did you know that according to Gartner
incorporative Global public Cloud
spending will increase by 20.7% to 5
91.8 billion in 2023 from $ 49.3 billion
in 2022 well the use of cloud computing
has been significantly impacted by the
recent years changing sub economic
environment companies accelerated the
digital transformation as a result of
the pandemic around the world and laws
requiring employees to work from home
this included switching to cloud-based
application to serve remote workers and
fast launching new cloud services to
keep clients initiatives for digital
transformation are still moving quickly
as 2022 comes to a close and if 2023
cloud computing Trends are any indicator
there will be no slowing down so
considering this growth popularity of
cloud computing our Channel simply learn
has brought you a new cloud computing
full course In This Cloud Computing full
course video you you will explore
various important Concepts around clown
Computing in depth in this video first
you will learn about what cloud
computing is and the basics of cloud
computing following that you will come
across the fundamentals of cloud
computing like types of cloud computing
cloud computing architecture Cloud
Computing Services models with VMware
and virtualization then you will be
introduced to various cloud computing
service providing platforms like AWS
aure and gcp first you will learn about
AWS and it's various interesting
Services following that you will learn
about Azure and how it differs from AWS
then you will learn about gcp and its
provided services and how it competes
with other two leading cloud computing
platforms AWS and Azure in the end of
this video you will learn about cloud
computing certifications that will help
you build your in cloud computing and
see how you can proceed to become a
cloud engineer which will cover all the
relatable data like eligibility criteria
qualifications and skills required for
you to proceed in this path by the end
of this video I can assure you that all
your cloud computing related questions
and doubts will have been cleared so for
this training with me I have our
experienced cloud computing specialist
Sam and Rahul together we'll work you
through the important cloud computing
Keynotes so let's start with an exciting
video on cloud computing full course but
before we begin make sure to subscribe
to our YouTube channel and hit the Bell
icon to never miss an update from Simply
learn imagine you're the owner of a
small software development firm and you
want to scale your business up however a
small team size the unpredictability of
demand and limited resources are
roadblocks for this expansion that's
when you hear about cloud computing but
before investing money into it you
decide to draw up the differences
between on premise and cloud-based
Computing to make a better decision when
it comes to scalability you pay more for
an on- premise setup and get lesser
options too once you've scaled up it is
difficult to scale down and often leads
to heavy losses in terms of
infrastructure and maintenance costs
cloud computing on the other hand allows
you to pay only for how much you use
with much easier and faster Provisions
for scaling up or down next let's talk
about server storage on Perm systems
need a lot of space for their servers
not withstanding the power and
maintenance hassles that come with them
on the other hand cloud computing
Solutions are offered by cloud service
providers who manage and maintain the
servers saving you both money and space
then we have data security on premise
systems offer less data security thanks
to a complicated combin of physical and
traditional it security measures whereas
cloud computing systems offer much
better security and let you avoid having
to constantly Monitor and manage
security protocols in the event that a
data loss does occur the chance for data
recovery with on- premise setups are
very small in contrast cloud computing
systems have robust disaster recovery
measures in place to ensure faster and
easier data recovery finally we have
maintenance on premises systems also
require additional teams for hardware
and software maintenance loading up the
costs by a considerable degree cloud
computing systems on the other hand are
maintained by the cloud service
providers reducing your costs and
resource allocation substantially so now
thinking that cloud computing is a
better option you decide to take a
closer look at what exactly cloud
computing is cloud computing refers to
the delivery of on-demand Computing
Services over the internet on a pay as
youo basis in simpler words rather than
managing files and services on a local
storage device you'll be doing the same
over the internet in a cost-efficient
manner cloud computing has two types of
models deployment model and service
model there are three types of
deployment models public private and
hybrid Cloud imagine you're traveling to
work you've got three options to choose
from one you have buses which represent
public clouds in this case the cloud
infrastructure is a available to the
public over the Internet these are owned
by cloud service providers two then you
have the option of using your own car
this represents the private cloud with
the private Cloud the cloud
infrastructure is exclusively operated
by a single organization this can be
managed by the organization or a third
party and finally you have the option to
Hell a cab this represents the hybrid
Cloud a hybrid cloud is a combination of
the functionalities of both public and
private cloud clouds next let's have a
look at the service models there are
three major service models available es
pass and SAS compared to on- premise
models where you'll need to manage and
maintain every component including
applications data virtualization and
middleware cloud computing service
models are hassle-free
is refers to infrastructure as a service
it is a cloud service model where users
get access to basic Computing
infrastructure they are commonly used by
administrators if your organization
requires resources like storage or
virtual machines is is the model for you
you only have to manage the data runtime
middleware applications and the OS while
the rest is handled by the cloud
providers next we have pass pass or
platform as a service provides Cloud
platforms and runtime environments for
developing testing and managing
applications this service model enables
users to deploy applications without the
need to acquire manage and maintain the
related architecture if your
organization is in need of a platform
for creating software applications pass
is the model for you pass only requires
you to handle the applications and the
data the rest of the components like
runtime middleware operating systems
servers storage and others are handled
by the cloud service providers and
finally we have SAS SAS or software as a
service involves cloud services for
hosting and managing your software
applications software and Hardware
requirements are satisfied by the
vendors so you don't have to manage any
of those aspects of the solution if
you'd rather not worry about the hassles
of owning any it equipment the SAS model
would be the one to go with with SAS the
cloud service provider handles all
components of the solution required by
the organization what is cloud computing
as we learn what is cloud computing we
will also be learning about how things
were before cloud computing and benefits
of cloud Compu Computing different types
of cloud computing available and some of
the famous companies that are using
cloud computing and they're getting
benefited out of it we're going to learn
all that before cloud computing existed
if we need any it servers or application
let's say a basic web server it does not
come easy now here is an owner of a
business and I know you would have
guessed it already that he's running and
successful business by looking at the
hot and fresh brewed coffee in his desk
and lots and lots of paperwork to review
and approve now he had a smart not only
smartlook but a really smart worker in
his office called Mark and on one fine
day he called Mark and said that he
would like to do business online in
other words he would like to take his
business online and for that he needed
his own website as the first thing and
Mark puts all his knowledge together and
comes up with this requirement that his
boss would need lots of servers uh
database and software to get his
business online which means a lot of
investment and Mark also adds that his
boss will need to invest on acquiring
technical expertise to manage the
hardware and software that they will be
purchasing and also to monitor the
infrastructure and after hearing all
this his boss was close to dropping his
plan to go online but before he made a
decision he chose to check if there are
any alternatives where he don't have to
spend a lot of money and don't have to
spend acquiring technical expertise now
that's when markk opened this discussion
with his boss and he explained his boss
about cloud computing and he explained
his boss the same thing that I'm going
to explain to you in some time now about
what is cloud computing what is cloud
computing cloud computing is the use of
a network of remote servers hosted on
the internet to store manage and process
data rather than having all that locally
and using local server for that cloud
computing is also so storing our data in
the internet from anywhere and accessing
our data from anywhere throughout the
internet and the companies that offer
those services are called Cloud
providers cloud computing is also being
able to deploy and manage our
applications services and network
throughout the globe and manage them
through the web management or
configuration portal in other words
cloud computing service providers give
us the ability to manage our
applications and services through
through a Global Network or Internet
example of such providers are Amazon web
servers and Microsoft Azure now that we
have known what cloud computing is let's
talk about the benefits of cloud
computing now I need to tell you the
cloud benefits is what is driving Cloud
adoption like anything in the recent
days if you want an IT resource or a
service now with Cloud it's available
for me almost instantaneously and it's
ready for production almost the same
time now this ready prodes the go live
date and the product and the service hit
the market almost instantaneously
compared to the Legacy environment and
because of this the companies have
started to generate Revenue almost the
next day if not the same day planning
and buying the rights Hardware has
always been a challenge in Legacy
environment and if you're not careful
when doing this we might need to live
with a hardware that's undersized for
the rest of our lives with Cloud we do
not buy any hardware but we use the
hardware and pay for the time we use it
if that Hardware does not fit our
requirement release it and start using a
better configuration and pay only for
the time you use that new and better
configuration in Legacy environments
forecasting demand is an full-time job
but with Cloud you can let the
monitoring and automation tool to work
for you and to rapidly scale up and down
the resources based on the need of that
R not only that the resources Services
data can can be accessed from anywhere
as long as we are connected to the
internet and even there are tools and
techniques now available which will let
you to work offline and will sync
whenever the internet is available
making sure the data is stored in
durable storage and in a secure fashion
is the talk of the business and Cloud
answers that million dollar question
with Cloud the data can be stored in an
highly durable storage and replicated to
multiple regions if you want and uh the
data that we store is encrypted and
secured in a fashion that's beyond what
we can imagine in local data centers now
let's bleed into the discussion about
the types of cloud computing very lately
there are multiple ways to categorize
cloud computing because it's ever
growing now we have more categories out
of all these six sort of stand out you
know categorizing cloud based on
deployments and categorizing cloud based
on services and again under deployments
categorizing them based on how they have
been implemented you know is it private
is it public or is it hybrid and again
categorizing them based on the service
it provides is it infrastructure as a
service or is it platform as a service
or is it software as a service let's
look at them one by one let's talk about
the different types of cloud based on
the deployment models first in public
Cloud everything is stored and accessed
in and through the internet and um any
internet users with proper permissions
can be given access to some of the
applications and resources and in public
Cloud we literally own nothing beat the
hardware or software everything is
managed by the provider AWS Azure and
Google are some examples of public Cloud
private Cloud on the other hand with
private Cloud the infrastructure is
exclusively for an single organization
the organizations can choose to run
their own cloud locally or choose to
Outsource it to a public cloud provider
as managed services and when this is
done the service the infrastructure will
be maintained on a private Network some
examples are wmware cloud and some of
the AWS products are very good example
for private Cloud hybrid cloud has taken
things to the whole new level with
hybrid Cloud we get the benefit of both
public and private Cloud organizations
will choose to keep some of their
applications locally and some of the
application will be present in the cloud
one good example is NASA it uses hybrid
Cloud it uses private Cloud to store
sensitive data and uses public Cloud to
store and share data are not sensitive
or confidential let's now discuss about
cloud based on servers model the first
and the broader category is
infrastructure as a service uh here we
would uh rent the servers network
storage and we'll pay for them in an
arly basis but we will have access to
the resources we provision and for some
we will have Ro root level access as
well ec2 in AWS is a very good example
it's a Wim for which we have root level
access to the OS and admin access to the
hardware the next type of service model
would be platform as a service now in
this model the providers will give me a
pre-built platform where we can deploy
our codes and our applications and they
will be up and running we only need to
manage the codes and not the
infrastructure here in software as a
service is the cloud providers sell the
end product which is a software or an
application and we directly buy the
software on an subscription basis it's
not the infra or the platform but the
end product or the software or a
functioning application and we pay for
the hours we use the software and in
here the client maintains full control
of the software and does not maintain
any equipment Amazon and Azure also sell
products that are software as service
this chart sort of explains the
difference between the four models
starting from on premises to
infrastructure as a service to platform
as a service to software as a service
this is self-explanatory that uh the
resource managed by us are huge in on
premises that towards your left as you
watch and it's little less in
infrastructure as a service as we move
further towards the ride and further
reduced in platform as a service and
there's really nothing to manage when it
comes to software as a service because
we buy the software not any
infrastructure component attached to it
there are lots of famous well-known
organization around the world they have
moved or have migrated to the cloud
environment Pinterest is one company
which is one of the world's largest
visual book marketing tool with more
than 70 million monthly active users now
Pinterest was able to scale its business
because it was all built on Amazon web
services with a company with less number
of employees Pinterest decided that they
do not want a dedicated staff time to
manage a data center instead Pinterest
users AWS or they used AWS to manage
their high performance social
application and store more than 8
billion objects and 400 terab of data in
AWS Cloud using a service called Amazon
simple storage service or in short
Amazon S3 and not only that they are
running 225,000 instance hours a month
and they run that on Amazon compute
service called Amazon ec2 or Amazon
elastic compute Cloud Spotify is an
another company that got benefited using
AWS it's an online music service
offering and it offers instant access to
over 16 million licensed songs Spotify
needed a peculiar requirement it needed
a storage solution that could scale very
quickly without incurring long lead
times for upgrades long story short
Spotify used Amazon S3 which is uh
Amazon simple storage service and Amazon
S3 gave them the confidence in their
ability and to expand their storage
quickly while also providing High data
durability the third biggest company I
want to mention is Netflix it's the
world's largest internet television
network with more than 100 million
members in more than 190 countries
Netflix uses Amazon web services for
nearly all its Computing and storage
needs now that includes database
analytic recommendation Engines video
transcoding and a lot more not only that
this company is planning to use AWS
Lambda to build an automated or an
rule-based self-managing infrastructure
and replace some of their inefficient
process to reduce the rate of errors and
replace inefficient processes to reduce
the rate of errors and save the valuable
time xedia is another famous company
that got benefited by using Cloud xedia
provides travel booking service through
its Flagship site called expedia.com and
not only that they also run 200 plus
travel booking sites around the world
xedia is all on AWS and by using AWS
xedia has become more resilient with an
high scalable infrastructure and better
cloud services because it had offloaded
the it management to the cloud provider
itself in this case AWS and because of
this expedia's developers have been able
to innovate faster while saving the
company millions and millions of dollars
all right so it was really an exciting
Journey beginning our discussion with
what is cloud computing and then started
to Branch out into the benefits of cloud
computing and then we understood about
the types of cloud computing that are
available and the difference between
them and how companies get benefited by
using cloud service providers like AWS
and Azure I really hope that you have
enjoyed the video please stay tuned for
more videos like this thank
you hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click
here
e
e
e
e
e
e
e
e
e
e
e e
now let's understand what do we mean by
the cloud computing
architecture so this diagram represents
the cloud computing architecture where
you can see there is a cloud
infrastructure which we call as a front
end front end means the infrastructure
that is Network facing that is internet
facing and the back end is where
actually the application the services
and and the devices are like storage
servers and all along with the
management and the security portals so
primarily the cloud computing
architecture has two components one is
the front end and the other one is the
back
end now with respect to the architecture
the cloud computing is architecture as
we discussed earlier is divided into two
parts front end and the back end it
provides applications and interfaces
that are required for the cloud-based
services so front end would be anything
which is facing the internet for example
the websites that you deploy they are
front end because they are accessed by
the public over the public
network these applications basically
such as web browser Google Chrome and
Internet Explorer over which you can
basically open up your web applications
your websites and also it includes
client and mobile devices along with
some apis so this is with respect to the
front-end architecture
now let's look into the further more
concepts related to the front end so in
the front end the cloud infrastructure
consist of a hardware and the software
components that includes a data storage
server virtualization software Etc also
it provides the GUI we call it as a
graphical user interface to the end
users so that they can perform the
respective task so it's a GUI based
access and those who are not not from
the coding background or they don't have
uh familiarity with the CLI it is an
advantage now let's understand the
backend
architecture so the backend is basically
it manages all the programs that runs
the application on the front end so
whatever is required to support the
applications for example web
applications which are running on a
front end would be available in the back
end it has the larger number of data
storage systems and server so it is
basically it comprises of the entire
infrastructure of a cloud provider and
it can also be a software or it can be a
platform as well so based on the
requirement the application provides
output to the end users in the backend
also it is one of the most important
components in the cloud because it is
you can say a backbone of a cloud
computing architecture
and its task is to provide utility in
the architecture and the few services
that are widely used among the end users
are storage application development
environments and the web
services in the back end with respect to
the storage it maintains and manages any
time of any amount of data over the
Internet some of the examples of storage
services on the cloud are S3 which is
called as a simple storage service
Oracle cloud storage and Microsoft Azure
storage uh the storage capacity varies
depending upon the service providers
available in the
market and also it allocates specific
resources to a specific task it handles
functions of cloud environment so this
is with respect to the management in the
backend architecture the management in
the backend architecture helps in the
management of components like
applications task service security data
storage and the cloud
infrastructure and in a simp terms it
establishes coordination among the
resources coming to the security aspect
of a backend architecture it is an
integral part of a crowd infrastructure
because uh since you it mostly relies on
the storage and the databases so it has
to be more secured it helps in
protecting Cloud resources system files
and the infrastructures also it provides
security to the cloud servers with
virtual firewalls and results in the
prevention data loss now the different
components of the cloud computing
architecture one is the hypervisor
second is the management software the
third one is the deployment
software fourth one is the
network fifth one is the Cloud Server
and you have the cloud storage now let's
look into what we have with the
hypervisor hypervisor as the name
suggests is a virtual operating platform
and that is basically used by every user
uh it runs a separate virtual machine on
the back endend which consists of a
software and the
hardware also it maintain objective it
main objective is to divide and allocate
resources so basically the hypervisor is
primarily used to virtualize the
physical machines so that it can be
shared across with many resources or
many
users the management software its
responsibility is to manage and monitor
the cloud operations so all the
operational task are basically done by
the management software it helps in
improving the performance of the cloud
since um you can do the admin task as
well for example high security
flexibility full-time access and the
access given to other uh users who would
be working on the cloud platform the
deployment software consists of all the
mandatory installations and
configurations that are required to run
a cloud
service every deployment of cloud
services performed using a deployment
software so it is primarily used for
deploying the applications or a software
so that basically saves the time of a
developer in terms of deploying the
codes now the three different models
which can be deployed are the software
as a service which we call as a SAS so
the software as a service means uh it
hosts the software and manage the
application of the end user example is
Gmail so Gmail is an example of a SAS
service where it ta as a mailing service
and uh the users just have to create
their email accounts and start using the
service uh the pass is basically a
platform as a service and uh it helps uh
to develop and build and deploy the
applications and the software primarily
used by the coders they can concentrate
more on their development activities and
less focus on the infrastructure part
example is Microsoft aure and uh the
infrastructure as a service IAS is
basically to provide services on pay as
you go pricing
model uh coming to the component of a
cloud computing architecture that is
Network so what is with respect to
network why do we need a network so it
connects the front end and the back end
together so the network can be the
internal private network of a cloud
computing provider as well as the public
network and also it allows every user to
access the cloud resources over the
Internet uh it helps users to connect
and customize the route and protocol so
let's say an admin wants um a particular
IPS to be blocked so you can specify
them and customize them in the route and
the
protocols uh it is a virtual server
which is hosted on the cloud computing
architecture so it is just like a
virtual uh networking that is available
on the cloud computing
platform it is highly flexible secure
and cost effective and it is basically
uh you can say the backbone in terms of
connecting multiple Services together
the cloud storage is basically where
every data is stored and that can be
accessed by a user from anywhere over
the Internet of course it depends on how
do you provide the access to it it is
scalable at runtime and it is
automatically
accessed and the data can be modified
and retrieved from the cloud storage
over the Internet itself now that we
know what cloud computing is let's look
at the types of cloud computing service
models for a better understanding let's
consider a scenario consider baking
lasagna you have four options you can
either bake it at home or you can go to
a restaurant to eat lasagna or walk into
an event and bake or get it delivered to
your doorstep but how is this related to
cloud computing well let's take a look
we have four boxes we have the
traditional method we have the
infrastructure as a service we have
platform as a service and we have
software as a service in the traditional
method everything is made at home and
all the components are manag bias that
is Kitchen electricity microwave lasagna
sheets then toppings meat and cooking
the lasagna but when it comes to
infrastructure service the main kitchen
appliances are managed by the vendor and
the other things are managed by us so
basically the infrastructure is managed
by the vendor when we look at platform
as a service the infrastructure and the
main ingredient for the software is
managed by the vendor and the other
things are managed by us but when we
look at software as a service everything
from the infrastructure till the end
product is managed by the vendor now
let's get into brief of all this when we
look at the traditional method
everything is done from the scratch from
choosing the right ingredients to the
mode of preparation and we have the full
control of the toppings this is similar
to traditional method where all the
hardware and software components are
built by our choice of requirements when
we look at infrastructure as a service
we can say kitchen electricity and
microwave is the infrastructure and this
is the code runs the next step is to
alter the software as per our
requirements when you look at platform
as a service uh it's similar to going to
event and baking so you have the main
ingredient that is the lasagna sheet and
then you have all the appliances that is
Kitchen electricity and microwave and
the rest is managed by us you can either
put toppings or you don't want to put
meat or you want it to be wed it's all
up to us so the components are altered
as per our requirements finally in
software as a service the entire thing
is modeled by the vendor so this is
where you just given your requirements
and you get the software delivered to
you it's basically where the deployment
and framework of the project is already
set now that we have a brief of all
these Services let's dive deep into each
one of them first we have infrastructure
as a service infrastructure cloud
provider gives a variety of
infrastructures such as storage Services
Network hardware and so on it also
maintains and supports these
infrastructures C customers can access
these resources over the Internet next
let's look at the benefits of
infrastructure as a service so the first
feature is that these resources can
easily be scaled up and down next the
cost depends on the consumption so
basically it's a pay as you go pricing
and you pay for what you use you pay for
the services you use a single piece of
server can give out a lot of information
to many users and finally the client has
complete control over the architecture
now let's look at the pros of
infrastructure as a service model
firstly it is highly flexible this is
because only the infrastructure is
provided and the rest depends on the
customer requirements next like I said
before it is cost effective and you pay
for what you use it is easy to use as
all the updates are deployed and all the
hardware is deployed automatically
management tasks are virtualized so that
the other employees have time for other
tasks now let's look at few of its G it
is a multitenant architecture due to
this there is an issue with data
security when a new infrastructure is
introduced team training is required to
learn all about this new infrastructure
and it consumes a lot of time if in case
the server crashes at the vendor side
the customers cannot access the data for
a while and they would have to wait for
quite a lot of time until the vendor
fixes this issue now that we know what
infrastructure as a service is let's
look at platform as a service platform
as a service cloud computing platform is
a develop programming platform which is
used for the programmers to develop test
and run and manage the applications a
developer writes the applications and
deploys it directly into this layer all
the infrastructure to run the
applications will be over the Internet
let's look at the benefits now firstly
the resources can be scaled Up and Down
based on the requirements of the user
multiple users can access the same
application it allows for testing and
hosting apps in the same environment the
web services and databases and servers
are integrated into one and finally the
teams can collaborate very easily next
let's look at the pros of P AAS firstly
the development process is quick and
easy as it is a developer programming
platform it is coste efficient you only
pay for the services you use when you
use platform as a service the coding is
done by the developer before so less
coding is required and then the
migration to hybrid cloud is very easy
next let's look at few of the cons of
platform as a service similar to IAS
data security is an issue because it has
a multi-tenant architecture there's a
compatibility issue with the existing
infrastructure pass is dependent on the
vendor speed reliability and support now
that we know about platform as a service
let's go ahead and dive deep into
software as a service in software as a
service everything is done by the
vendors the end users are only
responsible to give their requirements
and everything is is done by the
providers now let's go ahead and look at
its benefits the installations and
updations are done by the providers
resources are scaled Up and Down based
on the requirements of the users the
only requirement is that there has to be
a strong network connectivity and
finally the provider is responsible for
everything now let's go ahead and look
at the pros of SAS upgrades are
automatic firstly then next it is again
a pay as you go model it is easier to
custom in SAS rather than the other
service models it is accessible from any
location the only constraint is that you
need to have a strong internet
connectivity let's see the cons of SAS
the provider has entire control the end
users only have the control of giving
the requirements there are only few
solutions for software crashes the
devices should always be connected for
better performance now that we've seen
all the three models let's see few of
the companies providing these service
models the famous IAS provider ERS are
Amazon web services Rackspace digital
ocean lenoe and Microsoft aure these are
only a few there are many more apart
from these Amazon offers many features
such as autoscaling Cloud monitoring and
load balancing features when it comes to
Rackspace the cloud computing platform
vendor focuses primarily on Enterprise
level hosting Services when you look at
Famous past providers you have heroo
Apache Stratos open shift Microsoft
Azure and many other providers go going
ahead in SAS providers the most
important one is Google Apps then
there's Salesforce there is Cisco WebEx
there's Dropbox and many more finally
let's look at the end users of the cloud
computing service models we can picture
this as a pyramid so in IAS the end
users are system admins who are
responsible for maintaining everything
except the infrastructure then in pass
the end users are the developers who
code on the platform provided by the
vendors in SAS the end users are the
customer who give only the requirements
and the software is built based on those
requirements by the vendors now what
exactly is the need of VM now VMware is
uh a kind of virtualization concept
which is available there so let's try to
understand uh it with a small story over
here so that we will be able to get a a
good understanding about what is the
overall purpose of using a vmw here now
before the virtual machines or the
VMware uh I can use the terminology as
an virtualization virtual machines
VMware they all are pointing to the same
point so let's assume like we got a
developer who is uh working on the
application and the application is not
working uh is uh is not working as
expected now it could be like uh po due
to the poor data recovery poor
maintainance poor performance so the
overall benefits or the overall quality
of the product which we are expecting is
not coming up over here now um after the
VMware what really happened over here is
like we got uh certain Solutions you can
use the virtual machines to host your
applications the maintenance becames a
little bit more easy and little bit less
uh cost expective so uh we got like
reduced uh operation operating cost we
got like increased it productivity uh
efficiency and Agility there so we are
trying to see like how exactly uh we are
performing the executions we are
performing uh the setup here so we also
got like a faster provisioning of
applications so that also something we
are uh getting over here when we talk
about from the VMware so these are some
couple of benefits which we get uh as a
solution a complete full flash uh
solution from the vmw here so these are
the some uh Solutions which we are
talking about from the VMware
perspective and uh it also helps us to
go for a greater business continuity
there because it helps us to provide
High virtual machines uh in case any
existing virtual machine gets deleted we
can get a new one spend up within just
minutes of time duration um it helps us
to go for a disaster recovery like where
we can go for a Dr kind of setup also in
addition to production and uh it also
helps us to go for an easy management of
the data center that's the biggest
benefit because usually we have so many
servers so many infrastructure there so
if we really want to resolve if you
really want to maintain the things in a
much easier way then VMware or
virtualization is a score for
that right so what is the relationship
between the the VMware and the
virtualization here now in simple terms
VMware is something which uh we can call
it as an virtualization software U this
is something which we can use as an
virtualizer over here so virtualization
uh software is something which generates
a kind of a virtual environment uh just
on top of an uh physical infrastructure
the main focus of the virtualization is
to see like how we can create a virtual
uh set of resources by consuming or by
using the existing resources the
existing physical resources so in case
of virtualization um we can make use of
the elements like RAM memory storage and
uh these can be actually converted into
the virtual resources so uh and then uh
we can create a virtual machines which
will be consuming these virtual
resources so that uh they will be able
to uh get a full-fledged environment up
and running so VMware is a kind of a
tool which provides us a
virtualization software and
virtualization is a concept in which we
will be uh using or reusing the existing
physical infrastructure in such a way
that we will be able to uh make uh most
out of our uh infrastructure component
so that's the main uh component or main
reason why we are looking forward for
these kind of virtualization
here right now so what exactly is a
hypervisor here now hypervisor we talked
about the VMV we talked about
virtualization now hypervisor is a kind
of uh tool it's a kind of a management
tool which is uh available to manage the
virtual machines here the main focus of
hypervisor is to manage like how the
virtual machine can be done right so
that's the main focus of a particular
hypervisor here and if we talk about the
uh mechanisms like how hypervisor is
really going on there so that's where we
will be able to also understand that
this is something which can take up the
request from the virtual machines and uh
then send it to the uh host operating
system or to the physical infrastructure
so it acts like a intermediate between
the host operating system and the
virtual machine so as for this diagram
you can see like we got this virtual one
virtual machine 1 Virtual Machine 2 and
virtual machine 3 in virtual machine 1
you have the applications you have the
binaries you have the libraries you even
have a guest operating system in Machine
2 uh you also have the application
boundaries guest operating system third
also is having the same thing now we
also have a hypervisor between the
physical infrastructure and the virtual
environment which actually uh helps us
as a kind of a layer which will be doing
the collaboration between how exactly
these communication will be performed
and uh the overall uh communication or
the 2 and4 uh uh request can be
processed over here in this case so this
is something which we will be uh really
going to see like how the overall uh
setup and the overall modifications can
be really performed here so that's one
of the mechanism which is available
there like how uh we are going for this
specific approach over here right so
virtual uh machine is a very important
component because the hypervisor is
going to manage the virtual machine only
and what we have into our virtual
machine so we basically have uh a
full-fledged environment you can talk
about like in that you have a guest
operating system you can have an
application running libraries whatever
fe uh things you want to store you want
to uh deploy over there you can actually
do it so that's something we can easily
get it over here with the help of this
component so virtual machine is
something which is really helping you to
see like how uh you can deploy a certain
resources onto this one right so um it's
it's a kind of a virtual environment so
if we deploy it into a physical machine
or physical server then we call it as an
a kind of physical machine or server but
right now it's actually deployed onto a
virtualization or a virtual environment
that's why we call it as in Virtual
machines because it's not physically
available it's present as an onto a
virtual machine
here right and uh whatever applications
you feel application binaries libraries
whatever those components you want to
deploy you can actually get those things
deployed as such on this one so that's
the main benefit which you get over here
that with the help of these uh
components or with the help of uh these
uh virtual machines you can H uh host a
lot of boundaries and a lot of uh uh
softwares applications as such on this
one so it could be like you can use
these three virtual machines as a
different different environment so a
single server you have uh split up into
three environments machine one you can
use it as an kind of a Dev environment
machine two you can use it like a test
environment and machine three you can
use it like a prod environment or a
pre-pro so this really helps you to uh
have most out of your infrastructure now
earlier we used to have only one server
because we are using physical uh uh
machines but now we are talking about
virtual machines so here we have got
like different different set of
resources uh being utilized and uh we
can have multiple virtual machines
created and that's a very good benefit
also you can think about like okay I am
just pay I'm just paying for a cost of
one server and instead of that I'm
getting three different environments you
can have it like as an environment you
can have it like three different
applications deployed so it's completely
up to you like how you are going to
perform the customization there but it's
all something which can handle it at the
level of virtualization that's a main
focus which we are trying to explain
over
here right and uh like have I have
already explained over here hypervisor
is something which is uh going to deploy
to see like how uh the specific
modifications and the changes can be uh
really deployed over here in in this
case right so this is very important
thing to be considered because it's uh
something which where we will be able to
see like how uh we can go for the
overall implementation of this complete
virtualized environment
here right so now next thing is that we
have got like hypervisor also configured
in two ways you can have a hypervisor
type one which is uh a kind of
hypervisor which is available there for
example hyperkit for Max OS hyperv for
Windows and KVM for Linux so this is
something which is available there for
some couple of operating systems which
we have commonly like Windows Mac OS and
Linux so there we can install these type
one kind of hypervisors there so these
are actually deployed on top of the host
operating system then we have got like
uh the type two which is again a little
bit more complex hypervisors now these
are like VMware and the virtual box
these also provides you an additional
features like uh the Tai one uh
hypervisors are a little bit uh limited
there because they are just trying to
give you a flavor of virtual machines
but if you are looking forward for a
kind of a production environment or a
kind of a real environment so in that
case the uh type two infrastructure or
type two hypervisor makes more sense for
you in that situation so depending on
the situations and depending on the
utilization the selection between the
type one or type two hypervisor can be
decided there right now uh let's see
that uh you know which situations we can
use over here now you can consider an
example where you are running like two
applications U onto your server and uh
you want it to be done into complete
isolation now this will basically
require uh two operating system because
if you want to deploy the single
application on the same same operating
system or the same server machine uh
definitely they cannot be hosted
isolated there so the only thing only
reason why we can have a complete or
total isolation is by uh splitting by
having two different guest operating
systems there so in that case only you
can go for a true isolation on this case
right so this is the main benefit like
you know you can have different
environments created onto the virtual
environments and uh you can have uh
create a total isolated environment and
that's the biggest benefit so you save
the cost you have the infrastructure you
have uh made the uh particular
maintenance easy on top of that you made
the proper uh true isolation for each
and every environment so these are the
main thing which is being done by the
hypervisor so let's go ahead with the
benefits of hypervisor again one by one
cost efficient that's totally correct
because when we talk about hypervisor
it's just telling that we are converting
the physical machines or physical
servers in case of virtual machines so
which means as for the previous diagram
we were converting a single machine into
three different virtual machines so you
getting most out of your infrastructure
and at the same time that reduces the
cost also now rather than going for
three different servers you're just
using the single server as in three
virtual machines to come up with that
environment so that's the reason why
it's uh kind of a cost efficient over
here flexibility that's great you have
got a create flexibility over here in
order to work with how the main of these
virtual machines can be done you want to
create the virtual machines you want to
delete the virtual machines you want to
do the stop start all that activities
you can actually do it with the help of
hypervisor here so that's a great
flexibility which we get or which we are
getting from the hypervisor here
portability these virtual machines which
we are creating in VMware it's not like
I cannot use it in another system if I
have the VMware into one machine and I I
have another virtual VMware in another
environment I could literally use the
existing virtual machines there also so
it helps me to save my time because I
don't have to rebuild my virtual
machines again and again I can easily
Port it from one environment to another
environment so I can do a clone also
like I have got one virtual machine and
I want to clone it like into five six
virtual machines I can do that also so
it it can be extended to any level so
the these are all possibilities only
possible by the
hypervisor easy setup of maintainance a
single software maintaining all the
activities all the calls between the
physical and the virtual environment so
you don't have to do any kind of uh
maintainance as such over there the
backup restoration all those features
are already there in the
hypervisor better resource allocation so
you are using 100% utilization of your
resources and uh you're getting max out
of your uh existing resources so the
vage of resources is also quite less
when we talk about the hypervisor or the
virtualization here
now what are the different components
which we get with the virtualization
like how exactly the virtual
virtualization gets into the work so we
have got like three components CPU
memory and
storage now what is a CPU all about so
CPU is something where exactly um we are
um getting the cores the internal
process of the system so the virtual
machines when we create onto a physical
environment so these cores actually got
uh shared to the virtual environment
also so hypervisor is capable of taking
or having a virtual CPU created out of
this physical CPU there but it's not
like that that if you are having four
four cores or four CPU you can create
eight virtual machine of One Core each
it's not like that you cannot multiply
that the number of cores which you can
use for the virtual machine should
always be less than what you have
physically available there so that's the
true virtualization which we are talking
about of course um you know these cores
can actually be shared across multiple
virtual machines because uh let's say
like I have got two cores and I can uh
create three virtual machines so uh
definitely these two cores can be shared
across these three virtual machines but
you have to limit the number of CES
because it's not like you want to expose
your host operating system host system
and uh you should be into the limits
when you allocate these cores or these
CPUs and
memories right so uh the hypervisor is
actually taking all the request from the
virtual environment and then it send it
to the CPU there so the communication
between the virtual machine and the host
is not Direct in fact it's through the
hypervisor now virtual machine is just a
ram in the in case of the system in case
of the hardware terminologies when uh we
use the virtual machine it actually
requires certain uh memory also so we
can get it uh there from the host memory
just like in CPU here also the limit
apply lies so if we have the host
machine uh of 8GB Ram we could use
closely to 6gb of ram over there so that
we can have one or 2 GB left out for the
host environment also so we should not
be compromising the host environment the
number of virtual machines which we can
create on a particular server or a piece
of Hardware is also limited by the
resources which is allocated physically
there so that's something which we need
to take care on that so the um memory
size is available there so you can uh
Accord accordingly see like what kind of
resources you have and then according to
that you can allocate it to the virtual
machine so you need to be careful about
that how many resources you actually
allocate to a specific virtual machine
there so that should always be in a
particular limit and this is something
which we can actually modify whenever we
feel we can modify it also right so
that's the main reason that's the main
resource component which is available
there which we can use then uh we have
the storage uh ultimately after CPU and
memory it comes the storage also so we
have uh this as an different uh storage
devices which we can use with the
virtualization with the virtual machines
here so uh using the uh VMware or any
kind of hypervisor which you're using
you can actually manage the storage also
like what where exactly you want to
store these files or you want to um you
know consume the which which file system
you want to consume so all those things
we can actually mign we can uh configure
over there and in fact like if you want
to increase or if you want to decrease
the uh specific size so that also we can
do there now there are some couple of uh
extensions which is available there now
these extensions May differ from
different hypervisor which you're using
so vdi V vhdx then we have got vmdk then
we have got like HDD these are the
different uh uh kind of files extensions
which is available there when we talk
about the storage like where exactly the
virtual machines are getting stored so
these are getting stored in these uh
different different subsequent
extensions of the files here so these
are the different component which is
available there and uh of course uh
depending on different hypervisors these
configurations or these parameters may
change over the period of time but uh
it's something which we can see like how
these things can be a really configured
here we are going to talk about like how
we can create a new virtual machine with
the help of VMware hypervisor VMware
Workstation and also we're going to talk
about like if you have an existing
virtual machine how we going to do that
clone activity over there so this is the
official page of VMware where we can
download the vmw workstation 16 player
here now you can have this uh particular
software for Linux and windows platform
both there so let's go for the windows
platform so I have already got this uh
installed over here so you can see like
in my downloads it's going to show me
like this player is available there so
it's close to 2250 MB package which is
available there and all we need to do is
like we need to install this package as
such so I'm going to just open this
installer here
so it's going to be installed into my
system so once the installation is done
I will be able to have this specific uh
hypervisor over here installed and then
I can use it for implementing the
virtualization into my system so all I
need to do is like I need to just
install this software it may take a
little bit more time because it's in a
heavy software which is there and it's
doing a lot of installations components
there so that's what it's happening
there and uh apart from the installation
over here in this case like uh apart
from installing this uh particular uh
hypervisor of vmw software
virtualization software I also have to
uh go ahead with the installation of uh
an ISO image and a kind of image file I
require whatever operating system like
if you want to go for a Windows
operating system deployment or you want
to go for Ubuntu Linux any kind of
platform if you feel like you want to
install so in that case you have to
actually go ahead uh with the uh
particular installer so uh that is
something which we are looking forward
and uh we are trying to see like how
um we can download that IO image um if
it's Windows you can download it from
Windows website if it of to sent
anywhere you can do it so what I'm going
to do is like I'm going to download a
specific small kind of uh tiny Linux
over here which will be very easy for us
to do the installation I could easily go
for Ubuntu and those kind of iso or
image files also but they will take like
a lot of time for us to do it and uh for
this demo purpose we can start with the
tiny Linux and uh we can go for that
customized operating system but when we
actually require the implementation when
we feel like we want to go for a real
implementation at that moment of time we
can go for the image like obuntu and all
that stuff but it's all the same only
the kind of setup which you're looking
forward it's all the same only so my
installer is done for the uh particular
VMware so let's see like uh how we can
download the sa file so I can go for an
installer like I can download the image
file for my tiny Linux so now I'm going
to download
the tiny Linux ISO file so this is the
website which is available there now the
uh tiny core Linux micro Linux there are
different names which is available there
right so you have multiple uh links
which is available there from which you
can actually download it right so it's a
basic a very small competent version
which is available there so you can just
download it right and uh this is also
this is the official page which is there
for tiny cor Lin this is the uh
distributor link which is available from
which where you can download this one so
let's go ahead and uh let's download
this ISO file so core tiny core core
plus there are different kind of images
which is there so I'm just going for the
uh tiny core Linux over here um it's
just a kind of uh version which is
available there which is having a little
bit in uh like medium level of
executables available so that's what I'm
going to download so it's actually there
um you can see like it's having a size
of 20 MB alone that's the only size
which is available there so which means
like we will be able to easily download
it and uh get it onboarded onto our
virtual machine here so let's open the
VMware meanwhile so that I can go for
the basic configurations when I want to
do that so let's go with that so I have
opened the workstation 16 player so I'm
going for the free non-commercial use
over here otherwise if you have a
license you can go with that I'm just
going for this login here so this is the
application which comes up over here
right so let me maximize it so so here
you can actually create an virtual
machine you can open an existing uh
virtual machine also if for some reason
you got the virtual machine already
built up and you want to import it over
here you can simply open it up there
right so I'm going to just uh create a a
new virtual machine here very simple
options go to the player then file and
the uh new virtual machine here now in
this case you just have to provide the
iso image so I have already downloaded
now so I can just go with the uh
particular download option over here the
tiny core this is the iso image which is
available there now it could be a
possibility since it's a very small
compressed one so VMware may not be able
to load it up but the whole idea is like
we just require an ISO image that can be
OB 2 or that can be anything there now
OB 2 will take a lot of time because
it's more than 1 GB of size so I'm just
trying with the basic ISO image so that
uh we can actually get started uh much
faster over here in this one so which
operating system is this one so this is
like a Linux operating system which is
available there
and uh we can see into our options here
uh if anything related to Tiny Linux is
is available there but if it's not there
so we can just select for the uh UB 2 64
that's what we can select and we can say
like next whatever the version you have
like if you are going for the specific
one virtual machine you can select the
size accordingly now here we were
talking about the dis size so I'm just
giving 6 20 GB because that's a ma
maximum size I don't have to worry about
uh the particular size there 20 GB is
more than sufficient if you feel like
you want to create a single file a
single disk file for the complete
virtual machine you can just select with
this one if you want to split it in
multiple files so that it would be easy
for you to uh you know it provides a
better efficiency and if in future you
want to transfer the files from one
system to another that will also be much
easier so depending on the situation you
can see like whether you want to go for
a single file or you want to go for
multiple files
and uh the customizations like I said uh
in this one the course everything is
available there so we are allocating
like 2GB but you can see that there is
an maximum recommended memory which is
available there because my system is
having like 6 GB sorry 8 GB of RAM so
it's saying like till 6.1 you should be
going for that particular memory because
if we say 8GB then 8GB means like your
host machine is also compromised so the
maximum recommended memory is 6gb now
the recommended memory is 2 GB so that's
what we are trying to use here and the
processor definitely we have like 32 CES
which is available there so I can
actually use like uh whatever the CES I
want I feel I can use that according to
my uh particular configuration so I I
can actually use it accordingly and uh
if you want to enable the virtualization
CPU all these couple of options are
there then ISO image you have already
configured it like uh if you feel like
you can change it you can change it also
uh then network is there you got the
bridge hone only there are different uh
Network which is available there usually
Bridge connection uh will help you to uh
replicate uh the you know the connection
so it will directly get connected to the
physical Network and you will be able to
you know share the aspects like you can
get the internet activated and it's all
going on in that part mode there so
these are some couple of options which
is available there using which you will
be able to uh component like you can add
it you can configure it if you feel like
you want to add any one of the uh
Hardware type which is not being covered
that also you can do here right so I'll
just have to say close because this is
the configuration which I'm looking
forward but pretty much it picked up the
decent uh basic configuration so I can
just go ahead with the finish
part and now if I feel like I want to
edit it I can do that but I can just
start the play button to get the uh
particular virtual machine started all
right so as you can see like it's just
just downloading some couple of vmw
tools for Linux so I can just say like
download and install so these are some
uh BM tools which is required now these
tools are actually used when you are
dealing with the virtual machine so at
that moment of time these tools are
required here now in this one you have a
multiple options you can see like a boot
tiny core you want to use the boot core
which is only the command line interface
so these couple of options are available
there but I think it's going to wait for
close to 40 50 seconds it's already
passed like 10 15 seconds so after some
time it will automatically go for boot
tiny core but if I can just click on
this window inside and press enter it
will start proceeding with that uh tiny
core here right and if you feel like you
want to come out of this so you have to
do like Control Alt like it's hovering
over here so right now you are able to
see like my cursor is inside there but
if you want to go for Control Alt then
you will be back onto the host machine
so when you click inside you will not be
able to use see my cursor is not going
out this window it's limited over here
so if you press control alt then only
you will be able to see your cursor back
here right and uh you can go into the
full maximize mode uh again Control Alt
to come
out right and uh this is the particular
window which is coming up for you so let
this uh Tools installation okay it's
already going on so let it uh get
completed so these are some uh tools
which is getting installed so that uh it
will became very easy for you to
interact with the virtual machine so for
that purpose this is getting
installed now um you can do a lot of
work uh from the power you can actually
shut down it you can restart it you can
stop it all that stuff you can do it any
removable devices if you have so that
you can over the flly and disconnect it
you can see like you can disconnect over
here right and uh you want to make it uh
like a full screen you want to uh you
know do any kind of uh virtual machine
settings like here you will be able to
see like all these set settings you can
again recover it so these are some
couple of options which is available
there now inside this one so you have
the options here like in the bottom one
you have the exit option editor uh
control panel then apps run program
Mount tool terminal so terminal is there
so you can see like uh it's a little bit
uh different terminal which you're
getting but of course like it's more
than sufficient for you to do that
so you can run some couple of
uh components
here now I
can now the output is very small here
because of this uh particular one but
it's actually being used like it's using
uh a specific uh size of the file system
which is being allocated to this one and
uh this is where my operating system is
basically being used over here in this
case right so that's how the uh commands
are going to work um it's it's going to
execute some couple of commands some
executions some commands may not be
available there so you may have to
execute that so that's how it it it
happens over there right now if I have
to come out control alt same and I'll
just go here say that uh let me stop it
up so I can just shut down this virtual
machine
here and once there is done so I have to
have open the vmw player again to get it
on there so this is what we have got
over here in this one this is a kind of
virtual machine which is available there
so again I can edit the
configurations processor hard disk
everything is available there right so
if you see there's a very important
aspect about this one that the maximum
size which is being allocated is 20 GB
now it's not like the complete 20 GB is
being allocated uh yes 2.6 MB is being
allocated right now the current size so
gradually it will be increasing up to 20
GB but yes it's something which is
available and we can uh see like how
exactly we can uh you know what amount
of size is being utilized and what is
being used over
here right and this is just an virtual
machine you can have multiple virtual
machines which you can create here so so
it's just a matter of One matual
machines or 20 M machines you can just
have it all available there and these
are the different prefaces which is
available there which you can perform
and you can go ahead with that right you
can delete uh this one completely from
the specific uh file system here so if
you feel like you want to completely
wipe a to from the vile system so that
also you can do here so these are the
different options which is possible over
here just to see like how exactly we can
go ahead with this implementation all
together right again in the file you
have all these options which is
available there and uh you can also go
for the preferences which is available
there like how what are the different
options which is uh being provided over
here in this case so that's a mechanism
how exactly we work with the uh virtual
machines uh Creations we can manage it
and uh we can get it uh implemented all
together what exactly virtualization is
let's take a look at an example so beat
Jake he works as a software developer in
an IT firm and he often has to work on
different projects involving multiple
operating systems according to the
requirements of the project during which
you often come across cases where the
managing of data becomes problematic due
to the compatibility issues with the
different operating systems which makes
it hard for the completion of the
project so what can Jake do to overcome
such a problem well to overcome such a
problem he decided to use the way of
virtualization
but what is virtualization and how does
it help us let's take a look to begin
with we'll take a look at what exactly
virtualization is and how it can help us
then we'll understand what virtual
machines are after that we learn about
the role and different types of
hypervisor that are involved during the
process of virtualization after that we
learn what different types of
virtualizations are available and how
differently they affect our systems and
lastly we will see what benefits
virtualization provides us with to begin
with we'll understand what
virtualization is virtualization is
nothing but utilizing a software to
create a virtual layer over the hardware
which allows the system Hardware to be
used more efficiently and allows
appropriate return for a hardware cost
the software hypervisor also allows the
elements of the system like storage
memory processor and Etc to be
distributed among on multiple separate
and secure virtual computers known as
virtual machines to understand the
situation of virtualization much better
let's take a look at an example this is
a system installed with the Windows
operating system which is officially
known as the host OS where the
virtualization software known as the
hypervisor will run and then using the
hypervisor software we can have multiple
instances of different OS including Unix
Mac and Linux which are known as virtual
systems or guest OS the working of the
virtualization is only possible by using
a software known as a hypervisor and
later in the video we will also learn
about the working and different types of
hypervisor
involved now that we have understood
what virtualization is let's take a look
at what a virtual machine is so as the
name suggests virtualization is nothing
but an emulation or a virtual
representation of a physical operating
system on a hardware device the virtual
machines are also known as guest OS
whereas a physical system that they run
on is also known as the host OS now
let's take a look at the software that
makes the virtualization
possible hypervisor is a software layer
that manages the virtual
machines it forms an interface between
the physical system and the virtual
machine which ensures the proper access
of the resources it also manages the
virtual machine so that they don't
interfere with each other's resources
let's take a look at what different
types of hypervisors are there the first
type of hypervisor is known as the type
one hypervisor or the bare metal
hypervisor this type of hypervisor
directly interacts with the hardware
system and user resources the other type
of hypervisor is known as the type 2
hypervisor which runs as an application
on the host operating system and the
hyper visor also coordinates with the
virtual machine for resource
management let's take a look at the type
one hypervisor they run directly on top
of the host operating system and utilize
the hardware resources and that is why
they're also known as the bare metal
hypervisor they take up the place of the
host operating system and works as the
own operating system and since this type
of hypervisor Works directly on the
hardware system they are highly
efficient now that we we understand type
one hypervisor let's take a look at the
type two
hypervisor this type of hypervisor
doesn't directly work on the operating
system Hardware but instead it works as
an application in the host operating
system where they are suitable for
running individual systems users can
also have different multiple OS
installed in the physical system by
using this type of
hypervisor due to the application-based
working of the type 2 hypervisor they
are also known as virtual machine
monitors or in short
vmm now that we understand what
different types of hypervisors there are
let's take a look at what types of
virtualizations are present the first
type of virtualization is the desktop
virtualization then the network
virtualization storage virtualization
and lastly the application
virtualization let's take a look at them
one by one for desktop virtualization as
the name suggests in this type of
virtualization we can run multiple
operating systems on a single Hardware
system let's take a look at the
different types of desktop
virtualization the first one is virtual
desktop infrastructure or in short vdi
which runs numerous virtual machines on
a central server and then hosted to the
user according to the user's requirement
in this way the user can access any
operating system without having to
physically install the particular
operating system in its Hardware system
the second one is known as local desktop
virtualization as the name suggests it
uses a hypervisor software on a local
system which allows a user to run
multiple operating systems
simultaneously without having to affect
the host operating system the next type
of virtualization is Network
virtualization in this type of
virtualization the software creates a
virtual instance of the network that can
be used to manage from a single console
and it also forms the abstraction of the
hardware components and function
functions including switches routers and
Etc which simplifies the network
management different types of network
virtualization are softwar defined
virtualization which virtualizes the
hardware that controls the network
traffic routing and the other one is
Network function
virtualization virtualize is the
Hardware Appliances that provide Network
specific functions easier to configure
and manage for example a firewall let's
take a look at the third type of
virtualization known as the storage
virtualization this virtualization
enables all the storage devices on the
system to be accessed and be managed as
a single storage unit the storage
virtualization collects all the storage
into a single pool from which they can
allot another virtual machine on the
same network as required and this makes
it easier to assign storage for multiple
virtual machines with maximum efficiency
and the last virtualization that we'll
discuss is the application
virtualization in this type of
virtualization process the application
runs directly without the need of
installing it into the system as they
run on a virtual environment different
types of application virtualization app
the local application virtualization and
this type of application runs on the
host device but runs in a different
virtual environment but not in the
hardware the second one is application
virtualization and in this the
application is on the server side and it
sends some of the components to The Host
device according to the requirement and
last is the server-based application
virtualization this application runs
directly on the server side and sends
only the interface to the client system
now let's take a look at the benefits of
virtualization the first one is resource
efficiency as the name suggests before
virtualization each application server
used its own Hardware resources which
were being underused but with having
multiple virtual machines maximum
utilization of the hardware capacity
occurs then we have minimum downtime
which refers to the crashes of operating
systems and applications which can cause
a halt in the user productivity by using
virtualization the admin can run
multiple similar virtual machines
simultaneously and change over the
working instances in case of a crash
instead of having multiple dedicated
servers and then we have time maner
management buying installing and
configuring a new system is not only
costly but also a waste of time in such
a case virtualization can solve the
problem provided that the existing
Hardware resources are sufficient for
running the virtualization software
otherwise it can be configured for the
same as next thing let's talk about the
different Cloud providers Amazon web
services AWS is a cloud computing
service provided by Amazon it provides a
mix of infrastructure as a service IAS
platform as a service Pas and package
software as a service called SAS
offerings Microsoft Azure formerly known
as Windows Azure is a cloud computing
service by Microsoft and it sort of
specializes in using Cloud for building
testing deploying and managing the
applications through the servers
throughout a Global Network that
Microsoft manages it also provides
software as a service platform as a
service infrastructure as a service and
and it supports lots of different
programming languages and tools and
Frameworks including both Microsoft and
thirdparty software and systems IBM
cloud is a cloud computing service
offered by IBM IBM Cloud includes
infrastructure as a service software as
a service platform as a service now the
difference is here it offers through
public private and hybrid Cloud delivery
models VMware on the other hand is an
subsidiary of Dell Technologies and
provides cloud computing and platform
virtualization soft software and
services it was the first commercially
successful company to virtualize the x86
architecture Google Cloud platform on
the other hand is offered by Google it's
a suit of Cloud Computing Services that
run on the same infrastructure that
Google uses internally for its end user
products such as the Google search and
the YouTube you're familiar with
alongside a set of managed tools it also
provides cloud services including
Computing Services data storage Services
data analytics and machine learning
Services digital Ocean on the other hand
is headquartered in New York City with
data centers worldwide digital otion
provides developers cloud services that
help to deploy and scale applications
that run simultaneously on multiple
computers as of January 2018 digital
ocean was the third largest hosting
company in the world in terms of web
phasing computers let's talk about cloud
computing in AWS Amazon web services AWS
is an cloud computing service provided
by Amazon and these services are
accessible over the internet and because
AWS provides uh infrastructure as a
service it's a flagship offering we can
create and deploy any type of
application in the cloud on top of the
is that Amazon provides and you know the
best part here is the subscriptions are
pay as you go type you use less and pay
less and only for what you have used you
use more pay more but still less per
unit for the service used us attractive
isn't it now let's talk about the life
cycle of the cloud computing solution
the very first thing in the life cycle
of a solution or a Cloud solution is to
get a proper understanding of the
requirement I didn't say get the
requirement but said get a proper
understanding of the requirement it is
very vital because only then we will be
able to properly pick the right service
offered by the provider getting a sound
understanding the next thing would be to
define the hardware meaning choose the
compute servers that will provide the
right support where you can resize the
compute capacity in the cloud to run
application programs getting a sound
understanding of the requirement helps
in picking the right Hardware one size
does not fit all there are different
services and Hardwares for different
needs you might have like ec2 if you're
looking for is and Lambda if you're
looking for serverless computing and ECS
that provides containerized service so
there are a lot of Hardwares available
pick the right Hardware that's your
requirement the third thing is to define
the storage choose the appropriate
storage service where you can backup
your data and a separate storage service
where you can archive your data locally
within the cloud or from the internet
and choose the appropriate storage there
is one separately for backup called S3
and there's one separately for archival
that's for Glacier so you know you
knowing the difference between them
really helps in picking the right
service for the right kind of need
Define the network Define the network
that secur L delivers data video and
applications Define and identify the
Network Services properly for example
VPC for Network Route 53 for DNS and
direct connection for private P2P line
from your office to the AWS data center
set up the right Security Services IM am
for authentication and authorization and
KMS for uh data encryption at rest so
there are variety of security products
available we got to pick the right one
that suits our need and there are
variety of deployment and Automation and
monitoring tools that you can pick from
for example Cloud watch is for
monitoring Autos scaling is for being
elastic and cloud formation is Define
the management process and tools you can
have complete control of your Cloud
environment if you define the management
tools which monitors your AWS resources
and are the custom applications running
on AWS platform there are variety of
deployment Automation and monitoring
tools you can pick from like cloudwatch
for monitoring autoscaling for
Automation and cloud formation for a
deployment so knowing them will help you
in defining the life cycle of the cloud
computing solution properly and
similarly there are a lot of tools for
testing a process like code star and
code build and code pipeline these are
tools with which you can build test and
deploy your code quickly and finally
once everything is set and done pick the
analytics service for analyzing and
visualizing the data using the analytics
Services where where we can start
quering the data instantly and get a
result now if you want to visually view
the happenings in your environment you
can pick atena and other tools for
analytics are EMR and uh which is
elastic map produce and Cloud search all
right enough of theory and let's have a
quick look at how two services in AWS
ec2 and S3 work together and benefits us
here are two ID professionals talking to
each other one says I have an
application which takes lot of storage
and works only on Linux system which I
do not have at the moment and the other
one is a smart guy and he immediately
replies that he could use S3 to store
data and retrive data and use ec2 for
all his compute needs and then the
Curious conversation builds up and the
first person wants to know what is easy2
and S3 and the second guy starts to
explain that AWS ec2 is a web service
that provides a secure and resizable
compute capacity in the cloud and ec2 it
can also be used to launch as many
virtual servers as we need and about S3
he explains AWS S3 is a simple storage
service provided by AWS and about S3 he
explains further saying that using
Amazon S3 we can store Android Thrive
any amount of data at any time on the
web all right so the very first thing is
to have an AWS account so create an AWS
account and second thing is to create an
AWS S3 bucket and upload the files there
you know the files that ec2 server is
going to pull from will get stored in S3
as first thing all right so the first
thing is to create an S3 bucket so let's
create an S3 bucket and let's call it as
a website bucket and these names will
have to be unique so let's see if this
bucket name is available say bucket name
already taken or already exist so let me
top it up withl meaning simply Lear all
right so we we were able to create a
bucket and in that bucket let me upload
a Content that basically is going to be
my index file that's going to go and sit
in my web server so let me also make it
public and view it all right so this is
the content that I have stored in my S3
and this is the content I'm going to
push to my ec2 server all right Second
Step create an AWS S3 bucket and upload
files we're done with that third is to
create an ec2 instance that's in other
words create an virtual machine in the
cloud let's go and create it all right
the third thing is to create an ec2
instance so let's create an ec2 instance
it's plain and it's very simple I'm
going to keep everything as defaults
here all right so I've launched an ec2
instance as you see it's running I also
have an IP address public IP address
here with that I have logged into this
E2 instance and uh I'm in the folder /w/
dubdub du/ HTML and as you see there is
nothing in there at the moment and This
Server also has Apache installed in it
as you can see now I browsed to the IP
address of the ec2 instance and it's
showing a page which is the default
Apache page all right if I had any files
in this folder that would get shown as
the web page instead of the default page
our task is to save files in S3 and move
them to the ec2 instance so S3 is going
to act as a storage or a repository or a
source code control in this example all
right so let's do that all right the
third step is to create your ec2
instance and that's what we have done
and the ec2 instance has no files in it
and the fourth step is to synchronize
the source code bucket with the E2
instance let's do that the actual
command to do that would be AWS S3 sync
and then the name of the bucket from
which we're going to pull the code and
the folder in which we're going to put
the code or the data all right right so
it has downloaded something look at that
and now if we go back to the test page
and do a refresh there you go it showing
the page that it pulled from S3 so here
I can use S3 as the source code control
bucket and uh any information that I put
in there will get reflected in ec2
instance once I do a sync every day and
that's how I use S3 as my storage for
ec2 all right finally we were able to
synchronize S3 bucket and the ec2 2
instance and we were also able to view
the results in the web browser of the
ec2 instance the data was copied from S3
to the ec2 instance and that we were
able to view from the web browser meet
Rob he runs an online shopping portal
the portal started with a modest number
of users but has recently been seeing a
surge in the number of visitors on Black
Friday and another holidays the portal
saw so many visitors that the servers
were unable to handle the traffic and
crashed is there a way to improve
performance without having to invest in
a new server wondered rob a way to
upscale or downscale capacity depending
on the number of users visiting the
website at any given point well there is
Amazon web services one of the leaders
in the cloud computing Market before we
see how AWS can solve Rob's problem
let's have a look at how AWS reached the
position it is at now AWS was first
introduced in 2002 as a means to provide
tools and services to developers to
incorporate features of amazon.com to
their website in 2006 its first Cloud
Services offering was introduced in 2016
AWS surpassed its 10 billion Revenue
Target and now AWS offers more than 100
cloud services that span a wide range of
domains thanks to this the AWS cloud
service platform is now used by more
than 45% % of the global market now
let's talk about what is AWS AWS or
Amazon web service is a secure cloud
computing platform that provides
computing power database networking
content storage and much more the
platform also works with a paysu go
pricing model which means you only pay
for how much of the services offered by
AWS you use some of the other advantages
of AWS are security AWS provides a
secure and durable platform that offers
endtoend privacy and security experience
you can benefit from the infrastructure
management practices born from Amazon's
years of experience flexible it allows
users to select the OS language database
and other services easy to use users can
host applications quickly and securely
scalable depending on user requirements
applications can be scaled up or down aw
provides a wide range of services across
various domains what if Rob wanted to
create an application for his online
portal AWS provides compute services
that can support the app development
process from start to finish from
developing deploying running to scaling
the application up or down based on the
requirements the popular Services
include ec2 AWS Lambda Amazon light cell
and elastic beant stock for storing
website data Rob could use AWS storage
services that would enable him to store
access govern and analyze data to ensure
that costs are reduced agility is
improved and Innovation
accelerated popular services within this
domain include Amazon S3 EBS S3 Glacier
and elastic file storage Rob can also
store the user data in a database with
aw Services which he can then optimize
and manage popular services in this
domain include Amazon RDS Dynamo DB and
redshift if Rob's businesses took off
and he wanted to separate his Cloud
infrastructure or scale up his work
requests and much more he would be able
to do so with the networking Services
provided by AWS some of the popular
networking Services include Amazon VPC
Amazon Route 53 and elastic load
balancing other domains that AWS
provides services in are an analytics
blockchain containers machine learning
internet of things and so on and there
you go that's AWS for you in a
nutshell hello everyone let me introduce
myself as Sam a multiplatform cloud
architect and trainer and I'm so glad
and I'm equally excited to talk and walk
you through the session about what AWS
is and talk to you about some services
and offerings and about how companies
get benefited by migrating their
applications and infra into AWS so
what's AWS let's talk about that now
before that let's talk about how life
was without any cloud provider and in
this case how life was without AWS so
let's walk back and picture how things
were back in 2000 which is not so long
ago but lot of changes lot of changes
for better had happened since that time
now back in 2000 a request for a new
server is not and happy thing at all
because lot of uh money lot of
validations lot of planning are involved
in getting a server online or up and
running and even after we've finally got
the server it's not all said and done
there a lot of optimization that needs
to be done on that server to make it
worth it and get a good return on
investment from that server and uh even
after we have optimized for a good
return on investment the work is still
not done there will often be a frequent
increase and decrease in the capacity
and you know even news about our website
getting popular and getting more hits
It's still an Bittersweet experience
because now I need to add more servers
to the environment which means that it's
going to cost me even more but thanks to
the present day Cloud technology if the
same situation were to happen today my
new server it's almost ready and it's
ready instantaneously and with the Swift
tools and technologies that Amazon is
providing u in provisioning my server
instantaneously and adding any type of
workload on top of it and making my
storage and server secure you know
creating a durable storage where data
that I stored in the cloud never gets
lost with all that features Amazon has
got our back so let's talk about what is
AWS there are a lot of definitions for
it but uh I'm going to put together a
simple and a precise definition as much
as possible now let me iron that out
Cloud still runs on and Hardware all
right and uh there are certain features
in that infrastructure in that cloud
infrastructure that makes cloud cloud or
that makes AWS a cloud provider now we
get all the services all the
Technologies all the features and all
the benefits that we get in our local
data center like you know security and
compute capacity and uh databases and in
fact you know we get even more cool
features like uh content caching in
various global locations around the
planet but again out of all the features
the best part is that I get or we get
everything on a pay as we go model the
less I use the less I pay and the more I
use the less I pay per unit very
attractive isn't it right and that's not
all the applications that we provision
in AWS are very reliable because they
run on an reliable infrastructure and
it's very scalable because it runs on an
ond demand infrastructure and it's very
flexible because of the designs and
because of the design options available
for me in the cloud let's talk about how
all this happened AWS was launched in
2002 after the Amazon we know as the
online retail store wanted to sell their
reminding or unused infrastructure at as
a service or as an offering for
customers to buy and use it from them
you know sell infrastructure as a
service the idea sort of clicked and AWS
launched their first product first
product in 2006 that's like 4 years
after the idea launch and in 2012 they
held a big-sized customer event to
gather inputs and concerns from
customers and they were very dedicated
in making those requests happen and that
habit is still being followed it's still
being followed as U reinvent by AWS and
at 2015 Amazon announced its Revenue to
be 4.6 billion and in 2015 through 2016
AWS launched products and services that
help migrate customer services into AWS
well there were products even before but
this is when a lot of focus was given on
developing migrating services and in the
same year that's in 2016 Amazon Revenue
was 10 billion and not but not the least
as we speak Amazon has more than 100
products and services available for
customers and get benefited from all
right let's talk about the uh services
that are available in Amazon let's start
with this product called S3 now S3 is a
great tool for internet backup and it's
it's the cheapest storage option in the
object storage category and not only
that the data that we put in is
retriable from the internet S3 is really
cool and we have other products like
migration and data collection and data
transfer products and here we can not
only collect data seamlessly but also in
a realtime way monitor the data or
analyze the data that's being received
that they cool products like uh AWS data
transfers available that helps achieve
that and then we have products like uh
ec2 elastic compute Cloud that's an
sizable computer where we can anytime
anytime alter the size of the computer
based on the need or based on the
forecast then we have SIMPLE
notification Services systems and tools
available in Amazon to update us with
notifications through email or through
SMS now anything anything can be sent
through email or through SMS if we use
that service it could be alarms or uh it
could be service notifications if you
want stuff like that and then we have
some security tools like KMS key
management system which uses AES 256bit
encryption to encrypt our data at rest
then we have Lambda a service for which
we pay only for the time in seconds
seconds it takes to execute our code and
uh we're not paying for the
infrastructure here it's just the
seconds the program is going to take to
execute the code if it's a short program
will be paying in milliseconds of it's a
a bit bigger program will be probably
paying in 60 seconds or 120 seconds but
that's lot cheap lot simple and lots
cost effective as against paying for
service on an odly basis which lot of
other services are well that's cheap but
using Lambda is a lot cheaper than that
and then we have services like uh Route
53 at DNS service in the cloud now I do
not have to maintain an DNS account
somewhere else and my cloud environment
with AWS I can get both in the same
place all right let me talk to you about
um how AWS makes life easier or how
companies got benefited by using AWS as
their it provider for their applications
or for the infrastructure now unil is a
company and um they had a problem right
and they had a problem and they picked
AWS as a solution to their problem right
now this company was sort of spread
across 190 countries and they were
relying on a lot of digital marketing
for promoting their products and their
existing environment their legacy local
environment proved not to support their
changing it demands and they could not
standardize their old environment now
they chose to move part of their
applications to AWS because they were
not getting what they wanted in their
local environment and since then you
know rollouts were easy provisioning new
applications became easy and even
provisioning infrastructure became easy
and they were able to do all that in
push button scaling and uh needless to
talk about uh backups that are safe and
backups that can be securely access from
the cloud as needed now that company is
growing along with AWS because of their
Swift speed in rolling out deployments
and uh being able to access secure
backups from various places and generate
reports and in fact useful reports out
of it that helps their business now on
the same lines let me also talk to you
about kogs and how they got benefited by
using Amazon now kogs had a different
problem it's one of its kind now their
business model was very dependent on an
infra that will help to analyze data
really fast right because they were
running promotions based on the analyzed
data that they get so they being able to
respond to the analyzed data as soon as
possible was critical or vital in their
environment and luckily sap running on
Hannah environment is what they needed
and uh you know they picked that service
in the cloud and that sort of solved the
problem now the company does not not
have to deal with maintaining their
legacy infra and maintaining their heavy
compute capacity and maintaining their a
database locally all that is now moved
to the cloud or they are using Cloud as
their it service provider and and now
they have a greater and Powerful it
environment that very much complements
their business coming to the AWS
services so essential AWS services are
compute storage database migration
networking and content delivery
developer tools management tools Media
Services machine learning analytics
security identity and compliance mobile
services application integration arvr
which is augmented reality and virtual
reality customer engagement business
productivity desktop and app streaming
internet of thingss which is iot now
let's look into the compute service
which is one of the widely used service
on the AWS and what does the compute
service do these Services help
developers build deploy and scale an
application in the cloud platform that
is
ec2 Lambda elastic container service
elastic load balancer light sale and
elastic bean stock these are the
services which lie in the compute
service section only one of the most
widely used service within the compute
section is ec2 which stands for elastic
Cloud compute it is a web service that
allows developer to rent virtual
machines and help to resize the compute
capacity so here what you can do is you
can run the virtual machines and you
have the Privileges to select the type
of operating system that you want uh
should be running on your instances or
on your virtual machines and likewise
later you can customize as per your
requirement Lambda is a serverless
compute service it is also responsible
to execute code for a specific
application so those who are from the
development background they can focus
more on creation of a code they don't
have to create a server manage it
instead they can use a Lambda where you
they can deploy their code directly onto
the Lambda server then comes the storage
service now couple of storage services
are S3 glacia EBS which is elastic block
storage store storage Gateway now AWS
provides web data storage service for
archiving data also its main advantage
is disaster data recovery with high
durability let's look into some of the
essential storage services and one of
the most widely used storage servic is
S3 which stands for simple storage
service in the simple storage service
what you need to do is you have to
create a bucket and in that bucket you
have to put the files in it so so S3
gives open cloud-based storage service
which is utilized for online data backup
then comes the EBS which is an elastic
block storage now you can understand EBS
as a virtual hard drive also which
attaches with the ec2 and it provides
High availability storage volume for
persistent data it is mainly used by
Amazon ec2 instances then you have the
database Services AWS datab base domain
service offers coste efficient highly
secure and scalable database instances
in the cloud and some of the database
services are RDS which is a relational
database service Dynamo DB the
non-relational or no SQL or nosql
database service elastic C and Amazon
red shift now one of the essential
database service is the Dynamo DV it is
a flexible nosql database service
which offers fast and reliable
performance with no scalability issues
it is fast reliable highly scalable and
suitable for small scale applications
like mobile applications gaming
applications or anything with respect to
the Internet of Things devices there the
dynamodb is uh most widely used or
suitable then comes the relational
database service that is the RDS which
is a structure database service it is a
managed distributed relational database
cloud service that helps developers to
operate and scale database in a simple
manner so RDS has different vendors
platform with respect to the database U
usage and that includes the post gr SQL
MySQL then you have Oracle Microsoft or
mssql and uh they have their own
customized database uh as well that is
called as the Amazon Aurora along with
that they have a Maria database or Maria
DB as well so these are couple of
vendors that give their database engines
uh that you can use on the RDS now
coming to the networking Services it
offers a highly secure Cloud platform
and helps in connecting your physical
Network to your private virtual network
with high transfer speed now some of the
services in the networking and content
delivery or VPC which is a virtual
private Cloud a very important service
in order to make your applications or uh
Services more secure roote 53 which is a
DNS mapping Service Direct Connect which
directly connects with the AWS services
and with your data centers and the cloud
front that is basically a Content
delivery service now coming to the VPC
or a virtual private Cloud it helps a
developer to deploy AWS resources such
as Amazon ec2 instances in a private
virtual Cloud so that you can actually
make your ec2 isolated or make it more
secure and even you can make it for a
public access also depends on the
administrator that how they want to
customize it so the complete control of
a VPC and its networking is with the
admins then comes the root 53 service it
is a web service with highly available
domain name system or the DNS that helps
user to Route software by translating
text into IP address and that is why it
is called as a DNS mapping service and
that helps you to use your domains or
the external domains pointed to the AWS
services in case if you use AWS for
hosting your websites or the
applications note DNS translates text
into the IP address now coming to the
developer tool services it helps a user
build deploy and run an application
source code automatically it also
updates the server and instance on the
workload so first is the code star code
build code deploy code pipeline so codar
it is a service designed to manage
application development at a single
place here developers can quickly
develop build and deploy applications on
AWS so all the manage app development
can be done with the code star are code
build removes the hassle of managing
physical servers and helps developer
build and test code with continuous
scaling so security identity and
compliance Services helps in monitoring
a safe environment for your AWS
Resources by providing limited access to
specific users so in case let's assume
that you have to give an access to
someone but with limited privileges you
can primarily use IM am in that case and
if if you want to make uh your
applications or um deployments more
secure then you can use uh these
services like uh KMS IM Cognito uh WF
which acts as a firewall now the IM
service which is the identity access
management is a framework that helps in
maintaining access to AWS services in a
secure way so what happens is that the
admin who has the complete access of the
AWS console provide access to users and
there can be different users and they
would have the privilege accesses uh
defined by the Admin so what type of
permissions the admin gives them uh they
would have those limited access on the
AWS console KMS enables users to create
and manage the encryption keys that are
used for encrypting data coming to
management tool services with the help
of management tools using the Serv
service an individual can optimize cost
minimize risk and automate all the
resources running on the AWS
infrastructure efficiently so with the
management tools you can monitor the
resources application it's uh its tools
and the utilizations and along with that
you can scale up scale down the
resources likewise with the help of the
management tools you can also do the
auditing task so so one of the essential
service in the management tool servic is
the cloud watch it is a monitoring tool
for AWS resources and customer
applications running on AWS platform so
let's assume that you have used ec2
Dynamo DB S3 RDS and you want to monitor
those resources you can use the cloud
watch that can give you the results
coming to the cloud
formation this service helps you in
monitoring all your a s resources at one
place so that you can spend minimum time
in managing those resources and maximum
time on developing the application so
with the help of the cloud formation you
can deploy the entire solution with the
help of creation of a template you just
need to create one template and you have
to deploy it the rest of the things will
be done by the AWS and hence it is a
kind of an automation task only now
let's look into the demo of some of the
essential
services so we'll start with the ec2 and
I have already logged in into my AWS
account so where exactly you can find
the E2 just click on the
services under the compute section you
can find the ec2 ec2 stands for elastic
Cloud compute service primarily used for
creating the virtual machines so I'll
click on the service and quickly I'll
show you how the virtual machine is
created and how we can basically access
it so I would be creating one virtual
machine or an instance with the Windows
operating system so here you can see
there are three instances which are
already running let me create another
one you have to click on launch
instances and here you have to select
some configuration details so most of
the configuration details I'll be taking
as default and wherever it is necessary
I'll be making the
changes so first of all you have to
select the operating system in the form
of Ami and I would be looking for
Windows uh 2016 and then you have to
select the type of instance with respect
to the CPUs number of CPUs and the
memory capacity so I'll go with t2.
micro which is a free tier eligible
instance in the configure instance
details the rest of the things we'll
keep it as default as of now click on
add storage this is a basically the
virtual hard drive or the EBS the
elastic block storage that is attached
with the ec2 instance so primarily it is
giving us 30 GB of space without any
additional
cost we can leave the taxs as blank
click on configure security groups and
here you can see that the RDP Port is by
default open which will actually allow
us to RDP or to have a remote
connectivity of our
instance click on review and launch now
to give an authentication or to provide
an authentication we should have a key
pair with us so that the AWS
can understand or can tally the key pair
and give us the access so I already have
a key pair created what I'll do is I'll
create a new key pair for this instance
and put a random name let's say I put it
something like demo download the key
pair and make sure that you keep your
key pair in a safe and secure place
click on launch
instances now you will see that there is
an instance ID that has been created
which is a number alpha numeric number
that is randomly given by the
AWS you can name your
instances so let's assume let's say we
name our instance as Windows
2016 click on
Save and the AWS deploys are instance on
a respective infrastructure it gives us
the IP addresses the public IP and the
private IP which is an internal IP and
we have to access the instance from the
public IP only so now to access that
instance we have to open up the
RDP put the public IP click on
connect and when it asks Q for the
authentications you put a username as
administrator and the password you have
to generate by providing the key so
click on
connect and then uh you have to click on
RDP client here you have to get a
password so click on get
password browse and when you click on
browse you have to provide demo do PM
file so it should be in the download
section here itself just just provide
that key pair and it is going to give
you the password in the encrypted format
decrypt your
password copy that and then provide
those
details to the RDP client click on okay
and now it should allow you to log to
the instance let's wait for the windows
to appear so here you can see that we
have uh logged in into the windows
instance the windows 2016 screen is
available in front of us if getting your
learning started is half the battle what
if you could do that for free visit
scaleup by simply learn click on the
link in the description to know more in
this particular demonstration I would be
explaining you about how to use a
storage service specifically the S3
service which is most widely used
service under the storage section now
the S3 stands for simple storage service
primarily used for storing the objects
and the files
and uh what we need to do is we just
have to click on the S3 service under
the storage
section so when you click on S3 service
um you have to create the buckets inside
the
S3 and the buckets are the places where
you keep your folders or you upload the
files that are available on your
systems so here if you'll notice that
the S3 service is a global Service that
means it is irrespective of the region
and uh the buckets when you create they
are created in a specific region now
what is the
benefit the benefit is that if you
create a bucket in different regions all
those buckets in different regions can
be viewed from a single
dashboard and uh you don't have to
change the region again and again to
view the S3 buckets so what you need to
do is you have to click on create bucket
and here you have to specify the name of
the bucket so let's say I put something
like demo
AWS and
uh that's the bucket name I going to
select now make sure that the bucket
name starts with the lowercase and it
should be always unique now why it
should be unique because since S3 is a
global Service so it may be that
somebody else could be using
your the name that you have provided so
it should be always unique otherwise it
will not be
allocated now you have to select a
region and that would be let's say I go
with the Ohio
region and uh in the bucket settings you
can basically change and configure these
settings according to your requirement
by default uh when you create a
bucket it blocks all the public access
from the bucket so when you have all
public access blocked no object can be
viewed from the S3 Bucket from a public
network or from or by anyone
else and hence uh in order to view the
objects or the or the files in the
bucket you have to unblock or uncheck
this option so that first of all you
make the bucket accessible from the
public network likewise you can
customize as per the requirement and
then click on acknowledgement the rest
of the things will keep it as default
and click on create bucket now it says
that the bucket with the same name
already exists so that means it is not a
unique name somebody else might have
been using this name so I'll try to keep
it more unique and I'll try to assign
some number so let's say I put something
like
987 and it says it is already existed
let's say
9876 and then click on create bucket now
if it creates a bucket that means we
have been allocated with that bucket
name so let's wait for wait for a minute
to get that bucket created
so that happens quickly and you can see
there are a couple of buckets already
created here and these buckets can be
viewed in a single dashboard so some of
the buckets are in Mumbai region some of
the buckets are in Ohio region but they
are available in the same or a single
dashboard so I'll open up the bucket
that I have recently created and I'll
try to upload some documents now what
you can do is you can create a folder
also inside the S3 and when you create a
folder you can can upload the objects
accordingly otherwise you can directly
also upload the objects so click on
upload and we will upload any random
file from my system onto the
S so I'll uh basically
file and then click on
upload now it is uploading my object
from my system onto uh the S3 bucket
and now it has successfully done so it
says the message is successful now in
order to validate I will click on the
bucket from the S3 service and view this
object now you can see here is my bucket
so I'll just open up this bucket and uh
these DNS records. CSV file is available
with us now the S3 is not only limited
till storing the objects or the files it
has many other functionalities and the
features also
like you can enable versioning you can
host a static website on the
S3 along with that uh you can have a
cross region replication enabled so that
you can have a high availability of your
objects or you can have a redundancy of
uh your critical objects in different
regions so likewise there are more
features um that would be covered up in
the detailed section of an S3 service
now coming to another section of of the
essential services that is the
database now here you can see I have a
database section in the AWS and it has
multiple service within
in uh the RDS is there Dynamo DB is
there elastic and the other database
services are there uh so I would be
showing a demo on the Dynamo DB which is
a nosql
database
now when we say it is a nosql database
that means it is a non- relational
database service where here we can
create a database table directly from
the web console we don't require a
separate database engine uh like uh in
the case of
RDS and uh in the tables you can insert
the values and view those values
directly from the AWS dashboard
itself so it says Amazon Dynamo DB is a
fast and flexible no SQL database
service primarily suitable for iot and
web gaming and other mobile applications
so what you need to do is it is a
straightforward uh database service
which is which can be accessed while
creating a table itself and it's a
compute based database service that is
the reason that it is more fast so click
on create
table and what you need to do is you
just have to put a table name so I'll
just put something like
test and uh in the partition key so this
partition keys are unique entities so
what you need to do is you have to
specify a partition key so I'll put
something like
ID and the string instead of string I'll
use a
number likewise I can add sort Keys Also
let's say I put a
name and the name should be in the
string format now these are the unique
entries these are the fixed entries in
the table and after that we are going to
put the attributes and in the attri
rutes the data will be inserted so what
you need to do is rest of the things
we'll keep it as default as of now click
on
Create
and here you can see the table has been
created the test table has been created
now if I click on the items so you would
see that it has the sort Keys available
the ID and the sort key associated with
that that is the name but it does not
have any entry because we have not added
any value or the attributes so how we
can add or insert the values in this
table that can be done many ways you can
enter manually you can use the help of
CLI to enter the large chunk of data
directly uh upload it onto the Dynamo DB
table and also you can use the apis also
in order to insert the data inside the
table so what you will do is we will
click on create
item and in the ID we will put some
value let's say number one string uh
let's say we put something
like uh we will go with a random name so
instead of putting any name we'll put a
value like a BC D that should be fine
and
then we will insert some attributes so
let's say I put string as Rank and I'll
put something like rank
two and then click on
save so here you will see just refresh
the database
table close it and open it up
again click on the
items now here you can see the ID one
name is ABCD got the rank two likewise
you can click on create items let's say
the serial number or the ID 2 name let's
say we put something like uh we can go
with XY Z any random thing
put it up as number rank and let's say
this particular value got a rank
three likewise you can add some more
items string let's say you put something
like
ajk and you put a rank let's say it's a
rank number one holder rank one
holder right so likewise uh this is just
an example likewise you can add the
attributes as per your need and the
table can be filtered out based on the
attributes also so if you have to search
some values in the Dynamo DB table so
you can always use
filters uh to basically search the
values uh inside the table apart from
that the Dynamo TB table has lot of
other
functionalities um uh it can be
basically uh you can have a backup
of a table created in the Dynamo DB
table in the Dynamo database and then
you can retrieve or recover the data by
restoring the backups from the Dynamo DB
this database can be created in the
cluster format also so these are couple
of features uh that you can use with the
Dynamo
DB now let's move into the next
section now coming to the networking
services so there are some of the
networking Services which are very
useful and uh that includes a VPC which
is a virtual private Cloud cloudfront
root 53 API Gateway
Etc now I'll will uh basically
demonstrate about the root 53 Service uh
in this demo so in the root 53 is
basically a DNS mapping service so what
you can do is let's assume that you are
hosting any web application on the
server and you want to Route the domain
traffic onto those servers you need to
have the help of root 53 to do that so
what you need to do is you just have to
click on root 53 service and from this
service you can register your domains
also otherwise if you have domains
purchased from any external site you can
point them to the root 53 name servers
also so first of
all in the root 53 you have to create a
hosted zone so there is already one
hosted Zone all created now when you
create a hosted Zone you have to specify
the domain name
so let me show you I have one dummy
domain this is the domain uh that has
been defined so what you have to do is
when you click on create hosted Zone you
have to put a domain here you can see
example.com likewise you have to put
your own
domain and uh click on the public hosted
Zone click on create hosted Zone and it
is going to give you four name server
now those four name servers have to be
updated on the platform from where you
have purchased the domain so that is
mandatory in order to Route the traffic
to the root 53
service right so here you can see I
already have a hosted Zone created for a
domain and it has given
me four name servers these are the name
servers and these name servers have been
updated in a record set from where the
domain has been purchased right once it
is done then you have to Route the
domain traffic to a server so what you
have to do is you have to click on
create
record and in the create record you have
to specify the IP address or you can use
the alas also where the traffic should
be routed to where your application is
hosted at so ideally it is a server
details and it exists with certain
policies of also so you can see some
view existing records so here you can
see this particular domain is routed to
a DNS value which is hosted in the
elastic bean stock instead of that you
can put an IP address of the ec2
instance also you can use the S3 URL
also you can use the cloudfront URL also
so likewise what will happen is that the
domain traffic will be routed to the
server where actually the application or
a web application is hosted at now I'm
using a routing policy as simple routing
policy that means all the traffic should
be routed to that particular domain
whereas there are other routing policies
also in the Route 53 that includes the
weighted routing policy which acts as a
kind of a load balancing geolocation
routing policy multivalue answer routing
policy and then you have a redundancy
based routing policy which is a fail
over one so likewise you can select as
per your
requirement so what you need to do is
you just have to click on create records
uh you have to specify you have the
domain so you have to specify any uh
particular info you want to put before
the domain otherwise you can leave it as
blank and in the record type let's
assume that you are using an IP address
of a server where uh your web
application is hosted at so you can
basically use a routes traffic to an IP
V4 address and put an IP address make
sure that you put a public IP address or
the elastic IP address attached to your
instance in in case if you're not using
any particular IP address or the URL you
can use the Alias also so likewise um
the records can be created along with
that um the Route 53 Services used for
domain verifications also like uh if you
want uh the email services on onto your
web application then you can basically
verify your domain directly from the
roote 53 service you can uh get the
verification done for the SSL
certificate creation for that also the
records will be direct created from the
root 53 service because it is actually
managing or hosting your uh
domain now with respect to the security
services uh the most widely used service
is the IM am which is identity access
management that lies under security
identity and compliance so in from the
IM you can create the users uh whom you
want to give an access to your AWS
console you can create groups and you
can add multiple users in that group
give them the permissions you can create
the rules also so that multiple Services
can inte interact or integrate it
together so how the IM is used click on
the IM
service and I'll show you how a
particular user can be created and U how
the user is basically uses the
credentials to access the AWS console
now the IM dashboard is open and when
you open up the IM dashboard it gives
you the URL so this is the URL through
which uh the user has to actually
access the AWS console by providing the
user credentials now how the users are
created just click on the users and U
here you just have to click on add user
so I'll create one sample user let's say
I put something like sample
user and what type of an access you want
to give that particular user do you want
to give a programmatic access which is a
primarily the CLI access or you want to
give the AWS Management console so right
now we'll go with the AWS manag m m
console access now do you want an
autogeneration of the password or do you
want to customize the password so let's
assume that we customize the password so
put any
value make sure the password meets all
the
criterias and uh then click on next
permissions now what kind of permissions
you want to give to that user let's
assume that you want to give an access
of a particular service only to that
user so you have to actually search you
have to actually search a policy for
that particular particular service um
that can be given an access to a user
otherwise if you want to give an admin
policy or the admin access to that user
you can search an admin policy there so
the permissions are important otherwise
the user would not have privileges to
access um the AWS console so we'll
attach the existing policies directly so
click on it and here let's assume that I
want to give an admin access to that
user so I'll just click on the admin
access now to a particular user you can
give multiple policies also it is not
necessary that you have to give only a
single policy so you can provide a
multiple policies to the users also now
click on next tags let let's make the
tags blank click on review and create a
user now you'll see that our sample user
would be created and to access the
console we'll just copy this URL which
has the account information as well that
is the account
number so copy the link or can copy the
complete URL log out from the root
account and
then paste the URL that was copied from
the
console and then we have to give the
user credentials
so we'll put sample user as the username
and the password that we provided while
creating the
user click on sign in and if it is
correct then it should allow us to login
into the AWS console while asking
the password
change so we'll change the password
confirm password
change and now it should allow us to
login into the root accounts AWS console
with the admin
privileges now the next service is the
monitoring services and under the
monitoring Services uh primarily the
cloud watch uh the cloud trail the cloud
formation these are some of the services
that are most widely
used now the cloudwatch is a service
which is primarily used for monitoring
the metrics primarily of the servers
like for example if you create an ec2
instance and you want to uh basically
watch out for the metrics associated
with the CPU utilization or uh the
storage utilization the network in
network out all those information you
can get it from the cloudwatch now
cloudwatch is not
only related to monitoring the metric it
can generate the alarms also and uh here
you can get the events also generated
which or the events that can be created
which can trigger the Lambda function as
well so what you have to do is you just
have
to click on dashboard soing metrics are
viewed uh with the help of the cloud
watch so first of all you have to create
a
dashboard now you have to put a
dashboard name let's say I put something
like
monitoring ec2
click on create dashboard now how do you
want the reports to be published you
have to actually select a widget I want
that okay the report should be visible
in the numeric form so I'll select a
number now click on the dashboard that
was created and
uh let's again add the
widget and here uh we'll select the
metrix primarily for the ec2 per
instance metrix so we have one single
instance running now there are 14 metric
available for that particular instance
now I would be looking for the CPU
utilization for this particular instance
so I'll just select for the CPU
utilization and it is going to give me
some information about what is the
current CPU utilization of that
particular instance so that is somewhere
around
29.3% uh the CPU utilization has been
done for the single instance that is
running in my ec2 dashboard so likewise
you can add some more metrics and view
them in this particular dashboard and
the cloudwatch will keep on publishing
the data uh at a refresh interval of 5
minutes that is uh the default
value hi guys so today's session is on
AWS AG maker but before proceeding with
this topic I would request you guys to
like our video also subscribe to our
Channel you can find the link just below
at the right side of this video let's
look into what we have in our today's
session so what's in it for you we would
be covering what is
AWS why do we need AWS sagemaker what is
AWS sagemaker Services what are the
benefits of using the AWS
sagemaker machine learning with AWS
sagemaker how to train a model with AWS
sagemaker how to validate a model with
AWS and the companies that are using AWS
stagemaker along with that we will be
covering up one live demo on the AWS
platform now let's understand what is
AWS so what is AWS it's an Amazon web
services it's a largest or most widely
used public Cloud platform offered by
Amazon it provides services over the
Internet AWS Services can be used to
build Monitor and deploy any type of
application in the cloud AWS also uses
the subscription pricing model that
means you only pay for whatever the
services you to use
for now why do we need AWS sagemaker
let's look into it so let's consider an
example of one of the company that is
proquest now before AWS sagemaker the
proquest is a Global Information content
and technology company that provides
valuable content such as ebooks
newspapers Etc to the
users before AWS s maker the proquest
requirement was to have a better user
experience maximum relevant search
results now after aw s maker they were
able to achieve those uh results uh so
they achieved more appealing video user
experience they achieved more relevant
search results for the users now what do
we mean by aw sagemaker why this
Services primarily used so Amazon
sagemaker is a cloud machine learning
platform that helps users in building
training tuning and deploying machine
learning models in a production ready
hosted environment so it's kind of a
machine learning service which is
already hosted on the AWS platform now
what are the benefits of using AWS
Hemer uh the key benefits of using aw
sagemaker are it reduces machine
learning data cost so you can do the
cost
optimization while running uh this
particular service on the AWS all ml
components are stored in a particular
place in a dashboard so they can be
managed together highly scalable so it
can be scalable on you can scale this
particular service on the fly it trains
the models quite faster maintains the up
time so you can be assured that your
workloads will be running all the time
it will be available all the time high
data security so security becomes a
major concern on the cloud platforms and
it ensures that you have the high data
security along with that you can do a
data transfer to different AWS services
like S3 bucket and all with the simple
data transfer
techniques now machine learning with AWS
sagemaker let's look into it so machine
learning with aw sagemaker is a
three-step function so one is to build
second is to test and tune the model and
third is to deploy the model now with
the build it provides more than 15
widely used ml algorithms for training
purpose now to build a model you can
collect and prepare training data or you
can select from the Amazon S3 bucket
also choose and optimize the required
algorithm so some of the algorithms that
you can select are K means linear
regressions logistic regression
sagemaker helps developers to customize
ml instances with the Jupiter notebook
interface in the test and tune you have
to set up and manage the environment for
training so you would need some sample
data to train the
model so train and tune a model with the
Amazon Sage maker Sage maker implements
hyperparameter tuning by adding a
suitable combination of algorithm
parameters also it divides the training
data and stores that in the Amazon S3 S3
is a simple storage service which is
primarily used for storing the objects
and the data hence it is used for
storing and recovering data over the
internet and Below you can see that AWS
s maker uses Amazon S3 to store data as
it safe and secure also it devides the
training data and stores in Amazon S3
where the training algorithm code is
stored in the ECR ECR stands for elastic
container registry which is primarily
used for containers and Dockers ECR
helps users to save Monitor and deploy
Docker and the containers later Sage
maker sets up a cluster for the input
data trains it and stores it in the
Amazon S3 itself so this is done by the
sage maker itself after that you need to
deploy it so suppose you want to predict
limited data at at a time you use Amazon
sagemaker hosting services for that okay
but if you want to get prediction for an
entire data set prefer using Amazon
sagemaker batch transform now the last
step that is to deploy the model so once
tuning is done models can be deployed to
sagemaker
endpoints and in the endpoint realtime
prediction is performed so you would
have some data which you would reserve
and validate your model whether it is
working correctly or not now evaluate
your model and determine whether you
have achieved your business goals now
the other aspect is how we can train a
model with AWS
Hemer so this is basically a flow
diagram which shows you how to train a
model with the AWS sagemaker and here we
have used couple of Services of an AWS
to get that
done so model training in aw sagemaker
is done on machine learning compute
instances
and here we can see there are two
machine learning compute instances used
as helper code and the training code
along with that we are using 2 S3
buckets and the ECR for the container
registry now let's look into what are
the ways to train the model as for the
slides So Below are the following
requirements to train a model so here in
the diagram you can see these are the
following requirements to train a
model the URL of an Amazon S3 bucket
where the training data is stored that
is
mentioned the compute resources on
machine learning compute instances so
these are all your machine learning
compute
instances then the URL of an Amazon S3
bucket where the output will be
stored and the path of AWS elastic
container registry where the code data
is save the inference code image lies in
the elastic container registry now what
are these calls the these are called as
the training
jobs now when a user trains a model in
Amazon sagemaker he she creates a
training job so we need to First create
a training job and then the input data
is fetched from the specified Amazon S3
bucket once the training job is built
Amazon Sage maker launches the ml
compute instances so these compute
instances will be launched once the
training job is built then it trains the
model with the training code and the
data set and it shows the output and
model tic crafts in the AWS S3 bucket so
this is done automatically now here the
helper code performs a task when the
training code fails the interference
code which is in the elastic container
registry consist of multiple linear
sequence containers that process the
request for inference on data the ec2
container registry is a container
registry that helps users to save
Monitor and deploy container images
whereas container images are the ready
applications one once the data is
trained the output is stored in the
specified Amazon S3 bucket so here you
can see the output will be stored here
to prevent your algorithm being deleted
save the data in Amazon sagemaker
critical system which can process you on
your ml compute instances now how to
validate a model let's look into it so
you can evaluate your model using
offline or using the historical data so
first thing is that you can do the
offline testing to validate a model you
can do an online testing with the live
data so if you have a live data coming
or realtime streams coming you can
validate a model from there as well you
can validate using a holdout set and
also you can validate using the kfold
validation now use historical data to
send requests to the model through the
jupyter notebook in Amazon sagemaker for
the evaluation online testing with live
data deploys multiple models into the
endpoints of Amazon sagemaker and
directs live traffic to the model for
for validation validating using a
holdout set is part of the data is set
aside where which is called hold out set
so the part of the data is left which is
basically called as the hold out set
this data is not used for the model
training so later when the model is
trained with the remaining input data
and generalize the data based on what is
learned initially so whatever the data
which is left out will be used for
validating a model because we have not
used that data while training a model
the kfold validation is the input data
is split into two parts one part is
called K which is the validation data
for testing the model and the other part
is K minus one which is used as a
training data now based on the input
data the machine learning model
evaluates the final output now the
companies that are using AWS Hemer one
is the ADP Al so you must be knowing
about ADP
zelando Dow Jones which is the stock
market
proquest and the intute now let's look
into the demo that how we can actually
run the AWS stage maker so we'll use the
r algorithm and then package the
algorithm as a container for building
training and deploying a model we are
going to use the Jupiter notebook for
that for model building for model
training for model deployment and the
code for the demo is in the below link
so you can see here that from this link
you can get the code for the demo let's
try to do a demo on the
AWS now I would be using a link which is
uh provided by Amazon to build train and
deploy the machine learning model on the
sagemaker as you can see on my screen
and in this tutorial uh you would have
some steps where you can put those steps
and the code python codes into your AWS
sagemaker Jupiter lab so in this
tutorial you will learn how to use
Amazon sagemaker to build train and
deploy a machine learning model and for
that we will use the popular XT boost ml
algorithm for this
exercise so first of all what you need
to do is you have to go to the AWS
console and there you have to create a
notebook instance so in this tutorial
you will be creating a notebook instance
you will prepare the data train the
model to learn from the data deploy the
model evaluate your ml models
performance and once all those
activities are done then we'll see how
we can actually remove all the resources
in in order to prevent the extra
costing now the first step is we have to
enter to the Amazon sagemaker console so
here you can see I'm already logged in
into the sagemaker console you can click
on the services search for the sagemaker
here and here you get the Amazon
sagemaker
service now the next step
is that we have to create a notebook
instance so we will select the notebook
instance from the sagemaker service
and then after the notebook instance is
selected we'll put a name to our
instance and we'll create a new IM Ro
for that so let's wait for the sagemaker
studio to
open so here you can see the studio is
open and you just have to click on the
notebook instances and here you have to
create a notebook instance so here you
can see couple of notebook instances
have already been created one of them is
in service so this is The Notebook
instance that we are going to use for
uh creating the demo model I'll show you
how you can create a notebook instance
you just have to click on create
notebook instance button and put your
notebook instance name so you can put
something like demo Das sagemaker 97 or
we can put it as
model we'll go with notebook instance
type as default which is ml t2. media
and in the permission and encryptions
under the IM R we'll click on
create a new IM Ro now why we are
creating a new IM Ro so that we can
allow the sage maker to access any S3
bucket that has been created on our
account just click on create a role and
here you would see that the new IM Ro
will be created with the set of
permissions then rest of the things
we'll keep it as default and then you
just have to click on create a notebook
instance the notebook instance creation
takes some time so you just have to wait
for a couple of minutes to get that in
service we already have uh one of the
notebook instance that has been created
so we will be using uh that to create a
demo now going back to the steps so
these are the steps that we have already
performed now once the notebook instance
is created then we have to prepare the
data so in this step we will be using
the sagemaker notebook instance to
pre-process the data
that would require to train the machine
learning model and for that we would be
opening up the jupyter notebook and uh
then we have to select an environment a
kernel environment in the Jupiter
notebook that would be condore Python 3
so let's follow these steps go back to
the sage maker click on the notebook
instances select the running notebook
instance and here you would select
select the open Jupiter lab now here you
would see that the Hemer would try to
open up the jupyter
notepad and we would be performing all
our inputs uh into that Jupiter notebook
and uh executing the results there
itself so just wait for the notebook to
open now here you can see the Jupiter
lab notebook has been open so I would be
selecting one of the notebook that has
been created so this one so likewise you
can create your own notebook also how
you can do
that first of all let me select the
kernel environment so I would be
selecting Kore Python 3 and just click
on select so how you can create your own
notebook just have to click on file
click on new and here you can select the
notebook just name your notebook select
the environment condore Python 3 to run
this demo so I have my notebook open so
so in the tabs I would be putting up the
python codes and I would be executing
those codes to get the output directly
so the next step
is to prepare the data train the ml
model and deploy it we will need to
import some libraries and Define a few
environment variables in the Jupiter
notebook environment so I would be
copying this code which you can see that
would try to import numai pandas these
are all required to to run the python
syntax so just copy this
code and paste it into
your
notebook right so once you do that
execute your
code and here you can see that you get
the output which is that it has imported
all the necessary libraries that have
been defined in the code
now the next step is we would create an
S3 bucket into the S3 service and for
that you have to copy this python code
just that you have to edit it so you
have to specify this bucket name that
you want to get created so here I would
provide the bucket name which should be
unique should not
overlap so something like s
maker Dash
demo is the name that I have selected
for the bucket and now you have to
execute that
code it says that the S3 bucket has been
created successfully with the name
sagemaker - demo 9876 so this is
something which you can verify so you
can go to the S3 service and there you
can verify whether the bucket has been
created or not now the next task is that
we need to download the data to the AWS
sagemaker instance and load it into the
data frame and for that we have to
follow this URL so from this URL which
is build train deploy machine learning
model would have a data in the form of
bankor clean. CSV and this will be
deployed onto our sagemaker instance
we'll copy this
code and paste it
here and execute the
code so says that it has successfully
downloaded bankor clean. CSV which is
which has the data inside it and that
has been loaded into the S maker data
frame
successfully now we have a data to build
and train our machine learning model so
what we are going to do we are going to
shuffle the data and we are going to
split it one into the training data set
and the other one into test data set so
for the training data set we are going
to use 70% of the customers uh that are
listed in the CSV file and 30% of the
customers in the CSV file data we will
be using it as a test data to train the
model so we'll copy the following code
into a new code cell and then we are
going to run that Cod
cell so I'll just copy it for training
the data so that we can segregate the
data model 70% for building the model
and 30% for
testing the data so click
on run the execution and here you can
see that we got the output
successfully now we have to train the
model from that data so how we are going
to train that model and for that we'll
use sagemaker pre-built XG boost model
which is an
algorithm so you will need to reformat
the header and First Column of the
training data and load the data from the
S3 bucket so what I'll do is I'll
copy this
syntax and paste it in the node
shell so it has the train data it would
train the model click on run execution
now it is changing the S3 input class
which will be renamed to training input
because now we are training the model
with the training data so we just have
to wait for some time till it gets
executed
completely now the next thing is that we
need to set up the amazon sagemaker
session to create an instance of the XG
boost model so here we are going to
create this sagemaker session say we are
going to create an instance of the XT
boost model which is an estimator so
just copy that copy that
code and paste it
here execute it and here you can see
that it will start it has uh basically
changed the parameter image name to the
image URI in this stagemaker python SDK
V2 now we'll follow the next step that
that is with the data loaded in the XT
poost estimator we'll set up train the
model using gradient
optimization and uh we'll copy the
following code and that would actually
start the training of the model so copy
this code and this would actually start
training the model using our input data
that we have reserved 70% of that data
that we have reserved for train the mod
so just copy
that again initiate the execution and it
will start the training
job now we'll deploy the model and for
that I would copy the deploy code put
that in the
cell and execute
it
so it says param image will be renamed
to image URI and using already existing
model so XG boost was deployed already
if you have not done that uh if you're
doing it a first time so it will
initiate another XT poost instance so
where you can find your XT poost
endpoints created you just have to
scroll down and here under the inference
uh click on the end points and you
should find the exg Boost end points
defined here so here you can see that
today I have uh created one boost uh
endpoint and that is uh now in process
of creating so just refresh
it so it is uh still created is going to
take some time to get that in service
now our endpoint is uh in service state
so now we can use it so going forward
with the next steps uh we'll try to
predict whether the customer in the test
data enroll for the bank product or not
for that we are going to copy this code
put that in the Jupiter cell function
function and execute
it so here it gives you the output that
it has actually evaluated and the same
output we got in the screenshot uh of
the demo as well now we are going to
evaluate the model performance so what
we are going to do we are going to get
the prediction done so based on the
prediction we can conclude that you
predicted a customer that will enroll
for a certificate of deposit accurately
for 90% of the customer customers in the
test data with AE Precision of 65% for
enrolled and 90% which are which haven't
enrolled for it so for that we are going
to copy this
code and execute it here in the
cell so if it is predicted correctly
that means our model is working
absolutely fine so here you can say the
overall classification rate is
89.5% and uh there is the
accurate prediction that has been made
by the model and that's what the output
we can see here in the screenshot of a
model so that means our model is
absolutely working fine it has been
built deployed and trained correctly now
the next thing is that once you are done
with that you terminate your resources
and for that you just have to
copy uh this code and put that in the
cell function so that the additional
resources and the end points and the
buckets that have been created by the
Jupiter notepad should be uh terminated
so that you would not be incurred with
the extra costing so just execute it and
here you would see that it is triy to it
would try to terminate all the
additional resources that we have
created from the Jupiter AWS let me
start the session with this scenario
let's imagine how life would have been
without Spotify for those who are
hearing about Spotify for the first time
Spotify is an online music service
offering and it offers instant access to
over 16 million licensed songs Spotify
now uses AWS Cloud to store the data and
share it with their customers but prior
to AWS they had some issues imagine
using spotify before AWS let's talk
about that back then users were often
getting errors because Spotify could not
keep up with the increased demand for
storage every new day and that led to
users getting upset and users cancelling
the subscription the problem Spotify was
facing at that time was their users were
present globally and were accessing it
from everywhere and uh they had
different latency in their applications
and Spotify had a demanding situation
where they need to frequently catalog
the songs released yesterday today and
in the future and this was changing
every new day and the songs coming in
rate was about 20,000 a day and back
then they could not keep up with this
requirement and needless to say they
were badly looking for way to solve this
problem and that's when they got
introduced to AWS and it was a perfect
fit and match for their problem AWS
offered a dynamically increasing storage
and that's what they needed AWS also
offered tools and techniques like
storage life cycle management and
trusted adviser to properly utilize the
resource so we always get the best out
of the resource used AWS address their
concerns about easily being able to
scale yes you can scale the aw
environment very easily how easily one
might ask it's just a few button clicks
and AWS solved spotify's problem let's
talk about how it can help you with your
organization's problem let's talk about
what is AWS first and then let's bleed
into how AWS became so successful and
the different types of services that AWS
provides and what's the future of cloud
and AWS in specific let's talk about
that and finally we'll talk about a use
case where you will see how e easy it is
to create a web application with AWS all
right let's talk about what is AWS AWS
or Amazon web services is a secure cloud
service platform it is also pay as you
go type billing model where there is no
upfront or Capital cost we'll talk about
how soon the service will be available
well the service will be available in a
matter of seconds with AWS you can also
do identity and access management that
is authenticating and authorizing a user
or a pro prog on the flight and almost
all the services are available on demand
and most of them are available
instantaneously and as we speak Amazon
offers 100 plus services and this list
is growing every new week now that would
make you wonder how AWS became so
successful of course it's their
customers let's talk about the list of
well-known companies that has their it
environment in AWS Adobe adob uses AWS
to provide multi- terabyte operating
environments for its customers by
integrating its system with AWS Cloud
adob can focus on deploying and
operating its own software instead of
trying to you know deploy and manage the
infrastructure Airbnb is another company
it's an Community Marketplace that
allows property owners and travelers to
connect each other for the purpose of
renting unique vacation spaces around
the world and the rbnb community users
activities are conducted on the website
and through iPhones and Android
applications arbn B has a huge
infrastructure in AWS and they're almost
using all the services in AWS and are
getting benefited from it another
example would be Autodesk Autodesk
develops software for engineering
designing and entertainment Industries
using services like Amazon RDS or
rational database servers and Amazon S3
or Amazon simple storage servers
Autodesk can focus on deploying or
developing its machine learning tools
instead of spending that time on
managing in the infrastructure AOL or
American online uses AWS and using AWS
they have been able to close data
centers and decommission about 14,000
in-house and collocated servers and move
Mission critical workload to the cloud
and extend its Global reach and save
millions of dollars on energy resources
Bit Defender is an internet security
software firm and their portfolio of
softwares include antivirus and
anti-spyware products Bit Defender uses
PC2 and they're currently running few
hundred instances that handle about 5
tby of data and they also use elastic
load balancer to load balance the
connection coming in to those instances
across availability zones and they
provide seamless Global delivery of
service because of that the BMW group it
uses AWS for its new connected Car
application that collects sensor data
from BMW 7 Series cards to give drivers
dynamically updated map information
canons offers Imaging products division
benefits from faster deployment times
lower cost and Global reach by using AWS
to deliver cloud-based services such as
mobile print the office Imaging products
division uses AWS such as Amazon S3 and
Amazon Route 53 Amazon cloudfront and
Amazon IM for their testing development
and Production Services Comcast it's the
world's largest cable company and the
leading provider of internet service in
the United States Comcast uses AWS in a
hybrid environment out of all the other
Cloud providers Comcast chose AWS for
its flexibility and scalable hybrid
infrastructure Docker is a company
that's helping redefine the way
developers build ship and run
applications this company focuses on
making use of containers for this
purpose and in AWS the service called
the Amazon ec2 container service is
helping them achieve it the esa or
European Space Agency although much of
esa's work is done by satellites some of
the programs data storage and Computing
infrastructure is built on Amazon web
services Esa chose AWS because of its
economical pay as youo system as well as
its quick startup time the Guardian
newspaper uses AWS and it uses a wide
range of AWS services including Amazon
Kinesis Amazon red shift that power an
analytic dashboard which editors use to
see how stories are trending in real
time Financial Times ft is one of the
world's largest leading business news
organization and they used Amazon red
shift to perform their analysis A Funny
Thing Happened Amazon red Shi performed
so quickly that some analysists thought
it was malfunctioning they were used to
running queries overnight and they found
that the results were indeed correct
just as much faster by using Amazon red
Shi FD is supporting the same business
functions with costs that are 80% lower
than what was before General Electric GE
is at the moment as we speak migrating
more than 9,000 workloads including 300
desperate Erp systems to AWS while
reducing its data center footprint from
34 to 4 over the next 3 years similarly
Howard Medical School HTC IMDb
McDonald's NASA Kelloggs and lot more
are using the services Amazon provides
and are getting benefited from it and
this huge success and customer portfolio
is just the tip of the iceberg and if we
think why so many adapt AWS and if we
let AWS answer that question this is
what AWS would say people are adapting
AWS because of the security and
durability of the data and endtoend
privacy and encryption of the data and
storage experience we can also rely on
aw's way of doing things by using the
aw's tools and techniques and suggested
best practices built upon the years of
experience it has gained flexibility
there is a greater flexibility in aw us
that allows us to select the OS language
and database easy to use swiftness in
deploying we can host our applications
quickly in AWS be it a new application
or migrating an existing application
into AWS scalability the application can
be easily scaled up or scaled down
depending on the user requirement cost
saving we only pay for the compute power
storage and other resources you use and
that to without any long-term
commitments now let's talk about the
different types of services that AWS
provides the services that we talk about
fall in any of the following categories
you see like you know compute storage
database Security customer engagement
desktop and streaming Mission learning
developers tools stuff like that and if
you do not see the service that you're
looking for it's probably is because AWS
is creating it as we speak now let's
look at some of them that are very
commonly used within Computer Services
we have Amazon ec2 Amazon elastic beant
stock Amazon light sale and Amazon
Lambda Amazon ec2 provides compute
capacity in the cloud now this capacity
is secure and it is resizable based on
the user's requirement now look at this
the requirement for the web traffic
keeps changing and behind the scenes in
the cloud ec2 can expand its environment
to three instances and during no load it
can shrink its environment to just one
resource elastic beanock it helps us to
scale and deploy web applications and
it's made with a number of programming
languages elastic beanock is also an
easy to use service for deploying and
scaling web applications and services
deployed be it in Java doet PHP nodejs
python Ruby doer and lot other familiar
services such as Apache passenger and
IIs we can simply upload our code and
elastic beanock automatically handles
the deployment from capacity
provisioning to load balancing to
autoscaling to application Health
monitoring an Amazon light sale is a
virtual private server which is is easy
to launch and easy to manage Amazon
light sale is the easiest way to get
started with AWS for developers who just
need a virtual private server lightell
includes everything you need to launch
your project quickly on a virtual
machine like SSD based storage a virtual
machine tools for data transfer DNS
management and a static IP and that too
for a very low and predictable price AWS
Lambda has taken Cloud Computing
Services to a whole new level it allows
us to pay only for the compute time know
no need for provisioning and managing
servers and AWS Lambda is a compute
service that lets us run code without
provisioning or managing service Lambda
executes your code only when needed and
scales automatically from few requests
per day to thousands per second you pay
only for the compute time you consume
there is no charge when your code is not
running let's look at some storage
services that Amazon provides like
Amazon S3 Amazon Glacier Amazon abs and
Amazon elastic file system Amazon S3 is
an object storage that can store and
retrive data from anywhere websites
mobile apps iot sensors and so on can
easily use Amazon S3 to store and
retrive data it's an object storage
built to store and retrive any amount of
data from anywhere with its features
like flexibility in managing data and
the durability it provides and the
security that it provides Amazon simple
storage service or S3 is a storage for
the internet and Glacier Glacier is a
cloud storage service that's used for
archiving data and long-term backups and
this Glacier is an secure durable and
extremely lowcost cloud storage service
for data archiving and long-term backups
Amazon EBS Amazon elastic Block store
provides Block store volumes for the
instances of ec2 and this elastic Block
store is highly available and a reliable
storage volume that can be attached to
any running instance that is in the same
availability Zone ABS volumes that are
attached to the EC intenses are exposed
as storage volumes that persistent
independently from the lifetime of the
instance an Amazon elastic file system
or EFS provides an elastic file storage
which can be used with AWS cloud service
and resources that are on premises an
Amazon elastic file system it's an
simple it's scalable it's an elastic
file storage for use with Amazon cloud
services and for on premises resources
it's easy to use and offers a simple
interface that allows you to create and
configure file systems quickly and
easily Amazon file syst system is built
to elastically scale on demand without
disturbing the application growing and
shrinking automatically as you add and
remove files your application have the
storage they need and when they need it
now let's talk about databases the two
major database flavors are Amazon Aus
and Amazon red shift Amazon auds it
really eases the process involved in
setting up operating and scaling a
rational database in the cloud Amazon
aios provides cost efficient and
resizable capacity while automating time
consuming administrative tasks such as
Hardware provisioning database setup
patching and backups it sort of frees us
from managing the hardware and sort of
helps us to focus on the application
it's also coste effective and resizable
and it's also optimized for memory
performance and input and output
operations not only that it also
automates most of the services like
taking backups you know monitoring stuff
like that it automates most of those
Services Amazon red shift Amazon red
shift is a data warehousing service that
enables users to analyze the data using
SQL and other business intelligent tools
Amazon red shift is an fast and fully
managed data warehouse that makes it
simple and cost effective analyze all
your data using standard SQL and your
existing business intelligent tools it
also allows you to run complex analytic
queries against terabyte or structured
data using sophisticated query
optimizations and most of the results
they generally come back in seconds all
right let's quickly talk about some more
services that AWS offers there are a lot
more services that AWS provides but
we're going to look at some more
services that are widely used AWS
application Discovery Services help
Enterprise customers plan migration
projects by gathering information about
their on-premises data centers in a
planning a data center migration can
involve thousands of workloads they are
often deeply interdependent server
utilization data and dependency mapping
or important early first step in
migration process and this AWS
application Discovery service collects
and presentence configuration usage and
behavior data from your servers to help
you better understand your workloads Rod
53 it's a network and content delivery
service it's an highly available and
scalable Cloud domain name system or DNS
service and Amazon Route 53 is fully
compliant with IPv6 as well elastic load
balancing it's also a network and
content delivery service elastic load
balancing automatically distributes
incoming application traffic across
multiple targets such as Amazon E2
instance containers and IP addresses it
can handle the varing load of your
application traffic in a single
availabity zones and also across
availability zones a is auto scaling it
monitors your application and
automatically adjusts the capacity to
maintain steady and predictable
performance at a lowest possible cost
using AWS Autos scaling it's easy to set
up application scaling for multiple
resources across multiple services in
minutes autoscaling can be applied to
web services and also for DB Services
AWS identity and access management it
enables you to manage access to AWS
services and resources securely using IM
you can create and manage AWS users and
groups and use permissions to allow and
deny their access to AWS resources and
moreover it's a free service now let's
talk about the future of AWS well let me
tell you something cloud is here to stay
here's what in store for AWS in the
future as years pass by we're going to
have variety of cloud applications Bond
like I
artificial intelligence business
intelligence serverless Computing and so
on cloud will also expand into other
markets like healthare banking space
automated cars and so on as I was
mentioning some time back lot or greater
Focus will be given to artificial
intelligence and eventually because of
the flexibility and advantage that cloud
provides we're going to see a lot of
companies moving into the cloud all
right let's now talk about how easy it
is to deploy an web application in the
cloud so the scenario here is that our
users like a product and we need to have
a mechanism to receive input from them
about their likes and dislikes and uh
you know give them the appropriate
product as per their need all right
though the setup and the environment it
sort of looks complicated we don't have
to worry because AWS has tools and
Technologies which can help us to
achieve it now we're going to use
services like Route 53 services like
Cloud watch ec2 S3 and lot more and all
these put together together are going to
give an application that's fully
functionable and an application that's
going to receive the information uh like
using the services like Route 53
cloudwatch ec2 and S3 we're going to
create an application and that's going
to meet our need so back to our original
requirement all I want is to deploy a
web application for a product that keeps
our users updated about the happenings
and the new comings in the market and to
fulfill this requirement here is all the
services we would need ec2 here is used
for provisioning the computational power
needed for this application and ec2 has
a vast variety of family and types that
we can pick from for the types of
workloads and also for the intense of
the workloads we're also going to use S3
for storage and S3 provides any
additional storage requirement for the
resources or any additional storage
requirement for the web applications and
we also going to use cloud watch for
monitoring the environment and Cloud
watch monitors the application and the
environment and it uh provides trigger
for scaling in and and scaling out the
infrastructure and we're also going to
use Route 53 for DNS and Route 53 helps
us to register the domain name for our
web application and with all the tools
and Technologies together all of them
put together we're going to make an
application a perfect application that
CS are need all right so I'm going to
use elastic beanock for this project and
the name of the application is going to
be as you see GSG signup and the
environment name is GSG signup
environment 1 let me also pick a name
let me see if this name is available yes
that's available that's the domain name
so let me pick that and the application
that I have is going to run on node.js
so let me pick that platform and launch
now as you see elastic beant stock this
is going to launch an instance it's
going to launch uh the monitoring setup
or the monitoring environment it's going
to create a load balancer as well and
it's going to take care of all the
security features needed for this
application
all right look at that I was able to go
to that URL which is what we gave and
it's now having an default page shown up
meaning all the dependencies for the
software is installed and it's just
waiting for me to upload the code or in
specific the page required so let's do
that
let me upload the code I already have
the code saved
here that's my code and that's going to
take some time all right it has done its
thing and now if I go to the same URL
look at that I'm being thrown an
advertisement page all right so if I
sign up with my name email and stuff
like that you know it's going to receive
the information and it's going to send
an email to the owner saying that
somebody had subscribed to your service
that's the default feature of this app
look at that email to the owner saying
that somebody had subscribed to your app
and this is their email address stuff
like that not only that it's also going
to create an entry in the database and
Dynamo DB is the service that this
application users to store data there's
my Dynamo DB and if I go to tables right
and go to items I'm going to see that a
user with name Samuel and email address
so and so has said okay or has shown
interest in the preview of my site or
product so this is where or this is how
I collect those information right and
some more things about the
infrastructure itself is it is running
behind an load balancer look at that it
had created a load balancer it had also
created an autoscaling group now that's
the feature of elastic load balancer
that we have chosen it has created an
Autos scaling group and now let's put
this URL you see this it's it's not a
fancy URL right it's an Amazon given URL
a dynamic URL so let's put this URL L
behind our DNS let's do that so go to
Services go to R
53 go to hosted Zone and there we can
find the DNS name right so that's a DNS
name all
right all right let's create an
entry and map that URL to our load
balancer right and create now
technically if I go to this UR L it
should take me to that application all
right look at that I went to my custom
URL and now that's pointed to my
application previously my application
was having a random URL and now it's
having a custom URL so now what is azure
what's the big cloud service provider
all about so Azure is a cloud computing
platform provided by Microsoft now it's
basically an online portal through which
you can access and manage resources and
services now resources and services are
nothing but you know you can store your
data and you can transform the data
using services that Microsoft provides
again all you need is the internet and
being able to connect to the Azo portal
then you get access to all of their
resources and their services in case you
want to know more about how it's
different from its rival which is AWS I
suggest you click on the top right
corner and watch the AWS versus Azo
video so that you can clearly tell how
both these cloud service providers are
different from each other now here are
some things that you need to know about
a year it was launched in February 1st
2010 which is significantly later than
when AWS was launched it's free to start
and has a pay per use model which means
like I said before you need to pay for
the services you use through aor and one
of the most important selling points is
that 80% of Fortune 500 companies use
Azure Services which means that most of
the bigger companies of the world
actually recommend using AZ and then
azer supports a wide variety of
programming languages the C nodejs Java
and so much more another very important
selling point of a your is the amount of
data centers it has across the world now
it's important for a cloud service
provider to have many data centers
around the world because it means that
they can provide their services to a
wider audience now Azor has 42 which is
more than any cloud service provider has
at the moment it expects to have 12 more
in a period of time which brings its
total number of regions it covers to 54
now let's talk about Azure Services now
Azure Services have 18 categories and
more than 200 services so we clearly
can't go through all of them it has
services that cover compute a machine
learning integration management tools
identity devops web and so much more
you're going to have a hard time trying
to find a domain that Azure doesn't
cover and if it doesn't cover it now you
can be certain they're working on it as
we speak so first let's start with the
compute Services first virtual machine
with this service what you're getting to
do is to create a virtual machine of
Linux or Windows operating system it's
easily configurable you can add RAM you
can decrease RAM you can add storage
remove it all of it is possible in a
matter of seconds now let's talk about
the second service cloud service now
with this you can create a application
within the cloud and all of the work
after you deploy it deploying the
application that is is taken care of by
aor which includes know provisioning the
application load balancing ensuring that
the application is in good health and
all of the other things are handled by
aor next up let's talk about service
fabric now with service fabric the
process of developing a micros service
is greatly simplified so you might be
wondering what exactly is a microservice
now a microservice is basically an
application that consists of smaller
applications coupled together next up
functions now with functions you can
create applications in any program
pramming language that you want another
very important part is that you don't
have to worry about any hardware
components you don't have to worry what
Ram you require or how much storage you
require all of that is taken care of by
Azure all you need is to provide the
code to Azure and it'll execute it and
you don't have to worry about anything
else now let's talk about some
networking Services first up we have
Azure CDN or the content delivery
Network now the AZ CDN service is
basically for delivering web content to
users now this content is of high
bandwidth and can be transferred or can
be delivered to any person across the
world now these are actually a network
of servers that are placed in strategic
positions across the world so that the
customers can obtain this data as fast
as possible next up we have expess now
with this you can actually connect your
on promise Network onto the Microsoft
cloud or any of the services that you
want through a private connection so the
only communication that happens is
between your on promise Network and the
service that you want then you have
virtual Network now with virtual Network
you can have any of the a Services
communicate with each other in a secure
manner in a private manner next we have
azur DNS so azur DNS is a hosting
service which allows you to host their
DNS or domain name system domains in
Azure so you can host your application
using Azure DNS now for the storage
Services first up we have dis storage
with dis storage you're given a cost
effective op option of choosing HDD or
solid state drives to go along with your
virtual machines based on your
requirements then you have blob storage
now this is actually optimized to ensure
that they can store massive amounts of
unstructured data which can include Text
data or even binary data next you have
file storage which is a managed file
storage and can be accessible via the
SMB protocol or the server message block
protocol and finally you have q storage
now with Q storage you can provide
durable message queuing for an extremely
large workload and the most important
part is that this can be accessed from
anywhere in the world now let's talk
about how aor can be used firstly for
application development it could be any
application mostly web applications then
you can test the application see how
well it works you can host the
application on the internet you can
create virtual machines like I mentioned
before with the service you can create
these virtual Machines of any size or
Ram that you want you can integrate and
Sync features you can collect and store
metrices for example how the data Works
how the current data is how you can
improve upon it all of that is possible
with these services and you have virtual
hard drives which is an extension of the
virtual machines where these services
are able to provide you a large amount
of storage where data can be stored
let's talk about Azure Services as I
told you Azia provides services for a
wide range of domains now let's have a
look at some of these domains there's Ai
and machine learning compute containers
database I identity management tools
networking Security Storage and so much
more now let's have a look at some of
the individual services within these
domains firstly you have AZ virtual
machines with Az virtual machines what
you get is the opportunity to create
Windows or Linux virtual machines now
all of this is possible in a matter of
seconds with a large amount of
customization now let's have a look at
some of its features firstly you can
choose from a wide variety of virtual
machine options then you have a large
amount of optimization available to you
for for example what size of operating
system do you want how much size do you
want allocated to it what version of the
system is it and so much more then it
provides low cost and permanent billing
now AZ provides you per minute billing
which means that you're only charged for
how much time you use the service and
finally you have enhanced security and
protection for your virtual machines
next we have service fabric now with
service fabric you have a platform which
enables you to create microservices now
this also makes the process of
application life cycle man management a
whole lot easier as a direct result you
can create applications with a faster
time to Market it supports Windows Linux
on promises or other clouds and it
enables you to do a tremendous amount of
scaling up depending on your requirement
and finally we have functions now with
functions you can build applications
with the help of serverless computing
here the users only pay for the amount
of resources that they've used you can
create applications in any language that
you want and the only thing you need to
worry about is the code of the
application everything other than that
that is the hardware requirements are
taken care of by Azure now let's have a
look at the networking Services firstly
we have the Azor CDN or the content
delivery network with Azor CDN what you
get is the ability to deliver your
content with reduced load times fast
responsiveness and less bandwidth now
CDN can be integrated with several other
Azure services so that the process can
move at a fast rate it can handle heavy
loads and traffic spikes with ease it it
also provides a robust security system
now with the content that's delivered
you can get Advanced analytic data with
which you can understand how customers
are using your content next we have
express route with express route you can
connect your on premisis network to aure
through a private Network Now by default
this lowest latency it increases the
emphasis on reliability and speed and it
can be of great use when you have to
transfer large amounts of data between
networks now another way this can be
used useful is if it's used to add
compute or storage capacity to Data
Centers next we have Azure DNS domain
name service or Azure DNS can be used to
host your domains on Azure this provides
High availability and great performance
it provides fast responses to DNS
queries by taking advantage of
Microsoft's Global Network it also
provides High availability next we have
virtual Network Azo virtual Network
allows the Azo resources to communicate
with each other or other on networks via
the Internet and all of this is kept
extremely secure now with this users can
create their own private Network for
communication it provides users with an
isolated and extremely secure
environment for their applications to
run now all of the traffic stays
entirely within the Azure Network and it
also allows users to design their own
networks next we have traffic manager
now with traffic manager you can route
incoming traffic to improve your
performance and availability now one
thing it provides is multiple failover
options so if a particular situation
goes wrong there's always an option to
consider to salvage the situation it
helps reduce applications runtime and
enables the distribution of user traffic
across multiple locations it also helps
the people who are using it to know
where the customers connecting from
across the world next we have load
balancer with this you have provided the
ability to instantly scale applications
at the same time providing High
availability and improved Network
performance for users applications it
can be integrated into virtual machines
and cloud services it provides highly
reliable applications it also allows
users to secure and integrate security
groups finally we have AO VPN Gateway
now this allows users to connect their
on promise networks to aure using a
sight tosite VPN now this allows users
to connect their virtual machine to
anywhere in the world through a point to
site VPN and also it's very easy to
manage and is highly available now let's
talk about about the storage Services
first we have data Lake storage now with
this what you get is a scalable data
storage with an emphasis on cost
Effectiveness and scalability now it
comes of Maximum use when you've
integrated with other services so that
you can get analytics on how the data is
being used it is also integrated with
other services like the Azo blob storage
now it is also optimized for Big Data
analysis tools like Pache spark and
Hadoop next up we have blob storage now
blob storage provides a storage capacity
for data now depending on how often a
particular data is used it is classified
into different tiers now all the data
that is within the blob storage is
unstructured data now it has a way of
ensuring that the data Integrity is
maintained every time a particular
object is being changed or the data is
being accessed and it also helps improve
app performance and reduces bandwidth
consumption next we have q storage now
with this you have a message queuing
system for large workloads this allows
users to build flexible applications and
separate functions not to mention with
this you can be sure that your
individual components will not fail it
also makes sure that your application is
scalable Q storage provides Q monitoring
which helps ensure that the customers
demands are met then we have file
storage now with file storage you can
perform file sharing with the help of
the SMB protocol or the server message
block protocol now this data is
protected by SMB 3 0 and the https
protocol in this CL like we mentioned in
functions Azo takes care of all the
hardware needs and the operating system
deployments on its own it also improves
on promises performance and other
capabilities lastly we have table
storage with table storage you can
deploy semi-structured data sets and
nosql key value store now this is used
for creating applications which have a
flexible data schema and also
considering how it has a very strong
consistency model it's mainly aimed for
Enterprises next let's have a look at
some web and mobile services first we
have the Azo search now with azo search
you get a cloud search service which is
powered by artificial intelligence with
this you can develop web applications as
well as mobile applications now one big
Advantage is that you don't have to set
up or manage your search indices as your
takes care of that and by extension it
increases your development speed the
artificial intelligence also will
provide insights and structured
information that you can use to improve
the search and structured information
next we have logic apps now with this
you can create integration Solutions
which can connect applications that are
important to your business now with this
you can visually create business
processes and workflows you can
integrate SAS or software as a service
applications and Enterprise applications
and more importantly it allows you to
unlock data within a firewall and
securely connect to services next we
have web application
now with web apps you can create deploy
and scale web applications according to
business requirements now it supports
both windows and Linux platforms and it
helps with continuous integration or
deployment abilities another very
important aspect of this is that the
data can be deployed and hosted across
multiple locations in the world and
finally we have mobile apps with mobile
apps you can create applications for iOS
Android and Windows platforms one
advantage is that it automatically
scales Up and Down based on your
requirements now in situations where you
have network issues offline data syncing
ensures that your applications work
anyway and you can create crossplatform
applications or native applications for
iOS Android and Windows next let's have
a look at some container services first
let's talk about ACS or Azo container
services it is also known as the AIA
kubernets Services as it's a fully
managed kubernets container
orchestration service now what does
means is that it eases the process of
container integration and deployment it
also can be used with other resources
from security like virtual networks
cryptographic keys and so much more to
ensure that your container is kept
secure next we have container instances
now this is similar to functions in a
way just that in this we're using
containers without having to manage
servers now applications can be
developed here without managing virtual
machines or learning new tools all that
is a your's problem to take care of and
it enables building applications without
having to manage the infrastructure that
is all you need to worry about is
running the container next let's have a
look at some database Services first we
have the SQL database now with SQL
database what you get is a relational
cloud databased service now this means
that it helps accelerate your app
development and makes it easier for you
to maintain your application now SQL
database is also used extensively in
migrating workloads to the cloud and
hence and saves time and cost it also
helps improve your performance by
integrating machine learning and
Adaptive Technologies into your database
next we have Azure Cosmos DB now this is
a globally distributed multimodel
database service now what this means is
that with this you can create
application with support nosql it
provides a high-grade security system
has high availability and low latency
now this is usually used in situations
where you have a diverse and highly
unpredictable workload now let's have a
look at some security and identity
Services firstly we have the Azure
active directory now if you want to know
more about Azor active directory I
suggest you click on the top right
corner and watch our video on the Azor
active directory this is just an
introduction so with this you can manage
user identities and you can make sure
the resources are kept safe with the
help of access policies most of these
are intelligence driven now one of the
main features is that you can have
access to your applications from any
location or device it helps increase
your efficiency and helps down cutting
cost when it comes to having a help desk
it can also help improve security and
can respond to Advanced threats in real
time next you have a your active
directory b2c it helps provide customer
identity and access management in the
cloud now protecting customer identity
is extremely important for an
organization and that's what a your ad
b2c does now it also enables the
application to to be scaled to great
amounts even billions of customers next
we have the Azure security Center this
is basically like a command post with
which you get a complete view of the
security across users on your own
premises and Cloud workloads so with
this you given threat protection method
that adapts to situations and helps
reduce exposing you to threats it also
has rapid threat response and makes the
process of finding and fixing
vulnerabilities a whole lot easier next
up let's talk about monitoring and
Management Services so first let's have
a look at Azure advisor now Azure
advisor is basically a guide for the
best practices when it comes to Azure
now when you follow these it improves
performance security cost and increases
availability now it also learns from how
you use the services on your
configuration and usage pattern and the
adjustments that it suggests can be
implemented very quickly and easily next
we have Network Watcher now with this
you can mon monitor diagnose and
understand the working of your network
now you can monitor your network without
actually having to log in to your
virtual machine now you can also use
something known as network security flow
logs to understand the traffic pattern
how much traffic is coming towards you
how much you're giving and so much more
it also helps diagnose VPN problems that
you might have with detail logs and
finally you have the Azo resource
manager now with this you can ensure
that the resources that you have are
managed and deployed at a consistent
rate now this makes it extremely easy
for you to manage and visualize your
resources that are used in your
applications or some other requirements
and you can control who can access your
resources as well as perform actions on
it now let's have a look at how we can
use one of these services to satisfy a
particular requirement so the friend
asks that she needs one last bit of help
she wants her friend's help to host a
website on a z and here's how we can do
it so now we're going to to put one of
these Services into action so this is
the Azure dashboard so from here we're
going to create a virtual machine within
which we are going to create a website a
very simple HTML website nonetheless so
let's get started so first what we're
going to do is create a resource a
resource Group so what a resource Group
is is actually a repository of all the
resources like virtual machines or
storage that you're going to use in your
project so in this we need a virtual
machine so let's get that created so
first we'll name
it
demo es us and we'll create it now this
will take a few
seconds and that's it so now that that's
done we go on
ADD so we're going to be using a Windows
operating system so we'll just search
for virtual
machine we select this and
create okay so here we going to set
setup we going to select the resource
Group the one just that we just created
a virtual machine
name
okay let's uh
storage
[Music]
name
passwords and then just Breeze through
the rest of it
uh we can reduce the size we need this
big and
create now that the validation is done
the next step is
to
create now this will take a few minutes
so we'll wait and as you can see here
the virtual machine has been created
we'll go to
Resource so here all we need to do is
connect and download the RDP file let's
see if we run this file and see what
happens now this error shows up in a
whole lot of systems that I've used so
there's a very easy way to troubleshoot
this problem so here's what you need to
do now so these are the problems as
remote access to the server is not
enabled the remote computer turned off
the remote computer is not available on
the network so what we're going to do is
go to networking
and add an inbound Port Rule now we're
trying to access a computer remotely
right so to remotely access the computer
we would need to add an inbound Port
rule so that's what we're going to do
right now so we can keep the source as
any port range as any and our
destination Port as 3389 if you've
noticed there we'd actually have used
the 3389 as the inbound port and then we
change the name as
3389 and this rule is ADD
now after this is done all we need to do
you can see here that we refresh the
page this would be
here a new rule has been added now so
we'll go to
overview and try to connect
again do the same thing
again and this time it worked so here
we'll input the name
and our
password and that's it now we just press
yes and there you go the system has
started now your virtual machine is
started now in a while the server
manager will pop up so there's some
things that we need to do so that we can
host the website so the first thing is
to create an inbound Port so we'll turn
off the virtual machine for now come
back to that later first we'll go to a
resource
Group check the name of our
resource and then we look at all its
resources here so here we need to find
the network security group and add an
inbound security rule press add we take
any Source any Source Port range and
then we add Port 80 it is the one that
we going to use and we'll put the name
as Port
80 We'll add the security rule it will
take a few
seconds and that's that now it's allowed
now let's go back to our virtual
machine here you go
and let's get it
started log in
again and there you have it it's on
again now we'll go to server manager so
we need to add roles and features so
most of this we can just run through
we'll leave them at the default values
same and here we need to add a web
server which is the IIs so we'll select
this we add the features next so we can
add the net 3.5 features as well
next
and we'll install
now now we'll wait for the installation
to
finish and now the installation is done
so now what we going to see is if the
website can be reached we'll go to the
public address which is given here so
we've come back to our original system
click and you can see that the is Page
has been created now we need to work
more to create the HTML page here's what
we'll need to do to run our website so
right now what we getting is this the I
page now let's make it our own HTML page
a very simple HTML page from there on we
can create the website so we go back to
the machine and then we go to C drive so
the is is located in a location which is
in C drive in it Pub and www root so if
you click on this actually what you'll
get is the page I showed you earlier
which is this one so now we'll go to the
server manager click on
tools and find the IIs
manager so we click on this
and sites now there's a default
website we can now add a new
one your site
name
demo we need a physical path we'll put
it inside the folder we just saw which
is right here C drive init Pub ww route
within which we'll create a new folder
for simply learn
demo we save it and Port 80 and okay now
this is assigned to another site so
we'll change that but we'll wait for
that
okay now for this we'll just change it
we click on this so since both of them
have the same port we'll change that go
here edit
binding we change this
to something
else okay and close so now both are
different and now we can start our
website it's already started
now now we'll start the website right
click this so manage website and
start and that's it now we will go to
the to the inner Pub ww root and we have
simply learn demo so here we will create
the HTML website or the HTML the
document so index.
HTML
okay open
this it's a very simple website now so
we keep it save
it and so here what what we're going to
do is save
as an HTML file but for
all all
files we close this now to make sure
this is recognized as a HTML file it go
to
view
options here we'll hide extensions for
known file
types we just rename this
[Music]
and there you have it it's a HTML file
now let's go back to our original
computer and onun this page and there
you have it now this is just the initial
version more work has to be done on it
but your first step is already done with
the greatest debate of the century today
I'm joined by two giants of the cloud
computing industry they'll be going
head-to-head with each other to decide
who amongst them is better it's going to
be one hell of fight now let's meet our
candidates on my left left we have AWS
who's voiced by apea hi guys and on my
right we have Microsoft AO who's voiced
by Angeli hey there so today we'll be
deciding who's better on the basis of
their origin and the features they
provide their performance in the present
day and comparing them on the basis of
pricing market share and options free
tier and instance configuration now
let's listen to their opening statements
let's start with AWS launched in 2006
AWS is one of the most common use cloud
computing platforms across the world
companies like Adobe Netflix Airbnb HTC
Pinterest and Spotify have put their
faith in AWS for their proper
functioning it also dominates the cloud
computing domain with almost 40% of the
entire market share so far nobody's even
gotten close to beating that number AWS
also provides a wide range of services
that covers a great number of domains
domains like compute networking storage
migration and so much more now let's see
what Azar has to say about that Azure
was launched in 2010 and is trusted by
almost 80% of all Fortune 500 companies
the best of the best companies in the
world choose to work only with aure aure
also provides its services to more
regions than any other cloud service
provider in the world aure covers 42
regions already and 12 more are being
planned to be made AA also provides more
than 100 servic spanning a variety of
domains now that the opening statements
are done let's have a look at the
current market status of each of our
competitors this is the performance
route here we have the stats for the
market share of AWS Azo and other cloud
service providers this is for the early
2018 period Amazon web services takes up
a whopping 40% of the market share
closely followed by aor at 30% and other
cloud services adding 30% this 40%
indicates most organizations clear
interest in using AWS we are number one
because of our years of experience and
Trust we've created among our users sure
you're the market leader but we are not
very far behind let me remind you more
than 80% of the Fortune 500 companies
trust Azure with their cloud computing
needs so it's only a matter of time
before Azure takes the lead the rest of
the 30% that is in AWS or aure accounts
to the other cloud service providers
like Google Cloud platform Rackspace IBM
soft layer and so on now for our next
round the comparison round first we'll
be comparing pricing we'll be looking at
the cost of a very basic instance which
is a virtual machine of two virtual CPUs
and 8 GBS of RAM for AWS this will cost
you approximately
0.928 per hour and for the same instance
in a it'll cost you approximately 0.096
us per hour next up let's compare market
share and options as I mentioned before
8 ews is the Undisputed market leader
when it comes to the cloud computing
domain taking up 40% of the market share
by 2020 AWS is also expected to produce
twice its current Revenue which comes
close to $44 billion not to mention AWS
is constantly expanding its already
strong roster of more than 100 services
to fulfill the shifting business
requirements of organizations all that
is great really good for you but the
research company Gardner has released a
magic quadrant that you have to see you
see the competition is now neck to neck
between Azure and AWS it's only a matter
of time before Azure can increase from
its 30% market share and surpass AWS
this becomes more likely considering how
all companies are migrating from AWS to
Azure to help satisfy their business
needs Azure is not far behind AWS when
this comes to Services as well azure's
service offerings are constantly updated
and improved on to help users satisfy
their cloud computing requirements now
let's compare AWS and azor's free
offerings AWS provides a significant
number of services for free helping
users get hands-on experience with the
platform products and services the free
tier Services fall under two categories
services that will remain free forever
and the others that are valid only for
one year the always free category offers
more than 20 services for example Amazon
s s sqs cloudwatch Etc and the valid for
a year category offers approximately 20
services for example Amazon S3 ec2
elastic cache Etc both types of services
have limits on the usage for example
storage number of requests compute time
Etc but users are only charged for using
services that fall under the valid for a
year category after a year of their
usage a sh provides a free tier as well
it also provides services that that
belong to the categories of free for a
year and always free there are about 25
plus always free services provided by
Azure these include app service
functions container service active
directory and lots more and as of the
valid for a year there are eight
services offered there's Linux or
Windows Virtual machines blob storage
SQL database and few more Azure also
provides the users with credits of
$200 to access all their services for 30
days now this is a unique feature that
Azure provides where it users can use
their credits to utilize any service of
a choice for the entire month now let's
compare instance configuration the
largest instance that AWS offers is that
of a warping 256 GBS of RAM and 16
virtual CPUs the largest that Azure
offers isn't very far behind either 224
GBS of RAM and 16 virtual CPUs and now
for the final round now each of our
contestants will be shown facts and they
have to give explanations for these
facts we call it the rapid fire round
first we have features in which AWS is
good and Azure is better AWS does not
cut down on the features it offers its
users however it requires slightly more
Management on the user's part Azure go
slightly deeper with the services that
fall under certain categories like
platform as a service and infrastructure
as a service next we have hybrid Cloud
where AWS is good good and Azure is
better okay although AWS did not
emphasize on hybrid Cloud earlier they
are focusing more on technology now
Azure has always emphasized on hybrid
cloud and has features supporting it
since the days of its inception for
developers AWS is better and Azure is
good of course it's better because AWS
supports integration with thirdparty
applications well Azure provides access
to data centers that provide a scalable
architecture for pricing both AWS and
Azor are at the same level it's good for
AWS because it provides a competitive
and constantly decreasing pricing model
and in the case of aure it provides
offers that are constantly experimented
upon to provide its users with the best
experience and that's it our contestants
have finished giving their statements
now let's see who won surprisingly
nobody each cloud computing platform has
its own pros and cons choosing the right
one is based entirely on your
organization ation requirements let's
understand why Google Cloud platform so
Google Cloud platform is popular for
many reasons now let us see few of most
important reasons why it stands out when
you talk about pricing pricing is one of
the significant factors that make Google
Cloud Stand Out Among the other Cloud
providers it offers a monthly pricing
plan which is build according to monthly
usage and when we talk about billing
here the in can be in hours it can be in
minutes and it can also be in seconds
there are different options when you
talk about your pricing which can be
found from your Google Cloud web page
now pricing could be based on preemptive
machines pricing could be based on
reserved instances or reserved resources
I'll show you the link where you can
find more details on pricing part of it
so Google Cloud really has various
pricing options which help customers in
their different requirements whether
they would go for any of the service
models such as infrastructure as a
service platform as a service or even
software as a service one more
attractive thing about Google Cloud
pricing is that it provides committed
use discounts for example under this
scheme you can purchase a specific
amount of virtual CPU course and memory
for up to 57% Discount off regular
prices if you commit usage for either
one or three years now this is just one
option there are various such options
which you can learn about from the
Google Cloud's page and that really
suits different customers for their
different
requirements now when we talk about
speed we don't need to really challenge
this aspect when it comes to Google
services so Google provides its Google
cloud and Google apps customer speed up
to 10 terab because of its faster cable
system there are different kind of
machines which can be used if you're
talking about computation if you're
talking about memory hungry applications
or even storage intensive workloads all
in all speed is one of the defining
characteristics of Google cloud services
the cable has connections over us West
Coast main city in Japan and even major
hubs in Asia this speed enhances
performances and leads to customer
satisfaction now when we talk about
customers any or every customer would
prefer to have low latency High
throughput Based Services they would
want to use higher speeds to process
their data in as less time as possible
Google provides a low latency Network
infrastructure in fact you can say that
when you are are using Google cloud
services you are using the services from
the same infrastructure which Google
uses for its popular services such as
Google search or even YouTube which is
one of the second largest repository
which can be accessed for videos now
when we talk about Big Data big data is
data which is very complex has lot of
other characteristics such as your
volume you have velocity you have
variety you have veracity validity
volatility virality and so on so if an
organization has is working on Big Data
Google Cloud can be a better choice
because Google has many Innovative tools
for cloud warehousing for example such
as big query and even realtime data
processing tools such as data flow big
query is a data warehouse that allows
massive processing of data at high
speeds basically working on your
structured data Google also has launched
some new machine learning from
artificial intelligence tools now there
are various other services which we can
which we can use from Google Cloud
platform but let's understand what is
Google Cloud platform and what are some
of the services even the services which
are not listed here can be found in your
console from Google Cloud so what is
Google Cloud platform gcp it is a set of
Cloud Computing Services provided by
Google that runs on the same
infrastructure as I mentioned that
Google uses for its end user products
like YouTube Gmail and even Google
search the various set of services
offered by Google Cloud platform are so
you have Services which are specific to
Computing requirements and again in
Computing you have various different
options available you have machines
which are compute optimized machines
which are memory optimized machines
which are storage optimized and also we
have certain machines such as preff
which basically means that you could get
a machine to work on at a far lesser
price than any other machine but when we
talk about preemptive these are the
machines which can be requested on
demand and they can be taken back by
Google at any time you have networking
related Services which can be very
useful when you are setting up your
applications or your services across
Globe you also have different Services
which are specific to machine learning
and organizations which would be
interested in working on machine
learning or artificial intelligence
would be really interested in using
these Services there are also innovative
solutions to work on big data and Big
Data related Technologies now when we
talk about Google Cloud platform domains
so we can break down these Services into
specifics such as you have compute now
the compute service allows for computing
and hosting the cloud now when you talk
about Computing here there are different
services such as app engine you have
compute engine you have C in it is you
have Cloud functions and Cloud run when
you talk about storage and database so
the storage and database service allows
application to store media files backups
or other fil like objects now various
Services Under This are as follows you
have cloud storage Cloud SQL Cloud big
table for unstructured data you have
Cloud spanner cloud data store
persistent Diss and Cloud memory store
when we talk about networking the
networking service allows us to load
balance traffic across resources now as
I mentioned earlier resources could be
your different resources which you would
be using from a cloud platform such as
your devices your instances memory
optimized or CPU optimized instances or
other resources creating DNS records and
much more so various Services Under This
are VPC that stands for virtual private
Cloud you have Cloud load balancing
Cloud armor Cloud CDN you have Cloud
interconnect DNS and network service TS
when we talk about the big data service
this allows us to process and query big
data in Cloud now various Services under
these are as follows you have big query
cloud cloud data proc Cloud composer
cloud data lab cloud data prep Cloud Pub
sub which is published in subscribing
system you have cloud data studio now
you also have the developer tools and
developer Tools service includes tools
related to development of an application
now various Services under these are as
follows that is you have Cloud SDK
software development kit you have
deployment manager Cloud Source
repositories and Cloud test lab when we
talk about identity and security which
is one of the primary concerns for any
organization or any user who would be
interested in using a cloud platform
Google Cloud really has taken care of
this so when you talk about identity and
security domain this deals with Security
Services now various Services here are
Cloud identity you have identity and
access management that is cloud am you
have identity aware proxies you have
cloud data loss prevention API security
key enforcement Key Management Service
and many more you also have cloud
services which are related to internet
of things so very much would be used by
organizations who would be working on
iot devices or the data generated by
these devices so various Services here
are Cloud iot Core you have Edge TPU and
Cloud iot also when we talk about Cloud
AI that is artificial intelligence this
comprises of services related to machine
learning and much more so you have Cloud
autom ml Cloud TPU Cloud machine
learning engine job Discovery dialog
flow Enterprise natural language Cloud
text to speech and much more if you
would be interested in Services related
to API platform then there are different
Services Under This category or this
domain so you have Maps platform you
have APG API platform
monetization developer portal analytics
that is API analytics APG sense Cloud
endpoints and service infrastructure so
these are some of the listing of
services under each Cloud platform
domain now let's look at Ferrero use
case and let's also understand what was
done here so ferero is one of the famous
chocolate and ranks third am among
worldwide chocolate and confectionary
producers it was found in 1946 in Italy
I'm sure you would have seen the ferero
chocolates when you would have gone out
to buy some chocolates so the challenges
here was that ferero as we know is sold
in every supermarket and is known for
its quality once the busers grew some
issues Arrow right and that's what
happens when the business grows you have
issues popping up which could be related
to the volume of data the speed with
which the data is getting generated the
variety of data and also looking at your
platforms which support different
Dynamic applications or your scalability
requirements performance requirements
and so on so it needed data storage
processing and Analysis system for a
wide customer
database there was a huge gap between
the company and the people who bought
its Goods because the company relied on
data given sales outlets now that's one
of the challenge Ferrero wanted to
create a digital ecosystem where there
was a point of contact with its
customers and also a foundation for an
Innovative datadriven marketing strategy
what was the solution here so one of the
service of cloud platforms or Google
Cloud platform is Big query and this was
an answer to fero's challenges since it
was capable of hyper fast and efficient
data analysis as a solution now as I
mentioned big query is a data warehouse
which allows you to store structured
data now this could directly be used as
a service where you could store in any
amount of data and you would not be
paying for storage so there are
different pricing models and for data up
to 1 terabytes you would not be charged
in anything and if you would be
accessing the data that is reading the
data or processing the data from
bigquery that's where the pricing model
kicks in now using Google Cloud's big
query business analysts of ferero were
able to store and analyze massive data
sets in a very reliable fast and
affordable
manner consumer behavior and sales
pattern data reports were easy to build
and automate
and the analysis also followed ferero to
adopt advertising across various
marketing channels to serve the customer
needs in a better way what was the
result they could divide their database
into realtime actionable consumer
clusters to generate more accurate user
profiles fero was also able to
personalize its marketing strategies to
match the user needs now Google Cloud
platform completely tailored the website
mobile content and advertising and
created a very costeffective media by
strategy now these were some Basics on
Google Cloud now as I said you can
always find lot of details about pricing
about services and also all the services
are accessible in your free trial
account now let me just walk you through
here so one is you could always find
details on documentation on each of
these services so if I would click on
getting started you have quick start
which basically shows you short
tutorials you have trainings and you
have also certifications now if we click
on quick start now that takes me to this
page which shows me quick starts or if
you would want to understand about
different projects if you would want to
look into the documentation of creating
a virtual machine or storing a file and
sharing it deploying a container Docker
container image and so on so you have a
lot of quick starts here you can also
look at Cloud
minute on the same page on the right
side you have docs and once you click on
that it takes you to these links now
this shows you build Solutions which
shows you top use cases best practices
all the solutions it's always good to
learn from these use cases which are
available and here you have different
feature products and all the different
Services which Google Cloud offers now
you can click on featured products and
that basically shows you a list of these
products now here we have all the
solutions which you can look at
architecture database Enterprise level
big data and Analysis gaming related
internet of things and so on now if you
go down here it shows you featured
products and that shows you some of the
important products such as compute
engine which is belonging to the compute
domain you have Cloud run you have
anthers which is for
migration and basically Cloud adoption
when organizations would want to move
from on premise to cloud-based Solutions
you have Vision AI you have cloud
storage now that basically allows you to
store any kind of data whether that's an
object so this is acting as an object
storage you have cloudsql which is
basically a ready to use service where
you would be using MySQL post or any
other SQL Server database Services you
have big query which is a data warehouse
which basically allows you to store your
structured data and then you have your
AI and machine learning related products
so you have automl Vision AI video AI
text to speech spe to text and so on now
you also have different platform
accelerators which can be used and in
any of these cases for example if I
would click on compute engine which is a
featured product from Google Cloud now
that shows you basically your quick
starts using your Linux machines how to
guides which tells you completely
working on a VM instance or working on
storage working on persistent Diss and
so on and it shows you the documentation
here now you also have products and
price
option which you can see for GCB pricing
and you could straight away go to the
pricing you could be looking at
Solutions which talks about
infrastructure modernization now this is
something which organizations are
interested in when they would want to
move from their on premise solution to
your Google Cloud now here I can say for
example I click on infrastructure
modernization you can always find some
case studies what are the different
solutions you we have here right when
you talk about Google Cloud so you can
always click on C Solutions and you can
look at VM migration or sap Cloud right
why Google Cloud what it can be used for
VMware as a service or HBC that is high
performance Computing and about these
Services we will learn in detail later
now I can go back on the same page where
I was clicking on the services so you
have quick starts you have how to guide
guides you have deep understanding of
different concepts right and here if I
click on all how to guides so that shows
me what are the different ways in which
you can work with compute engine and
working with different instances
although it's quite exhaustive content
but then if you follow steadily then you
can learn a lot about Google Cloud now
here I can just go back so this is just
giving you some idea on the different
products which are available from Google
Cloud looking into different sections
finding right documentation here right
and you can always look into each one of
these in detail for each product which
Google Cloud
offers now if we scroll down we could
see all the options or all the domains
which we see here right let me just go
back because we got into Solutions now
we have quick starts right
and then you can basically scroll all
the way down to look at Cloud SDK which
can be set up on your Windows machine
like I have set it up on my Windows
machine plus when you use Google console
you have a GUI which I'll show you in a
couple of minutes and also a cloud shell
where you can use your command line
options to work with Google Cloud
platform you have Cloud console which is
nothing but your GUI to access your
resources now you can also look at
in-depth tutorials pricing and you have
different other options so let's just
click on pricing here and then we have
your price list which basically gives
you details of different Services what
are available and what are the services
or what are the prices so for example if
I click on computer engine right and
this shows me the pricing aspect of
compute engine which belongs to compute
domain
and here you can see you have VM
instance pricing you have networking
pricing so tenant noes which are
specific to particular organizations or
if organizations would want to have
dedicated nodes you have GPU based
pricing right so General processing
units you have disk and image pricing
and you can click on any of the links
and you can see pricing which also shows
you your different kind of machine types
so here you have different kind of
machine types which says N1 N2 n2d E2
you have memory optimized machine types
you have compute optimized you have
premium images and you can basically
look at all the categories you can look
at disk pricing which involves your
persistent disk pricing for ssds or sdds
what are the kind of images what are the
different network Services right and you
can always look at your machine types
you can choose a particular region right
there is the concept of reg region and
availability zones here and as per
region you could look at the prices you
can also look at standard prices you
could be looking for what are the free
tier machines if I'm specifically
looking for VM instance pricing I can
click on this one and that takes me to
the VM instance pricing what is the
building module what is the instance up
time what is the resource based pricing
and then you can also look at different
kind of discounts such as sustained use
discounts committed use discounts
discounts for preemptable FM instances
and so on and this is how you can be
looking at pricing for all different
resources and you can choose the
resource which you are interested in and
then look for the pricing
benchmarks reservations if you would
want to use and you would want to
benefit if you would want to look at
what are the quotas and limits and so on
so please explore this link and you can
find lot of information when it comes to
technical options looking at different
Google Cloud products which we briefly
discussed when it comes to domains and
what each product can be used for you
can always come back to the main page
where I was showing you different
Services we went into pricing straight
away right now you can always come back
to this page on your cloud.google.com
and then you have your
Solutions which talks about different
products right and you can click on
these or you can look into technical
documentation so this talks about your
different featured Solutions
infrastructure Solutions you have data
center migration related and so on so I
could be talking more and more about
Google Cloud platform it's a ocean it's
a huge chunk of different services for
different organizational requirements so
look into this link and also what you
can do is create a account on Gmail I
mean you could create a free account on
or you could go to cloud.google.com
and then you could create a free account
like I have created here and I can just
click on Console now that's my GUI or
Google Cloud console which basically
allows me to work with Google Cloud now
there are lot of options here by default
when you create an account now in my
case on the top it says it's a free
trial account I have $300 credit and out
of that $251 is left and $237 days out
of a year are left for my account now
here I can basically see the project
right and by default we can create an
project or by default there is a project
existing for any user so when you log in
it creates a particular project and you
could create a new project and project
would be to dedicate your different
services or different resources per
different project so this is your
dashboard which basically shows your
project which shows you a project number
and a project ID which is always unique
now you can click on this and this if
you would want you can hide this
information you can also look into the
documentation part of it then it shows
you the different services or a
graphical information of services which
you might have used in past so you have
compute engine which shows you how much
percentage of CPU was used you can
always go to compute engine as a service
you can look at your Google Cloud
platform status and look at the Google
Cloud status now this is billing now
since this shows me the billing period
for April month and I can always look at
detail charges I can look at reporting I
can look at different apis which Google
offers and for your different kind of
work you can always go to the API
overview or the API link and enable or
disable any API now once you enable or
disable any AP
then you can use that so you have news
section you have documentation and you
have getting started guides which tells
you how to work or enable different apis
if you would want to deploy a pre-build
solution at Dynamic logging monitoring
errors deploying a Hello World app and
so on now this is your dashboard I can
click on activity and that would show me
what kind of activity I would have done
in my cloud platform I can always choose
the kind of activities by saying
activity type I can choose the resources
now it shows me that I created some VM
instances I deleted them I updated some
metadata I basically worked on instances
I changed some firewall rules here I
have then also worked on some other
services or API so I created some
buckets which is for object storage and
again I have been working with some
instances and creating some firewall
rules here I have granted some
permissions I am setting up a policy
right so this activity gives me a
history of what things I have done over
the past couple of months while working
on Google Cloud platform now on the top
left corner you have the hamburger menu
which you can click on this so that's
your navigation menu and when you click
on this this shows you home it takes you
to the marketplace it specifically takes
you to billing you can always look at
apis and services so here in API
Services I can pretty much go to a
dashboard and I can see a report on the
traffic or errors or latency and the
kind of apis which are available so you
have compute engine API you have big
query API big query data transfer API
big query storage cloud data proc so
these are some of the apis or Services
I've us used in past might be I was
evaluating a product might be I was
using a particular product so you have
whenever you would want to use a
particular service from Google Cloud
platform you would be enabling these
apis so here we see some apis for data
proc you have logging monitoring you
have resource manager API you have
cloudsql which allows you to directly
use MySQL or postgress you have cloud
storage right and so many apis so if I
would want to enable a particular API
which is not right now required but then
it's good to know you can always click
on this one you can search for an API so
for example if I would say data proc
right and that shows me the cloud data
proc API which manages Hadoop based
clusters and jobs on Google Cloud
platform so I can spin up a Hadoop
cluster and I can start running some
jobs on a Hado cluster on demand and as
I'm done with it I can just get rid of
it so this is an API which I would have
to enable and then for every particular
service you also have an admin API which
you would enable now going back so
coming out from this API Library I got
into API and services now there is you
can look into the library you can look
at credentials and you can also look at
different time intervals for which you
can see the usage of your different apis
now going back to the menu you have
support wherein you can always reach out
to the Google support team if you are
using a paid version even with free
trial you can try to reach out and you
will find someone to help but it is
always good when you have a billing
cycle reaching out the customer care you
have identity access management in admin
and this is required when you're working
with different apis or Services when you
would want to have relevant access you
have getting started you have security
related options you have anthos which is
mainly for migration now here you can
look at your different domains such as
which we discussed so you have compute
and in compute you have different
services so you have app engine you have
compute engine kubernetes you have Cloud
functions and Cloud run you can look
into the storage aspect which shows you
big table which is usually and mainly
for your unstructured data big table
which is something which gave rise to
popular nosql databases like hbas and
cassendra you have data store which can
be used you have fire store file store
storage which can be used to put in data
of any kinds you have SQL for structured
data you have spanner as a service you
have memory store and you have data
transfer now when it comes to the
networking domain you have all these
services such as VPC virtual private
network network related service services
for your load balancing for using a
cloud-based DNS or Cloud defined Network
you have hybrid connectivity network
service tis security and intelligence
then you have other options for your
operations which can be used and here
you have different other tools which can
be used such as Cloud build you have
Cloud tasks container registry
deployment manager and many more big
data specific you have the services here
so you have data proc which can be used
to spin up your clusters you have
published subscribe messaging systems
such as now Kafka which you might have
heard of originates its idea from here
so Pub sub you have data flow you have
iot core big query which is a data
warehouse package or data warehousing
solution from Google Cloud now then you
have your artificial intelligence
related services and other Google
Solutions so you can always find a huge
list of Serv serves or you can say at
high level Solutions offered by Google
cloud and we can use these to basically
test some of the solutions use them and
work on them so I'll give you a quick
demo on different Services which can be
used here from your Google Cloud
platform now this is your Google Cloud
platform and this is your console now
this also gives you a cloud shell which
can be activated so that you can work
from command command line and you can
always find lot of documentation on that
so you can also set up your Cloud shell
that is SDK on your Windows machine and
if you have set it up then basically you
could be doing something like g-cloud
right if the cloud has been set up so in
my case I had set up the cloud SDK and
basically I can get into that by looking
at what path I have set up so Cloud SDK
and then I can be using it from my
Windows machine as per my comfort I can
also activate the cloud shell here which
will basically open up a terminal and at
any point of time you can open up this
one Cloud shell in a different window it
is preparing the cloud shell where it
will by default set up your project it
will set up the metadata and now I am
logged into my Google Cloud account from
command line and here I can basically
use g-cloud right and that basically
shows you the different options which
are available which you can be using so
if you would want to use a particular
service you can also try giving a help
and that shows you how do you manage
your Google Cloud platform right so you
have different options for billing to
work with your different services and
basically you could be working on Google
Cloud using this Cloud shell from your
command line usually for people who are
learning Google cloud in the beginning
it is always good to go for console and
use your different services from here in
an easier way but as I said you can
always use the command line for example
if I would just go ahead and type
gcloud create
instances and that will directly take me
to the cloud console documentation which
shows you different options which you
can use to work with instances so I
could do a g-cloud compute instances
create and then I can give my instance
name and so on and I'll show you some
examples on that so you can be using
your Cloud console right that is your
Cloud shell and you can straight away
start working from command line however
I would suggest using console in the
beginning and when you are well
experienced then you can start using
Cloud shell to do things from command
line and when you are fully experienced
you can always switch and certain things
are usually useful or easier when done
from command line and some some are
easier when you do it from the console
and you can use any one of these options
now we can go to Google Cloud console
now as of now I can close this one I
still have my cloud shell open if I
would want to look into it I can click
on this one and I can straight away go
to compute engine where I would want to
work on creating some instances on
Google Cloud platform using the compute
engine service and then basically
connecting to those instances
and basically trying out some basic
things so you can click on VM instances
and then once this comes up I can always
create some instances now if you see
here I have some instances already
created right and I can continue working
on those I can create new instances I
can use different options while creating
instances and I'll show you that in a
demo in quick seconds so let's have a
quick demo on setting up gcp inst es now
before that let's have a quick recap so
when you talk about instance or a
virtual machine it is hosted on Google's
infrastructure right and you can create
your Google Cloud instance using this
Google Cloud console that is by clicking
on this and then going to compute engine
and clicking on VM instances now you can
also do that from Google Cloud command
line tool that is cloud shell and you
can be doing that using compute engine
API so compute engine instances can run
the public images for Linux or Windows
servers that Google provides you also
have option of creating or using custom
images that you can create an import
from your existing systems you can
deploy Docker containers which are
automatically launched on instance
running containers optimized OS now when
you talk about instances and projects or
always remember that in each instance
belongs to a Google Cloud console
project and a project can have one or
more instances when you talk about
instances and storage options each
instance has a small boot persistent
disk which I will show you in further
screens that contains the OS you can add
more storage space if needed and when
you talk about instances and network a
project can have up to five VPC networks
so VPC network is virtual private
networks wherein you can virtual private
Cloud networks where you can have your
resources within your own subnet and
each instance belongs to one VPC Network
now instances in same network can
communicate with each other through
local area network protocol an instance
uses internet to communicate within any
machine so that could be virtual
physical or outside its own network when
you talk about instances and containers
you should remember that compute engine
instances support a declarative method
for launching your application using
containers now you can create a VM
instance or a VM instance template you
can provide a Docker image and launch
the configuration so there are different
ways in which you can create these
instances and once you create these
instances you you can say for example
you're creating a Linux instance you can
associate SSH keys with your Google
account or your G Suite account and then
manage your admin non-admin access to
the instance using IM roles if you
connect to your instance using g-cloud
or SSH from console which we will see
later compute engine can generate SSH
keys for you and apply that to your
Google cloud or G Suite account now what
we can do is we can see how we can
create your Google Cloud instances from
your console or from the command line
let's have a look in creating instances
using GCB console now that we have
learned on some basics of Google Cloud
platform what are the different Services
what are different domains basically
looking at your Google Cloud console or
even Cloud Shell let's go ahead and
create some VM instances now that's from
your compute engine service and let's
try connecting to these instances and
see how this works let's also see what
are the different options which are
available when you would want to create
the virtual machine instances here so
when you click on your drop down from
the top left and choose compute engine
so it brings you to this page so I can
show that again so you can click on
compute engine click on VM instances and
that basically brings you to this this
page now here it tells you compute
engine lets you use Virtual machines
that run on Google's infrastructure so
we can create micro VMS or large
instances running different
distributions of Linux or Windows using
standard images that is public images
and you can also have your own image so
let's create a VM instance by clicking
on create now that's basically helping
me to create an instance here so it
shows me the instance name I can give it
something so let me say C1 now I can add
labels so what is label it basically
allows you to organize your project add
arbitrary labels as key value pairs to
your resources this so this is basically
categorizing your labels in projects if
you have multiple projects now remember
if you have your Cloud console and if
you have created your free trial account
it will allow you to do these things if
not you may have to go back to the
billing section and see if the billing
is enabled which also means that when
you are creating your Google Cloud
account it will ask you to Keen your
credit card details but they do not
charge anything or they might charge
might be $1 or one rupee depending on
your location and that's also refunded
but that's just to verify your card now
once I've given the name I can choose a
region so I would basically choose
Europe since I'm in Europe I would be
clicking on Europe West 3 and here I can
choose choose an availability Zone
availability zone is basically to make
your services or instances or any other
resources highly available so you can
choose one of the availability zones now
I would choose save S 3 now this one we
can scroll down and here it says machine
configurations you have general purpose
machines you have memory optimized
machines which is M1 series and you can
always go back to the Google Cloud page
and see what a particular kind of
machine specializes in so I would click
on generals purpose and in general
purpose you have different categories so
you have N1 which is powered by Intel
Skylake CPU platform or you have E2
which is CPU platform selected selection
based on availability so let's have N1
selected now this one shows me the
machine type and if you are using your
free tier account then you can start
with correcting a micro machine which is
one virtual CPU code you can go for a G1
small which is one virtual CPU code and
1.7 GB Ram you can even go up to a
high-end machine and then you can
basically see if you're using a free
account how many of these machines you
can use if I would be selecting eight
virtual CPU CES and 32 gab of memory
then it would allow me to create at
least two instances by this
configuration we will use N1 standard
which is one virtual CPU core 3.75g V
memory now then we can also deploy a
container image to this VM instance if
you would be interested in deploying a
container image let's not get into that
right now here it shows you the boot dis
and it shows you dbn gnu 9 Linux 9 what
I would do is I can go for this
distribution or I can choose a Linux
distribution of my choice so here you
have public images you have custom
images you also have snapshots if you
have created backup of your previous
images here I can choose for example
ubben 2 and then I can choose a
particular version so let's go for to
18.04 you can go for the latest one also
18.04 would be good enough and here it
tells me what is a boot dis so you have
ssds or you have standard persistent
discs now ssds are little expensive in
comparison to your standard persistent
discs or your sdds but then SSD are
faster so as of now we can choose
standard persistent disk as it is and we
can let the gigabyte be 10 now depending
on your requirement you can increase
this you can even add diss later that's
not a problem click on select now here I
have access Scopes so here I will say
allow default access you can also set
access for each API or you can give full
access to all Cloud apis so so that
based on your requirement you can
anytime change later also we will also
say allow sttp traffic and we can also
choose allow https traffic that
basically allows me to access this
machine or Services which are HTTP based
accessible from this machine now I can
just click on create however it would be
good to basically enable connectivity to
this machine now we can do that in
different ways one is when you bring up
your machine it will have an SSH access
which you can log in from the cloud
platform here itself or what you can do
is you can create a private and a public
key using some softwares like puty or
puty gen so for example if you do not
have that on your machine you can
download you can just type in download
puty that takes you to the p.org page
and here you can click on download puty
and scroll down which shows you for your
64-bit machine which I have in my case
you can download putty.exe which is
basically your SSH andet client to
connect to your machines you can also
use puty gen which will be allowing you
to create a private and a public key
which I have done in my case let me show
you how so what you can do is you can go
to puty gen to begin with and here you
can click on generate now then to have
this key generated just move your cursor
on the top here in the empty space and
that creat creates your key you can give
it a name so for example let's give a
username I would give hdu now I can give
a password for this one so let me give a
simple
password and what I can also do is I can
copy this public key from here and for
my later usage I can just keep it in my
notepad file which I can use later and
I'll show you when so now we can save
this private key and this will basically
allow me to save my private key I can
choose desktop and I can give it a name
so let's say new key new key and that
will be getting saved in a PPK file so
let's save it and that's done so we also
have our public key and we have saved
our private key now we know that when
you would want to connect using SSH you
need your private keys to the client and
public key also has to be existing so
let's take this public key key so let's
do a contr a I'm going to copy this and
I'm going to come back to my Google
Cloud console and here you can click on
security so you can click on the
security tab here scroll down and just
give you your public key once you give
that it resolves and shows you the name
and this is good enough so that I can
use my SSH client to connect to this
machine I can click on Create and this
will basically create my VM instance it
will take some time and then your
instance will have an internal IP which
will show up here external IP and it
will also show you options to connect to
these machines so this is my internal IP
this is my external IP which I can use
to connect from a client I can easily
connect from the option here which says
SSH and I can say open in a browser
window I can even open this in a custom
Port I can look at the g-cloud command
which you can give from cloud shell or
you can use another SSH so let's first
do a open in browser window and let's
see if this connects so we can easily
connect to our instance the UB 218
instance which I just set up here easily
in couple of seconds now this is trying
to establish a connection using your SSH
keys and when you do that it also
basically brings up this web browser so
I'm already connected it shows my
username which is my CL cloud account
username and I have been connected to
this machine here using SSH it did not
ask me for any password and basically
now I can just check what do I have in
my Linux file system right and I can
anytime login as root by doing a Pudo Su
for example let's try installing a
package and I can say app get install
whm or appt get install WG get or apt
get install open SSH and all these
packages are already existing so not an
issue now I can start using this
instance I can just look at the disk
what is available okay now we gave
around 10 GB out of which we see 8.3 GB
here for the dev SDA 1 and then 1.8 GB
available and you can continue using
this machine this was the easiest way of
connecting to a VM instance using
SSH now what we can also do is I can
just leave this and I will now try to
connect using an external SSH client
right and here you can copy the public
IP so when you want to connect to an
instance you will have to get the public
IP also remember that if you select and
stop this machine which will stop your
billing counter and if you start it
again the internal IP will remain same
but the external IP is the one which
will change I can obviously select this
machine Any time and I can do a cleanup
and I can delete it I can do a start of
the machine if it is stopped and I can
even import the virtual machine to be
used later so there are different
options which you can always use so this
is my instance and if you would want to
look at the details click on this one C1
and that should basically allow you to
look at the details so it shows you what
is the instance ID what is the machine
type is it reservation specific what is
the CPU platform what's the zone and all
the other details at any point of time
if you would want to edit you can always
click on edit and you can change the
details as you need now you can also
look at the equivalent rest command to
basically use the rest API to connect to
this instance for now I have copied the
public IP and I would want to connect it
using puty so let's go in here and let's
give it the host name so I can give
ubben
I can give my IP address so that's in my
session now I will click on SSH I would
go into authentication I'll click on
browse and this is where I need to
choose the PPK file so this is the one
which we created new key let's select
this and then I can come back to session
I can even save this and I can call it
as my instance one let's save it and you
can create any number of instances so
you see I have created different
instances here for my Google cloud or
Amazon related instances and I can click
on open it says the service host key is
not cast in the registry that's fine
just click on yes and basically it says
no authentication method supported now
this could be because we have not
enabled your SSH access so let's look at
that so now let's see if we were trying
to connect using puty what was the issue
here so if I I go back to my puty select
my instance load it it says Port 22 I'm
giving the username which is window or
sorry that's the wrong username we gave
and that might be the reason we had set
up the user as sdu so let's save this
again and now let's try connecting to
this one and it asks for my password and
you are able to connect this right now
if there was any other issue related to
network connection AC ity then we could
look at the rules the inbound and
outbound rules which allow us to look
into the machine now we are connected to
our machine here using hdu user I can
log in as root and I can continue
working so not only from the SSH within
the cloud console but you can use an
external SSH client and connect to your
machine so this is your uin to machine
and we can basically look at the space
and that basically confirms we are
connecting to to the same machine which
shows 8.3 GB here and 1.8 GB here which
we were seeing from the ssh in the
browser now let's close this and let's
go back to our instance page now here
you can always look at the network
details and this will show you your
different kind of rules that is ingress
or egress rules which basically allows
to connect to this machine from an
external network or for this machine to
connect to an external network so here
we have have different firewall rules
which shows default allow HTTP that's
your Ingress Rule and it tells you apply
to all it shows me what are the IP
ranges where I can specifically give the
IP of my machine it shows the protocols
it shows what are the different ports
which you have used for these services
for example RDP or SSH which shows 22
you have icmp HTTP and https now anytime
if you would want to make a change to
the these rules that can be done by
going into your network details and say
for example you would want to work on
firewall rules so we have these firewall
rules here you can click on this one so
right now we are looking into the
network domain and we are looking into
VPC Network right and this shows me what
are the different rules we have now if I
would want to create a different
firewall rule for a different protocol I
can always click on on create firewall
rule I can give it a name okay I can say
what if you would want to turn on the
firewall logs you can basically say what
is the kind of traffic so Ingress
applies to incoming traffic and egress
applies to outgoing traffic and you can
then basically choose what are the IP
ranges from where you want that
connection to be coming in or to going
out you can choose a particular protocol
you can give a protocol here with with
comma separated values and you can
create a firewall rule so this might be
required depending on the services which
you are running wherein you might want
to enable your access to your machine
from an external service or to an
external service so you can always go to
network details from here you can then
go into firewall rules create a firewall
rule apply that to your instance and
restart your instance so as of now we
don't need to create any any network
details here because my Ingress or
egress rules are already available right
now I can basically then have my
instance stopped and removed so I can
just do a stop or since this is running
the ideal wave would be to do a stop I
could also do a reset now what does
reset do reset does not basically delete
the machine what it does is it does a
cleanup of the machine and it brings it
to the initial state so sometimes we
might have installed certain things on
our machine we would want to clean them
up and at that time reset can be useful
I can basically click on delete right
and I can select this and I can do a
cleanup and this is good always when
you're are using a free trial account
try to use different Services play
around with them and then you can clean
up so that you do not waste your free
billing credit right and you can use it
for Meaningful stuff now I have clicked
on delete and within few few seconds my
instance which I had created will be
deleted also to remember is if you are
creating multiple instances then you can
connect from one machine to other
machine using SSH by using the private
file so for that we can learn in detail
later so this is just a simple example
of using your compute engine creating
your instances connecting to it from an
internal SSH or from an external SSH
client such as puty where you have
already created your private and public
Keys now I can click on the browser here
and then I can basically come out and I
can basically be looking into any
particular service so we just looked
into compute engine right you have
different other options you have
instance groups you have instance
templates for example let's click on
instance templates here and that
basically shows you that you don't have
any instance template and this basically
facilitates say for example you are
working as a admin and you would want to
create an instance template so that
using that you can describe a VM
instance and then you can basically use
this template to create different
instances you can go for soul tenant
notes you can look for machine images
you can also look at your TKS you can
create a snapshots and you can look at
different options here now let's come
back here and let's click on home and
that should take you back to your your
homepage which basically shows you if a
particular API which you have used
recently which shows me in the graph
here so I can go to the API overview
right and that basically shows me if
there were any errors if I was using the
compute engine API to basically create a
VM instance and that's why we were
seeing some spike in the graph so this
is a quick demo of using your compute
engine service provided by Google Cloud
wherein you can create VM instances and
use those VM instances for your
application installation for any other
purpose now that we have seen how you
use your Cloud console to create an
instance and also clean it up let's also
understand how you can do using your
command line options and let's see what
it takes or what are the different
commands which you can use to create
your instances now you can always when
when you're creating an instance you can
use compute engine which Provisions
resources to start the instance so
instance basically has different states
which we can see when we are creating
the instance so you basically start the
instance instance moves into staging
that is prepared for your first boot
finally it boots up and then it moves
into running so when you look at
instance states which we will create
create instance and C it will basically
have different states such as
provisioning where resources are being
allocated for instance but instance is
not yet running then it goes into
staging where resources have been
acquired and instance is being prepared
for your first boot then instance is
booting up and running and if you are
stopping an instance it goes into being
stop status and it would be moved to the
terminated option you can also do a
repairing of instance and finally you
can terminate your instance or clean it
up by stopping and then deleting it now
when you say stopping and resetting an
instance you can stop the instance as I
showed earlier if you no longer need it
but if you need for future use you can
just use the reset option which will
basically wipe the contents of instance
or any application State and then
finally you can stop it and have it
terminated now when you would want to do
that using your Cloud console uh we have
seen the options now let's also see from
the command line tool how you can do it
so here I have the cloud shell which I
brought up from here and I basically
opened it in a new window so command
line tool enables you to easily manage
your compute engine resources in a
friendlier format than using your
compute engine API now gcloud which is
part of cloud SDK is the main command
here and then you can always
autocomplete the different options here
so when you would want to create or work
on g-cloud you can just type in gcloud
here and then for example I could just
say help to see different options of
gcloud which will show me different
options which you can use here now we
would be interested in compute instances
so I could also do a g Cloud
compute compute instances and then I can
basically say create and then I can do a
help now that should show me different
options which work with g-cloud compute
instances create command which is
expecting an instance name so your
Google Cloud SDK which we can set up on
our Windows machine or even on your
Linux machine is set of tools that helps
you to manage resources and applications
hosted on your gcp that is your Google
Cloud platform now here you have options
such as gcloud which I'm showing you
right now you have Gs util and then you
have BQ so that also can be used so you
can set up a Google Cloud compute if
using gcloud now what we can do is if we
are setting up our SDK on our Windows
machine then we would have to do a
gcloud CL in it which basically
initializes your configurations for
gcloud now here we are using a cloud
shell which was started from the console
and we don't have to basically give your
g-cloud init command because it's
already initialized now you can always
look at your default Zone you can look
at your region what is being used all
those things are coming from the
metadata which is being used so for
example I could be looking at the
metadata for my particular project by
just doing a
gcloud and then I can say
compute and I would be interested in
Project info so I can just do a project
minus info and then I can do a describe
and then I need my project ID so I can
say minus minus project and I can get my
project ID from here so you can click on
this one and that's my project ID so I
can click on this one and here I can
just do a right click and if it does not
paste then you can do a control V and
then I can try to look at my project
info so this basically will give me the
metadata which is by default set and we
can always look at what are the regions
or what is the Zone which has been set
so if it has been set so I'm looking at
my details it is showing me my SSH keys
and then it can be using your default
region and your default available zones
it also shows my username and other
details so this is basically to look at
the metadata which is available now at
any point of time I can basically say
add metadata I can basically choose
metadata option and I can say my default
region should be Europe which I was
choosing earlier so I can just bring up
this command again and what I can do is
I can say here where I have gcloud
compute project info where I did a
describe earlier now what I can also do
is I can just say add
metadata and then I can specify what is
the metadata I would be interested in
adding I can then say Google
compute default
region and then I can basically give a
region for example Europe and then I can
say West
three and if you are not well
experienced you can always do it from
console or you can be doing it from here
so I'm giving Google Cloud compute
Google compute default region then I can
also give a Google compute default
Zone and then I can pass in a value for
default zone so I can say Europe West 3
and then basically I can give an
availability zone so if it is basically
giving me some error where I'm trying to
pass in these values so I can just give
it this way and then it basically says
that if you can look into particular
help to see what is the command which
you would have to do now I can basically
do a
gcloud in it here initial configuration
and this basically says that it is
initializing my default configuration it
says reinitialize this configuration
from cloud shell you want to create a
new configuration so if I had updated my
metadata then basically I could just do
a cloud in it and I could be
reinitializing my default configuration
or properties which I have passed in so
as of now I will not activate or change
the default region let's go for a simple
way in which you can create your compute
engine so for example I'll just say one
and it is reinitializing it asks me what
is the username and let's select that it
says what is the project and let's
select that and do you want to configure
a default compute region and zone right
and I will just say Yes And basically
then it shows me different options which
we have here so there are too many
options here and then here we were
interested in Europe West 3 so let's
choose 21 right and then that basically
allows me to choose my region and my
zone so it also gives you some
specifications so it says your project
default compute Zone has been set to
Europe West 3A you can change it by
running g-cloud config set and you can
give a compute Zone right so I can
always give these commands I can get
help information here so I can just say
gcloud config set and I can be
specifying the compute zone so I can say
compute /zone and I can give a Zone I
can say compute SL region and I can give
a region name if I would want to do it
or the easier ways like what I did right
now so you can basically do a command
g-cloud in it and then basically it
allows you to change your configuration
or set default things here you can all
the times you can say gcloud config
unset to basically remove a compute zone
or a compute region now there are
different ways so if you were working on
a Linux machine you could always use an
export command something like this so
you could do export compute sorry Cloud
SDK and and then you can say compute
uncore Zone and then give your Zone name
or compute _ region and give your region
or you could add it in your bash RC file
right now that is when you have your
Cloud SDK set up on your Linux machine
or on Windows machine and you would want
to specifically set a Zone and a region
for all your Compu related resources so
we don't need to do that the default
settings are already given here right
and what we can do is we can start by
quickly looking into g-cloud compute
instances options so I can say g-cloud
compute
instances and then I can just do a list
at any point of time if you would need
help you can just do a g-cloud compute
and then say minus minus help or I could
just do a g-cloud compute and that shows
me different options which I have here
from which I use instances I can always
type instances and again hit enter and
it shows me different options what you
would want to do so I can initially just
type list which should show me what are
the list or what are the instances
available and as of now we don't have
any instances I can always do a list and
then I can specify minus minus format
and then try to get the information in a
Json format or EML or minus minus form
format text so you can always do a list
you can do a filter so there are
different options with your list and you
can you can try doing a help here and
that basically shows you what are the
options so for example if I do a help
and it shows me with list what are the
things you can do so you can give a name
you can give a regular expression you
can say in particular Zone you can use a
minus filter so there are different
commands which are available and you can
always find all those options here as I
showed you earlier so now what we would
be interested in is basically going for
your compute instances I can always do a
SSH and I can create an instance I can
add and remove metadata right for my
instances by giving a particular Zone by
giving a particular region and if you
would want to do that now here what we
get do is we can basically create an
instance by just giving a name to that
particular instance and we can then go
back to our console and see what has it
done so I can say here create so that's
an option let's see what does the create
do so it says okay you are giving a
create option but you would have to give
a name so let's say let's call it E1 and
that will be the name of my instance and
this one is a an easy command from your
Cloud shell which basically creates an
instance you see the Zone which has been
set it is the standard machine type it
is not preemptible it has an internal IP
and it has an external IP now I have
created an instance and here I can just
go back and then I can just do a Refresh
on this page and that already shows me
the instance which I have created and
then you can use the same method to
connect to it using an SSH so I have an
instance created from my cloud shell and
what I can do is I can basically look
into instances and see what are the
different options with instance so if we
did a create right you have an option
delete we did a create you have an
option delete you can be then deleting
the instance from the command line or
you can use other options so you can do
a list you can do a stop you can start
so I can basically do a stop and let's
say even let's go ahead and do a stop
commands are pretty easy here to
remember or you can always use the help
option and now I'm trying to stop the
instance and then basically I can go
ahead and delete it so this is a simple
way where I created a instance from the
command line or from the console which I
showed earlier and similarly you can be
working with other options
so now I've created stop the instance
let's do a listing to see if my instance
shows up it shows up right and it says
the status is terminated so you have
stopped it it's in the terminated status
and now what I can do is I can go ahead
and delete
it so it says this will be lost are you
sure you would want to continue just say
yes and that should take care of
deleting your instance so this is a
quick demo on creating your instance we
have already seen how you can connect to
these instances using SSH now that we
have created instances let's also see
how to use Google Cloud's storage
service which can be used to load data
or upload data so for this we will have
to look into your Google cloud storage
options and here you can look in cloud
storage so let's go back here on the top
which shows me different Services which
we have here and let's look into storage
so this one shows me your storage option
now you can click on browser and that
basically shows me your storage browser
which shows me on the top you have
options for creating a bucket now Google
Cloud Storage allows you to store any
kind of data here the easiest and the
simplest way would be by using Cloud
console although you can again use cloud
shell and in that you can use your GS
util command and GS util basically has
different options where you can be using
say for example I would want to work on
buckets so I can be using MB and then I
can use my command line option to create
buckets I can put in data there I can
browse it and I can access the data from
command line so let's create a bucket
here let's click on this let's give it a
name so let's say my data important
that's the name now I can click on
continue straight away I can look into
all of these options if I'm interested
in so you can basically be looking at
your monthly cost estimate in the
beginning now I can click on continue or
you can say choose where to store your
data and this one shows you different
options so you have region specific
so let's also give the bucket name with
all lowercase that's what is required
yeah so coming back to the location type
you can choose multi- region which
basically allows High availability so
your bucket or your storage option will
be accessible across regions you can
also give dual region and high
availability and low latency across two
regions I can also say region specific
so for our use case we can just give
region specific
which can keep our cost low but in
business use cases you would be going
for multi- region now here you have
location and I would again go for say
Europe and let's choose Europe west3
Frankfurt it is always a good practice
that when you create your instances when
you create your storage or you use
different Services try to have a
geographical region Chosen and then you
would try to put things or your service
within the particular region within a
particular Zone unless you would want to
make it accessible and available across
regions now I can then choose a default
storage class so when you say default
storage class there are different
storage class and each one is for a
different use case so you have standard
which is best for short-term storage and
frequently accessed data you have near
line so this is basically best for
backups and data access less than once a
month you can also have have cold line
so these are basically like your cold
storage or freezing storage which you
might have heard generically as terms so
you can choose one of these storage
classes depending on what will be the
use case for this particular storage
bucket so let it be standard now how to
control access to objects so you can
basically say specify access to
individual objects by using object level
permissions now you can give permissions
to the bucket you can just say uniform
access to all objects in the bucket by
using only Bucket Level permissions so
you can choose that you can also go into
advanced settings and here you can see
different ways in which you can have
configuration set up now you can also
have a retention policy to specify the
minimum duration where that this buckets
object must be protected from deletion
and you can set a retention policy we
will not get into all that we will just
first try creating a bucket so just
click on Create and that should create
your bucket wherein I have my bucket now
I can click on overview to see the
details what is the region which region
it belongs what is the storage class
anytime you can always click on edit
bucket and make some changes here you
can look at the permissions so bucket
uses fine Grand access allowing you to
specify access to individual objects and
then you can basically look at who who
has access to this so in my case editors
of the project they basically are the
bucket owners owners of project viewers
of the project right and you can
basically choose what kind of access you
need so for this you can always go into
cloud storage and then you can decide
what kind of access you would want to
give whether that's a storage admin it's
an object admin object Creator object
viewer and so on you can always look at
storage storage Legacy and for any other
services depending on what apis you have
enabled right you can always control
your om missions so right now it is
storage Legacy bucket reader and that's
fine and this one is bucket owner and
then might be I was using some other
services like data proc which uses cloud
storage and that's why I've given data
proc service agent also so these are
some of the members you can remove you
can view by specific members you can
view by roles so what are the different
roles which have access to this so it
says storage Legacy bucket owner there
are two owners based on this particular
project so these are autocont controlled
but then you can add or remove machines
you can save storage cost by adding a
life cycle rule to delete objects after
the duration of current retention policy
so you can add different policies and
you can basically control your bucket
now my bucket is already created right
so I can go back and then I see my
bucket is already here I can click on
this option and you can anytime edit the
bucket permissions you can edit the
labels the default storage class you can
just go ahead and delete the bucket you
can export it to Cloud pup sub so
basically if you would want to have the
content from this bucket being
accessible in a message queuing system
you can go for Pub sub you can process
with Cloud functions and you can scan
with cloud data loss prevention so there
are different options which are
available we can always select this
bucket and delete it now I can click on
the bucket and that basically shows me
different ways in which I can upload
some data here so I can just click on
upload files and here then I can choose
some files so for example I'll go in
here I'll go into data sets and I have
different data sets so for example let's
choose this one which is a CSV file and
then I'm just upload loing this to my
cloud storage right it's as simple as
this so you can drag and drop your files
or you can just upload your files and my
file is uploaded here now I can
basically edit permissions for this one
right so this is in the bucket so anyone
who has access to this bucket can
basically download this file it can they
can copy move or rename this you can
export it and you can look at the
permissions of this file so let's look
at edit permissions and it says for this
project whoever is the owner it has
access I've also given a specific user
my Gmail ID and I have given access so
at any point of time you can just say
add item and then you can start giving
different kind of accesses so let's
click on cancel here now my data is
already uploaded into this particular
bucket and that's a simple usage of your
cloud storage now what we can also do is
I can basically select this and I can
delete it I can also create specific
folders and then basically uh load data
into it I can click on this one I can
just do a download and then I can
download it anywhere on my machine so
let's go to desktop and let's download
this file so we not only uploaded some
content in the bucket but we also
downloaded that in a different location
which will be accessible now what I
could have also done is I could have
created a folder here and I could have
specified say for example immediate
immediate
data okay and I'm clicking on this one
so that's my immediate data I can click
on this one and now I can upload my data
specifically to this particular folder
by using the same mechanism you can just
do upload file let's choose air
passengers let's do a open open and my
file will be
uploaded so you can always choose what
is the retention expiry date for this
one so as of now there is nothing right
but you can be deleted objects can be
deleted or modified until they reach
their minimum duration you can control
all of that you can basically download
it right you can copy it and you can
move it and rename it so this is a
simple example where I created a
particular bucket right now if for
example you click on this transfer so it
says cloud storage data transfer
products have moved you can now find
storage transfer service on premise data
and transfer Appliance in new data
transfer section right so you can always
go back to cloud storage you can also
look at transfer options which are
mainly when you are using a on premise
service and you would want to upload
data in a Google Cloud right so this is
my bucket here now I can go to Cloud
console and here what I can do is if I
would be interested in working so I can
just say GS util right and then we have
uh for example let's try a help okay and
that basically shows me my GS util now
you can always go to Quick Start GS util
tool and this is one more way wherein
basically you can do it from the command
line so working with buckets so you can
do a GS util MB minus B and then on a
particular Zone if you would be
interested in and then you can say GS
colon and give a name so my awesome
bucket this is how you will create a
bucket which will show you it is
creating a bucket and then basically you
can upload an image or some data from
internet you can just do a wget download
the file and then you can just do a
gsutil copy command and that basically
will pick up the file from your Cloud
shell location put it in your bucket
once you have done that you can always
do a copy right so here you're doing a
copy from your local machine that is
your Cloud shell machine to your bucket
and you can do the reverse of that that
is you can do a copy and you can point
your bucket and the file and download it
to your desktops so you can just
download that using the command line and
you can copy the object to a folder in
the bucket so that is by creating a
particular folder and you pushing the
file into a particular bucket you can
list the contents of a bucket using LS
and then give your bucket name so GS
colon so we can just try this one to be
simple and rest you guys can try so here
you can just do a GS util you can do a
LS GS colon SL slash and then I need to
give my bucket name so in my case the
bucket name was my data important so
let's try that so let's say my data
important and I'm trying to list the
bucket and the content it has right and
then you have a folder and then you can
look into the folder and look into the
files so this is a simple quick option
wherein you can use your Cloud shell you
could be using your Google Cloud SDK
from your local machine or you can be
using Cloud console to create a bucket
upload some data on it download the data
see if the data is accessible and that
basically shows you the power of cloud
storage where you can easily upload any
kind of data now what we can also do is
as you're using a free account or even a
paid account unless and until you would
want to keep the bucket you can select
it and you can just go ahead and delete
it so this will basically ask you to ke
the bucket name and let's say my data
important you need to confirm your
bucket name click on confirm and you
could have done that using GS util and a
delete command from the command line
that is from cloud shell so we have now
created a bucket we have uploaded some
data into it we saw how you can download
it or create a folder and upload some
specific data into it and also you can
do the same thing using GS util tools
wherein you can list your bucket create
your buckets delete it create some
folders into it and do everything from
the command line so this is in simple
way you can use your gcp where either
you can spin up your machines or you can
use cloud storage to basically use a
storage or a easy to use instance on
Google Cloud platform in this video we
will compare and contrast AWS Azure and
gcp based on a few related Concepts
around these cloud computing platforms
it will help us understand the
functioning of these top Cloud platforms
and will also let us figure out the
individuality of each one of them but
before starting with the comparison
let's have a quick introduction of AWS
versus Azure versus gcp so let's get
started Amazon web services or AWS is a
cloud computing platform that manages
and maintains hardware and
infrastructure reducing the expense and
complexity of of purchasing and running
resources on site for businesses and
individuals these resources are
available for free or for a fee per
usage Microsoft Azure is a cloud
computing service that offers a
collection of Cloud Computing Services
for building testing deploying and
managing applications in the cloud
including remotely hosted and managed
versions of Microsoft
Technology Google Cloud platform offers
a variety of Cloud Computing Services
for building deploying scaling
monitoring and operating a cloud the
services are identical to those that
power Google products such as Google
search Gmail YouTube and Google Drive
now let's move on to the comparison
between AWS Azure and
gcp we will be comparing them based on a
few major parameters like origin
service
integration availability
Zone Cloud tools like
compute
storage
networking market
share
pricing and at last who uses
them now let's move ahead and start with
the first comparison
origin in the year 2006 Amazon web
services or AWS was introduced to the
market and in the year 2010 Azure
launched its services whereas on the
other hand gcp was established in the
year
2008 from the start AWS has been
supportive of the open source concept
but the open- source Community has a
tense relationship with
Azure on the other hand gcp similar to
AWS provides Google cloud with managed
open-source services that are tightly
linked AWS offers services on a large
and complex scale that could be
manipulated but as your support is
comparatively low quality whereas gcps
monthly support price is almost $150 for
the silver class which is the most basic
of services and is quite
expensive now let's move on to the
service integration of these Cloud
platforms service integration is a set
of tools and Tech technology that
connects different applications systems
repositories and data and process
interchange in real time AWS makes it
simple for users to combine services
such as Amazon ec2 Amazon S3 beanock and
others and on the other hand Azure
allows customers to effortlessly combine
Azure VMS as your app service SQL
databases and other services whereas
users can utilize gcp to combine
services such as compute engine cloud
storage and cloudsql now that we know
briefly about all these Cloud platforms
let's have a look at the availability
zones of these platforms because AWS was
the first in the cloud domain they have
had more time to build and extend their
network but Azure and gcp both have
various locations around the world but
the distinction is in the amount of
availability zones they have AWS now
offers 66 availability zones with an
additional 12 on the pipeline close to
it Azure is available in 140 countries
and is available in 54 regions
throughout the world but Google Cloud
platform is now available in 20 Global
areas with three more on the
way now let's move on to the next
important factor which is tools now
let's move ahead head and have a look at
the first feature which is
compute elastic compute cloud or ec2 is
aws's compute service which offers a
wide range of features including a large
number of instances support for both
windows and Linux high performance
Computing and more Azure on the other
hand as virtual machines is Microsoft
azure's core cloud-based compute
solution it includes Linux windows
server and other operating systems as
well as better security and Microsoft
program
integration in comparison to its
competitors Google's Computing Services
catalog is somewhat smaller compute
engine that company's principal service
offers custom and predefined machine
types per second invoicing Linux and
Windows support and carbon neutral
infrastructure that uses half the energy
of traditional data
centers within the compute category
Amazon's different container services
are gaining prominence it has Docker
kubernetes and it's also fargate service
which automates server and cluster
management when using containers as well
as other
Alternatives Azure unlike AWS uses
virtual machine scale sets of two
container services Azure container
services is based on kubernetes and
container service uses Docker Hub and
azzure container registry for management
for Enterprises interested in deploying
containers Google offers the kubernetes
engine and it's also worth noting that
Google was significantly involved in the
kubernetes project providing an
extensive knowledge in this
field now let's move on to the next
parameter of comparison which is
storage simple storage service for
object storage elastic block storage for
persistent block storage and elastic
file system for file storage are among
aws's storage
offerings block storage for rest based
object storage of unstructured data Q
storage for a large volume workload file
storage and disk storage are among
Microsoft Azure core storage Services
gcp offers an increasing number of
storage options its unified object
storage service cloud storage also has
has a persistent dis option relational
database service or RDS Dynamo DB no SQL
database elastic and memory data store
redshift data warehouse Neptune graph
database and database migration service
are all SQL compatible databases offered
by
Amazon the database choices in Azure are
specifically wide SQL database mySQL
database and post grid fre SQL database
are the three SQL based choices now when
it comes to databases gcp offers the
sql-based cloudsql and Cloud spanner a
relational database built for Mission
critical workloads now the next
parameter is
networking AWS uses Amazon virtual
private cloud or
VPC on the other hand Azure uses Azure
virtual Network or vnet
and gcp uses Cloud virtual Network now
let's move on to another factor which is
market share and pricing all these cloud
services are based on comparative
pricing strategies which means you need
to pay on the basis of its usage
according to Canalis the worldwide Cloud
Market Rose 35% to 41.8 billion in the
first quarter of
2021 AWS accounts for 32% of the market
with measure accounting for 19% and
Google accounting for
7% on one hand Amazon charges on a
yearly basis and on the other hand
Microsoft Azure and Google services
charge on a minute basis and also all of
them provide you a standard price for
you to access these Services AWS charges
roughly $69 per month for a very basic
instance with two virtual CPUs and 8 GB
of RAM and AWS largest instance with
3.84 TB of RAM and 128 vcpus will set
you back roughly
$3.97 per hour but in Azure the same
type of instance one with two vcpus and
8 GB of RAM cost roughly $70 us per
month and azure's largest instance has
3.89 TB of RAM and 128 virtual CPUs and
it cost about
$6.79 per
hour compared to AWS gcp will supply you
with the most basic instance which
includes two virtual CPUs and 8 GB of
RAM for 25% less and as a result it will
set you back roughly $52 every month and
the largest instance that includes is
3.75 TB of RAM and 160 vcpus and it will
cost you about about
$532 per hour Amazon other than this
also provides spot instances reserved
instances and dedicated host where you
can look for multiple offers and
discounts but for Azure it provides
special prices to developers based on
situations or even Azure hybrid benefit
which benefits your organization up to
40% if it uses Microsoft software in
their data centers
whereas Google offers quite a sorted
pricing to its customer compared to the
other two it gives you sustained use
discounts which activate if you use the
same instance for a
month the prunable instance which is
very similar to Amazon spot instances
but one thing is common in all three
cloud services which is that they all
offer long-term
discounts now let's have a look at the
last comparison which is the companies
that are using them because AWS is the
oldest player in the cloud business it
has the largest user base and Community
Support as a result AWS has the largest
number of high-profile and well-known
clients including Netflix Airbnb
Unilever BMW Samsung MinGa and others
with time Azure is getting a large
number of high-profile customers aure
currently boasts around 80% of Fortune
500 firms as customers Johnson Controls
polycom Fujifilm HP Honeywell apple and
others are among its key clients Google
Cloud on the other hand uses the same
infrastructure as Google search and
YouTube and as a result many high-end
interprises trust Google Cloud HSBC
PayPal 20th Century Fox Bloomberg
Domino's and other are among Google
Cloud's many clients the cloud is much
more than just a reliable storage
solution the year 2021 like the year
before proved to be a watershed event
for cloud computing it gave businesses
more freedom to operate in the phase of
covid-19 cloud computing according to it
experts will be at the Forefront of all
Technologies used to address key
businesses concerns in the coming
years cloud computing has generated more
anticipation excitement and investment
than any other it area in the last
decade according to ID see the cloud
will increase at a rate of 22% next year
with the market value of $277
billion as a result there will surely be
a significant demand for cloud
professionals therefore a right
certification in cloud computing can
help you earn a handsome salary of
$145,000 to
$227,000 in the United States and close
to 16 lakh to 24 lakh rupees in India so
let's have a look at the top 10
certification in cloud
computing the first one is AWS certified
Solutions architect
associate AWS certified Solutions
architect associate is a set of
technical certifications offered by
Amazon web services for beginers and
professionals working in Enterprise
architecture and solutions
architecture this certificate AIDS in
the identification and development of
personnel with crucial capabilities for
cloud
implementation AWS certified solution
architect associate certification
verifies your competence to build and
deploy distributed systems on
AWS so this certification is for persons
who can do solution architecture such as
deploying and securing web applications
and who have worked with AWS services
for at least a year and its difficulty
level is
easy skills required for the
certification is understanding of
specific programming or scripting
languages like Java Python chash and
many other programming language
available data storage fundamentals
networking AWS service selection Cloud
specific patterns and
Technologies moving on here are a few
certification details you will need to
proceed with its
examination cost of examination is 150
USD
and the average annual salary is
$13,810 the duration of this examination
is 130 minutes with number of questions
65 you have multiple choice and multiple
response questions and the passing score
is
70% and the renewal time of the
certification is 2
years the companies hiring are Amazon
IBM Cap Gemini M tree and equ V are
these are the top companies that
hire second is AWS certified Cloud
practitioner the AWS certified Cloud
practitioner certification is a
foundational level test designed for
people who can successfully show an
overall understanding of the AWS Cloud
according to
Amazon this exam verifies a candidate's
understanding of essential Cloud
infrastructure and Architectural
Concepts as well as key AWS
Services the certification provides an
overview of AWS key Services as well as
security and network so this is the idle
Cloud certification for beginners or
anyone interested in learning more about
cloud computing and the AWS Cloud
platform so the difficulty level is
easy the skills required for this
certification
is oversight on creating architecture
for AWS platform ability to deploy AWS
Solutions and applications building
Cloud
Solutions developing plans for the
adoption of cloud Solutions management
and monitoring of cloud
platforms moving on here are a few
certification details you will need to
proceed with this examination the cost
of examination is 100 USD average annual
salary is
$13,995 exam examination duration is 90
minutes number of questions is
65 types of questions are multiple
choice and multiple response
question passing score 70% and again the
renewable time for the certification is
2 years and the famous company hiring
are Amazon net app enture Deo and solal
LLC moving on third is AWS certified
develop
associate AWS certified developer
associate is an Amazon web service
examination that measures a person's
ability to successfully demonstrate
deploying and maintaining applications
on the AWS
platform this is the idle Cloud
certification for programmers and
software developers interested in
building Cloud native applications and
the difficulty level is
intermediate skills required for the
certifications are
use the
AWS Services API CLI and software
development kits to write
applications identify key features of
AWS Services understand the AWS shared
responsibility model use a continuous
integration of continuous delivery
pipeline to deploy application on AWS
and use an interact with AWS
services so here are the few
certification
details cost of examination is 150
USD average annual salary is
$3,272 exam duration is 130 minutes
number of questions is
$65 type of questions multiple choice
and multiple response questions the
passing scorer 70% renal time is 2 years
and the company hiring are Amazon IBM
Cap Gemini TCS and
Oracle fourth is aw
certified system operation administrator
associate the AWS
certified CIS Ops administrator
associate certification is designed to
demonstrate technical skills for system
administrators in Cloud operations jobs
this certificate assists organizations
in identifying and develop in personnel
with crucial abilities for cloud
implementation the ability to develop
manage and operate workloads on AWS is
demonstrated by earning the
certification this associate
certification is for professionals who
have at least one year of experience
with AWS deployment management and
operations it teaches you how to
identify the right service for your
needs and the difficulty level is
intermediate
so these are the skills required for the
certification understanding of AWS
tenets hands-on experience with the AWS
CLI and sdks tools understanding of
Network Technologies understanding of
security concept with hands-on
experience in implementing security
controls and compliance
requirements understanding of
virtualization
technology moving on here are a few
important details related to this
certification so cost of examination is
150 USD average annual salary is
11,966 the duration of examination is
130 minutes and the number of questions
is 65 which are multiple choice and
multiple response
questions with passing score of 70% and
renewal time of 2 years and here are the
companies hiring Amazon IBM Cap Gemini
TCS and
Oracle fifth is Microsoft certified
Azure
fundamentals Azure fundamental
certifications allows you to demonstrate
your understanding of cloud Concepts
Azure Services Azure workloads Azure
security and privacy and Azure pricing
and support the Azure Foundation
certification intended for people who
already have a basic understanding of
cloud services it will explain you Cloud
Concepts as well as how to use them and
the difficulty level is
easy skills required for this
certification is the different cloud
computing
Concepts main Azure Services describing
Azure security priv privacy compliance
and
Trust aure pricing and
support and here are a few certification
details required for the certification
cost of examination is 99 USD which can
vary average annual salary is
$110,000 the duration of this
examination is 85
minutes and the number of questions
varies between 40 to 60 and the type of
questions are multiple choice and
multiple response
questions the passing score for this
examination is 70% and the renewal time
is 6 months at the top comp companies
hiring are Microsoft Dell asentia and
cognizant sixth is Microsoft certified
Azure administrator
associate an Azure administrator is in
charge of deploying monitoring and
managing Microsoft Azure Solutions which
include significant compute storage
Network and Security Services by earning
these this certification you'll be able
to demonstrate that you can deploy and
manage as your compute
resources this certificate will prepare
you to develop administer and monitor
cloud services such as Storage security
and virtual environments among other
things and the difficulty level is
intermediate so the skills needed for
this certification
is Main Azure Services describing Azure
security private and compliance and
Trust knowledge of azure pricing and
support and knowledge of different cloud
computing
Concepts and here are the further
important details needed for the
certification cost of examination is 165
USD average annual salary is
$17,620.63 is 120 Minutes with number of
questions varying between 40 to
60 and the type of questions are
multiple choice and multiple response
questions with the passing score of
70% renewal time is 6 months and the top
companies hiring are Microsoft asentia
TCS and
Yahoo seventh is Google associate Cloud
engineer an ass associate Cloud engineer
delivers and protects
applications and infrastructure oversees
various projects operations and
maintains corporate solution to ensure
that they fulfill performance goals this
person has worked with both public
clouds and on premises
systems for programmers developers and
software Engineers this is the greatest
Google Cloud certification its Holder
will be responsible for deploying web
applications in the cloud
as well as monitoring and managing
operations and the difficulty level is
intermediate here are the some important
skills required for you to excel this
certification basic understanding of
Google Cloud platforms products and
services and basic understanding of
cloud Concepts such as virtual machines
containers and
networking and these are the few
important certification details that you
need to take care of cost of examination
is 125 USD with average annual salary of
$119,450 exam duration is 120 Minutes
with number of questions 50 and the type
of questions are multiple choice and
multiple response questions with the
passing score of
70% and the renewal time is 2 years and
the top companies hiring a Google
Goldman
sets here comes the e one that is
Microsoft certified as your solution
architecture
expert to implement Solutions and as
your solution architect work with Cloud
administrators cloud dbas and clients
these this credential displays the
ability to provide advice to
stakeholders and translate business
requirements into safe scalable and
dependable
Solutions this certificate is for
experienced programmers developers and
devops Engineers who wish to become
Asher
professionals this is the idle Cloud
certification and the difficulty level
is
hard and these are the skills required
for this Advanced
certification deploying and configuring
infrastructure implementing workloads
and security creating and deploying apps
implementing authentication and securing
data developing for the cloud and for
the Azure storage
and these are the details you need to
remember cost of
examination 165
USD average annual salary
$135,000 with exam duration of 150
minutes and number of questions varies
between 40 to 60 type of questions are
multiple choice and multiple response
questions with the passing score of 70%
and the renewal time is 6 months and the
company's hiring are IBM Microsoft
enosis and
medine coming to the ninth one is Google
professional Cloud
architect a Google Certified
professional Cloud architect means you
demonstrate the ability to design and
plan A Cloud solution architecture
manage and provision the Cloud solution
infrastructure design for security and
compliance
the professional Cloud architect exam
measures your abilities to design and
plan A Cloud solution architecture
manage and provision Cloud solution
infrastructure and design for security
and
compliance this is the idle Google Cloud
certification for experienced it
professionals interested in becoming
solution Architects for Google Cloud
Technologies such as big table big query
and other gcp platform services and the
difficulty level is
hard and these are the skills you need
to focus
on proficiency with command command line
Linux operating system and system
operations and three plus years of
Industry
experience and here are the further
details you need to take take care of
cost of examination is 200 USD with
average annual salary of $140,000
the examination duration is 120 Minutes
with number of questions 40 the type of
questions are multiple choice and
multiple response questions with a
passing score of
70% and the renewal time is 2 years and
the company's hiring are Google and
golden
SATs and here comes the last but not the
least 10th certification that is AWS
architect professional
the AWS certified Solutions architect
professional exam verifies Advanced
technical knowledge and experience in
designing distributed applications and
systems on the Amazon web services
platform this certification AIDS in the
identification and development of
personnel with crucial capabilities for
cloud
implementation the AWS certified
Solutions architect
credential verifies your ability to
develop Implement and evaluate
applications on AWS under a variety of
conditions this Advanced certification
teaches you how to create and deploy
scalable web applications on Amazon AWS
servers as well as how to choose the
right service and power for your
application and the difficulty level is
hard and the the skills required for
this
certification
is familiarity with AWS CLI AWS apis AWS
cloud formation templates and windows
and Linux environments ability to
provide best practice guidance on the
architectural design across multiple
applications and projects of the
Enterprise ability to evaluate Cloud
application requirements and ability to
design a hybrid architecture using key
AWS
Technologies and here are the details
you need to know before applying for the
exam cost of examination is 300 USD with
average annual salary of
$188,200 minutes with number of
questions
75 types of questions are multiple
choice and M multiple response questions
with the passing score of
70% and the renewal time is 2 years and
the top companies hiring are Amazon
centure IBM and M
tree so this was all about top 10 Cloud
certification in 202 let's have a look
at some of the skills required to become
a cloud administrator first off you need
experience with programming languages
like C and.net you'll also need to be
experienced with devops tools like
Jenkins Docker anible and Chef you'll
need to have an understanding of
database configuration so that you can
take advantage of all the relevant
information about the hardware and
software components used by the
organization you'll also need to have
experience with Cloud infrastructure
systems like servers storage Network and
visualization software finally you'll
also need to configure virtual machines
VPN and Cloud servers now let's have a
look at the salary offered to Cloud
administrators now in the United States
the average salary of a cloud
administrator is approximately $65,000
per anom similarly in India it's
approximately 7 lakh rupees per anom now
let's have a look at some of the
companies hiring for cloud
administrators we have companies like
vipro infosis Accenture IBM and so on so
here I see a comment which says I'm a
fresher and want to learn about AWS how
can I start so actually any fresher can
make a career in AWS as long as you
interest towards the subject now if you
are from a non-technical background you
need to put in a lot more hard work and
have a lot of patience so I would
recommend that before you take up the
AWS course you have a look at the course
curriculum types of AWS courses
opportunities in the market for AWS
certified candidates and so on so now
let's go to number six Cloud application
developer now a cloud application
developer mainly focuses on implementing
and maintaining an organization's cloud
infrastructure now they're involved with
designing building creating analyzing
and maintaining Cloud systems now these
individuals are also involved with
ensuring that there's an effective
design of business processes in the
cloud they're also involved with a
number of different tasks like
optimizing efficiency and performance
scaling application components and
security issues now let's have a look at
some of the skills required to become a
cloud application developer first off
you need to have experience with
database languages like MySQL SQL and
mongod DB then you need to have
experience with programming languages
like python Ruby and pearl so that you
can code and create cloud computing
applications you'll also need to have
experience with Linux now this is
because most Cloud infrastructures are
created with Linux servers you'll also
need to have experience with cloud
service providers like Amazon web
services Google Cloud platform and
Microsoft is your you'll need to have
experience with information security
with the help of certifications you'll
also so need to know how to create
microservices and to create Cloud
applications now let's have a look at
the salary offered to a cloud
application developer in the United
States it's approximately $70,000 per
anom in India the average salary of a
cloud application developer is
approximately 8 lakh rupees perom now
let's have a look at some of the
companies hiring Cloud application
developers you have companies like Bosch
meca sap and so on so now let's go to
number five Cloud network engineer a
cloud Network NW engineer is responsible
for implementing supporting maintaining
and optimizing the network Hardware
software and communication Links of the
organization's Cloud infrastructure
hioshi basically focuses on Automation
and security while building the cloud
infrastructure enhancing Network tooling
visibility and improving productivity
now let's have a look at some of the
skills required to become a cloud
network engineer first off obviously you
need to have experience with networking
which means the familiarity with the
internet and Van communication
Technologies protocols and best
practices you need to have an
understanding of cloud security and how
you can design a public cloud with
multiple options you'll need to have an
experience with data center
Administration which basically means an
understanding on infrastructure design
operations and life cycle management and
finally you need to have experience with
cloud computing platforms like Amazon
web services Microsoft as your and
Google Cloud platform next let's have a
look at the salaries offered to a cloud
network engineer firstly the average
salary of a cloud network engineer in
the United States is approximately
$72,000 perom in India the average
salary of a cloud network engineer is
approximately 4 lakh rupees perom now
that we're done with this let's have a
look at the companies hiring Cloud
Network Engineers we have companies like
Amazon web services Rakuten Cisco and so
much more now for number four we have
Cloud automation engineer a cloud
automation engineer focuses on Cloud
Automation orchestration and integration
he or she implements optimizes and
supports the cloud infrastructure and
ensures it has high availability they
also have to increase the cost
Effectiveness and availability of the
cloud now let's have a look at some of
the skills required to become a cloud
automation engineer first off since the
role is cloud-based experience with a
cloud service platform like Microsoft is
your AWS or Google Cloud platform is an
absolute must you'll also need to have
experience with programming languages
like Python and go that will help you
make the process of automation easier
next experience with devops tools like
chef anible and puppet are seen as a
huge plus you'll also need to have an
understanding of Docker and containers
experience with databases like post SQL
and MySQL that can handle multiple
workloads would be very useful and
finally you need to have experience with
virtualization which means you need to
know how you can work with virtual
servers applications storage and
networks with this you can reduce the
amount of resources required by the
organization now let's have a look at
the salary offered to Cloud automation
engineers in the United States the
average salary of a cloud automation
engineer is approximately $79,000 per
anom the average salary of a cloud
automation engineer in India is
approximately 5 lakh rupees perom now
that we're done let's have a look at
some of the companies hiring Cloud
automation Engineers we have companies
like Oracle Google Kappa Gemini this
Disney and so on and now to number three
Cloud security manager a cloud security
manager is someone who's responsible for
providing security for cloud-based
digital platforms and protecting the
organization's data he or she may be
involved with creating new security
methods or to analyze existing ones they
may also have to create cloud-based
applications performing threat
simulations and providing security
recommendations now let's have a look at
some of the skills required to become a
cloud sec manager first off you'll need
to have experience with at least one of
the popular cloud service providers like
Amazon web services Microsoft asur and
gcp you'll also need to have experience
in programming languages like python
experience with commonly used devops
tools like jenin so that you can
Implement continuous integration or
continuous deployment models would be
very helpful you'll also need to be well
wored with TCP IP protocols and other
networking Concepts you need to be well
vered with pki SSL SSH https and so on
experience with web-based analytics and
services would also be very helpful now
let's have a look at the salary offered
to a cloud security manager in the
United States the average salary of a
cloud security manager is approximately
$99,000 perom in India the average
salary for cloud security manager is
approximately 11 lakh rupees per anom
now let's have a look at some of the
companies hiring Cloud security managers
we have companies like deor VMware Cisco
Dell and so on now let's move on to
number two Cloud engineer a cloud
engineer is an individual who is
responsible for any technological duties
that are associated with cloud computing
including design planning management
maintenance and support they're also
involved with orchestrating and
automating cloud-based platforms
throughout the organization let's have a
look at the skills required to become a
cloud engineer first off you'll need to
have experience with at least one of the
popular cloud service providers like AWS
Microsoft a your or the Google Cloud
platform you'll need to have an
understanding of networking Concepts
like building and accessing servers
virtual networks and so on this enables
Cloud engineers make sure that the
network is responsive to the users next
you need to have experience with
virtualization with virtualization you
can reduce how many Hardware units you
need with the help of virtual machines
it also makes resources scalable and
fall tolerant in an organization
experience with the Linux operating
system would also be very helpful
considering how 30% of the servers that
powers your are Linux based you'll need
to know about apis to design restful
Services experience with devops is also
a major skill to have since it'll be
able to help handle work dependencies
between the development and operation
teams and finally you need to have
programming experience with languages
like Java python C++ and Ruby next let's
have a look at the salary offered to
Cloud engineer in the United States the
average salary of a cloud engineer is
approximately
$105,000 per anom the average salary in
India is approximately 5 lakh rupees
perom now let's have a look at some of
the companies hiring Cloud Engineers we
have companies like Mecha IBM JP Morgan
visa and so on now I see a pretty good
question in the chat how do you become a
cloud engineer so to become a cloud
engineer you need to have experience
with the programming language experience
with at least one cloud service platform
which is Amazon web services or
Microsoft as your or Google Cloud
platform the choice is yours and you
need to specialize in a particular
service be it storage networking
disaster recovery and so on and now for
number one Cloud architect a cloud
architect is responsible for managing
cloud computing architecture in an
organization he or she handles
everything to do with front end
platforms servers storage delivery and
networks now let's have a look at some
of the skills required to become a cloud
architect first off you need to have
knowledge in programming knowledge about
Pearl python Ruby PHP Java and net can
provide users with the ability to build
deploy and manage applications quickly
you'll need to know about the basics of
networking since most of the work what
you'll be doing would be web related
knowledge about networking can be really
helpful knowledge about data storage
fundamentals are also really important
this provides you with the knowledge to
determine which data storage option to
use and when being experienced with
cloud service platforms is also really
important platforms like AWS Google
Cloud platform and Microsoft aure are
what you are going to be basing most of
your work on experience with them is
pretty important and finally Cloud
security this will help you make sure
that your data is secure and only
authorized code is run and the right
people are allowed to run it now let's
talk about the salary of a cloud
architect the the average salary of a
cloud architect is approximately
$107,000 per anom in India the average
salary of a cloud architect is
approximately 16 lakh rupees perom and
there you go those are the top job roles
in the field of cloud computing now
let's have a look at some of the
companies hiring Cloud Architects we
have companies like ey vpro Huawei hulet
Packard Enterprise and so on and today
I'm going to tell you how you can become
a successful cloud computing engineer
now cloud computing is one of those
Technologies that's rapidly rising and
with any technology that's growing
rapidly it comes with several job
opportunities for the people who are
skilled in it so before we get into it
let's have a brief look at what is cloud
computing cloud computing refers to
services like storage databases software
analytics machine learning artificial
intelligence and so much more all of
which made accessible via the Internet
the cloud tech services Market is
expected to grow
17.3% in the span of 2018 to 19 which
which means there's a growth from $1
175. billion to a whopping $26 billion
in 2019 and as of 2020 it's expected
that 90% of all organizations in the
world would be using cloud services not
to mention several organizations around
the world suggest that using Cloud
Computing Services has enabled their
employees to experiment a lot more with
Technologies like machine learning and
artificial intelligence so here's what
we'll be going through today firstly
we'll be talking about who is a cloud
computing engineer the steps you need to
take to become a cloud computing
engineer and the cloud computing
engineer salaries so first off who is a
cloud computing engineer now a cloud
computing engineer is an IT professional
who takes care of all the technical
aspects of cloud computing now be it
design planning maintenance and support
now a cloud computing engineer can take
up a number of different career paths
this could be that of a cloud developer
security engineer a full stack developer
isops administrator Solutions architect
Cloud architect and so much more now
let's have a look at some of the major
cloud computing roles first off we have
Solutions architect now these are
individuals who are responsible for
analyzing the technical environment in
which they are going to produce the
solutions the requirements and the
specifications secondly they are
required to select an appropriate
technology that satisfies set
requirements they need to estimate and
manage the usage and the operational
costs of the solutions they provide and
they need to support support project
management as well as solution
development next we have ssops
administrators they are involved in
deploying managing and operating highly
scalable and fall tolerance systems they
need to select an appropriate service
based on compute security or data
requirements they need to estimate and
manage usage and operational costs and
they need to be able to migrate on
premises workloads onto an appropriate
cloud computing platform so among both
of these roles there are certain
requirements that are remaining constant
now let's have a look at the steps you
need to take to become a cloud computing
engineer your first step is to gain
Proficiency in a cloud computing
platform now the first step is to become
proficient in at least one of the three
major cloud computing platforms bws
Azure or the Google Cloud platform now
there are a huge number of resources
that you can find on the internet it
could be YouTube videos articles virtual
or physical classrooms and so much more
now after you're done learning you can
get certified by Microsoft as your AWS
or the Google Cloud platform now for AWS
you have a number of different
certifications which can be divided into
three categories which are the
foundational which is just the basics
the associate level certifications the
professional level certifications and
the specialty certifications similarly
with Microsoft Azure you have
certifications that enable you to become
an Azure developer associate an Azor
administrator associate an Azure
architect professional and a devops
engineer now most cloud computing
platforms have a free here that you can
take advantage of these provide a number
of free services for a period of time
some of which are free forever so you
can use these platforms to your
advantage and do as much practice as you
can on them now if you want to learn
more about cloud computing you can also
check out Simply learns YouTube channel
then you can go on to the playlist
section right here and you can find
comprehensive videos on a number of
different cloud computing platforms AWS
and Microsoft as your our AWS tutorial
videos talk about what exact ly is AWS
how you can become an AWS Solutions
architect Amazon ec2 S3 some of the
other services and so much more we also
have detailed tutorials on Azure which
talks about what exactly is azure the
certifications provided by Azure some of
the services like machine learning a
your active directory and so much more
and now we're at step two being
experienced in at least one programming
language unlike general purpose
programming languages like C C++ C and
so on cloud computing requires ones that
are a lot more data oriented now some of
the major programming languages that are
used in cloud computing are go Python
clure and Java now as I said before
there is a wealth of resources that you
can learn from there are free websites
that you can practice your code on like
quick code code academy and several
others there's also resources like
YouTube videos as well as there's the
option of online or offline classes now
we're at step three specialization
you'll also need to be well vered with a
number of number of key Concepts these
are storage and networking now with
storage you need to know how data can be
stored and where it can be accessed from
you need to know how it can be accessed
from multiple different resources you'll
also need to have some experience with
the services provided by Azor and AWS
like the Amazon S3 in AWS and the
appropriately named Azor storage from
Microsoft aure with networking you need
to have a strong understanding of the
networking fundamentals as well as virtu
networks next up we have virtualization
and operating systems with
virtualization you need to know how
virtual networks which is just a
combination of different virtual
machines can be used to emulate
different components in a particular
system with operating systems you need
to have a very strong understanding
operating systems like Windows and Linux
next up we have security and Disaster
Recovery now you need to understand how
data application as well as
infrastructure can be protected from
malicious attacks With Disaster recovery
you need to be prepared for any
unexpected circumstance by making sure
your systems are always safe and are
regularly backed up to prevent any sort
of loss of data then we have web
services and devops now you need to have
a strong understanding of apis or
application program interfaces and web
services some amount of experience with
web design also can be of great help
with devops you need to have a strong
understanding of how cloud computing is
able to provide a centralized platform
on which you can perform testing
deployment and production for devops
automation moreover with devops you
understand the Synergy that the
operations as well as the development
teams have with each other and for the
success of any project and finally
you're a cloud computing engineer now
let's have a look at the salaries of
cloud computing engineers in the United
States cloud computing Engineers earn
around
$116,000 perom in India a cloud
computing engineer is paid approximately
666,000
per anom now how can simply learn help
you become a cloud computing engineer so
let's head on to Simply learn's website
here we have the cloud architect Masters
program now this deals with a number of
different courses all of which that can
help you get started in your journey to
becoming a cloud computing engineer this
Masters program covers a number of
different courses like AWS technical
Essentials Microsoft a your fundamentals
AWS developer associate and so much more
it provides you 40 plus in demand skills
and 25 plus Services provides you a
master certification it has 16 plus real
life projects and helps you get a salary
that ranges between 15 to 25 lakh rupees
per anom it also covers a variety of
tools like Amazon ec2 Azo data Factory
virtual machines and so much more so why
don't you head on to Simply learn.com
and get started on your journey to
getting certified and getting ahead and
with that we have come to the end of
this video on cloud computing full
course I hope you found it useful and
entertaining please ask away any
questions from the topics covered in
this video in the comments box below our
experts will assist you in addressing
your problems thank you for watching
stay safe and keep
learning hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to ner up and get certified click
here