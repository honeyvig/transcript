welcome to devops fundamentals for
beginners 2023 if you are new to the
world of devops you are in the right
place in this video session we will
break down devops Concepts and practices
into simpler terms you will learn how
devops make software development faster
more reliable and more collaborative
we'll start from Basics and gradually
build your understanding whether you're
a developer ID professionals or just
curious about devops join us on this
journey to uncover the fundamental
principles that drive modern software
development so let's get started so if
you are interested in taking your career
to the next level look no further than a
postgraduate program in devops this
comprehensive course is designed to
empower you with the skills and
knowledge needed to excel in the dynamic
world of devops this program offers over
50 hours of self-paced learning master
classes led by Caltech ctme 20 plus real
life projects in integrated labs and the
opportunity to acquire 40 plus in demand
skills and master 15 plus essential
tools top it all with Capstone project
spanning in three domains and you will
be well on your way to a successful
devops career when it goes through a
number of key elements today the first
two will be reviewing models that you're
already probably using for delivering
Solutions into your company and the most
popular one is waterfall followed by
agile then we'll look at devops and how
devops differs from the two models and
how it also borrows and leverages the
best of those models we'll go through
each of the phases that are used in
typical devops delivery and then the
tools used within those phases to really
improve the efficiencies within devops
finally we'll summarize the advantages
that devops brings to you and your teams
so let's go through waterfall so
waterfall is a traditional delivery
model that's being used for many decades
for delivering Solutions not just IT
solutions and digital Solutions but even
way before that it has its history it
goes back to World War II so waterfall
is a model that is used to capture
requirements and then Cascade each key
deliverable through a series of
different stage Gates that is used for
building out the solution so let's take
you through each of those stage Gates
the first that you may have done is
requirements analysis and this is where
you sit down with the actual client and
you understand specifically what they
actually do and what they're looking for
in the software that you're going to
build and then from that requirements
analysis you'll build out a project plan
so you have an understanding of what the
level of work is needed to be able to be
successful in delivering the solution
after that you've got your plan then you
start doing the development and that
means that the programmers start coding
out their solution they build out their
applications to build out the websites
and this can take weeks or even months
to actually do all the work when you've
done your coding and development then
you send it to another group that does
testing and they'll do full regression
testing of your application against the
systems and databases that integrate
with your application you'll test it
against the actual code you'll do manual
testing you do UI testing and then after
you've delivered the solution you go
into maintenance mode which is just kind
of making sure that the application
keeps working there's any security risks
that you address those security risks
now the problem you have though is that
there are some challenges however that
you have with the waterfall model the
cascading deliveries and those complete
and separated stage Gates means that
it's very difficult for any new
requirements from the client to be
integrated into the project so if a
client comes back and it's the project
has been running for six months months
and they've gone hey we need to change
something that means that we have to
almost restart the whole project it's
very expansive and it's very time
consuming also if you spend weeks and
months away from your clients and you
deliver a solution that they are only
just going to see after you spend a lot
of time working on it they could be
pointing out things that are in the
actual final application that they don't
want or are not implemented correctly or
lead to just general unhappiness the
challenge you then have is if you want
to add back in the client's feedback to
restart the whole waterfall cycle again
so the client will come back to you with
a list of changes and then you go back
and you have to start your programming
and you have to then start your testing
process again and just you're really
adding in lots of additional time into
the project so using waterfall model
companies have soon come to realize that
you know the clients just aren't able to
get their feedback in quickly
effectively it's very expensive to make
changes once the teams have started
working and the requirement in today's
digital world is that Solutions simply
must be delivered faster and this has
led for a specific change in agile and
we start implementing the agile model so
the agile model allows programmers to
create prototypes and get those
prototypes to the client with the
requirements faster and the client is
able to then send their requirements
back to the programmer with feedback
this allows us to create what we call a
feedback loop we're able to get
information to the client and the client
can get back to the development team
much faster typically when we're
actually going through this process
we're looking at the engagement cycle
being about two weeks and so it's much
faster than the traditional waterfall
approach and so we can look at each
feedback loop as comprising of four key
elements we have the planning where we
actually sit down with the client and
understand what they're looking for we
then have coding and testing that is
building out the code and the solution
that is needed for the client and then
we review with the clients the changes
that have happened but we do all this in
a much tighter cycle that we call a
Sprint and that typically a Sprint will
last for about two weeks some companies
run sprints every week some run every
four weeks it's up to you as a team to
decide how long you want to actually run
a Sprint but typically it's two weeks
and so every two weeks the client is
able to provide feedback into that Loop
and so you were able to move quickly
through iterations and so if we get to
the end of Sprint 2 and the client says
hey you know what we need to make a
change you can make those changes
quickly and effectively for Sprint three
what we have here is a breakdown of the
ceremonies and the approach that you
bring to Agile so typically what will
happen is that a product leader will
build out a backlog of products and what
we call a product backlog and this will
be just a whole bunch of different
features and they may be small features
or bug fixes all the way up to large
features that may actually span over
multiple Sprints but when you go through
the Sprint planning you want to actually
break out the work that you're doing so
the team has a mixture of small medium
and large solutions that they can
actually Implement successfully into
their Sprint plan and then once you
actually start running your Sprint again
it's a two-week activity you meet every
single day the two with the actual
Sprint team to ensure that everybody is
staying on track and if there's any
blockers that those blockers are being
addressed effectively and immediately
the goal at the end of the two weeks is
to have a deliverable product that you
can put in front of the customer and the
customer can then do a review the key
advantages you have are running a Sprint
with agile is that the client
requirements are better understood
because the client is really integrated
into the scrum team mean they're there
all the time and the product is
delivered much faster than with a
traditional waterfall model you're
delivering features at the end of each
Sprint versus waiting weeks months or in
some cases years for a waterfall project
to be completed however there are also
some distinct disadvantages the product
itself really doesn't get tested in a
production environment it's only been
tested on the developer computers and
it's really hard when you're actually
running agile for the Sprint team to
actually build out a solution easily and
effectively on their computers to mimic
the production environment and the
developers and the operations team are
running in separate silos so you have
your development team running their
Sprint and actually working to build out
the features but then when they're done
at the end of their Sprint and they want
to do a release they kind of fling it
over the wall at the operations team and
then it's the operations team job to
actually install all the software and
make sure that the environment is
running in a stable fashion that is
really difficult to do when you have the
two teams really not working together so
here we have is a breakdown of that
process where the developers submitting
their work to the operations team for
deployment and then the operations team
may submit their work to the production
service but what if there is an error
what if there was a setup configuration
error with the developers test
environment that doesn't match the
production environment there may be a
dependency that isn't there there may be
a link to an API that doesn't exist in
production and so you have these
challenges that the operations team are
constantly faced with and their
challenge is that they don't know how
the code works so this is where devops
really comes in and let's dig into how
devops which is developers and operators
working together is the key for
successful continuous delivery so devops
is as an evolution of the agile model
the agile model really is great for
Gathering requirements and for
developing and testing out your
Solutions and what we want to be able to
do is kind of address that challenge and
that gap between the Ops Team and the
dev team and so with devops what we're
doing is bringing together the
operations team and the development team
into a single team and they are able to
then work more seamlessly together
because they are integrated to be able
to build out solutions that are being
tested in a production-like environment
so that when we actually deploy we know
that the code itself will work the
operations team is then able to focus on
what they're really good at which is
analyzing the production environment and
being able to provide feedback to the
developers on what is being successful
so we're able to make adjustments in our
code that is based on data so let's step
through the different phases of a devops
team so typically you'll see that that
the devops team will actually have eight
phases now this is somewhat similar to
Agile and what I'd like to point out at
the time is that again agile and devops
are very closely related that agile and
devops are closely related delivery
models that you can use with devops it's
really just extending that model with
the key phases that we have here so
let's step through each of these key
phases so the first phase is planning
and this is where we actually sit down
with a business team and we go through
and understand what their goals are the
second stage is as you can imagine and
this is where it's all very similar to
Agile is that the code is actually start
coding but they typically they'll start
using tools such as git which is a
distributed Version Control software it
makes it easier for developers to all be
working on the same code base rather
than bits of the code that is rather
than them working on bits of the code
that they are responsible for so the
goal with using tool set such as git is
that each developer always has the
current and latest version of the code
you then use tools such as Maven and
Gradle as a way to consistently build
out your environment and then we also
use tools to actually automate our
testing now what's interesting is when
we use tools like selenium and junit is
that we're moving into a world where our
testing is scripted the same as our
build environment and the same as using
our get environment we can start
scripting out these environments and so
we actually have scripted production
environments that we're moving towards
Jenkins is the integration phase that we
use for our tools and another Point here
is that the tools that we're listing
here these are all open source tools
these are tools that any team can start
using we want to have tools that control
and manage the deployment of code into
the production environments and then
finally tools such as ansible and Chef
will actually operate and manage those
production environments so that when
code comes to them that that code is
compliant with the production
environment so that when the code is
then deployed to the many different
production servers that the expected
results of those servers which is you
want them to continue running is
received and then finally you monitor
the entire environment so you can
achieve zero in on spikes and issues
that are relevant to either the code or
changing consumer habits on the site so
let's step through some of those tools
that we have in the devops environment
so here we have is a breakdown of the
devops tools that we have and again one
of the things I want to point out is
that these tools are open source tools
there are also many other tools this is
just really a selection of some of the
more popular tools that are being used
but it's quite likely that you're
already using some of these tools today
you may already be using Jenkins you may
already be using git but some of the
other tools really help you create a
fully scriptable environment so that you
can actually start scripting out your
entire devops tool set this really helps
when it comes to speeding up your
delivery because the more you can
actually script out of the work that
you're doing the more effective you can
be at running automation against those
scripts and the more effective you can
be at having a consistent experience so
let's step through this devops process
so we go through and we have our
continuous delivery which is our plan
code build and test environment so what
happens if you want to make a release
well the first thing you want to do is
send out your files to the build
environment and you want to be able to
test the code that you've been created
because we're scripting everything in
our code from the actual unit testing
being done to the all the way through to
the production environment because we're
testing all of that we can very quickly
identify where whether or not there are
any defects within the code if there are
defects we can send that code right back
to the developer with a message saying
what the defect is and the developer can
then fix that with information that is
real on the either the code or the
production environment if however your
code passes the the scripting test it
can then be deployed and once it's out
to deployment you can then start
monitoring that environment what this
provides you is the opportunity to speed
up your delivery so you go from the
waterfall model which is weeks months or
even years between releases to Agile
which is two weeks or four weeks
depending on your Sprint Cadence to
where you are today with devops where
you can actually be doing multiple
releases every single day so there are
some significant advantages and there
are companies out there that are really
zeroing in and on those advantages if we
take any one of these companies such as
Google Google any given day will
actually process 50 to 100 new releases
on their website through their devops
teams in fact they have some great
videos on YouTube that you can find out
on how their devops teams work Netflix
is also a similar environment now what's
interesting with Netflix is that Netflix
have really fully embraced devops within
their development team and so they have
a devops team and Netflix is a
completely digital company so they have
software on phones on Smart TVs on
computers and on websites interestingly
though the devops team for Netflix is
only 70 people and when you consider
that a third of all internet traffic on
any given day is from Netflix it's
really a reflection on how effective
devops can be where you can actually
manage that entire business with just 70
people so there are some key advantages
that devops have it's the actual time to
create and deliver a software is
dramatically reduced particularly
compared to Waterfall complexity of
maintenance is also reduced because
you're automating and scripting out your
entire environment now you're improving
the communication between all your teams
so teams don't feel like they're in
separate silos but that are actually
working cohesively together and that
there is continuous integration and
continuous delivery so that your
consumer your customer is constantly
being delighted if you're into the tech
industry are just curious about the role
of devops and software development you
have come to the right place so what
exactly is devops in simple terms it's a
set of practices and tools that help
developers and operational team work
better together releasing software
faster with higher quality at its core
devops is about breaking down barriers
between development and operations and
creating a culture of collaboration that
for focuses on delivering value to
customers as quickly and efficiently as
possible of course this is a vast
oversimplification and there are many
different aspects of devops that we
could spend hours diving into but for
now let's focus on some of the key
responsibilities of a devops engineer
who is the person responsible for
implementing and overseeing devops
practices and processes but before we
begin if you're new to the channel and
haven't subscribed already consider
getting subscribed to Simply learn to
stay updated with all the latest
Technologies and with that Bell icon to
never miss an update from us having said
that the demand for devops professionals
has overgrown in recent years as more
and more companies adapt devops
practices to improve their software
development and delivery processes so
are you ready to advance your
professional career to the next level
taking our step-by-step simply learn
spojet program in devops in
collaboration with IBM will help you
start your devops journey that will
prepare you for the devops engineer role
in order to match your skill set market
demand this devops training program is
designed in collaboration with Caltech
ctme our Cutting Edge branded learning
combines live online devop certification
classes with interactive labs to give
you practical experience this post
Argent program covers topics including
git GitHub Docker CI CD practices using
Jenkins kubernetes and much more
so what else can you expect from this
program well this devops trading program
will cover skills like devops
methodology continuous integration
devops and Cloud deployment Automation
and you'll also get hands-on experience
with the latest tools and techniques
including terraform Maven and symbol
Jenkins Docker junit and many more this
program will cover industry projects
like Docker Rising junkins pipeline
deploy angular application and Docker
container branching development model
and many more exotic projects so if you
are looking to pursue your career as a
devops engineer and acquire skills that
will prepare you for your job consider
enrolling in this intensive training
program we will leave the link in the
description box below make sure to check
that out so without any further Ado
let's get started with today's topic
firstly let us understand what is devops
now devops is a software development
approach that emphasizes collaboration
Automation and communication between
development and operations team it aims
to streamline the entire software
development life cycle by integrating
and optimizing processes tools and
methodologies it encourages the culture
of shared responsibility where
developers and operations team work
together closely throughout the entire
software development life cycle from
planning and coding to testing
deployment and monitoring
now the question is who is a devops
engineer well you got it right a devops
engineer is a professional who combines
software development expertise with
operations knowledge to facilitate
collaboration streamline processes and
improve software delivery and
infrastructure management within an
organization a devops engineering role
is to bridge the gap between development
and operations team enabling efficient
and reliable software development and
deployment practices
but the question is how to become a
devops engineer what are the skills that
you need to persist to become a good
devops engineer well a devops engineer
possess a wide range of skills including
Proficiency in scripting and programming
languages knowledge of various tools and
Technologies expertise in
systemadministration Cloud platforms and
containerization Technologies as well as
strong problem solving and communication
skills are necessary firstly having a
good coding knowledge well tools like
Confluence jira git these tools can
support and enhance collaboration and
project management within a devops
environment next having a good knowledge
on deployment tools are also necessary
now tools like dcos provides
orchestralization capabilities for
distributed applications Docker enables
containerization for consistent and
scalable deployments and AWS offers a
broad range of cloud services for
infrastructure provisioning scalability
and managed Services next you need to
have a good knowledge on operations
tools as well now chef and ansemble
focus on infrastructure automation and
configuration management while
kubernetes specializes in container
orchestration and management these tools
are utilized in devops to automate
various aspects of software like
development life cycles including
infrastructure provisioning
configuration management application
deployment and scaling moving ahead you
need to have a strong grip on monitoring
tools nagios Splunk and datadog are
three common use tools in the field of
monitoring and observatability now each
rule serves a specific purpose in
monitoring and managing system and
applications nagio specializes
infrastructure and application
monitoring Splunk focuses on log
analysis and data visualization datadog
provides comprehensive monitoring and
analytic capabilities in Cloud
environments these tools play a crucial
role in maintaining the health and
performance of systems and applications
moving ahead you need to have a good
knowledge on Jenkins and code ship now
Jenkins and courtships are both
essential tools and demo practices
Jenkins is a flexible and extensible
automation server that supports
continuous integration testing and
deployment on the other hand core ship
is a cloud-based CI CD platform that
offers Simplicity and ease of use
particularly for cloud native
applications both these tools contribute
to improving development productivity
and code quality and finally having a
good knowledge on testing tools like
selenium J unit are necessary for a
devops engineer junit is primarily
focused on unit testing and automated
testing of java code while selenium is
geared towards functional testing and
automation of web applications both
these tools play critical roles and
devops workflow contributing to faster
feedback Cycles improve code quality and
Reliable Software releases
so these are some of the main and
important skills that you need to
possess as a day offs engineer before
moving ahead let's take a minute and
listen to the experiences of our
Learners who have enrolled in the devops
pgp program which has proven to be
highly beneficial for many aspiring
engineers and professionals leading them
to achieve New Heights in the field of
devops I started my It Journey with
Accenture three years ago I joined the
asset Cloud architect there I worked
with AWS and Azure Technologies looking
for higher paying job and devops same
the right career choice so I decided to
go with the postgraduate program in
devops in collaboration with Caltech
ctme the course was divided into modules
and we had assignments
I was really impressed by how many job
interviews I landed after I added the
certification to my portfolio and now I
am earning 40 more than my previous job
it didn't only boost my career but also
my confidence
well now comes the main part what
exactly are the day-to-day roles and
responsibilities of a devops engineer
now a devops engineer play a crucial
role in Bridging the Gap between
development and operations team as we
discussed earlier so here are some of
the top five roles and responsibilities
of a devops engineer in detail first on
the list we have collaboration and
communication now devops engineer of a
state effective communication and
collaboration with development and
operations team the actively participate
in meetings and discussions to align
goals and expectations now as a devops
engineer you need to engage in regular
meetings and discussions regular
engagement ensures that they are up to
date with ongoing projects challenges
and goals enabling them to better align
their efforts and contribute effectively
regular engagement ensures that they are
up to date with ongoing projects
challenges and goals enabling them to
better align their efforts and
contribute effectively actively listen
and understand the requirement concerns
and feedback now when engaging with
development and operations team day offs
Engineers practice active listening they
close attention to the requirements
concerns and feedback expressed by team
members from both the sites by
understanding their perspectives pain
points and suggestions devops Engineers
can better assess the needs of their
teams and collaborate to find suitable
Solutions frustrated effective
communication channels now devops
Engineers take the initiative to
establish and maintain effective
communication within the organization
this often involves settling up
dedicated chat platforms like slack
Microsoft teams or collaboration tools
like jira to Foster better collaboration
and ensure that information flows
smoothly between the teams and finally
encourage cost functional collaboration
now day of Engineers recognize the value
of cross-function collaboration and
knowledge sharing among the team members
they actively encourage them from
development and operations team to
collaborate exchange ideas and share
their expertise second on the list we
have infrastructure Automation and
configuration management now devops
Engineers focus on automating
infrastructure provisioning and managing
configurations using certain tools they
Define infrastructure as code enabling
efficient deployment and scaling of
resources
now as a devops engineer you have to
identify the infrastructure requirements
effectively now day offs Engineers work
closely with development teams to
understand the infrastructure
requirements of the application this
involves analyzing the needs of
applications in terms of Computer
Resources Storage security networking
and scalability by gathering all these
requirements devops engineer can ensure
that infrastructure is provisioned and
configured to meet the application
specific need and future growth write
infrastructure automation scripts and
templates now once the infrastructure
requirements are identified devops
Engineers use automation tools and
techniques to define the desired state
of infrastructure components the right
scripts and templates that specify how
the infrastructure should be provisioned
configured and managed
automate the provisioning configuration
and management of servers well devops
Engineers leverage infrastructure as
code or in short IAC principles to
automate the provisioning configuration
and management of servers networks and
other infrastructure resources they use
tools like ansemble chefs or puppet to
automate the deployment and
configuration of infrastructure
components and finally regularly update
infrastructure code now devops engineer
uses Version Control Systems like get to
track changes collaborate with team
members and manage different versions of
infrastructure code by regularly
updating and versioning infrastructure
code devops Engineers can easily track
and reverse changes whenever necessary
now third on the list we have continuous
integration and continuous deployment or
CI CD in short now devops Engineers are
responsible for establishing and
maintaining cicd pipelines which enable
developers to integrate code changes
seamlessly and deploy applications
rapidly
so for that they have to set up a
version control system now Version
Control System like git play a crucial
role in devops by providing a
centralized repository for managing code
and tracking changes setting up a
Version Control System involves creating
a repository initializing it with the
code and defining branching and merging
strategies
configure a build server now a build
server automates the process of
compiling testing and packaging
application code tools like Jenkins and
gitlab cicd allows you to Define build
pipelines that specify the steps to be
executed these pipelines typically
involve tasks such as pulling code from
repository compiling source code
generating artifacts and packaging the
application
next automate the deployment process now
automation of the deployment process is
crucial for achieving rapid and
consistent software releases
containerization tools like Docker
provide a lightweight and portable way
to package application and their
dependencies Docker containers can be
created and deployed consistently across
different environments ensuring
consistency between development testing
and production Define an inverse quality
Gates and monitor CI CD kpis quality
Gates ensure that the code meets
predefined quality standards before it
is promoted to the next stage of the
cicd pipeline automated testing
including unit test integrated test and
end-to-end test integration test and
end-to-end test help catch bugs and
validate the functionality of the
managing applications and finally
measuring K pairs of the CI CD pipeline
provide insights into its performance
and help identify areas for improvement
well next we have monitoring and
performance optimization now day of
Engineers monitor system performance
identify and
infrastructure and application Stacks
whenever necessary they Implement
monitoring tools to collect and analyze
metrics logs and traces so for that
select and configure monitoring tools
now monitoring tools like Prometheus or
grafina can be used to collect and
visualize these metrics allowing tapes
to identify bottlenecks or optimized
processes and enhance the overall
efficiency of the CI CD pipeline
also we have to collaborate with
development and operations team to
fine-tune application performance
so continuously optimizing the
infrastructure which will ensure High
availability scalability and reliability
of that application and finally we have
security and compliance now deox
Engineers play a critical role in
implementing security measures and
ensuring compliance with industry
standards and regulations they work
closely with security teams to Define
and Implement security controls
throughout the software delivery
pipeline so they have to continuously
collaborate with the security teams to
identify and Define security
requirements and controls and Implement
security measures such as vulnerability
scanning access management and secure
configuration they have to continuously
integrate security testing and code
analysis into the CI CD pipeline
and monitor for any sort of potential
security risks or breaches and respond
promptly to mitigate any identified
vulnerabilities
so these were some of the main or top
five roles and responsibilities of a
devops engineer talking about the salary
figures of a senior devops engineer
according to glassdo a senior devops
engineer working in the United States
earns a whooping salary of 178 362
dollars the same senior devops engineer
in India earns 18 lakh rupees annually
to sum it up as you progress from entry
level to mid level and eventually to
experience devop engineer your roles and
responsibilities evolved significantly
each level presents unique challenges
and opportunities for growth all
contributing to your journey as a
successful devops professional so
excited about the opportunities devops
offers great now let's talk about the
skills you will need to become a
successful devops engineer
coding and scripting strong knowledge of
programming languages like python Ruby
or JavaScript and scripting skills are
essential for Automation and Tool
development
system administration familiarity with
Linux Unix and Windows systems including
configuration and troubleshooting cloud
computing Proficiency in Cloud platforms
like AWS Azure or Google Cloud to deploy
and manage applications in the cloud
containerization and orchestration
understanding container Technologies
like Docker and container orchestration
tools like kubernetes is a must
continuous integration or deployment
experience with CI CD tools such as
Jenkins gitlab Ci or Circle CI to
automate the development workflow
infrastructure as code knowledge of IAC
tools like terraform or ansible to
manage infrastructure programmatically
monitoring and logging familiarity with
monitoring tools like promptials grafana
and logging Solutions like elk stack
acquiring these skills will not only
make you a valuable devops engineer but
will also open doors to exciting job
opportunities so to enroll in the
postgraduate program in devops today
click the link mentioned in the
description box below don't miss this
fantastic opportunity to invest in your
future let's take a minute to hear it
out from all Learners who have
experienced massive success in their
career through a postgraduate program in
devops hello everyone and welcome to
Simply learns YouTube channel you might
already know from the title and
thumbnail of the video that today we
will discuss the difference between
agile and devops but before that let's
take a step back and understand what
these two are and why it's important to
know these two methodologies and their
differences a software development
methodology is like a set of
instructions or guidelines that tell the
team and individual involved in the
software development how to build
software just like a set of instruction
helps the team make sure they do
everything right a software development
methodology provides a roadmap for a
building's software products it
involved in the software development
process from planning and designing to
testing and deployment a software
development methodology is like a set of
best practices that help team build high
quality software products in a timely
and efficient manner and agile and
devops are two of the mini software
development methodologies hope this
quick introduction helped you understand
why we need to understand the software
development methodologies and their
differences so let's quickly start with
our topic agile versus devops on that
note if you aspect to advance your
career in the devops field and secure
position in the top tire companies
consider enrolling in the post-graduate
program in devops and intensive training
course that equips you with the top
tools and skills developed in
collaboration with Celtic City yummy
this professional development program
ensures your expertise alliance with
industry standards other Innovative
Blended learning approach combines live
online devop certification classes with
interactive Labs providing valuable
hands-on experience also accelerate your
career growth with the highly regarded
certified fights Grand Master
certification gain the necessary
Knowledge and Skills to excel in agile
project management through simply
learn's comprehensive two-day CSM
training course led by experienced
instructors this course covers all
essential scrum Frameworks Concepts
preparing you thoroughly for the CSM
certification now let's begin with the
topic of agile versus devops Agilent
devops a two distinct approaches in
software development each with its focus
and objectives here is a detailed
comparison between agile and devops
first methodology agile software
development methodology emphasis
flexibility adaptability and iterative
development whereas devops is a cultural
and operational philosophy emphasizing
collaboration and integration between
development and operation teams next we
have objectives agile aims to deliver
high quality software through
incremental development collaboration
and continuous feedback whereas devops
aims to streamline software development
and deployment process enhance as
collaboration and Achieve faster time to
Market next let us see the focus of
agile and devops agile focuses on
effective project management closer
customer collaboration and delivering
working software in short iteration
devops focuses on automating process
continuous integration and delivery
infrastructure as code and improving
communication and collaboration between
teams next let us look at the agile and
devops tools first the popular agile
tools widely used in the industry are
zeera Trello Azure devops version 1 and
monday.com so this tools offers a range
of features to support agile practices
including backlog management Sprint
planning task tracking collaboration and
Reporting the popular devops tools
widely used in industry include Jenkins
Docker get ansible and kubernet this
tools are top rated and widely adapted
in various organization to streamline
development and operation process
improve efficiency and enable faster and
more Reliable Software delivery next we
have the key skill set practices first
agile methodologies a solid
understanding of agile principles
Frameworks methodologies and Proficiency
in agile practices enable effective
Planning iterative Development and
continuous improvements next iterative
and incremental development agile
professionals should be comfortable
working in short iterations and
delivering incremental value and next we
have adaptability and flexibility agile
professionals should be open to change
and adapt plans and prioritize as needed
some standard agile practices include
scrum can burn and extreme programming
some key skills and knowledge areas
associated with devops are Automation
and scripting so Proficiency in
scripting languages like bash python or
partial is essential for automating
routine tasks infrastructure
provisioning and deployment process next
CI CD pipelines setting up and managing
continuous integration and deployment
pipelines and then we have configuration
management managing and auto meeting the
configuration of systems and
applications and some more key skills
and knowledge area associated with
devops are infrastructure as code Cloud
platforms monitoring and logging and
many more and next let's have a look at
the benefits of agile and devops agile
enables faster response to change
improved customer satisfaction and early
and frequent delivery of working
software whereas devops facilitates
faster software delivery improved
quality through automating and testing
increased efficiency and better
alignment between development and
operation teams and next let's have a
look at some of the popular career
opportunities in agile and devops so in
agile include this scrum Master agile
coach product owner agile project
manager and many more and some popular
career opportunities in the devops
include devops engineer site reliability
engineer automation engineer Cloud
engineer and many more next let's have a
look at the salaries the average salary
for an agile software engineer in the
United States is around 175
000 annually in India it is 9 lakh per
annum the average salary for a devops
engineer in the United States is around
104 000 annually in India at a 7 lakhs
per annum and some notable companies
known for focusing on agile and
frequently hiring in this area are
Spotify Amazon Google IBM Microsoft and
many more some companies are actively
hiring and known for their prominency in
devops include Cisco Amazon Google
Infosys Microsoft and many more but
before we go on to see the differences
between the two phases let's first have
a look at the basic definitions of the
boat henceforth let's begin with what is
continuous delivery
continuous delivery is a software
engineering practice in which code
changes are prepared to be released to
the production it is to be kept in mind
that the codes are to be passed through
the automated unit testing integration
testing and system testing before being
pushed to production
the transition between the continuous
integration phase and the continuous
delivery phase is automatically
completed which includes automated
testing at unit integration and system
levels before continuing with continuous
delivery let me briefly explain what is
continuous integration continuous
integration as the name suggests is a
devops practice that allows the
developers to integrate their code
changes or to merge their code changes
in the central repository so that the
automated builds and tests can be run
let's come back to continuous delivery
the tests that happen in the integration
phase provide validation that enables
the developers to fix the bugs before
the public release the process of
continuous delivery also enables the
developer to release the software at any
rate of their own choice this means that
the decision of initiating the release
is made manually and after that decision
is made continuous delivery takes place
now moving on to the next topic we shall
see what is continuous deployment
continuous deployment is a further step
for continuous delivery and this step is
referred to be as the final stage in the
pipeline the phase of the step refers to
the automatic releasing of any developer
changes from the repository to the
production
the ultimate goal is to release a newer
version whenever the developer makes
changes and automatically get those
changes to the end users
in continuous deployment codes are run
in a simulated environment that ensures
that the best quality factor is taken
into consideration there is also a
feature of real-time tracking to keep a
track of arising issues so that they
could be resolved at the earliest now
when we know the definition of both
continuous delivery and continuous
deployment let's see the contrast
between the two we have drawn the
contrast between the two on the basis of
three major factors the three major
factors are the definition of the two
advantages that they both have and for
whom or what kind of organizations use
them
let's begin with the first Factor
the first Factor we have is the
definition continuous delivery is a
devops practice that looks forward
towards releasing the code changes to
the production after all the testing has
been performed whereas continuous
deployment is a divorce practice that
focuses on continuously deploying or
releasing the code changes into the
production environment
then the second factor is advantages
here we will see the advantages of the
two continuous delivery is known for
frequent releases while continuous
deployment is known for completion of
each deployment phase second Advantage
is continuous delivery is known to
complete the releases in smaller
segments while continuous deployment is
known for quick and more reliable
completion
the next advantage in our list is
continuous delivery is known for its
instant response to defects and bugs
whereas continuous deployment focuses on
automating the entire process the final
advantage of continuous delivery is that
it is known for its comfortable stable
and very reliable releases whereas
continuous deployment is popular for the
creation of fully automated CI CD
pipeline the third considerable factor
is for whom in this Factor we will see
what kind of organizations or companies
use these two practices continuous
delivery is used by organizations that
need to release new features on a
frequent schedule
on the other hand continuous deployment
is mainly used by organizations that
look forward to releases on a daily or
hourly basis the process of continuous
deployment ensures cross-department
coordination like development support
marketing business
Etc now when we know continuous
integration continuous delivery and
continuous deployment it is important
for us to understand how these three are
related to each other
in the diagram on the screen we can see
the process of integration delivery and
deployment in a simultaneous flow we can
see that first the build is initiated
which is then tested and in the next
step it is merged these three steps
together make continuous integration
then after these three steps comes the
next phase where the automatic release
to the repository takes place and this
is called The Continuous delivery phase
lastly the release reaches the phase
where it is continuously released to the
production and this step is known as The
Continuous deployment phase
now to understand these better we shall
see each of these phase one by one to
understand their relation with the CI CD
pipeline
the first phase in the cicd pipeline is
the continuous integration phase the
process refers to the integration of all
the code changes into a shared
repository continuously
the process of continuous integration
ensures that this code is tested and
Incorporated very smoothly
after the integration phase Comes The
Continuous delivery phase
as the name implies the phase looks
forward to delivering the changes being
made in the code after several
iterations and feedbacks in the
continuous delivery phase the team
finalizes what is to be deployed to the
customers and when after the delivery
phase comes the last phase that is the
continuous deployment phase
continuous deployment phase is the phase
that is completely free from Human
interference there is a common goal that
continuous delivery and continuous
deployment share that goal is to
automate the entire development process
these phases are sometimes combined to
each other to give the maximum
productivity
so by now I hope that you have
understood the relevance and importance
of all these three phases and also how
are these phases different yet related
with respect to each other this topic by
simply learn is going to explain you
that what exactly is all about the
continuous integration process
continuous deployment and continuous
delivery so let's talk about these
Concepts one by one so continuous
integration is in kind of a process
which is which comes up in the first
part right so first The Continuous
Integrations comes into the picture then
the continuous delivery and the
deployment comes into the part so the
main important component or the main
task which we primarily perform in the
continuous integration is all about the
build automations test executions and
merging in various branches so it's
nothing but the integration of the
source code from the each and individual
developers machines to a shared location
which we often call it as any kind of a
version control system so continuous
integration is integrating the source
code into the shared repositories and
then depending on the order triggers
once the changes are checked into the
Version Control System the automated
build test cases all these executions
can really happen now in the build
process and the test cases automation
it's going to validate that how the
source code is really working uh how the
compilation is going on how the test
cases are getting executed so all these
things are getting validated during the
execution of these things now once the
continuous integration is successful we
can not assure give a hundred percent
surety that the code base is working
fine because it's just taking care of
the compilation and the test cases there
could be a possibility that some kind of
Errors may also come up into the
production environment because those
test cases or those scenarios are not
being covered in the test cases so test
cases are very important aspect over
here because it's giving us the
authority whether we need to proceed
further or we need to hold there itself
so after the continuous integration the
next process which comes into the
picture is the automatic release to the
repository over here right so it's just
a kind of a mechanism where we will be
releasing the source code to the
artifacts we are uploading the artifacts
to the particular component so that kind
of automation is available but that is
also happening when we are saying that
yes the test cases execution is
successful and we have to proceed
further and at the end it's nothing but
the automated deployment to the
production environment which we call it
as a kind of continuous deployment over
here so these are the overall process
which we are looking forward so that we
will be able to have an end-to-end
automations going on over here with the
help of cicd automations
now continuous educations again the
build test merge these are the different
options which we do
so continuous integration is a kind of a
process a kind of a development process
which is being used by most of the
development teams nowadays that
eventually enables the developers to
merge their source code uh whatever the
changes or whatever the modifications
they are doing they can uh particular uh
you know like merge their source code
from their local systems to the
centralized repository so that after
that with the help of CI tools we can
actually have the build automations and
the test cases automation there so this
is the reason why we prefer the
continuous integration process because
it depends on the basis of our
modifications that how the whole build
and test cases automations can really
happen here now continuous integration
is considered as a very important
process because of the various reasons
let's talk about those reasons one by
one the very first one is that it avoids
the merge conflicts because if we
frequently sync back our source code
from our systems to the shared
repository and the same thing can be
done by other developers also So
eventually it definitely helps the
different developers to collaborate
their source code into a single shared
repository and that can really help you
to avoid the merge conflict situations
because you are always going to work on
the latest source code so the scope of
merge conflicts is not going to be there
it also helps us to decrease the need of
the code review because we are just
doing the collaboration frequently so we
don't require any person to be sitting
there and you know have uh to perform a
kind of a heavy duty code review because
uh that is something which you can
easily manage and we can work with that
so code review uh is something which we
can easily decrease with the help of
this continuous integration process
it also speeds up the development
process yes definitely because uh the
developers can easily collaborate with
each other so from collaboration
perspective also it's beneficial for us
to do and that can really help us to see
that how easily we can integrate our
source code and that differs or that
basically helps us to go for the overall
mechanism and you know going for a
profit automation process over here so
it also increases the overall
development process which needs to be
implemented during the CI process
next it also reduce the product backlog
because we are doing our changes
frequently into the repositories so uh
any kind of product backlog which is
there which is spending from long that
is also can be managed easily because
it's keep on you know reduced there so
we keep on implementing our changes we
keep on implementing uh merging us
particular changes to the shared
repository so that reduces the amount of
backlog which is available
now next thing is the continuous
delivery which is an automatic release
to the repositories here so what exactly
is in continuous delivery all about
so continuous delivery is a kind of a
practice that refers to the particular
uh building testing delivering
improvements to the software code so the
important part of continuous delivery is
that the code is always in the
Deployable state so we are saying that
we are done with the delivery we are
ready with the delivery and whichever
environment whichever particular server
you want us to deploy we are ready to do
that so that is where we call it is in
continuous delivery so we have our
artifacts ready and whenever is desired
whenever we feel that we want to do the
deployments we want to proceed further
we will be able to do that with the help
of this process so it's in kind of an
end-to-end process you do the builds you
do the test cases executions once the
test case execution is done once the
testing is done then you create and
artifacts you create a particular War
files jar files ER files whatever the
artifacts we have we prepare that
artifacts and then we deliver it to a
specific environment we say that yes
let's go with the delivery let's make it
into a particular state where we will be
able to deploy to pretty much any
environment which we feel as such over
here so that's a real Beauty about the
continuous delivery where we can
actually deliver it to whatever
environment we feel we want to deliver
now continuous delivery is uh also
something which is a very important
concept for most of the organizations so
uh each and every organizations are
actually looking forward to invest in
this one for a variety of regions
because uh it's something where we can
invest some of our particular efforts
budget money a lot of things are there
but the kind of benefits which we get in
the return that's what we are going to
talk about over here as such so let's go
with that so it provides the higher
quality of products yes the efforts are
there yes the investment is also there
but in the return the prime the very
important concept or very important
thing which we get is the high quality
of products here so the kind of quality
which we are getting with this one is
quite high and that's the biggest
benefit which we get when we talk about
any kind of continuous integration
continuous delivery or continuous
deployment here so the quality is always
Top Notch when we work of these
particular processes
it also helps us to go for a quicker and
less risky releases so that's another
benefit which we get this process
customers are also satisfied because
they feel that yes we are going with
this uh particular automated process
um within you know few days we are
getting the deliveries uh we don't have
to wait for months for getting our
delivery so that's the biggest benefit
which a customer usually gets when he
talks about the continuous delivery
and it's more efficiency is there and
the cost of Fitness of the team also so
from the cost and from the uh particular
efficiency all those parameters are
fully fulfilled when we talk about the
continuous delivery so this process
delivers a lot of benefits which can
eventually help us to go for an perfect
high quality product which we can
develop as such over here
now the next thing is the continuous
deployment which is an automatic
deployment to the production environment
so let's see that what exactly is in
continuous deployment all about here so
continuous deployment refers to the
final stage in the pipeline so which
means that the we are not only preparing
our artifacts we are not only doing the
uh particular compilations or test case
executions we are actually going one
step ahead and this one step ahead is
nothing but the deployment to the
production environment so this is what
we are typically doing when we talk
about the continuous deployment over
here that we are going to one step ahead
and that's where we are going for the
deployment of our particular artifact so
we are deploying to the production
environment so that our changes whatever
the changes we are trying to do we are
trying to make that should be deployed
to the production environment right now
so continuous deployment again is can be
deployed to a higher whatever the
organizations want to feel or want to
deploy they can actually go with that
and there are a lot of benefits actually
which we get when we talk about the
continuous deployment so let's see about
all these particular deployments one by
one so first of all the faster
development of the products is there so
we are looking forward that when we are
going for the automations when we are
going for this deployment so if we get a
kind of a faster uh you know development
process which is there so the products
are getting developed very quickly very
fast that's the biggest benefit which
you are getting when we talk about this
continuous deployment here and it's less
risky releases also there and it's easy
to fix problems whatever the automations
we are feeling we are trying to perform
that can definitely help us to go that
how we can automate and how we can go
for this uh particular releases over
here so these releases are very less
risky because we are doing all the
things automated there is not much of a
scope which is there for the manual
errors so usually manual errors is
something which is the biggest uh
problem or which is the biggest risk
when we go for any release but that is
something which is totally removed with
this automation here
now it also helps in continuous
Improvement in the quality so
continuously improve quality is also
getting improved as such here now let's
talk about that why exactly as when we
talk about at the industry level in the
market level also that the cicd is one
of the best practices now what is the
reason behind that why those are known
as the best development and devops
practices as such
now continuous integration and
continuous delivery are the best
practices as they create an effective
process the kind of integration and
delivering the source code these are the
prime things which we need to take care
when we talk about the development uh
teams over here so they want an
automated process first of all and they
also want to streamline process which
can you know take up the things which
should not fail it should be a Fail-Safe
process mechanism through which they can
release various kind of changes to the
production environment because
ultimately they are looking forward for
the deployment of the changes to the
production environment so how they are
going to do that that is where the CI CD
or the continuous integration and
continuous delivery helps on that part
and that's the reason why they can help
the teams the development teams to go
for an effective
process implementations when we go for
these implementations of these two
practices
small code changes can be easily made in
the software course so you don't require
much of the efforts if you are going for
even small changes in the source code
that can also be effectively deployed to
a production environment in a simple
straightforward and easy mechanism the
changes can go from the developer
machines to the production environment
that's the biggest benefit which we get
due to which we feel that yes these two
processes should be implemented into a
devops platform
right so CA and CD provides continuous
feedback from the customers and also
from the development teams the devops
teams so the increase of transparency is
also something which is important which
is implemented with these processes here
so the process enables faster release of
the product so we don't have to wait for
months to release the source code there
within a couple of hours within couple
of days we can actually release it to
the production environment provided we
do all these steps like build
compilations testing development uh
validation all these things will be done
in that duration of time so the failures
can be easily detected faster and we can
easily find out that what is the problem
with the code base what is the problem
the changes and we can even do the hot
fixes now hot fix is these kind of ad
hoc changes are also can be easily
deployed to a production environment
within just couple of hours there so so
if you are interested in taking your
career to the next level look no further
than a postgraduate program in devops
this comprehensive course is designed to
empower you with the skills and
knowledge needed to excel in the dynamic
world of devops this program offers over
50 hours of self-paced learning master
classes led by caltex ctme 20 plus real
life projects in integrated labs and the
opportunity to acquire 40 plus in-demand
skills and master 15 plus essential
tools top it all with Capstone project
spanning in three domains and you will
be well on your way to a successful
devops career
learning objectives by the end of this
lesson you will be able to describe the
importance of continuous integration and
continuous deployment list the features
of Jenkins and demonstrate their uses
list the features of Team City and
demonstrate their uses and select a
suitable tool for your organization
the overview and importance of
continuous integration and continuous
deployment
overview of continuous integration
continuous integration is a development
practice of combining code from a number
of different developers into a common
code base intended for deployment
each integration event is verified by
automated build and automated tests
there are many aspects to a continuous
integration also known as CI process
continuous integration is a development
practice of combining code from a number
of different developers into a common
code base intended for deployment
each integration event is verified by
automated build and automated tests
there are many aspects to a continuous
integration also known as CI process
these are develop and compile code
perform unit tests integrate code with
various types of databases perform
pre-production deployment activities for
example moving code to various types of
computing environments such as testing
and staging also part of a continuous
integration process is to perform
functional testing and apply labels
against release points in the code
repository additional activities in CI
are to generate reports and analyze the
code continuous integration is about
combining code from each developer into
a common deployment path automatically
with a high degree of accuracy
continuous integration is more than a
developer just creating some code and
committing it to a feature Branch he or
she is working on for automation to pick
up and deploy instead the developer
needs to make sure that he or she writes
unit tests that exercise each line of
code written
the purpose of unit testing is to prove
that the code that's written works the
expectation a standard feature of most
shops that practice continuous
integration is to have the process run
all the unit tests and the developers
work Branch before merging the code into
a common code base
if the unit tests don't pass in the
automated process the developer's code
is not merged into the common Branch
then the developers typically notified
either by email or by another
Communication channel that there's a
problem that needs to be corrected
overview of continuous deployment
continuous deployment is an extension of
continuous integration its purpose is to
reduce the time between a development
team writing code and then using it in
production
the benefits of continuous deployment
are not only that it provides faster
feedback for men users as each new
feature is released to production but it
also realizes a faster return on
investment for each feature as it gets
developed
for the most part continuous deployment
is a highly automated process that
reduces the need for human interaction
in the deployment process
scripts take over most of the work that
used to be done by humans previously
involved in the physical deployment of
code
not only does automation copy code from
one Computing environment to another the
scripts will actually create the
Computing environments that need to be
in place before you can move the code
along
automation reduces the time it takes to
get code from developer to end user and
increases the accuracy of the code
that's being produced
continuous deployment makes sure that
the right code is in the right place at
the right time
popular tools in continuous integration
and continuous deployment
the more common CI CD tools use in it
shop practicing devops are Jenkins
Travis CI bamboo teamcity and get lab
continuous integration with Jenkins
Jenkins is one of the more popular tools
in the devops landscape it's a powerful
framework that supports a broad plug-in
ecosystem instead of having to write
code on a line-by-line basis to extend
the power of Jenkins developers write
custom plugins to add new functionality
these plugins are highly reusable
so much so that a whole Marketplace has
evolved around the product there are now
over a thousand plugins available and
most of these are free
also Jenkins integrates with over a
hundred other devops tools such as
GitHub and testing tools from a variety
of vendors Jenkins allows devops
Personnel to reliably orchestrate the
release tool chain and it's designed to
be an end-to-end solution
in order to get full benefit from using
Jenkins aspiring devops Personnel need
to understand the standard phases in the
CI CD pipeline these phases are code and
commit build and configure scan and test
release and deploy
you're going to incorporate of a variety
of tools with Jenkins during each phase
of the process
during code and commit developers use an
integrated development environment also
known as an IDE to create code the
popular ones are visual studio Visual
Studio code and eclipse and then they'll
check the code into a Version Control
System such as get GitHub perforce or
mercurial
some code might require binary
dependencies that are stored in an
artifact repository such as artifactory
after code is created and committed it
needs to be built
if the code requires compilation into a
binary file such as a dll exe or jar
file Jenkins will use a build system
such as Ms build a c compiler Maven for
Java or and for more general purposes if
the code needs to be containerized
you'll use docker
some it shops will automatically create
Computing environments during building
configure in anticipation of impending
code deployments
tools such as puppet chef and ansible
can provision virtual machines
automatically according to predefined
configurations
and during this scan and testing phase
you'll integrate Jenkins with testing
tools such as junit jmeter cucumber or
Ms test front-end testing can be
conducted using selenium appium sauce
lab or Hewlett Packard products
the purpose of scan and test is to make
sure the code meets quality standards
and is safe to move forward in the CI CD
process
the release phase is the last stage
before code is released to users
you might integrate Jenkins with tools
such as you deploy Serena midvision or
Excel release finally for deploy you'll
go on premises to a cloud service or a
hybrid combination of both a private
on-premises cloud and a public cloud
for public Cloud deployments you'll
Target Amazon web services Google Cloud
Azure or IBM Cloud also we're seeing
Cloud orchestration Technologies such as
kubernetes take an increased presence as
a release platform
for a private Cloud there's openstack
if you're working directly with virtual
machines you'll integrate Jenkins with
VMware products or Docker at a high
level in terms of application run times
you'll need something like the JBoss
Java virtual machine or the websphere
application server
Jenkins provides operational consistency
in the various stages of release
regardless of the particular products
you plan to use thus you'll do well to
keep these phases of deployment in mind
as you continue on your journey to
devops mastery
continuous deployment with Jenkins as
you can see in the slide continuous
deployment which we abbreviate a CD
consists of five operational stages the
first stage is coding which is typically
done on the developers local workstation
the next stage is storage during the
storage phase a developer commits his or
her code to a source control management
system such as get moving the code into
a separate test Computing environment
ensuring that the code is implemented in
the best way possible in addition to
ensuring that the code Works according
to expectation so that it can be
released to production is done in the
quality assurance and testing stages
production is the final stage in the
release process production is where the
code is made available to end users
the value that Jenkins brings to the
software development life cycle is that
it can be configured to automate every
stage of the entire process this is very
important because it not only speeds up
the software development process overall
but also increases the accuracy and
reliability of the code under
development companies such as LinkedIn
Google and Facebook have made continuous
deployment a fundamental part of the way
they make software
it's only by using an automation tool
such as Jenkins that effective
continuous deployment is possible
doing deployment manually is time
consuming and costly those who adhere to
the principles of devops understand that
continuous deployment is an essential
part of the practice of software
development in the modern Enterprise and
automation is key to the practice
continuous integration with Team City
team city is a free continuous
integration tool from jetbrains the
company is best known for its integrated
development environments for writing
software
the abbreviation for integrated
development environment is IDE
team city is a complete continuous
deployment continuous integration system
designed to automatically manage all
aspects of the software development life
cycle
team city is designed to integrate with
all the popular products used in modern
software development it integrates well
with the standard web browsers as we've
learned previously communication among
and between teams is important thus
teamcity can be configured to send out
notifications using email RSS chat tools
and social media platforms such as Slack
it supports Version Control by
integrating with Source Control
Management systems such as get
subversion per force as well as many
others teamcity supports all the build
tools and development Frameworks
commonly found in the Enterprise such as
ant Maven Ms building grunt also
teamcity can be integrated with popular
Ides such as eclipse the various
jetbrain tools such as IntelliJ webstorm
and pycharm
teamcity also integrates easily with the
industry standard for developing
Microsoft Technologies visual studio and
visual studio code
continuous deployment with Team City
a good way to understand the workflow
that team city is designed to manage by
way of automation is to conceptualize
the software development process as
having three areas of activity artifact
deployment and operations
let's take a look at each area in detail
artifact activity is about managing the
various code components that make up a
software system
some shops use the term deployment unit
to describe the distinct component of
code that needs to be released into
production
other shops use the term deployment
artifact
some of these components might be a
binary file such as Java or a jar file
or in.net shops the artifacts can be an
exe or a dll file
Linux doesn't have the concept of file
extension based executables thus any
file can be an executable you just need
to have the right permission set
the important thing to understand about
the artifact area is that it's about
creating and managing deployment units
that will be released to production
deployment
once an artifact or a collection of
artifacts are ready for release they
need to go through a company's
deployment process there are a number of
tools and processes associated with
deployment activity the ultimate goal of
the deployment activity is to make sure
that working valuable software gets to
the customer as quickly as possible
typically companies will use a ticketing
system to control and monitor the work
that needs to be done in deployment
artifacts will be prepared for
deployment
Computing instances will be provisioned
sometimes the provisioning might involve
creating virtual Computing instances
such as a virtual machine or container
other times it might require installing
physical Hardware such as computers and
networking devices then after
provisioning is complete the deployment
artifacts are configured and pushed into
relevant Computing environments
and finally we get to operations
it's important to understand that a
comprehensive CI CD system such as team
City goes beyond just deployment
activity team City will also automate a
lot of operations activity
release Personnel can use team City
features to orchestrate Computing
environments and monitor activity in
those environments also it coordinates
important aspects of Enterprise level
devops such as logging and aggregating
security alerts
security is a critical aspect of any
Enterprise's operation and is of
particular concern to any devops
practitioner working within the
Enterprise
overview and features of Jenkins
Jenkins as a continuous integration tool
Jenkins is a Java based open source
automation tool it functions as a server
and is a software development and
cross-platform tool used for continuous
integration and continuous deployment
let's look at what Jenkins can do as a
continuous integration server it can be
used as a CI server or a continuous
delivery hub for a project
in terms of distribution it can easily
distribute work across different
machines and help trigger builds test
and deployments to multiple machines and
platforms quickly
Jenkins is designed to work
cross-platform
it allows programming and software
development environments such as
ios.net Android Ruby and Java
architecture of Jenkins
Jenkins is a comprehensive CI CD
framework that is designed to be
flexible and extensible it has a
distinct object model that has classes
like project and build these classes
describe the various components of the
framework it uses jelly as the view
technology
it uses the file system to store its
data directories are created at the
location on disk defined by the
environmental variable Jenkins
underscore home
it supports plugins which are plugged
into those extension points and extend
the capabilities of Jenkins
popular features of Jenkins let's take a
look at the features
one Jenkins is platform independent
two it has a rich plug-in system
three it has the support of a large
community of third-party developers and
expert users
four it's designed to scale to meet the
needs of the large Enterprise
5. it's automation capabilities enable
immediate detection and resolution of
issues
6. it's open source and user friendly
and seven it's easy to configure modify
and extend
build status and job health
every time a Jenkins job runs the status
of the build gets reported
the figure on the left side of the slide
shows the results of a few runs of a
build project
notice that the result of each build is
reported
sometimes the build succeeds other times
it fails if an administrator aborts or
disables the build
that gets reported too
the figure on the right side of the
slide provides a description of the
overall health of the project according
to many builds
assisted practice set up Jenkins
let's take a look at the assisted
practice
you are given a project to install and
configure Jenkins on the Ubuntu
operating system of your learning lab
virtual machine
lab 3.1 installing and configuring
Jenkins in this demo we're going to
describe three steps to install and
configure Jenkins
first we're going to modify the file
sources.list
then we're going to use app get to
install the Jenkins package and then
finally we're going to run Jenkins in
the browser
so let's get started
first at the command line we type mate
dir lab 3.1
then the command line we type CD lab 3.1
to navigate into the directory we've
just created
now we need to add the key from the
Jenkins package repository into our
local environment
at the command line we type
wget
Dash Q Dash o
https
colon slash slash
package.jenkins dot IO slash Debian Dash
stable slash Jenkins dot IO Dash key
type
sudo app dash key add Dash and this will
add the key directly into the app get
file system
now we need to update the file
sources.list
we'll use the vi editor that's already
installed in your classroom virtual
machine
at the command line we type sudo space
VI space forward slash
Etc forward slash apt forward slash
sources dot list
sources.list is the file that act get
uses to locate package repositories on
the internet
we strike the I key on the keyboard to
put VI into insert mode
we scroll down to an empty space in VI
and type Deb https colon forward slash
forward slash
pkg.jenkins.io forward slash Debian Dash
stable space
binary forward slash
then we hit the Escape key to get out of
insert mode and type colon
W for write Q to quit fi
now we have to update the package
manager
at the command line we type sudo apt
Dash get Space update
update executes
once we see the line reading package
lists dot dot dot done the update is
complete
now we need to see if Java is installed
and if not we need to install it
at the command line we type Java space
Dash version
as you can see Java and the Java
development kit is already installed
if Java was not installed you would type
sudo space
apt-get space install space o p e n
jdk-7 Dash j r e
this command will execute app get to
install the openjdk package
now we install Jenkins type sudo space
app dash get Space install space Jenkins
strike the y key to confirm installation
the installation process will take a few
seconds
now we're going to open the initial
installation of Jenkins in a browser
we go back to the virtual machine
desktop
in this case we'll load the Firefox
browser we're going to enter the IP
address of the Jenkins installation
the IP address was assigned to the
virtual machine earlier
in the address bar of the Firefox
browser we enter the IP address
172.31.26.234 colon 8080 8080 is the
port where the Jenkins server is running
please be advised the IP address of your
virtual machine will be different and
the way you can discover the IP address
is at the command line type if config
the IP address of your virtual machine
will be in the information displayed
the first screen we're shown is the
unlock Jenkins screen
we're going to need to enter the
administrator password in order to
continue
as you can see in the red type the
location of the password is a file slash
VAR slash live Jenkins slash Secrets
slash initial admin password
where initial admin password is the name
of the file that contains the password
we return to the terminal
we type clear to clear the screen
at the command line we type sudo space
cat space forward slash VAR forward
slash live forward slash Jenkins forward
slash Secrets forward slash initial
admin password
we strike the enter key to display the
password
we highlight the password in the
terminal
and then copy it by right clicking on
the mouse and selecting copy
we return to the browser and paste the
password into the administrator password
text box then we click the continue
button
Firefox will offer to save the password
but will click don't save
we have the option to install suggested
plugins or we can select specific
plugins to install
we're going to go with the install
suggested plugins method
we click the Box labeled install
suggested plugins
the getting started page appears and the
plugin installation continues
once downloading and installing the
plugins is complete you'll be presented
with the create first admin user page
at this point we've successfully
installed Jenkins now the next steps
will be to create the admin user by
putting in a username password
confirmation of password a full name for
the user and the user's email address
let's recap the important steps
assisted practice guidelines to install
and configure Jenkins
log into Ubuntu lab provided with the
course
open the terminal and execute the
command available in the lab document
3.1 to add the key to the system edit
the sources.list file and add the
command to the file and then save it
update the apt-get package install jdk 8
plus version
install Jenkins using App get package
navigate to the IP address of the
machine denoted as X point x point x
point x colon 8080 in the browser of
your virtual machine
get the password and enter it into the
Jenkins window
create a new role job in Jenkins explore
the freestyle project and build section
overview and the features of Team City
team city as a continuous integration
tool
teamcity is a Java based management and
continuous integration server its
licensed commercial software that's used
for continuous integration and
continuous deployment
teamcity supports a number of features
that are important to the continuous
integration process for example gated
commits ensure only working code is
committed to deployment branches and
Source control
team City supports integration with
various integrated development
environments such as eclipse and
IntelliJ
it has a build grid that describes build
activity also it has cross-platform
support for a variety of operating
systems and programming languages
also teamcity has seamless code
integration
teamcity workflow
team City supports the CI CD process
that has become the standard workflow
for many companies and software
development shops let's take a look at
the overall workflow as you can see in
the slide at point one the first step is
that a developer or tester checks code
in to the source control management
system
upon committed 0.2 a build trigger
message is sent to the teamcity server
then at point three teamcity works with
system unit testing to start the system
unit testing automation process if the
code passes the test artifact building
archiving and deployment tasks are
performed in an automated manner at 0.4
team City reports the status of all the
activities at 0.5
popular features of Team City
team city has many features that make
the CI CD process more efficient let's
take a look at the features
teamcity has pre-tested commit instant
notifications it has code coverage and
monitoring capabilities it provides a
comprehensive build infrastructure and
it has enhanced Version Control System
integration
teamcity makes it easy to verify code
and it offers configurable test reports
and finally teamcity supports managing
users and roles
assisted practice set up teamcity now
it's time to do an assisted practice to
set up team City
you're going to be given a project to
install and configure team City on the
Ubuntu operating system in your learning
lab virtual machine so let's get started
with lab 3.2 install and configure team
City
first we create a directory for the work
we type MK dir lab 3.2
then we navigate to that directory by
typing CD space lab 3.2
first we're going to download the
compressed gz file for team City
then we're going to extract the files
from the compressed file
we're going to move the extracted
directory that contains the files into
another location
we're going to start up teamcity
and then we're going to go to the
browser and register a user with
teamcity
let's download teamcity
we go to the desktop of our classroom
virtual machine and then we click on the
icon for Google Chrome
appears
in the Chrome address bar we type
www.jetbrains.com
forward slash team City forward slash
download
we click the download button
gz file downloads and the thank you page
appears
we return to the terminal
at the command line we type CD space
forward slash home
forward slash Ubuntu forward slash
Capital downloads forward slash
we strike the enter key this takes us to
the directory where the gz file has been
downloaded
at the command line we type LS to
confirm that the gz file is indeed in
the directory notice the file
teamcity-2018.2.1.tar.gz is listed
now we need to extract the files
at the command line we type tar space c
x v f space
teamcity Dash
2018.2.1.tar.gz and we hit the enter key
this will extract all the files into a
directory named Team City
now we need to move the directory team
City into the directory forward slash
opt forward slash jetbrains
we create a directory by typing sudo
space
mkdir space forward slash opt forward
slash Capital jet Capital brains
now we need to move the directory team
City into the directory forward slash
opt forward slash jetbrains
at the command line we type sudo space
MV Space Team City space forward slash
opt forward slash jetbrains forward
slash team City
we strike the enter key
now we need to give full read write and
execute permissions to the files in the
newly created directory team City
at the command line we type CD space
forward slash opt forward slash
jetbrains
we strike the enter key
then we type chmod space Dash capital r
777
teamcity forward slash this says to give
all rights to the directory and the
contents of the directory team City
we type LS just to make sure the
directory team city is indeed there
we type CD space teamcity forward slash
and hit the enter key to enter the
directory team City
then we type LS to take a look at the
contents of the directory team City
notice there is a bin directory
we navigate into the bin directory by
typing CD space bin and hit the enter
key
let's clear the screen to make things
easier to see
to invoke team City
we will execute the bash file dot
forward slash teamcity Dash server dot
sh space start
it's very important that you use the dot
forward slash characters
we hit the enter key and team city
starts up spawning team City and then
team City restarter running with
pid11618 and the PID the process
identifier will vary according to
systems
now team city is running we go back to
the browser
but before we do take notice of the IP
address of the virtual machine
which is shown on the command line on
the left hand side
the IP address of this virtual machine
is 172.31.26.234
in the address bar of the Chrome browser
we type
172.31
.26.234 colon
8111 which is the port on the IP address
where team city is running
the team City first start web page
appears
we click the proceed button
the team city is starting page appears
then we see the database connection
setup page
teamcity allows you to choose from a
variety of databases we can use postgres
MySQL Oracle or Ms SQL Server
we're going to use the internal database
teamcity creates the database
and initializes its server components
this phase of the setup can take a few
minutes
the page will reload automatically once
team city is through installing its
server components
we're almost done with Team cities
installation process then all we'll need
to do is to create an account in team
City
when the installation process is
complete we'll be presented with the
licensing agreement that we have to
agree to
we scroll to the end of the terms and
conditions
we click the check box for accept
license agreement
we're going to uncheck send anonymous
usage statistics
then we'll click the continue button
the web page create administrator
account appears
we enter the username simply learn
then we add a password and confirm the
password
finally we click the button create
account
we'll also save the username and
password information in Google Chrome
upon successfully entering a username
and password will be presented with the
my settings and tools webpage in team
City
the page allows us to access projects
and changes
and agents
as well as build cues
in the next demo we'll look at more
details
let's recap the important steps
guidelines to install and configure
teamcity
number one log into your Ubuntu lab
provided with the course number two
download team City from the official
site
three unzip the folder and install
teamcity
four provide the read write and execute
mode access to seam City
number five run teamcity at x dot x dot
x dot X colon 811 where x dot x dot x
dot X is the IP address of your local
machine or the machine where the server
is running
six create an account in team City and
add the basic details to complete the
setup process
seven explore options such as projects
changes agents and build queue build
tools and their uses
build tools
build tools are programs that automate
the creation of applications from source
code
automation tools allow the build process
to be more consistent
some common build tools are make bash
which ships with Linux and Apache Maven
for Java Grunt and gulp
popular features of build tools well we
can start with compiling to binary code
for languages such as c-sharp and Java
build tools will package a compiled
program for deployment
also it will run automated test cases
against the code the build tool will
deploy artifacts to production and also
generate documents for developers to use
for later reference
and finally a build tool will generate
release notes an overview of Apache ant
Apache ant is a Java library and command
line tool its aim is to provide
processes described in build files as
targets and extension points dependent
on each other let's take a look at some
of the distinct features
Apache and supplies a number of built-in
tasks that allow it to compile test and
run code
it's flexible and does not impose coding
conventions
it builds solutions by combining build
tools and dependencies with Apache ivy
it solves makes portability problem
and users can develop custom and lives
using Java and finally you can use ant
to Pilot any type of process
the limitations of Apache ants and
Apache and there are a few limitations
some of the limitations are number one
configuration error checking is limited
undefined properties are not raised as
errors but are left as unexpanded
references number two ant has limited
fault handling rules lazy property
evaluation is not supported thus you
need to be very exact when configuring
it it is not a forgiving environment
number three older tasks use default
values which are not consistent and can
cause problems you need to pay attention
changing defaults can break existing and
scripts and finally number four and
build files are complex and verbose as
they are hierarchical and partly ordered
an overview of Maven Apache Maven is a
comprehensive software project
management tool based on the project
object model also known as the Palm
Maven can manage a Project's build
reporting and documentation from a
central XML file
let's take a look at the key points of
Maven number one is backward
compatibility and auto parent versioning
the hierarchical structure of the Palm
XML file reduces the burden on Version
Control and Reconciliation of
dependencies
number two
may even supports model-based builds
number three Maven supports parallel
builds and better error Integrity
reporting you can kick off a number of
project builds that run asynchronously
number four release management and
distribution publication is built into
maven the project object model is
comprehensive and covers all aspects of
the software development life cycle from
development through testing and onto
release
number five instant access to the latest
features with less or no additional
configuration Maven allows artifact
Version Control that can be set to get
the latest release of a dependency thus
you can make sure the code is always up
to date
6. large and growing repositories there
is a complete ecosystem of Maven
artifact repositories that make it so
you have access to popular components
automatically
the maven repository founded https slash
slash
mavenrepository.com is a popular one
also a company can set up its own
private Repository
the drawbacks of Maven Maven does have
some drawbacks number one unable to
depend on outcome status Maven builds
can fail leaving nothing behind other
than error messages that can be very
hard to understand number two Maven is
verbose and complex it does take a while
to master the details of the platform
number three slow and partial Black Box
Maven does a lot of things automatically
and sometimes these black box activities
can be very opaque and finally number
four Maven can be unreliable when
integrated for use within the Eclipse
IDE
comparing Maven over ant
well Maven has better dependency
management it provides more powerful
builds
there's better collaboration and the
debugging capabilities are a bit better
than ant
there's more componentization in the
builds and there's reduced duplication
and finally the Palm provides a more
consistent project structure the project
object model also known as the palm
the project object model is an XML
representation of a maven project it
provides General configuration
information such as a Project's name its
owner and its dependencies on other
projects
the XML excerpt in the slide shows a
rudimentary implementation of maven's
project object model
the power the Palm XML needs to define
the group ID the artifact ID and version
the packaging needs to be declared too
the default deployment package is a jar
the project described in the XML file in
the slide has an artifact idea of
calculator you can think of the artifact
ID as the name of the artifact that's
going to be created the Palm shows a
version of 1.0 and it belongs to the
group with the identifier EU dot total
eclipse
you can think of the group ID as a way a
company organizes a number of artifacts
under a common name
overview of grunt grunt is a JavaScript
based task Runner which is used to
automate repetitive tasks in a workflow
can be used as a command line tool for
JavaScript objects
let's take a look at some of the key
points to understand about grunt
an in-grunt developers write grunt setup
files to ease workflow thus grunt speeds
up the development flow and enhances
performance
it helps with the automation of
repetitive tasks with less effort it
supports a small infrastructure which is
a best fit for new code bases grunt
minifies files such as HTML and CSS thus
reducing the potential problems with
network latency
front aims at reducing the chances of
Errors during repetitive tasks and it
includes built-in tasks to extend
functionality of plugins
currently grunt has over 4 000 plugins
and can be used in large production
sites
an overview of gulp
gulp is an open source JavaScript
toolkit used as a build system in
front-end web development it automates
time consuming and repetitive tasks
involved in development
some of the key features of grunt are
code minification and concatenation
usage of pure JavaScript code grunt
converts files created under less or SAS
and a CSS compilation process less which
stands for leaner style sheets is a
dynamic preprocessor style sheet
language that can be compiled into
cascading style sheets also known as CSS
and these style sheets are run on the
client side or the server side
SAS is a preprocessor scripting language
that is interpreted or compiled into
cascading style sheets grunt also
manages file manipulation in memory
some of the advantages of grunt are that
it's easy to code it's easy to test with
web apps and the plugins are simple to
use
some of the disadvantages of grunt are
there's a more than the usual number of
dependencies multiple tasks cannot be
performed in parallel and configuration
can become tedious assisted practice
continuous integration with Jenkins and
maven you are given a project to
configure in Jenkins you need to make
Jenkins Paul get commits and build a
project code using Maven on the Ubuntu
operating system on your practice lab VM
okay so let's get started with lab 3.3
continuous integration with Jenkins git
and maven
in this lab we're going to configure
Jenkins to support get and maven then
we're going to configure a maven project
and then finally we're going to build
the maven project using source code from
get
so in a web browser we open Jenkins and
what we do in our classroom virtual
machine as we go to the IP address and
in this case it's
172.31.26.234 colon 8080 and Jenkins
typically runs by default installation
we click the new item link which is on
the upper left of the Jenkins webpage
the new item web page appears
in the new project text box we enter the
project name in this case Java maven
then we select freestyle project
then scroll down the web then scroll
down the web page and click the OK
button on the left hand side
the web page for the newly created
project loads we add text to the
description text box this is a simple
project to test Jenkins
we go to the menu bar at the top of the
projects web page and we click the build
tab
we're taken to the build section of the
web page
we click the add Bill step button to
display a drop down of options
from that drop down list we select the
item execute shell
in the command text box we enter the
Linux command Echo space quotation mark
Hello space from space Jenkins quotation
we click the save button on the lower
left
now we'll do a test build to make sure
everything's working all right on the
left hand side of the project web page
we click the link build now
the build starts and when it completes
you'll see a blue circular icon in the
build history section of the project web
page on the left side and you can see
there's a build there and it's labeled
number one
to the right of build number one is a
drop down arrow which we click to get a
list of options and we select console
output
selecting console output will display in
the web page the build log activity
notice there's a line that says hello
Jenkins this is from the echo command we
entered in the Jenkins build
configuration let's develop the project
a little more what we need to do is to
add get integration and get the project
to work with maven
we go to the simply learned site on
GitHub and that's simply learned devops
official
and we go to the repository Java Maven
app
on the GitHub site we click the green
clone or download button to display the
URL that we can use to access
to access the Java Maven app repository
on GitHub
so we highlight the URL and copy it
we go back to the Jenkins webpage
in the Jenkins webpage on the menu bar
we click the tab for the Java Maven
project the Java Maven project appears
on the left side of the Project's web
page we click the configure hyperlink
this loads the Project's configuration
page in which we click the source code
management tab we're navigated to the
source code management section of the
configuration page
there we click the option get
we paste the URL of our simply learn
Java Maven app repository into the text
box repository URL
entering a valid URL will make the error
message please enter get repository
disappear
we're going to select some predefined
credentials from the credentials drop
down
we added those earlier but if you need
to add credentials in the future simply
click the add button and then select
Jenkins
now if you're using your own GitHub
repository you'll be presented with the
Jenkins credentials provider dialog
you're going to need to enter your
username and also your password
once we configure access to the get
repository we click the save button
now what we've done is we've effectively
given Jenkins the right to access our
GitHub account using our credentials in
other words Jenkins is impersonating us
after we save the get configuration
we'll be brought back to the projects
page we need to go back to Jenkins home
we click the Jenkins tab on the upper
left of the project page
this takes us back to the main Jenkins
dashboard page which we can think of as
home
now we click manage Jenkins on the left
side of the Jenkins home page
on the manage Jenkins page we select
Global tool configuration
the global tool configuration page
appears
we scroll down to the maven section of
the global tool configuration page
we click on a button add Maven to add a
maven configuration
we're going to give this installation a
name and we're going to call it local
maven
then Maven needs to know where its home
is on the classroom virtual machine
in the text box labeled Maven underscore
home we enter the value slash USR slash
share slash maven
an alternative method is to again click
on ADD Maven add a name this time maven
and then from the install from Apache
drop down select the version of Maven
that's on the machine
we've now let Jenkins know about Maven
so we can click the save button
we return to the manage Jenkins page
we need to return to the project
in the upper left hand corner of the
manage Jenkins page we click the Jenkins
button we're brought back to the Jenkins
main dashboard you can see Java Maven in
the list of projects on the main
dashboard when we hover over java Maven
we'll see a down arrow we click that
down arrow and some options appear we
select configure from the options drop
down
the Java Maven projects configuration
page appears we click the build tab in
the menu bar
you can see the echo command that we
created previously in the build section
we want to get rid of it so what we're
going to do is we're going to go to the
right side of the execute shell section
and click the red button with the X
label clicking the red button deletes
the execute shell command Echo from the
build section now we want to add a new
build step
we click the add build step button and
we notice that in other drop down
appears
from net drop down we select invoked top
level Maven targets
after we add the maven build task will
be presented with a maven version drop
down
this will contain the maven
configurations we created previously
we select one of the configurations in
this case we'll select maven
let's recap the important steps assisted
practice guidelines to configure Jenkins
and maven
Number One login to Ubuntu lab provided
with the course number two log into
Jenkins and create the first Jenkins job
number three install and configure Maven
number four configure Jenkins with Java
get and Maven number five create a
Jenkins job for your Maven build project
and run the project number six poll get
for commits and automatically trigger
the build number seven build a trigger
using the push mechanism instead of pull
number eight repeat step six and seven
multiple times to observe the results in
the console output section so if you are
interested in taking your career to the
next level look no further than a
postgraduate program in devops this
comprehensive course is designed to
empower you with the skills and
knowledge needed to excel in the dynamic
world of devops this program offers over
50 hours of self-paced learning master
classes led by Caltech ctme 20 plus real
life projects in integrated labs and the
opportunity to acquire 40 plus in demand
skills and master 15 plus essential
tools top it all with Capstone project
spanning in three domains and you will
be well on your way to a successful
devops career so what are we going to
cover today so we're going to introduce
the concept of Version Control that you
will use within your devops environment
then we'll talk about the different
tools are available in a distributed
Version Control System will highlight a
product called git which is typically
used for Version Control today and
you'll also go through what are the
differences between get and GitHub you
may have used GitHub in the past or
other products like getlab and we'll
explain what are the differences between
get and get and services such as GitHub
and gitlab will break out the
architecture of what a get process looks
like how do you go through and create
forks and clones how do you have
collaborators being added into your
projects how do you go through the
process of branching merging and
rebasing your project and what are the
list of commands that are available to
you in get finally I'll take you through
a demo on how you can actually run git
yourself and in this instance use the
software of git against a public service
such as GitHub all right let's talk a
little bit about Version Control Systems
so you may have already been using a
version control system within your
environment today you may have used
tools such as Microsoft's team
Foundation services but essentially the
use of a virgin control system allows
people to be able to have files that are
all stored in a single repository so if
you're working on developing a new
program such as a website or an
application you would store all of your
Version Control software in a single
repository now what happens is that if
somebody wants to make changes to the
code they would check out all of the
code in the repository to make the
changes and then there would be an
addendum added to that so there will be
the the version one changes that you had
then the person would then later on
check out that code and then be a
version 2 and added to that code and so
you keep adding on versions of that code
the bottom line is that eventually
you'll have people being able to use
your code and that your code will be
stored in a centralized location however
the challenge you're running is that
it's very difficult for large groups to
work simultaneously within a project the
benefits of a VCS system a Version
Control System demonstrates that you're
able to store multiple versions of a
solution in a single repository now
let's take a step at some of the
challenges that you have with
traditional Version Control Systems and
see how they can be addressed with
distributed Version Control so in a
distributed Version Control environment
what we're looking at is being able to
have the code shared across a team of
developers so if there are two or more
people working on a software package
they need to be able to effectively
share that code amongst themselves so
that they constantly are working on the
latest piece of code so a key part of a
distributed Version Control System
that's different to just a traditional
version control system is that all
developers have the entire code on their
local systems and they try and keep it
updated all the time it is the role of
the distributed VCS server to ensure
that each client and we have a developer
here and developer here and developer
here and each of those our clients have
the latest version of the software and
then that each person can then share the
software in a peer-to-peer like approach
so that as changes are being made into
the server of changes to the code then
those changes are then being
redistributed to all of the development
team the tool to be able to do an
effective distributed VCS environment is
get now you may remember that we
actually covered get in a previous video
and we'll reference that video for you
so we start off with our remote git
repository and people are making updates
to the copy of their code into a local
environment that local environment can
be updated manually and then
periodically pushed out to the kit
repository so you're always pushing out
the latest code that you've code changes
you've made into the repository and then
from the repository you're able to pull
back the latest updates and so you'll
get repository becomes the kind of the
center of the universe for you and then
updates are able to be pushed up and
pulled back from there what this allows
you to be able to accomplish is that
each person will always have the latest
version of the code so what is get get
is a distributed Version Control tool
used for source code management so
GitHub is the remote server for that
source code management and your
development team can connect their get
clients to that certain remote Hub
server now git is used to track the
changes of the source code and allows
large teams to work simultaneously with
each other it supports a non-linear
development because of thousands of
parallel branches and has the ability to
handle large projects efficiently so
let's talk a little bit about git versus
GitHub so get is a software tool whereas
GitHub is a service and I'll show you
how those two look in the moment you
install the software tool forget locally
on your system whereas GitHub because it
is a service it's actually hosted on a
website git is actually the software
that used to manage different versions
of source code whereas GitHub is used to
have a copy of the local repository
stored on the service on the website
itself git provides command line tools
that allow you to interact with your
files whereas GitHub has a graphical
interface that allows you to check in
and check out files so let me just show
you the two tools here so here I am at
the get website and this is the website
you would go to to download the latest
version of git and again git is a
software package that you install on
your computer that allows you to be able
to do Version Control in a peer-to-peer
environment for that peer-to-peer
environment to be successful however you
need to be able to store your files in a
server somewhere and typically a lot of
companies will use a service such as git
Hub as a way to be able to store your
files so git can communicate effectively
with GitHub there are actually many
different companies that provide similar
service to GitHub gitlab is another
popular service but you also find that
development tools such as Microsoft
Visual Studio are also incorporating git
commands into their tools so the latest
version of Visual Studio team Services
also provides this same ability but
GitHub it has to be remembered is a
place where we actually store our file
miles and can very easily create public
and shareable is a place where you can
store our files and create public
shareable projects you can come to
GitHub and you can do a search on
projects you can see at the moment I'm
doing a lot of work on blockchain but
you can actually search on the many
hundreds of projects here in fact I
think there's something like over a
hundred thousand projects being managed
on GitHub at the moment that number's
probably actually much larger than that
and so if you are working on a project I
would certainly encourage you to start
at GitHub to see if somebody's already
maybe done a prototype that they're
sharing or they have an open source
project that they want to share that's
already available and in GitHub
certainly if you're doing anything with
um Azure you'll find that there are
thousands 45
000 Azure projects currently being
worked on interestingly enough GitHub
was recently acquired by Microsoft and
Microsoft is fully embracing open source
Technologies so that that's essentially
the difference between get and GitHub
one is a piece of software and that's
get and one is a service that supports
the ability of using the software and
that's GitHub so let's dig deeper into
the actual git architecture itself so
the working directory is the folder
where you are currently working on your
get project and we'll do a demo later on
where you can actually see how we can
actually simulate each of these steps so
you start off with your working
directory where you store your files and
then you add your files to a staging
area where you are getting ready to
commit your files back to the main
branch on your git project you want to
push out all your changes to a local
repository after you've made your
changes and these will commit those
files and get them ready for
synchronization with the service and
will then push your services out to the
remote repository an example of a remote
repository would be GitHub later when
you want to update your code before you
write any more code you would pull the
latest changes from the remote
repository so that your copy of your
local software is always the latest
version of the software that the rest of
the team is working on one of the things
that you can do is as you're working on
new features within your project you can
create branches you can merge your
branches with the mainline code you can
do lots of really creative things that
ensure the that Aid the code remains at
very high quality and B that you're able
to seamlessly add in new features
without breaking the core code so let's
step through some of the concepts that
we have available and get so let's talk
about forking and cloning and kit so
both of these terms are quite old terms
when it comes to development but forking
is certainly a term that goes way way
back long before we had distribute CVS
systems such as the ones that we're
using with get to Fork a piece of
software is a particularly open source
project you would take the project and
create a copy of that project and but
then you would then associate a new team
and new people around that project so it
becomes a separate project in entirety a
clone and this is important when it
comes to working with get a clone is
identical with the same teams and same
structuring as the main project itself
so when you download the code you're
downloading exact copy of that code with
all the same security and access rights
as the main code and then you can then
check that code back in and potentially
your code because it is identical could
potentially become the mainline code in
the future now that typically doesn't
happen your changes are the ones that
merge into the main branch but also but
you do have that potential where your
code could become the main code with Git
You can also add collaborative is that
can work on the project which is
essential for projects where
particularly where you have large teams
and this works really well when you have
product teams where the teams themselves
are self-empowered you can do a concept
what's called branching in git and so
say for instance you are working on a
new feature that new feature and the
main version of the project have to
still work simultaneously so what you
can do is you can create a branch of
your code so you can actually work on
the new feature whereas the rest of the
team continue to work on the main branch
of the the project itself and then later
you can merge the two together pull from
remote is the concept of being able to
pull in Services software the team's
working on from a remote server and git
rebase is the concept of being able to
take a project and re-establish a new
start from the project so you may be
working a project where there have been
many branches and the team has been
working for for quite some time on
different areas and maybe you're kind of
losing control of what the true main
branch is you may choose to rebase your
project and what that means though is
that anybody that's working on a
separate Branch will not be able to
Branch their code back into the mainline
Branch so going through the process of a
get rebase essentially allows you to
create a new start for where you're
working on your project so let's go
through forks and clones so you want to
go through the process so you want to go
ahead and Fork the code that you're
working on so this is a scenario that
one of your team wants to go ahead and
add a new change to the project the team
member may say yeah go ahead and you
know create a separate Fork of the
actual project so what does that look
like so when you actually go ahead and
create a fork of the repository you
actually go and you can take the version
of the mainline Branch but then you take
it completely offline into a local
repository for you to be able to work
from and you can take the mainline code
and you can then work on a local version
of the code separate from the mainland
Branch it's now a separate Fork
collaborators is the ability to have
team members working on a project
together so if you know if someone is
working on a piece of code and they see
some errors in the code that you've
created none of us are perfect at
writing code I know I've certainly made
errors in my code it's great to have
other team members that have your bag
and can come in and check and see what
they can do to improve the code so to do
that you have to then add them as a
collaborator now you do that in GitHub
you can give them permission within
GitHub itself that's really easy to do
supervisual interface that allows you to
do the work quickly and easily and
depending on the type of permissions you
want to give them sometimes it can be
very limited permissions it may be just
to be able to read the files sometimes
it's being able to go in and make all
the changes you can go through through
all the different permission settings on
GitHub to actually see what you can do
but you'll be able to make changes so
that people can actually have access to
your repository and then you as a team
can then start working together on the
same code let's step through branching
and get so suppose you're working on an
application but you want to add in a new
feature and this is very typical within
a devops environment so to do that you
can create a new branch and build a new
feature on that Branch so here you have
your main application on what's known as
the master branch and then you can then
create a sub branch that runs in
parallel which has your feature you can
then develop your feature and then merge
it back into the master Branch at a
later point in time now the benefit you
have here is that by default we're all
working on the master Branch so we
always have the latest code the circles
that we have here on the screen showed
various different commits that have been
made so we can keep track of the master
branch and then the branches that have
come off which have the new features and
there can be many branches in git so git
keeps you the new features you're
working on in separate branches until
you're ready to merge them back in with
the main branch so let's talk a little
bit about that merge process so you're
starting with the master branch which is
the blue line here and then here we have
a separate parallel branch which has the
new features so if we're to look at this
process the base commit of feature B is
the branch f is what's going to merge
back into the master branch and it has
to be said there can be so many
Divergent branches but eventually you
want to have everything merge back into
the master Branch let's step through git
rebase so again we have a similar
situation where we have a branch that's
being worked in parallel to the master
branch and we want to do a get rebase so
we're at stage C and what we've decided
is that we want to reset the project so
that everything from here on out with
along the master branch is the standard
product however this means that any work
that's been done in parallel as a
separate Branch will be adding in new
features along this new rebased
environment now the benefit you have by
going through the rebase process is that
you're reducing the amount of storage
space that's required for when you have
so many branches it's a great way to
just reduce your total footprint for
your entire project so get rebase is the
process of combining a sequence of
commits to form a new base commit and
the prime reason for rebasing is to
maintain a linear project history when
you rebase it you unplug a branch and
re-plug it in on the tip of another
branch and usually you do that on the
master branch and that will then become
the new Master Branch the goal of basing
is to take all the commits from a
feature branch and put it together in a
single Master Branch it makes it the
project itself much easier to manage
let's talk a little bit about pull from
remote Suppose there are two developers
working together on an application the
concept of having a remote repository
allows the code to the two developers
will be actually then checking in their
code into a remote repository that
becomes a centralized location for them
to be able to store their code it
enables them to stay updated on the
recent changes to the repository because
they'll be able to pull the latest
changes from that remote repository so
that they are ensuring that as
developers they're always working on the
latest code so you could pull any
changes that you have made to your
thought remote repository to your local
repository the command to be able to do
that is written here and we'll go
through a demo of how to actually do
that command in a little bit good news
is if there are no changes you'll get a
notification saying that you're already
up to date and if there is a change it
will merge those changes to your local
repository and you get a list of the
changes that have been made remotely so
let's step through some of the commands
that we have in git so get init
initializes a local git repository on
your hard drive git adds one or more
files to your staging area git commit
Dash M commit message is a commit
changes the git command commits changes
to head up to the git command commits
changes to your local staging area get
status checks the status of your current
repository and lists the files you have
changed get lock provides a list of all
the commits made on your current Branch
get diff the user changes that you've
made to the file so you can actually
have files next to each other you can
actually see the differences between the
two files get push origin Branch name so
the name of your branch command will
push the branch to the the remote
repository so that others can use it and
this is what you would do at the end of
your project git config Dash Global
username will tell get Who You Are by
configuring the author name and we'll go
through that in a moment git config
Global user email will tell get the
author of by the email ID get clone
creates a git repository copy from a
remote Source get remote ad origin
server connects the local repository to
the remote server and adds the server to
be able to push to it git branch and
then the branch name will create a new
Branch for you to create a new feature
that you may be working on get checkout
and then the branch name will allow you
to switch from one branch to another
Branch git merge Branch name Will merge
a branch into the active Branch so if
you're working on a new feature you're
going to merge that into the main branch
a get rebates will reapply commits on
top of another base tip and get rebase
will reapply AI commits on top of
another base tip and these are just some
of the popular git commands there are
some more but you can certainly dig into
those as you're working through using
get so let's go ahead and run a demo
using get so now we are going to do a
demo using get on our local machine and
GitHub as the remote repository for this
to work I'm going to be using a couple
of tools first I'll have the deck open
as we've been using up to this point the
second is I'm going to have my terminal
window also available and let me bring
that over so you can actually see this
and the terminal window is actually
running git bash as the software in the
background which you'll need to download
and install you can also run get bash
locally on your Windows computer as well
and in addition I'll also have the
GitHub repository that we're using for
simply learn and already set up and
ready to go all right so let's get
started so the first thing we want to do
is create a local repository so let's go
ahead and do exactly that so the local
repository is going to reside in my
development folder that I have on my
local computer and for me to be able to
do that I need to create a drive in that
folder so I'm going to go ahead and
change the directory so I'm actually
going to be in that folder before I
actually create make the new folder so
I'm going to go ahead and change
directory
and now I'm in the development directory
I'm going to go ahead and create a new
folder
and let's go ahead and created a new
folder called hello world
I'm going to move my cursor so that I'm
actually in the hello world folder
and now that I'm in the hello world
folder I can now initialize this folder
as a git Repository
so I'm going to use the git command init
to initialize and let's go ahead and
initialize that folder so let's see
what's happened so here I have my Hello
wall folder that I've created and you'll
now see that we have a hidden folder in
there which is called dot get and we
expand that we can actually see all of
the different subfolders that git
repository will create so let's just
move that over a little bit so that we
can see the rest of the work
and now if we check on our folder here
we actually see this is users Matthew
development hello world dot get and that
matches up with hidden folder here
so we're going to go ahead and create a
file called readme.txt in our folder so
here is our hello world folder and I'm
going to go ahead and using my text
editor which happens to be Sublime
I'm going to create a file and it's
going to have in the text hello world
and I'm going to call this one
readme.txt
if I go to my Hello World folder you'll
see that we have the readme.txt file
actually in the folder what's
interesting is if I select the get
status command what it'll actually show
me is that this file has not yet been
added to the commits yet for this
project so even though the file is
actually in the folder it doesn't mean
that it's actually part of the project
for us to do that we actually have to go
and select
foreign
for us to actually commit the file we
have to go into our terminal window and
we can use the get status to actually
read the files that we have there so
let's go ahead and use the git status
command and it's going to tell us that
this file has not been committed you can
use this with any folder to see which
files and subfolders haven't been
committed and what we can now do is we
can go and actually add the readme file
so let's go ahead I'm just going to
select at git add so the git command is
ADD
readme.txt so that then adds that file
into our main project and we want to
then commit those files into the main
repositories history and so it's that do
that we'll have the the get command
commit and we'll do a message in that
commit and this one will be
first commit
and it has committed that project what's
interesting is we can now go back into
readme file and I can change this so we
can go hello get
get is a very popular
Version Control solution
and we'll
we'll save that now what we can do is we
can actually go and see if we have made
differences to the readme text so to do
that we'll use the disk command forget
so we do get
div
and it gives us two releases the first
is what the original text was which is
hello world and then what we have
afterwards is what is now the new text
in green which has replaced the original
text
so what we're going to do now is you
want to go ahead and create an account
on GitHub we already have one so what
we're going to do is we're going to
match the account from GitHub with our
local account so to do that we're going
to go ahead and say get config
and we're going to do Dash and it's
going to be a
globaluser.name and we'll put in our
username that we use for GitHub and this
instance we're using the simply learn
Dash
GitHub account name
and under the GitHub account you can go
ahead and create a new repository name
in this instance we called the
repository hello dash world
and what we want to do is connect the
local GitHub account with the remote
hello world.get account and we do that
by using this command from get which is
our remote connection and so let's go
ahead and type that in open this up so
we can see the whole thing so we can
type in git remote add origin https
GitHub
.com slash
simply learn
Dash GitHub and you have to get this
type in correctly when you're typing in
the location hello dash world dot get
that creates the connection to your
hello world account
and now we want to do is we want to push
the files to the remote location using
the get push command commit git push
origin
master
so we're going to go ahead and connect
to our local remote GitHub so I'm just
going to bring up my terminal window
again and so let's select get remote add
origin
and we'll connect to the remote location
github.com slash
simply learn
Dash GitHub
slash
hello dash world dot get
oh we actually have already connected so
we're connected to that successfully and
now we're going to push the master Gish
so get
push origin
master and everything is connected and
successful
and if we go out to GitHub now
we can actually see that our file was
updated just a few minutes ago
so what we can actually do now is we can
go and Fork a project from GitHub and
clone it locally so we're going to use
the fork tool that's actually available
on GitHub let me show you where that is
located and here is our branching tool
it's actually changed more recently with
a new UI interface
and once complete we'll be able to then
pull a copy of that to our account using
the fox new HTTP URL address
so let's go ahead and do that
so we're going to go ahead and create a
fork of our project now to do that you
would normally go in when you go into
your project you'll see that there are
Fork options in the top right hand
corner of the screen now right now I'm
actually logged in with the default
primary count for this project so I
can't actually Fork a project as I'm
working on the main branch however if I
come in with a separate ID and here I am
I have a different ID and so I'm
actually pretending I'm somebody else I
can actually come in and select the fork
option and create a fork of this project
and this will take just a few seconds to
actually create the fork
and there we are we have gone ahead and
created the fork
so you want to say clone or download
with this and so this is the I select
that'll actually give me the web address
I can actually show you what that looks
like I'll open up my text editor
that's the correct
I guess that is correct so I'm going to
copy that
and I can Fork the project locally and
clone it locally I can change the
directory so I can create a new
directory that I'm going to put my files
in and then post in that content into
that file so I can now actually have
multiple versions of the same code
running on my computer
I can then go into default content and
use the patchwork command
20
so I can create a copy of that code that
we've just created in the college that's
a clone and we can create a new folder
that we're actually putting the work in
and we could for whatever reason we
wanted to we could call this photo
Patchwork and that would be maybe a new
feature and then we can then paste in
the URL of the new directory that has
the forked work in it and now at this
point we've now pulled in and created a
clone of the original content
and so this allows us to go ahead and
Fork out all of the work for our project
onto our computer so we can then develop
our work separately
so now what we can actually do is we can
actually create a branch of the fork
that we've actually pulled in onto our
computer so we can actually then create
our own code that runs in that separate
branch
and so we want to check out um the the
branch and then push the origin Branch
down to our computer
this will give us the opportunity to
then add our collaborators so we can
actually then go over to GitHub and we
can actually come in and add in our
collaborators
and we'll do that under settings and
select collaborators and here we can
actually see we have different
collaborators that have been added into
the project and you can actually then
request people to be added via their
GitHub name or by email address
or by their full name
one of the things that you want to be
able to do is ensure that you're always
keeping the code that you're working on
fully up to date by pulling in all the
changes from your collaborators
you can create a new branch and then
make changes and merge it into the
master Branch now to do that you would
create a folder and then that folder in
this instance would be called test we
would then move our cursor into the
folder called test and then initialize
that folder so let's go ahead and do
that so let's call create a new folder
and we're going to first of all change
our root folder and we're going to go to
development
create a new folder
call it test and we're going to move
into the test folder and we will
initialize
that folder
and we're going to move some files into
that test folder
close one test one
and then we're going to do file save as
and this one's gonna be test
two
and now we're going to commit those
files
Okay add and then we'll use the dot to
pull in all files
and then git commit
m
files
edited
make sure I'm in the right folder here I
don't think I was
and now that I'm in the correct folder
let's go ahead and
and get commit
and it's gone ahead and added those
files and so we can see the two files
that were created have been added into
the master
and we can now go ahead and create a new
Branch we call this one get branch
test underscore
branch
and let's go ahead and create a third
file to go into that folder
this is
file three
into file save as we'll call this one
test three dot text
and we'll go ahead and add
that file I need to get add
test three Dot txt
and we're going to move from the master
Branch to the test run branch
kit
check
out test underscore
branch
I switched to the test branch
and we'll be able to list out all of the
files that are in the tech in that
Branch now
and we want to go through and merge the
files into one area so let's go ahead
and we'll do git merge test underscore
branch
as well we've already updated everything
so that's good so otherwise it would
tell us what we would be merging
and now all the files are merged
successfully into the master branch
there we go all merged together
fantastic
and so what we're going to do now is
move from Master Branch to test branch
so get
check out
test underscore branch
and we can modify the files the test3
file that we took out
and pull that file up
and we can
now modified
okay
and we can then
commit
that file
back
in and we've actually been able to then
commit the file with one changes and now
we've seen as the text free change that
was made
now we can now go through a process of
checking the file back in switching back
to the master branch and ensuring that
everything is in sync correctly
we may at one point want to rebase all
of the workers kind of a hard thing you
want to do but it will allow you to
allow for managing for changes in the
future so let's switch to it back to our
test branch which I think we're actually
on we're going to create two more files
let's go to our folder here and let's go
copy those
and that's created
we'll rename those tests
four
and
five
and so we now have additional files
and we're going to add those into our
branch that we're working on so we're
going to go and select get add Dash a
and we're going to commit those files
get
commit Dash a dash m
adding
two new files
and it's added in the two new files
so we have all of our files now we can
actually list them out and we have all
the files that are in the branch
and we'll switch them to our Master
Branch we want to rebase the master
so we do get rebase
master
and that will then give us the command
that everything is now completely up to
date
we can go
get
check out
Master to switch to the master account
this will allow us to then continue
through and rebase the test branch and
then list all the files that are all in
the same area
so let's go get rebase
test underscore
branch
and now we can list and there we have
all of our files listed incorrectly
if we talk in the literal sense Maven
means accumulator of knowledge Maven is
a very powerful project management tool
or we can call it a build tool that
helps building documenting and managing
a project
but before we move forward and dive deep
into the basics of Maven let's
understand what is meant by the term
build tool
a build tool takes care of everything
for building a project
it generates a source code generates
documentation from a source code it even
compiles the source code and packages
the compiled codes into jar of zip files
along with that the build tool also
installs the packaged code in local
repository server repository or Central
Repository
coming back to Maven it is written in
Java or C sharp and it is based on
Project object model or pom
again let's have a pause and understand
what is meant by this term project
object model
a project object model or pom is a
building block in maven
it is an XML file that contains
information about the project and
configuration details used by Maven to
build a project this file resides in the
base directory of the project as
pom.xml file
the pum contains information about the
project and various configuration
details
it also includes the goals and plugins
used by Maven in a project
Maven looks for the pom in the current
directory while executing a task or a
goal it reads the pom gets the needed
configuration information and then runs
the goal
coming back Mayville is used to building
and managing any Java based project
it simplifies the day-to-day work of
Java developers and helps them in their
projects
now when we know the basics of Maven
let's have a look at some reasons to
know why is Maven so popular and why are
we even talking about it so let's have a
look at the need for maven
Maven as by now we know is properly used
for Java based projects it helps in
downloading libraries or jar files used
in the project
to understand the part of why do we use
Maven or the need of Maven let's have a
look at some problems that may even
solve
the first problem is getting the jar
files in a project getting the right jar
files is a difficult task where there
could be conflicts in the versions of
the two separate packages however it
makes sure all the jar files are present
in its repositories and avoid any such
conflicting scenarios
the next problem it sorted was
downloading dependencies
we needed to visit the official website
of different software which could be a
tedious task
now instead of visiting individual
websites we could visit
mvnrepository.com which is a central
repository of the jar files
then Maven plays a vital role in the
creation of the right project structure
in servlets struts Etc
otherwise it won't be executed
then Maven also helps to build and
deploy the project so that it may work
properly
so the next point is what exactly Maven
does
it makes the building of the project
easy
the task of downloading the dependencies
and jar files that were to be done
manually can now be done automatically
all the information that is required to
build the project is readily available
now
finally mirin helps manage all the
processes such as building documenting
releasing and other methods that play an
integral part in managing a project
now when we know everything about Maven
let's look at some companies that use
maven
there are over 2000 companies that use
Maven today
the companies that use Maven are mostly
located in the United States and in the
computer science Industry
Maven is also used in Industries other
than computer science like information
technology and services financial
service banking hospital and care and
much more
some of the biggest corporations that
use Maven are as follows
first we have via varijo then comes
Accenture followed by JP Morgan Chase
and Company then comes craft base and
finally we have red hat the first topic
that is the birth of selenium
selenium was primarily created by Jason
Huggins in 2004. Jason an engineer at
thoughtwork was working on a web
application that needed to be tested
frequently he realized that the repeated
manual testing of the application is
becoming inefficient
so he created a JavaScript language that
automatically controlled the browser's
actions this program was named
JavaScript test Runner
after he realized that his idea of
automating the web applications has a
lot of potential he made the JavaScript
test Runner open source and it was later
renamed as selenium core
so we know that Jason was a person who
initially created selenium but then we
must also know that selenium is a
collection of different tools and since
there are different tools there will be
several developers too to be exact about
the number of different tools there are
four different tools that have their own
creators
let's have a look at all of them
the first tool is the selenium remote
control or the selenium RC that was
created by Paul Hammond
then comes selenium grade that was
developed by Patrick lightbody
the third tool is the selenium IDE that
was created by shinia kasathani and the
fourth tool that is the selenium web
driver was created by Simon Stewart
we shall be learning about all these
tools in great detail As you move forth
in our video
first let's have a look at what is
selenium
selenium is a very popular open source
tool that is used for automating the
test carried on the web browsers there
may be various programming languages
like Java C sharp python Etc that could
be used to create selenium test Scripts
this testing that is done using selenium
tool is referred to as selenium testing
we must understand that selenium allows
the testing of web applications only we
can neither test any computer software
nor any mobile application using
selenium
selenium is composed of several tools
with each having its own specific role
and serving its own testing needs
moving forward let's have a look at the
features of selenium which will help us
understand the reason behind its
widespread popularity
so here we have a set of features of
selenium
selenium is an open source and portable
framework that has a playback and record
feature it is one of the best
cloud-based testing platform and
supports various OS and languages
it can be integrated with several
testing Frameworks and supports parallel
test execution
we will be talking about all these
features in detail as we move forward
so here the first feature that we have
is open source and portable framework
this feature states that selenium is an
open source and portable framework for
testing web applications
in addition to that selenium commands
are categorized in terms of different
classes which make it easier to
understand and implement
the second feature is the playback and
record feature the feature states that
the deaths can be authorized without
learning a test scripting language with
the help of playback and record features
the next feature says that selenium is a
cloud-based testing platform
selenium is a leading cloud-based
testing platform that allows testers to
record their actions and Export them as
a reusable script with a simple to
understand and easy to use interface
moving forth the next feature states
that selenium supports various operating
systems browsers and programming
languages
the tool supports programming languages
such as c-sharp Java python PHP Ruby
Pearl and JavaScript
if you talk about the operating systems
then selenium supports operating systems
like Android iOS Windows Linux Mac and
Solaris
and the tool also supports various
browsers like Google Chrome Mozilla
Firefox Internet Explorer Edge Opera
Safari Etc
then the next feature we have is the
integration with testing Frameworks
selenium can be well integrated with
testing Frameworks like test NG for
application testing and generating
reports and also selenium can be
integrated with Frameworks like and and
Maven for source code compilation
the last feature in our list is the
parallel test execution selenium enables
parallel testing which reduces time and
increases the efficiency of the tests
selenium requires fewer resources as
compared to other automation testing
tools
now let's move further and get to know
different selenium tools by now we know
that there are four different tools that
come under the selenium suit
the four tools are selenium remote
control selenium grid selenium IDE and
selenium Webdriver
we shall have a look at these four Tools
in detail one after the another
beginning with selenium remote control
selenium remote control enables the
writing of automated web applications in
languages such as Java c-sharp Perl
Python and PHP to build complex tests
such as reading and writing files
querying a database and emailing the
test results
selenium RC was a sustainable project
for a very long time before selenium
Webdriver came into existence and hence
selenium RC is hardly in use today as
the Webdriver offers more powerful
functionalities
the second tool we shall see is the
selenium grid
selenium grid is a smart proxy server
that enables the running of tests in
parallel on several machines
this is made possible when the commands
are routed to the remote web browser
instances and one server acts as the Hub
The Hub here is responsible for
conducting several tests on multiple
machines
selenium grid makes cross browser
testing easy as a single test can be
carried on multiple machines and
browsers all together
making it easy to analyze and compare
the results
here there are two main components of
selenium grade Hub and the node
Hub is a server that accepts the access
request from the Webdriver client
routing the Json test commands to the
remote drivers on nodes and here the
node refers to the Remote device that
consists of a native OS and a remote web
driver
it receives the request from the Hub in
the form of a Json test commands and
executes them using the Webdriver moving
for the third tool is the selenium IDE
if you want to begin with this selenium
IDE it needs no additional setup except
installing the extension of your browser
it provides an easy to use tool that
gives instant feedbacks
selenium IDE records multiple locators
for each element it interacts with if
one locator fails during the playback
the others will be tried until one is
successful
the IDE makes the test debugging easier
with features like setting breakpoints
and pausing exceptions
through the use of the Run command you
can reuse one test case inside one
another
and also selenium IDE can be extended
through the use of plugins they can
introduce new commands to the IDE or
integrate with a third-party service
the last and the fourth tool in the
selenium suit is the selenium Webdriver
selenium Webdriver is the most critical
component of the selenium tools suit
that allows cross browser compatibility
testing
the Webdriver supports various operating
systems like Windows Mac Linux Unix Etc
it provides compatibility with a range
of languages including python Java and
Perl along with it it provides supports
different browsers like Chrome Firefox
Opera Safari and Internet Explorer
the selenium Webdriver completes the
execution of test scripts faster when
compared to other tools and also it
provides compatibility with iPhone
driver HTML unidriver and Android driver
now after you know about the tools the
question arises which tool to choose
so in the next topic we shall see
different factors on the basis of which
we may decide which tool would be more
suitable for us here we shall have a
look at the reasons why one should
choose that particular tool we shall
begin with selenium remote control
selenium remote control or selenium RC
should be chosen to design a test using
a more expressive language than Celanese
is a set of selenium commands that are
used to test our web applications
then the selenium RC might be chosen to
run tests against different browsers on
different operating systems the RC may
be chosen to deploy tests across
multiple environments using selenium
grid it helps in testing the
applications against a new browser that
supports JavaScript and web applications
with complex Ajax based scenarios
now the second tool for which we shall
see the reasons are selenium grid
selenium grid as we learned in the
reasons for selenium RC is used to run
selenium RC scripts in multiple browsers
and operating systems simultaneously
the grid is used to run a huge test suit
that needs to be completed at the
earliest possible time now comes the
third tool the third tool is selenium
IDE
Al IDE is used to learn about Concepts
on automated testing and selenium these
Concepts include selenis commands such
as type Open click and weight assert
verify Etc
the concepts also include locators such
as ID name XPath CSS selector
Etc
selenium IDE enables the execution of
customized JavaScript code using run
script and exporting test cases in
several different formats the IDE is
used to create tests with a limited
amount of knowledge in programming and
these test cases and test suits can be
exported later to RC or Webdriver now
finally let's have a look at the reasons
to choose a last tool that is selenium
Webdriver
selenium Webdriver uses a certain
programming language in designing a test
case the Webdriver is used to test
applications that are rich in Ajax based
functionalities execute tests on HTML
unit browser and create customized test
results so if you are interested in
taking your career to the next level
look no further than a postgraduate
program in devops this comprehensive
course is designed to empower you with
the skills and knowledge needed to excel
in the dynamic world of devops this
program offers over 50 hours of
self-paced learning master classes led
by caltex ctme 20 plus real life
projects in integrated labs and the
opportunity to acquire 40 plus in demand
skills and master 15 plus essential
tools top it all with Capstone project
spanning in three domains and you will
be well on your way to a successful
devops career so let's take a slow
scenario of developer and a tester
before you had the world of Docker a
developer would actually build their
code and then they send it to the tester
but then the code wouldn't work on their
system encoders will work on the other
system due to the differences in
computer environments so what could be
the solution to this well you could go
ahead and create a virtual machine to be
the same of the solution in both areas I
think Docker is an even better solution
so let's kind of break out what the main
big differences are between Docker and
virtual machines as you can see between
the left and the right hand side both
look to be very similar what you'll see
however is that on the docker side what
you'll see as a big difference is that
the guest OS for each container has been
illuminated Docker is inherently more
lightweight but provides the same
functionality as a virtual machine so
let's step through some of the pros and
cons cons of a virtual machine versus
Docker so first of all a virtual machine
occupies a lot more memory space on the
host machine in contrast Docker occupies
significantly less memory space the boot
up time between both is very different
Docker just boots up faster the
performance of the docker environment is
actually better and more consistent than
the virtual machine Docker is also very
easy to set up and very easy to scale
the efficiencies therefore are much
higher with a Docker environment versus
a virtual machine environment and you'll
find it is easier to Port Docker across
multiple platforms than a virtual
machine finally the space allocation
between Docker and a virtual machine is
significant when you don't have to
include the guest OS you're eliminating
a signal significant amount of space and
the dark environment is just inherently
smaller so after darker as a developer
you can build out your solution and send
it to a tester as long as we're all
running in the docker environment
everything will work just great so let's
step through what we're going to cover
in this presentation we're going to look
at the devops tools and where Docker
fits within that space we'll examine
what Docker actually is and how Docker
works and then finally we'll step
through the different components of the
docker environment so what is devops
devops is a collaboration between the
development team the operation team
allowing you to continuously deliver
Solutions and applications and services
that both delight and improve the
efficiency of your customers if you look
at the Venn diagram that we have here on
the left hand side we have development
on the right hand side we have operation
and then there's a crossover in the
middle and that's where the devops team
sits if we look at the areas of
integration between both groups
developers are really interested in
planning code building and testing and
operations want to be able to
efficiently deploy operate a monitor
when you can have both groups
interacting with each other on these
seven key and elements then you can have
the efficiencies of an excellent devops
team so planning and code base we use
tools like jit and gearer for building
we use Gradle and Maven testing we use
selenium the integration between Dev and
Ops is through tools such as Jenkins and
then the deployment operation is done
with tools such as Docker and Chef
finally nagias is used to monitor the
entire environment so let's step deeper
into what Docker actually is so Docker
is a tool which is used to automate the
deployment of applications in a
lightweight container so the application
can work efficiently in different
different environments now it's
important to note that the container is
actually a software package that
consists of all the dependencies
required to run the application so
multiple containers can run on the same
Hardware the containers are maintained
in isolated environments they're highly
productive and they're quick and easy to
configure so let's take an example of
what Docker is by using a house that may
be rented for someone using Airbnb so in
the house there are three rooms and only
one cupboard and kitchen and the problem
we have is that none of the guests are
really ready to share the cupboard and
kitchen because every individual has a
different preference when it comes to
how the cupboard should be stocked and
how the kitchen should be used this is
very similar to how we run software
applications today each of the
applications could end up using
different Frameworks so you may have a
framework such as rails perfect and
flask and you may want to have them
running for different applications for
different situations this is where
Docker will help you run the
applications with the suitable
Frameworks so let's go back to our
Airbnb example so we have three rooms
and a kitchen and cupboard how do we
resolve this issue well we put a kitchen
and cupboard in each room we can do the
same thing for computers Docker provides
the suitable Frameworks for each
different application and since every
application has a framework with a
suitable version this space could also
then be utilized for putting in software
and applications that are long and since
every application has its own framework
and suitable version the area that we
had previously stored for a framework
can be used for something else now we
can create a new application in this
instance a fourth application that uses
its own resources you know what with
these kinds of abilities to be able to
free up space on the the computer it's
no wonder Docker is the right choice so
let's take a closer look to how Docker
actually works so when we look at Docker
and we call something Docker we're
actually referring to the base engine
which actually is installed on the host
machine that has all the different
components that run your Docker
environment and if we look at the image
on the left hand side of the screen
you'll see that Docker has a client
server relationship there's a client
installed on the hardware there is a
client that contains the docker product
and then there is a server which
controls how that Docker client is
created the communication that goes back
and forth to be able to share the
knowledge on that Docker client
relationship it's done through a rest
API this is fantastic news because that
means that you can actually interface
and program that API so we look here in
the animation we see that the docker
client is constantly communicating back
to the server information about the info
infrastructure and it's using this rest
API as that Communication channel the
docker server then will check out the
requests and the interaction necessary
for it to be the docker Daemon which
runs on the server itself well then
check out the interaction and the
necessary operating system pieces needed
to be able to run the container okay so
that's just an overview of the docker
engine which is probably where you're
going to spend most of your time but
there are some other components that
form the infrastructure for Docker let's
dig into those a little bit deeper as
well so what we're going to do now is
break out the four main components that
comprise of the docker environment the
four components are as follows the
docker client and server which we've
already done a deeper dive on Docker
images Docker containers and the docker
registry so if we look at the structure
that we have here on the left hand side
you see the relationship between the
docker client and the the docker server
and then we have the rest API in between
now if we start digging into that rest
API particularly the relationship with
the docker Daemon on the server we
actually have our other elements that
form the different components of the
docker ecosystem so the docker client is
accessed from your terminal window so if
you are using Windows this can be
Powershell on Mac it's going to be your
terminal window and it allows you to run
the docker Daemon and the registry
service when you have your terminal
window open so you can actually use your
terminal window to create instructions
on how to build and run your Docker
images and containers if we look at the
images part of our registry here we
actually see that the image is really
just a template with the instructions
used for creating the containers which
you use within Docker the docker image
is built using a file called the docker
file and then once you've created that
Docker file you'll store that image in
the docker Hub or registry and that
allows other people to be able to access
the same structure of a Docker
environment that you've created the
syntax of creating the image is fairly
simple it's something that you'll be
able to get your arms around very
quickly and essentially what you're
doing is you're creating the option of a
new container you're identifying what
the image will look like what are the
commands that are needed and the
arguments for within those commands and
once you've done that you have a
definition for what your image will look
like so if we look here at what the
container itself looks like is that the
container is a standalone executable
package which includes applications and
their dependencies it's the instructions
for what your environment will look like
so you can be consistent in how that
environment is shared between multiple
developers testing units and other
people within your devops team now the
thing that's great about working with
Docker is that it's so lightweight that
you can actually run multiple Docker
containers in the same infrastructure
and share the same operating system this
is its strength it allows you to be able
to create those multiple environments
that you need for multiple projects that
you're working on interestingly though
within each container that container
creates an isolated area for the
applications to run so while you can run
multiple containers in an infrastructure
each of those containers are completely
isolated they're protected so that you
can actually control how your Solutions
work there now as a team you may start
off with one or two developers on your
team but when a project starts becoming
more important and you start adding in
more people to your team you may have 15
people that are offshore you may have 10
people that are local you may have 15
Consultants that are working on your
project you have a need for each of
those developers or each person on your
team to have access to that Docker image
and to get access to that image we use
the docker registry which is an Open
Source server-side service for hosting
and distributing the images that you
have defined you can also use Docker
itself as its own default registry and
Docker Hub now something that has to be
bear in mind though is that for publicly
shared images you may want to have your
own private images in which case you
would do that through your own registry
so once again public repositories can be
used to host the docker images which can
be accessed by anyone and I really
encourage you to go out to Docker and
see the other Docker images that have
been created because there may be tools
there that you can use to speed up your
own development environments now you
will also get to a point where you start
creating environments that are very
specific to the solutions that you are
building and when you get to that point
you'll likely want to create a private
repository so you're not sharing that
knowledge with the world in general now
the way in which you connect with the
docker registry is through simple pull
and push commands but you run through
terminal window to be able to get the
latest information so if you want to be
able to build your own container what
you'll start doing is using the pull
commands to actually pull the image from
the docker repository and the command
line for that is fairly simple in
terminal window you would write Docker
pull and then you put in the image name
and any tags associated with that image
and use the command pause so in your
terminal window you would actually use a
simple line of command once you've
actually connected to your Docker
environment and that command will be
Docker pull with the image name and any
Associated tags around that image what
that will then do is pull the image from
the docker repository whether that's a
public repository or a private one now
in Reverse if you want to be able to
update the docker image with new
information you do a push command where
you take the script that you've written
about the docker container that you
define and push it to the repository and
as you can imagine the commands for that
are also fairly simple in terminal
window you would write Docker push the
image name any Associated tags and then
that would then push that image to the
docker repository again either a public
or a private repository so if we recap
the docker file creates a Docker image
that's using the build commands Docker
image then contains all the information
necessary for you to be able to execute
the project using the docker image any
user can run the code in order to create
a Docker container and once a Docker
image is built it's uploaded to a
registry or to a Docker Hub where it can
be shared across your entire team and
from the docker Hub users can get access
to the docker image and build their own
new containers so let's have a look at
what we have in our current environment
so today when you actually have your
standard machine you have the
infrastructure you have the host
operating system and you have your
applications and then when you create a
virtual environment what you're actually
doing is you're actually creating
virtual machines but those virtual
machines actually are now sitting within
a hypervisor solution that sits still on
top of your host operating system and
infrastructure and with a Docker engine
what we're able to do is we're able to
actually reduce significantly the
different elements that you would
normally have within a virtualized
environment so we're able to get rid of
the the bins and the so we're able to
get rid of the guest OS and we're able
to eliminate the hypervisor environment
now this is really important as we
actually start working and creating
environments that are consistent because
we want to be able to make it so it's
really easy and stable for the
environment that you have within your
Dev and Ops environment now critical is
getting rid of that hypervisor element
it's just a lot of overhead so let's
have a look at a container as an example
so here we actually have a couple of
examples on the right hand side we have
different containers we have one
container that's running Apache Tomcat
in it with Java a second container is
running SQL server and microsoft.net
environment the third container is
running python with mySQL these are all
running just fine within the docker
engine and sitting on top of a host OS
which could be Linux it really could be
any host OS within a consistent
infrastructure and you're able to have a
solution that can be shared easily
amongst your teams so let's have a look
at an example that you'd have today if a
company is doing a traditional Java
application so you have your developers
working in JBoss on his system and he's
coding away and he has to get that code
over to a tester now what will happen is
that tester will then typically in your
traditional environment then have to
install JBoss on their machine and get
everything running and cool and
hopefully set up identically to the
developer chances are they probably
won't have it exactly the same but
they're trying to get it as close as
possible and then at some point point
you want to be able to test this within
your production environment so you send
it over to a system administrator who
would then also have to install JBoss on
their environment as well yeah this just
seems to be a whole lot of duplication
so when I go through the problem of
installing JBoss three times and this is
where things get really interesting
because the challenge you have today is
that it's very difficult to almost
impossible to have identical
environments if you're just installing
software locally on devices the
developers probably got a whole bunch of
development software they could be
conflicting with the JBoss environment
the tester has similar testing software
but probably doesn't have all the
development software and certainly the
system administrator won't have all the
tools of the developer and tester have
their own tools and so what you want to
be able to do is kind of get away from
The Challenge you have of having to do
local installations on three different
computers and in addition what you see
is that this uses up a lot of effort
because when you you having to install
software over and over again you just
keep repeating doing really basic
foundational challenges so this is where
Docker comes in and Docker is the tool
that allows you to be able to share
environments from one group to another
group without having to install software
locally on a device you install all of
the code into your Docker container and
simply share the container so in this
presentation we're going to go through a
few things we're going to cover what
Docker actually is and then we're going
to dig into the actual architecture of
Docker and kind of go through what
Docker container is and how to create a
Docker container and then we'll go out
through the benefits of using Docker
containers and then the commands and
finalize everything out with a brief
demo so what is darker so Docker is as
you'd expect because all the software
that we cover in this series is an open
source solution and it is a container
solution that allows you to be able to
containerize all of the necessary files
and applications needed to run the
solution you're building so you can
share it from different people in your
team whether it's a developer a tester
or system administrator and this allows
you to have a consistent environment
from one group to the next so let's kind
of dig into the architecture so you
understand why Docker runs effectively
so the docker architecture itself is
built up of two key elements there is
the docker client and then there is a
rest API connection to a Docker Daemon
which actually hosts the entire
environment within the docker host and
the docker demo you have your different
containers and each one has a link to a
Docker registry the docker client itself
is a rest service so as you'd expect a
rest API and that sends command line to
the docker Daemon through a terminal
window or command line interface window
and we'll go through some of these demos
later on so you can actually see how you
can actually interact with darker the
docker Dem then checks the request
against the docking components and then
performs the service that you're
requesting now the docker image itself
all it really is a collection of
instructions used to create container
and again this is consistent with all
the devops tools that we have the devops
tools that we're looking to use
throughout this series of videos are all
environments that can be scripted and
this is really important because it
allows you to be able to duplicate and
scale the environments that you want to
be able to build very quickly and
effectively the agile container itself
has all of the applications and the
dependencies of those applications in
one package you kind of think of it as a
really effective and efficient zip file
it's a little bit more than that but
it's one file that actually has
everything you need to be able to run
all of your Solutions the actual Docker
registry itself is an environment for
being able to host and distribute
different Docker images is among your
team so say for instance you had a team
of developers that were working on
multiple different solutions so say you
have a team of developers and you have
50 developers and they're working on
five different applications you can
actually have the applications
themselves the containers shared in the
docker registry so each of those teams
at any time check out and have the
latest container of that latest image of
the code that you're working on so let's
dig into what actually is in the
container so the important part of a
docking container is that it has
everything you need to be able to run
the application it's like a virtualized
environment it has all your Frameworks
and your libraries and it allows the
teams to be able to build out and run
exactly the right environment that the
developer intended what's interesting
though is the actual applications then
will run in isolation so they're not
impacting other applications that using
dependencies on other libraries or files
outside of the container because of the
architecture it really uses a lot less
space and because it's using less space
it's a much more lightweight
architecture so the files and the actual
folder itself is much smaller it's very
secure highly portable and the boot up
time is incredibly fast so let's
actually get into how you would actually
create a Docker container so the docket
container itself is actually built
through command line and it's built of a
file and Docker image so the actual
Docker file is a text file that contains
all the instructions that you would need
to be able to create that Docker image
and then we'll actually then create all
of the project code with inside of that
image then the image becomes the item
that you would share through the docker
registry you would then use the command
line and we'll do this later on select
Docker run and then the name of the
image to be able to easily and
effectively really run that image
locally and again once you've created
the document you can store that in the
docker registry making it available to
anybody within your network so something
to bear in mind is that Docker itself
has its own registry called Docker Hub
and that is a public registry so you can
actually go out and see other Docker
images that have been created and access
those images as your own company you may
want to have your own private repository
so you want to be able to go ahead and
either do that locally through your own
repository or you can actually get a
licensed version of Docker Hardware you
can actually then share those files now
something that's also very interesting
to know is that you can have multiple
versions of a Docker image so if you
have a different version control
different release versions and you want
to be able to test and write code for
those different release versions because
you may have different setups you can
certainly do that within your Docker
registry environment okay so let's go
ahead and we're going to create a Docker
image using some of our basic Docker
commands and so there are essentially
really you know kind of just two
commands that you're going to be looking
for one is the build command another one
is to actually put it into your registry
which is a push command so if you want
to get a image from a Docker registry
then you want to use the pull command
and a pull command simply pulls the
image from the registry and in this
example using ngi next as our registry
and we can actually then pull the image
down to our test environment on our
local machine so we're actually running
the container within our Docker
application on a local machine we're
able to then have the image run exactly
as it would in production and then you
can actually use the Run command to
actually use the docker image on your
local machine so just a you know a few
interesting tidbits about the docking
container once the container is created
a new layer is formed on top of the
docker image layer is called the
container layer each container has a
separate read write container layer and
any changes made in that docking
container is then reflected upon that
particular container layout and if you
want to delete the container layer the
container layer also gets deleted as
well so you know why would using Docker
and containers be of benefit to you well
you know some of the things that are
useful is that containers have no
external dependencies for the
applications they run once you actually
have the container running locally it
has everything it needs to be able to
run the application so there's no having
to install additional pieces of software
such as the example we gave with JBoss
at the beginning of the presentation now
the containers are really lightweight so
it makes it very easy to share the
containers amongst your teams whether
it's a developer whether it's a tester
whether it's somebody on your operations
environment it's really easy to share
those containers amongst your entire
team different data volumes can be
easily reused and shared among multiple
contain and again this is another big
benefit and this is a reflection of the
lightweight nature of your containers
the container itself also runs in
isolation which means that it is not
impacted by any dependencies you may
have on your own local environment so
it's a completely sandboxed environment
so some of the questions you might ask
us you know can you run multiple
containers together without the need to
start each one individually and you know
what yes you can with Docker compose
docking compose allows you to run
multiple containers in a single service
and again this is a reflection on the
lightweight nature of containers within
the docker environment so we're going to
end our presentation by looking at some
of the basic commands that you'd have
within Docker so we have here on the
left hand side we have a Docker
container and then the command for each
item we're actually going to go ahead
and use some of these commands and the
demo that we're going to do after this
presentation you'll see that in a moment
but just you know some of the basic
commands we have are committing the
docker image into the Container kill is
a you know standard kill command to you
know terminate one or more of the
running containers so they stop working
then restart those containers but
suddenly you can look at all the image
all of the commands here and try them
out for yourself so we're going to go
ahead and start a demo of how to use the
basic commands to run Docker so to do
this we're going to open up terminal
window or command line now depending
whether you're running Linux PC or Mac
and we're going to go ahead and the
first thing we want to do is see what
our Docker image lists are so we can go
sudo Docker images and this will give us
well first of all enter in our password
so let's go enter that in and this one
that'll give us a list of our Docker
images and here are the docker images I
have already been created in this system
and we can actually go ahead and
actually see the processes that are
actually running so I'm going to go
ahead and open up this window a little
bit more but this will show you the
actual processes and the containers that
we actually have and so on the far left
hand side you see under names we have
learn simply learn be unscore cool these
are all just different ones that we've
been working on so let's go ahead and
create a Docker image so I'm going to do
sudo
docker
run
Dash D Dash p
0.0.0.0 go on a t colon 80
Ubuntu and this will allow us to go
ahead and run an Ubuntu image and this
will run the latest image and what we
have here is a hash number and this hash
number is a unique name that defines the
container that we've just created and we
can go ahead and we can check to make
sure that the container actually is
present so we're going to do
pseudo docker.ps and this actually show
us down there so it's not in a running
state right now but that doesn't mean
that we don't have it so let's list out
all the containers that are both running
and in the exits see so let's do sucker
PS Dash a and this lists all the
containers that I have running on my
machine
and this shows all the ones that have
been in the running State and in the
exit State and here we see one that we
just created about a minute ago and it's
called learn
and these are all running Ubuntu and
this is the one that we had created just
a few seconds ago
let's open it up and
there we go so let's change that to that
new dock container to a running state so
scroll down and we're going to type sudo
docker
run
Dash it Dash Dash
name my
um so this is going to be the new
container name it's going to be my
Docker so this is how we name our Docker
environment
and we'll put in the image name which is
Ubuntu
and dash bin Dash Bash
and it's now in our root and we'll exit
out of that
so now we're going to go ahead and start
the new my Docker container so sudo
docker
start
my
and we'll get the container image which
will be my docker
my docker
return and that started that Docker
image and let's go ahead and check
against the other running Docker images
to make sure it's running correctly so
sudo docker PS
and there we are underneath name on the
right hand side you actually see my
Docker along with the other Docker
images that we created and it's been
running for 13 seconds
quite fast so we want to rename the
container let's use the command sudo
docker
rename we can take another Docker image
this so let's grab this one and we'll
put it in rename
and we'll rename I'm putting the old
container name which is image and then
we'll put in the new container name and
let's call it
purple
so now the container image that had
previously been called image is now
called Purple
so do sudo Docker PS
to list all of our Docker images
and if we scroll up and there there we
go purple
how easy is that to rename an image
and we can go ahead and use this command
if we want to stop container so we're
going to write sudo docker
stop
and then we'll have to put in the
container name
and we'll put in my Docker the container
that we originally created
and that image has now stopped
and let's go ahead and prove that we're
going to list out all the docker images
and what you see is that it's not listed
in the active images it's uh not on the
list on the far right hand side
but if we go ahead and we can list out
all of the docker images so you actually
see it's still there as an image it's
just not in an active State just what's
known as an exit state
so here we go
and there's my Docker it's in an exited
state so that happened 27 seconds ago
so if we want to to remove a container
we can use the following command
so sudo docker
RM
will remove
my docker
and that will remove it from the X hit
state
and we're going to go ahead and we're
going to double check that
and and yep
yep it's not not listed there on the
exit State anymore
it's gone
and there we go
there that's where it used to be all
right let's go back
so if we want to exit a container in the
running state so we do sudo kill and
then the name of the container
I think one of them is called yellow
let's just check and see if that's going
to kill it
oops no I guess we don't have one called
yellow so let's find out name of
container that we actually have
so pseudo Docker kill
oh we're gonna list out of the ones that
are running oh okay there we go now
yellow isn't in that list so let's take
I know let's take simply learn and so we
can actually go ahead and let's write
sudo Docker kill
simply learn
and that will actually kill an active
Docker container
boom there we go
and we list out all the active
containers you can actually see now that
they simply learn container is not
active anymore
and these are all the basic commands for
Docker container we're going to break up
this presentation into four key areas
we're going to talk about life before
kubernetes which some of you are
probably experiencing right now what is
kubernetes the benefits that kubernetes
brings to you particularly if you are
using containers in a devops environment
and then finally we're going to break
down the architecture and working
infrastructure for kubernetes so you
understand what's happening and why the
actions are happening the way that they
are so let's jump into our first section
of life before kubernetes so the way
that you have done work in the past or
you may be doing work right now is
really building out and deploying
Solutions into two distinct areas one is
a traditional deployment where you're
pushing out code to physical servers in
a Data Center and you're managing the
operating system and the code that's
actually running on each of those
servers another environment that you may
potentially be using is to Blind code
out to Virtual machines so let's go
through and look at the two different
types of deployments that you may be
experiencing when you have applications
running on multiple machines you run
into the potential risk that the setup
and configuration of each of those
machines isn't going to be consistent
and your code isn't going to work
effectively and there may be issues with
uptime and errors within the
infrastructure of your entire
environment there's going to be around
problems with resource allocation and
you're going to have error issues where
applications may be running effectively
and not not effectively and not load
balance effectively across the
environment the problem that you have
with this kind of infrastructure is that
it gets very expensive uh you can only
install one piece of software one
service on one piece of Hardware so your
Hardware is being massively
underutilized this is where virtual
machines have become really popular with
a virtual machine you're able to have
better resource utilizing Asian and
scalability a much less cost and this
allows you to be able to run multiple
virtual machines on a single piece of
Hardware the problem is is that VMS are
for virtual machines are not perfect
either some of the challenges you run
with VMS is that the actual hardware and
software need needed to manage the VM
environment can be expansive there are
security risks with virtual with VMS
there are security risks with VMS there
have been data breaches recorded about
solutions that run in virtualized
environments you also run into an issue
of availability and this is largely
because you can only have a finite
number of virtual machines running on a
piece of hardware and this results in
limitations and restrictions in the
types of environment you want to be
running and then finally setting up and
managing a virtualized environment is
time consuming it can take a lot of time
and it can also get very expensive so
how about kubernetes well kubernetes is
a tool that allows you to manage
containerized deployment of solutions an
inherently kubernetes is a tour that is
really a Next Level maturity of
deployment so if you can think of your
maturity curve as deploying code in
directly to Hardware in a Data Center
and then deploying your solutions to
Virtual machines the next evolution of
that deployment is to use containers and
kubernetes so let's kind of go through
and look at the differences between a
virtual machine and kubernetes we've got
a few here that we want to highlight and
you'll get an understanding of what the
differences are between the two so first
of all with virtual machines there is
inherently security risk and what you'll
find as we get dig through the
architecture later in the presentation
is that kubernetes is inherently secure
and this is largely because of the
Legacy code the legacy of kubernetes and
where it came from we will talk about
that in just a moment but kubernetes is
inherently secure virtual machines are
not easily portable now with that said
they they are technically portable
they're just not very easily portable
whereas with kubernetes it's working
with Docker container Solutions it is
extremely portable and it means that you
can actually spin up and spin down and
manage your infrastructure exactly the
way that you want it to be managed and
scale it on the demands of the customers
as they're coming in to use the solution
from a time consuming point of view
kubernetes is much less time consuming
than with a virtual machine a few other
areas that we want to kind of highlight
from differences virtual machines use
much less isolation when building out
the encapsulated environment then
kubernetes does for instance with a
virtual machine you have to run
hypervisor on top of the OS and hardware
and then inside side of the virtual
machine you also have to have the
operating system as well whereas in
contrast on a kubernetes environment
because it's leveraging a Docker
container and or container-like
Technologies it only has to have the OS
and the hardware and then inside of each
container it doesn't need to have that
additional OS layer it's able to inherit
what it needs to be able to run the
application this makes the whole
solution much more flexible and allows
you to run many more containers on a
piece of Hardware than versus running
virtual machines on a single piece of
Hardware so as we highlighted here VMS
are not as portable as kubernetes and
kubernetes is portable directly related
to the use of containerization and
because kubernetes is built on top of
containers it is much less time
consuming because you can actually
script and automatically allocate
resource to nodes within your kubernetes
environment because it allows the
infrastructure to run much more
effectively and much more efficiently so
this is why if we look at our evolution
of the land of time before kubernetes
while we are running into a solution
where kubernetes had to come about
because the demand for having more
highly scalable solutions that are more
efficient was just really a natural
evolution of this software deployment
model that started with pushing out code
to physical hardware and then pushing
code out to Virtual machines and then
needing to have a solution much more
sophisticated kubernetes would have come
about at some point in time I'm just
really glad it came back when it did so
what is kubernetes does this dig into
the history of kubernetes and how it
came about so in essence kubernetes is
an open source platform that allows you
to manage and deploy and maintain groups
of containers and the container is
something like Docker and if you're
developing code you're probably already
using Docker today consider kubernetes
as the tool that manages multiple Docker
environments together now we talk a lot
about DACA and as a container solution
with kubernetes the reality is is that
kubernetes can actually use other
container tools out there but Docker
just simply is the most popular
container out there both these tools are
open source that's why they're so
popular and they just allow you to be
able to have flexibility in being able
to scale up your Solutions and they were
designed for the post-digital world that
we live and exist in today so a little
bit of background a little bit of trivia
around uh kubernetes so kubernetes was
originally a successor to a project at
Google and the original project was
Google
Borg it does exactly what kubernetes
done does today but kubernetes was
Rewritten from the ground up and then
released as an open source project in
2014 so that people outside of Google
could take advantage of the power of
kubernetes containerization management
tools and today it is managed by the
cloud native Computing foundation and
there are many many companies that
support and manage kubernetes so for
instance if you're signing up for
Microsoft Azure AWS Google Cloud all of
them will leverage kubernetes and it's
just become the the de facto tool for
managing large groups of containers so
let's kind of Step through some of the
key benefits that you'd experience from
kubernetes and so we have nine key
benefits the first it is highly portable
it is 100 open source code and this
means that you can actually go ahead and
contribute to this code project if you
want to through GitHub the ability to
scale up the solution is incredible
um what's the the history of kubernetes
being part of a Google project for
managing the Google network and
infrastructure kind of really sets the
groundwork for having a surgeon that is
highly scalable the out of the high
scalability also comes the need for High
availability and this is the desire to
be able to have a highly efficient and
highly energized environment but also
you can really rely on so if you're
building outer kubernetes management
environment you know that it's going to
be available for the solutions that
you're maintaining and it's really
designed for deployment so you can
script out the environment and actually
have it as part of your devops model so
you can scale up and meet the demands of
your customer then what you'll find is
that the load balancing is extremely
efficient and it allows you to
distribute the load efficiently across
your entire network so your network
remains stable and then also the tool
allows you to manage the orchestration
of your storage so you can have local
storage such as an SSD on the hardware
that the kubernetes is maintaining or if
the kubernetes environment is pulling
storage from a public Cloud such as
Azure or AWS you can actually go ahead
and make that a available to your entire
system and you can inherit the security
that goes back and forth between the
cloud environments and one of the things
you'll find consistent with kubernetes
is that it is designed for a cloud-first
environment um kubernetes as well is
that it's it's really a self-healing
environment so if something happens or
something fails uh kubernetes will
detect that failure and then either
restart the process kill the process or
replace it and then because of that you
also have automated rollouts and
rollbacks in case you need to be able to
manage the state of the environment and
then finally you have automatic bin
packaging so you can actually specify
the compute power that's being used from
CPU and ramp for each container so let's
dig into the final area which is the
actual kubernetes architecture I'm going
to cover this at a high level there's
actually another video that you can that
simply learn has developed which digs
deeper into the kubernetes architecture
and so the kubernetes architecture is a
custard-based architecture and it's
really about two key areas you have the
kubernetes master which actually
controls
um all of the activities within your
entire kubernetes infrastructure and
then you have nodes that actually are
running on Linux machines
um outs that are controlled by the
master so let's kind of go through some
of these areas so if we look at the
kubernetes master to begin with
um we'll start with uh Etc is this is a
tool that allows for the configuration
and the information and the management
of nodes within your cluster and one of
the key features that you'll find with
all of the tools that are managed within
either a the master environment or
within a node is that they are all
accessible via the API server and what's
interesting about the API server is that
it's a restful based infrastructure
which means that you can actually secure
each connection with SSL and other of
security models to ensure that your
entire infrastructure and the
communication going back and forth
across your infrastructure is tightly
secured scheduler goes ahead and
actually as you'd expect actually
manages the schedule of activities
within the actual cluster and then you
have the control and the controller is a
Daemon server that actually manages and
pushes out the instructions to all of
your nodes so the other tools really are
the the infrastructure and you can
consider them the administration site
I'm of the master whereas control is the
management it actually pushes out all of
the controls via the API server so let's
actually dig into one of the actual
nodes themselves and there are three key
areas of the nodes one is the doctor
environment which actually helps and
manage and maintain the container that's
actually inside of the node and then you
have the Kubler which is responsible for
information that goes back and forth and
it's going to do most of the
conversation with the API server on the
actual health of that no mode and then
you have the actual kubernetes proxy
which actually runs the services
actually inside of the node so as you
see all of these infrastructures are
extremely lightweight and designed to be
very efficient and very available for
your infrastructure and so here's a
quick recap of the different tools that
are available and it really breaks down
into two key areas you have your
kubernetes master and the kubernetes
node now the kubernetes Mazda has the
instructions of what's going to happen
within your kubernetes infrastructure
and then it's going to push out those
instructors to an indefinite number of
nodes that will allow you to be able to
scale up and scale down your solution in
a dynamic way so let's have an overview
of the kubernetes architecture so
kubernetes is really broken up into
three key areas you have your
workstation where you develop your
commands and you push out those commands
to your master and the master is
comprised of four key areas which
essentially control all of your nodes
and the node contains multiple parts and
each part has your Docker container
built into it so consider that you could
have a really almost an infinite number
of PODS I'm sorry infinite number of
nodes are being managed by the master
environment so you have your cluster
which is a collection of servers that
maintain the availability and the
compute power such as RAM CPU and disk
utilization and you have the master
which is really components that control
and schedule the activities of your
network and then you have the node which
actually hosts the actual Docker virtual
machine itself and be able to actually
control and communicate back to the
master the health of that pod and we'll
get into more detail now later in the
presentation so you know you keep
hearing me talk about containers but
they really are the center of the work
that you're doing with kubernetes and
can the concept around kubernetes and
containers is really just a natural
evolution of where we've been with
internet and digital Technologies over
the last 10 15 years so before
kubernetes you had tools where you
either running virtual machines or
you're running data centers that had to
maintain and manage and notify you of
any interruptions in your network
kubernetes is the tool that actually
comes in and helps address those
interruptions and manages them for you
so the solution to this is the use of
containers so you can think of
containers as that Natural Evolution
from you know uh 15 20 years ago you
would have written your code and posted
it to a Data Center and more recently
you probably posted your code to a
virtual machine and then move the
virtual machine and now you actually
just work directly into a container and
everything is self-contained and can be
pushed out to your environment and the
thing that's great about containers
they're they're isolated environments
very easy for developers to work on them
but it's also really easy for or
operations teams to be able to move a
container into production so let's kind
of step and back and look at a competing
product to kubernetes which is Docker
swarm now one of the things we have to
remember is that Docker containers which
are extremely popular
um built by the company Docker and made
open source and Docker actually has
other products one of those other
products is Docker swarm and Docker
swarm is a tool that allows you to be
able to manage multiple containers so if
we look at some of the uh the benefits
of using Docker swarm versus kubernetes
now one thing that you'll find is that
both tools have strengths and weaknesses
but it's really good that they're both
out there because it helps keep they
really kind of justifies uh the
importance of having these kind of tools
so kubernetes was designed originally
from the ground up to be Auto scaling
whereas took a swarm isn't the load
balancing is automatic on Docker Swan
whereas with kubernetes you have to
manually figure load balancing across
your nodes the installation for Docker
swarm is really fast and easy I mean you
can be up and running within minutes
kubernetes takes a little bit more time
is a little bit more complicated
eventually you'll get there
um I mean it's not like it's going to
take you days and weeks but it's it is a
tool that's a when you compare the two
Docker swarms much easier to get up and
running now what's interesting is that
kubernetes is incredibly scalable and
it's you know that's it's real strength
is its ability to have strong clusters
whereas with Docker swarm it's cluster
stream isn't um as strong when compared
to kubernetes now you compare it to
anything else on the market it's really
good um so this is kind of a splitting
hairs kind of comparison but kubernetes
really does have the advantage here if
you're looking at the two compared to
each other for scalability I mean
kubernetes was designed for by Google to
scale up and support Google Cloud
Network infrastructure they both allow
you to be able to send share storage
volumes with Docker you can actually do
it with any container with that is
managed by the docker swamp whereas with
kubernetes it manages the storage with
the pods and our product can have
multiple containers within it but you
can't take it down to the level of the
container interestingly kubernetes does
have a graphical user interface for
being able to control and manage the
environment the reality however is that
you're likely to be using terminal to
actually make the controls and Commands
to control your either Docker swarm or
kubernetes environment and it's great
that it has a GUI and to get you started
but once you're up and running you're
going to be using terminal window for
those fast quick administrative controls
that you need to make so let's look at
the hardware components for kubernetes
so what's interesting is that kubernetes
is extremely light of all the systems
that we're looking at is extremely
lightweight
um it's allows you to have you compare
it to like a virtual machine which is
very heavy you know kubernetes is
extremely lightweight and other uses any
resources at all interesting enough
though is that if you are looking at the
usage of CPU it's better to actually
take it for
um the cluster as a whole rather than
individual nodes because the nodes will
actually combined together to give you
that whole compute power again this is
why kubernetes works really well in the
cloud where you can do that kind of
activity rather than if you're running
in your own data center
um so you can have persistent volumes um
such as a local SSD or you can actually
attach to a cloud data storage again
kubernetes is really designed for the
cloud I would encourage you to use cloud
storage wherever possible rather than
relying on physical storage and the
reason being is that if you connect to
cloud storage and you need to flex your
your storage the cloud will do that for
you I mean that's just an inherent part
of why you'd have storage whereas if
you're connecting to physical storage
you're always restricted to the
limitations of the physical Hardware so
that's um kind of pivot and look at the
software components as compared to the
hardware components so the main part of
the components is the actual container
and all of the software running in the
container runs on Linux so if you have
documents stored as a developer on your
machine it's actually running inside of
Linux and that's what makes it so
lightweight and really one of the things
that you'll find is that most data
centers and Cloud providers now are
running predominantly on Linux inside of
the um the the container itself is then
managed inside of a pod and a pod is
really just a group of containers
bundles together and the kubernetes
scheduler and proxy server then actually
manage what how the pods are actually
pushed out into your kubernetes
environment the the parts themselves can
actually then share resources both
networking and storage so the parts
aren't pushed out manually they're
actually managed through a layer of
abstraction and part of their deployment
and and this is the strength of
kubernetes you use to find your um
infrastructure and then kubernetes will
then manage it for you and there isn't
that problem of manual management of
PODS if you have to manage the
deployment of them and you that's simply
taken away and it's completely automated
and the the final area of software
Services is on Ingress and this is
really the secure way of being able to
have communication from outside of the
cluster and passing of information into
that cluster and again this is done
security through SSL layers and allows
you to ensure that security is at the
center of the work that you have within
your kubernetes environment so let's
dive now into the actual architecture
before we start looking at a use case of
how kubernetes is being employed so
kubernetes again is um we looked at this
uh diagram at the beginning of the
presentation and there were really three
key areas there's the workstation where
you develop your commands and then you
have your master environment which
controls the scheduling the
communication and the actual commands
that you have created and pushes those
out and manages the health of your
entire node Network and each node has
various pods so if we like break this
down so the master node is the most
vital component with the master you have
four key controls you have Etc the
controller manager schedule an API
server the cluster store Etc this
actually manages the details and values
that you've developed on your local
workstation and then we'll work with the
out the control schedule and API server
to communicate that out those
instructions of how your infrastructure
should look like to your entire network
the control manager is really an API
server and again this is all about
security so we use wrestle apis which
can be packaged in SSL to communicate
back and forth across your pods and the
master and indeed the services within
each of them as well so at every single
layer of abstraction the communication
is secure the schedule as the name would
imply really schedules when tasks get
sent out to the actual nodes themselves
the nodes themselves are are dumb nodes
they just have the applications um
running on them the master and the scum
is really doing all of the work to make
sure that your entire network is running
efficiently and then you have the API
server which has your rest commands and
the communication and back and forth
across your networks that is secure and
efficient so your node environment is
where all the work that you do with your
containers gets pushed out too so um a
work is really a it's a combination of
containers and each container will then
logically run together on that node so
you'd have a collection of containers on
a node that all make logical sense to
have together within each node you have
a Docker and this is your isolated
environment for running your container
you have your cubelet which is a service
for conveying information back and forth
to the service about the actual health
of the kubernetes node itself and then
finally you have the proxy server and
the proxy server is able to manage the
nodes the volumes the the creation of
new containers and actually helps pass
the community the the health of the
container back up to the master to see
whether or not the containers should be
either killed stop started or updated so
finally let's look at see where
kubernetes is being used by other
companies so you know kubernetes is
being used by a lot of companies and
they're really using it to help manage
complex existing systems so that they
can have greater performance and with
the end goal of being able to Delight
the customer increase value to the
customer and enhance increased value and
revenue into the organization so example
of this is a company called BlackRock
where they actually went through the
process of implementing kubernetes so
they could so BlackRock had a challenge
where they needed to be able to have
much more Dynamic access to their
resources uh they were running a complex
installations on people's desktops and
it was just really really difficult to
be able to manage their entire
infrastructure so they actually went and
pivoted to using kubernetes and this
allowed them to be able to be much more
scalable and expansive in the in the
management of their infrastructure and
as you can imagine kubernetes was then
hooked into their entire existing system
and has really become a key part of the
success that BlackRock is now
experiencing of a very stable
infrastructure um and the bottom line is
that BlackRock is now able to have
confidence in their infrastructure and
be able to give their confidence as back
to their customers through the
implementation and more rapid deployment
how of additional features and services
so if you are interested in taking your
career to the next level look no further
than a postgraduate program in devops
this comprehensive course is designed to
empower you with the skills and
knowledge needed to excel in the dynamic
world of devops this program offers over
50 hours of self-paced learning master
classes led by caltex ctme 20 plus real
life projects in integrated labs and the
opportunity to acquire 40 plus in demand
skills and master 15 plus essential
tools top it all with Capstone project
spanning in three domains and you will
be well on your way to a successful
devops career so in this session what we
can do is we're going to cover what and
why you would use puppets what are the
different elements and components of
puppet and how does it actually work and
then we'll look into the companies that
are adopting puppet and what are the
advantages that they have now received
by having puppet within their
organization and finally we'll wrap
things up by reviewing how you can
actually write a manifest in puppet so
let's get started so why puppet so here
is a scenario that has as an
administrator you may already be
familiar with you as an administrator
have multiple servers that you have to
work with and manage so what happens
when a server goes down it's not a
problem you can jump onto that server
and you can fix it but what if the
scenario changes and you have multiple
servers going down so here is where
public shows its strap with puppets all
you have to do is write a simple script
that can be written with Ruby and write
out and deploy to the servers your
settings for each of those servers the
code gets pushed out and to the servers
that are having problems and then you
can choose to either roll back to those
servers to their previous working States
or set them to a new state and do all of
this in a matter of seconds and it
doesn't matter how large your server
environment is you can reach to all of
these servers your environment is secure
you're able to deploy your software and
you're able to do this all all through
infrastructure as code which is the
advanced devops model for building out
Solutions so let's dig deeper into what
puppet actually is so puppet is a
configuration management tool maybe
similar tools like Chef that you may
already be familiar with it ensures that
all your systems are configured to a
desired and predictable state public can
also be used as a deployment tool for
software automatically you can deploy
your software to all of your systems or
to specific systems and this is all done
with code this means you can test the
environment and you can have a guarantee
that the environment you want is written
and deployed accurately so let's go
through those components of Puppets so
here we have a breakdown of the puppet
environment and on the top we have the
main server environment and then below
that we have the client environment that
would be installed on each of the
servers that will be running within your
network so if we look at the top part of
the screen we have here our puppet
master store which has and contains our
main configuration files and those are
comprised of manifests that are actual
codes for configuring the clients we
have templates that combine our codes
together to render a final document and
you have files that will be deployed as
content that could be potentially
downloaded by the clients wrapping this
all together is a module of manifest
templates and files you would apply a
certificate authority to sign the actual
documents so that the clients actually
know that they're receiving the
appropriate and authorized modules
outside of the master server where you'd
create your manifest templates and files
you would have public clients as a piece
of software that is used to configure a
specific machine there are two parts to
the client one is the agent that
constantly interacts with the master
server to ensure that the certificates
are being updated appropriately and then
you have the factor that the current
state of the client that is used and
communicated back to through the agent
so let's step through the workings of
puppet so the puppet environment is a
Master Slave architecture the clients
themselves are distributed across your
network and they are constantly
communicating back to a Master server
environment where you have your puppet
modules the client agent sends a
certificate with the ID of that server
back to the master and then the master
will then sign that certificate and send
it back to the client and this
authentication allows for a secure and
verifiable communication between clients
and master the factor then collects the
state of the client and sends that to
the master based on the facts sent back
the master then compiles manifests into
the catalogs and those catalogs are sent
back to the clients and an agent on the
client will then initiate the catalog a
report is generated by the client that
describes any changes that have been
made and sends that back to the master
with the goal here of ensuring that the
master has full understanding of the
hardware running software in your
network this process is repeated at
regular intervals ensuring all client
systems are up to date so let's have a
look at companies that are using puppets
today there are a number of companies
that have adopted puppet as a way to
manage their infrastructure so companies
that are using company today include
Spotify Google ATT so why are these
companies choosing to use puppet as
their main configuration management tool
the answer can be seen if we look at a
specific company Staples so Staples
chose to take and use puppet for their
configuration management tool and use it
within their own private Cloud the
results were dramatic the amount of time
that the it organization was able to
save in deploying and managing their
infrastructure through using puppets
enabled them to open up to allow to
expand with other and new projects and
assignments a real tangible benefit to a
company so let's look at how you write a
manifest in puppets so so manifests are
designed for writing out in code how you
configure a specific node in your server
environment the manifests are compiled
into catalogs which are then executed on
the client each of the manifests are
written in the language of Ruby without
dot PP extension if we step through the
five key steps for writing a manifest
they are one create your manifest and
that is written by the system
administrator two compile your manifest
and it's compiled into a catalog three
deploy the catalog is then deployed onto
the clients for execute the catalogs are
run on the client by the agent and then
five and clients are configured to a
specific and desired state if we
actually look into how manifest is
written it's written with a very common
syntax if you've done any work with Ruby
or really configuration of systems in
the past this may look very familiar to
you so we break out the work that we
have here you start off with a package
file or service as your resource type
and then you give it a name and then you
look at the features that need to be set
such as IP address then you're actually
looking to have a command written such
as present or start the Manifest can
contain multiple resource types if we
continue to write our manifesting puppet
the default keyword applies a manifest
to all clients so an example would be to
create a file path that creates a folder
called sample in a main folder called
Etc the specified content is written
into a file that is then posted into
that folder and then we're going to say
we want to be able to trigger an Apache
service and then ensure that that Apache
service is installed on a node so we
write the Manifest and we deploy it to a
client machine now on that client
machine a new folder will be created
with a file in that folder and an Apache
server will be installed you can do this
to any machine and you will have exactly
the same results on those machines
the field of devops has grown
exponentially over the past few years
and it's no surprise why devops is a set
of practices and tools that aims to
break down the silos between development
and operations team and streamline the
software delivery process by doing so
devops enables organizations to deliver
high quality software faster and more
efficiently than ever before
then this video on how to choose devops
as a career is for you with the proper
roadmap on how to get started with it
also do not forget to subscribe to our
YouTube channel and hit the Bell icon to
never miss an update from Simply learn
so without any further Ado let's get
started first is to understand the role
of a devops professional before diving
into the specifics of how to become a
devops professional it's essential to
understand the roles and
responsibilities of a devops
professional a devops professional is
typically responsible for Designing
implementing and maintaining the
infrastructure automation tools and
processes required for the efficient
delivery of software this includes
collaborating with development teams to
integrate automation into the software
development process setting up and
maintaining the infrastructure required
for the software delivery pipeline such
as servers databases and network systems
developing and implementing automation
scripts to improve the efficiency of
software delivery process
continuously monitoring and analyzing
the software delivery process to
identify and resolve bottlenecks and
improve efficiency
ensuring that the software delivery
process adheres to Industry best
practices and regulatory requirements so
now that you understand the roles and
responsibilities of a devops
professional let's take a look at the
roadmap to become a devops professional
so first is to learn the basics of
software development to Be an Effective
devops professional it's essential to
have a good understanding of software
development processes including
programming languages software
development methodologies and Version
Control tools you can start by learning
programming languages like python Java
or Ruby well a devops engineer course
will prepare you for a career in devops
Technologies through this devops
engineer course you will learn to review
deployment methodologies CI CD pipelines
observability and use devop tools like
gate Docker and Jenkins with this devops
engineer certification you can check out
the link for this course in the
description following that learn about
infrastructure and operations as a
devops professional you will be
responsible for managing infrastructure
including servers databases and network
system
so it's essential to have a good
understanding of infrastructure and
operations this include learning about
servers operating systems Network
protocols and storage systems
then explore about automation tools and
cloud services
automation is a critical part of devops
learning automation tools such as
ansible puppet or shirt can help you
streamline the software delivery process
and improve efficiency
most organizations today use cloud
services to deliver software learning
cloud services such as AWS Azure or gcp
can help you design and Implement Cloud
architecture that are efficient and
scalable
well to help you with learning devops
and cloud services simply learn has to
offer Azure devopsolution expert
master's program to help you become an
industry ready professional
in this course you will learn to plan
smarter collaborate better and shift
faster with a set of modern development
services the link for this course is
mentioned in the description do check
them out
once done with Skilling you need to gain
experience and get certified as
discussed before the best way to gain
experience in devops is to work on real
world projects this can be achieved
through internships volunteering for
open source projects or even taking on
small projects on your own this will
allow you to put your theoretical
knowledge into practice and gain
hands-on experience along with it there
are various certification programs
available such as the devops Institute
certification program AWS certification
devops engineer and Microsoft Azure
devops Solutions certification once you
have gained experience and acquired the
necessary skills it's time to start
looking for job opportunities
various job titles in devops such as
devops engineer site reliability
engineer automation engineer and release
engineer in top hiring companies in the
field include Amazon Google Microsoft
IBM and many others well a postgraduate
program in devops in collaboration with
Caltech ctme is a professional
development option that will Square your
skills with industry standards in this
course you will learn how to formalize
and document development processes and
create a self-documenting system devops
certification course will also cover
Advanced tools like puppet soft stack
and ansible that will help
self-governance and automated management
at scale link is mentioned in the
description do check it out
this video on devops engineering
we will be covering
devops engineer salary and companies
hiring let me ask you a quick question
so which of the following is a key
objective of devops option A is slowing
down software development option b is
increasing the gap between development
and operations option C is automating
and streamlining software delivery
optionally is reducing collaboration
among teams now you can pause this video
and answer in the comment section below
alright so now let's start with the
first topic which is what is devops
so devops is a collaborative approach
that brings together software
development that is Dev and ID
operations that is Ops teams to
streamline the software delivery process
it emphasizes communication
collaboration automation to enable
faster and more Reliable Software
releases devops aims to break down the
wall between teams allowing for seamless
integration and continuous feedback
throughout the development life cycle by
automating tasks such as code testing
deployment and infrastructure management
devops primary and stability ultimately
devops Fosters a culture of
collaboration Innovation and continuous
Improvement enabling organizations to
deliver high quality software at faster
Pace to meet customer demands if you
inspire to become a devops engineer
simply learns professional certificate
program in cloud computing and devops is
your ideal choice in partnership with E
and ICT Academy IIT guwahati this
program equips you with the skills
needed to excel in cloud computing and
devops learn to deploy robust Microsoft
Azure and AWS applications through a
blend of live virtual class Hands-On
Labs self-paced videos and collaborative
peer interactions don't miss this
opportunity to master the art of cloud
and devops enroll today and pave your
way to become a cloud expert
all right now let's move on and
understand the devops engineering
roadmap so the devops engineers need to
possess a diverse set of skills so let's
explore the importance of key skills in
the devops engineer roadmap first we
have is Linux Linux is an open source
operating systems widely used in the
devops world devops Engineers need to be
familiar with Linux as it is commonly
used for Server management scripting and
automation understanding Linux commands
and system administrations helps in
effectively managing infrastructure
next skill is Jenkins CI CD continuous
integration and continuous delivery
deployment that is CI CD are essential
practices in devops Jenkins is a popular
tool for automating the CI CD pipeline
allowing for frequent code integration
testing and deployment devops Engineers
need to understand Jenkins to enable
faster and more Reliable Software
releases the next skill that you must
possess in order to become a devops
engineer is Docker and Docker swamp
containerization with Docker allows for
consistent and portable application
deployments devops Engineers use Dockers
to package applications and their
dependencies into containers making them
easier to manage and deploy Docker swarm
helps in orchestralization and scaling
containerization applications ensuring
High availability and efficient resource
utilization
coming to the next one which is
kubernetes kubernetes is crucial for
managing containerization application at
a scale it automates the deployment
scaling and management of containers
making it easier to manage complex
environments devops Engineers need to
understand kubernetes to ensure
efficient resource allocations High
availability and seamless application
scaling all right now moving on to the
next skill which is cloud platforms so
Cloud platforms provide on-demand
resources and services enabling
scalability and flexibility devops
Engineers need to be proficient in Cloud
platforms like AWS Azure or Google Cloud
to deploy and manage applications in the
cloud this allows for efficient resource
provisioning scalability and cost
optimization
next up we have is Version Control
System
so Version Control Systems like git are
essential for collaboration and managing
code changes devops Engineers use
Version Control System to track changes
collaborate with team members and ensure
code integrity
it enables efficient code management
versioning and collaboration across
teams following this there is monitoring
and logging monitoring and logging tools
are crucial for tracking applications
and infrastructure performance devops
Engineers set of monitoring systems to
proactively identify issues troubleshoot
problems and ensure Optimal Performance
and availability logging helps in
capturing and analyzing applications
logs for debugging and performance
optimization each of these skills plays
a vital role in enabling collaboration
Automation scalability and reliability
in the devops workflow they help devops
Engineers streamline process improve
efficiency and deliver high quality
software at a faster pace now let's talk
about the responsibilities of a devops
engineer so as a devops engineer your
responsibilities revolve around Bridging
the Gap between development and
operation teams to ensure smooth and
efficient software delivery here are
some key responsibilities explained in
the understandable manner first is
collaboration here collaboration means
you work closely with development
operations and other cross-functional
teams to Foster collaborations and
streamline communication
and performance hair Monitor and
performance means you set up monitoring
systems to track the performance and
health of applications and
infrastructure this helps in identifying
and resolving issues proactively then
third is release Management in this you
manage the release process coordinating
with different teams to plan and execute
software releases
all right next is continuous integration
and deployment in this you automate the
process of integration code changes from
developers and deploying them to the
production environments next is
troubleshooting and support in
troubleshooting and support you
investigate and resolve issues related
to application or infrastructure
performance working closely with
development and operation teams
then there is continuous Improvement in
continuous Improvement you continuously
evaluate and improve existing processes
tools and infrastructure to enhance
efficiency and scalability right then
there is security and compliance in this
you implement security measures and best
practices to protect applications and
infrastructure from potential threats
all right so this was about the
responsibilities of a devops engineer
now let's talk about the interesting
part which is the salary all right so on
an average a devops engineer in the
United States can expect to earn between
138 thousand dollars by the positions
may start around 70 to 90 000 per year
it's important to note that these
figures are approximate and can vary
significantly based on individual
circumstances all right so this was the
salary now let's talk about the
companies hiring for devops engineer so
first of all there is Mercedes-Benz
research and development India a company
based in Bengaluru India that requires
devops Engineers with skills in AWS
kubernetes Docker Jenkins and python the
next big company is IBM a multinational
company that has openings for devops
engineer then there is Flipkart Flipkart
a leading e-commerce company that is
looking for devops engineers with
expertise in Linux git terraform and
monitoring tools next company is core
over a remote company that offers
full-time positions for devops engineers
with experience in Integrations updates
fixes and technical support
then there is Accenture Accenture is
currently seeking devops Engineers to
optimize their ID operations and
software development processes then
Oracle Oracle is actively recruiting
devops Engineers to enhance their ID
operations and streamline software
development
at last we have is sap sap is looking
for devops engineers to improve their
I.T operations and software development
practices as we follow this devops
roadmap keep in mind that it's not just
a path but an ongoing Journey embrace
the idea of always learning things
easier with automation as you hit each
Milestone you will not only make things
run smoother but also create a culture
of teamwork and Improvement in your
workplace what is devops devops is like
a teamwork approach for making computer
programs better and faster it combines
the work of software developers and
operation team the goal is to help them
work together and use tools that speed
up the process and make fewer mistakes
they also keep an eye on the programs
and fix problems early This Way
businesses can release programs faster
with few errors and everyone works
better together if you want to learn
more about this then check post graduate
program in devops to understand from the
basics two advanced concepts this
postgraduate program in devops is
crafted in partnership with Caltech ctme
this comprehensive course aligns your
expertise with industry benchmarks
experience are Innovative Blended
learning merging live online devops
certification sessions with immersive
labs for practical Mastery Advance your
career with Hands-On training that meets
industry demands alright now let's move
on to the first question of devops
interview question which is what do you
know about devops so think of a devops
like teamwork in the it gold it's become
really important because it helps teams
work together smoothly to make computer
programs faster and with fewer mistakes
imagine a soccer team everyone works
together to win the game devops is
similar it's when computer developers
and operations people team up to make
software better they start working
together from the beginning when they
plan what the software will be like
until they finish and put it out for
people to use this Teamwork Makes sure
things go well and the software works
great so this was about devops now
moving on to the second question which
is how is devops different from agile
methodology devops is like a teamwork
culture where the people who create the
software and the people who make it work
smoothly join forces this helps in
always improving and updating the
software without any big breaks agile is
a way of making softwares that's like
taking small steps towards instead of
big jumps it's about releasing small
parts of the software quickly and
getting feedback from the users this
helps in solving any issues or
differences between what users want and
what developers are making so you can
answer this question in this way all
right moving on to the third question
which is what are the ways in which a
build can be scheduled slash run in
Jenkins
so as you can see there are four ways by
source code management commits second is
after the completion of other builds
third is scheduled to run at a specified
time and fourth one is manual build
requests so if the interviewer asks then
you can answer these four ways in which
a build can be scheduled in Jenkins
alright now the fourth question is what
are the different phases in devops so
the various phases of devops lifecycle
are as follows so as you can see first
is plan so initially there should be a
plan for the type of application that
needs to be developed getting a rough
feature of the development process is
always a good idea then code the
application is coded as per the end user
requirements then there is build build
application by integrating various codes
formed in previous steps after that
there is test this is the most crucial
step of the application development test
the application and rebuild if necessary
then there is integrate so multiple
codes from different programmers are
integrated into one after integrate
there is deploy so code is deployed into
a cloud environment for further usage it
is ensured that any new changes do not
affect the functioning of a high traffic
website after that there is operate so
operations are performed on the code if
required then there is Monitor
applications performance is monitored
changes are made to meet the end user
requirements so these all were the
different phases of devops and here we
have explained each one of these you can
go through it and if the interviewer
have asked you this question you can
answer it in a similar way all right now
moving on to the next question which is
mention some of the core benefits of
devops so the core benefits of devops
are as follows first of all we'll say
technical benefits first technical
benefit of devops's continuous software
delivery then second is less complex
problems to manage third is early
detection and faster correction of
defects then here comes the business
benefits first benefit of devops that is
business benefit of devops is faster
delivery of features so it allows faster
delivery of features then there is
stable operation environment
then third one is improve communication
and collaboration between the teams so
these were the business benefits so here
we have discussed both technical
benefits and business benefits all right
now the next question is how will you
approach a project that needs to
implement devops so here are the simpler
terms here's how we can bring devops
into a specific project using these
Steps step one would be first we look at
how things are currently done and figure
out where we can make them better this
makes about two to three weeks then we
can plan for what changes to make then
step two would be we create a small test
to show that our plan works when
everyone agrees with it we can start
making the real changes and put the plan
into action then third step would be we
are all set to actually use devops we do
things like keeping track of different
versions of our work putting everything
together testing it and making it
available to the users we also watch how
it's working to make sure everything
goes smoothly by doing these steps right
the keeping track of changes putting
everything together testing and watching
how it's going we are all set to use
devops in our project
so this is how you can approach a
project that needs to implement in
devops and this is how you can answer
this question all right moving on to the
question number seven which is what is
the difference between continuous
delivery and continuous deployment all
right so first would be continuous
delivery so it ensures code can be
safely deployed on production whereas
continuous deployment here every change
that passes the automated test is
deployed to production automatically
then in continuous delivery it ensures
business applications and services
functions as expected whereas in
continuous deployment it makes software
development and the release process
faster and more robust in continuous
delivery delivers every change to a
production like environment through
rigorous and automatic testing whereas
in continuous deployment there is no
explicit approval from a developer and
requires a developed culture of
monitoring so these were the three
points that you you can highlight while
differentiating between continuous
delivery and continuous deployment all
right so this was the question number
seven now moving on to the question
number eight which is name three
security mechanisms of Jenkins uses to
authenticate users so here we have to
name three security mechanisms so first
one would be Jenkins uses an internal
database to store user data and
credentials second is Jenkins can use
the lightweight directory access
protocol that is ldap server to
authenticate users third one is Jenkins
can be configured to employ the
authentication mechanism that the
deployed application server uses so
these were the three mechanisms that
Jenkins uses to authenticate users all
right now moving on to the question
number nine which is how does continuous
monitoring help you maintain the entire
architecture of the system so continuous
monitoring within devops involves the
ongoing identification detection and
reporting of any anomalies or security
risk across the entire system
infrastructure it guarantees the proper
functioning of services applications and
resources on servers by overseeing
server statuses it accesses the accuracy
of application operations this practice
also facilitates uninterpreted audits
transactions to utility and regulated
surveillance
so this was about the question number
nine that was how does continuous
monitoring help you maintain the entire
architecture of the system now moving on
to the question number 10 which is what
is the role of AWS in devops so in the
realm of devops AWS assumes several
rules first one would be adaptable
services so it offers adaptable
pre-configured Services eliminating the
necessity for software installation or
configuration second one is design for
expansion whether managing one instance
or expanding two thousand AWS Services
accommodates seamless scalability then
there is automated operations AWS and
Powers task and process automation
freeing up valuable time for inventing
Pursuits the next one is enhanced
security
AWS and entity and accesses management
that is IAM allows precise user
permissions and policy establishment
then the last one is extensive partner
Network so AWS Fosters a vast partner
Network that integrates with and
enhances its service offerings so these
were the role of awfs in the realm of
devops all right now moving on to the
question number 11 name three important
devops kpis the three important kpis are
as follows first one is mean time to
failure recovery this is the average
time taken to recover from a failure
second kPa is deployment frequency the
frequency in which the deployment occurs
is called deployment frequency third one
is percentage of field deployments the
number of times the deployment Fields is
called percentage of field deployments
so these were the three important devops
kpis this question can also be asked all
right moving on to question number 12
which is how is IAC implemented using
AWS
so comments by discussing traditional
methods involving scripting commands
into files followed by testing in
isolated environments prior to
deployment notice how this practice is
giving way to infrastructure as code IAC
comparable to code for various Services
IAC aided by AWS empowers developers to
craft accesses and manage infrastructure
components descriptively utilizing
formats like Json or yaml this Fosters
streamline development and expeditious
implementation of alterations in
infrastructure all right now moving on
to question number 13 which is describe
the branching strategies you have used
they'll ask you this question so to test
our knowledge the purpose of branching
and our experience of branching at a
past job this question is usually asked
so here we will discuss topics that can
help considering this devops interview
question so release branching we can
clone the develop Branch to create a
release branch once it has enough
functionality for a release this Branch
kicks off the next release cycle thus no
new features can be contributed Beyond
this point the things that can be
contributed are documentation generation
bug fixing and other release related
tasks the release is merged into master
and given a version number once it is
ready to ship it should also be merged
back into the development branch which
may have evolved since the initial
release so this was the first one then
there is future branching this branching
model maintains all modifications for a
specific feature contained within a
branch the branch gets merged into
Master once the feature has been
completely tested and approved by using
tests that are automated then the third
branching is Task branching in this
branching model every task is
implemented in its respective Branch the
task key is mentioned in the branch name
we need to Simply look at the task key
in the branch name to discover which
code which tasks so these were the
branching strategies that you can say
that you have used or if you have really
used it otherwise you can say something
else
all right moving on to question number
14 which is can you explain the shift
left to reduce failure Concept in devops
so shifting left within the devops
framework is a concept aimed to enhance
security for performance and related
aspects to illustrate consider the
entity of Davos processes currently
security valuations occur before the
deployment stage by employing the left
ship approach we can introduce security
measures during the earlier development
phase denoted as the left
this integration spans on multiple
phases encompassing pre-development and
testing not confined to development
alone this holistic integration is
likely to elevate security measures by
identifying vulnerabilities at initial
stage leading to a more fortified
overall process now moving on to the
question number 15 which is what is blue
teen deployment pattern so this approach
involves seamless deployment aimed at
minimizing downtime it entails shifting
traffic from one instance to another
requiring the placement of outdating
code and a new version to integrate
fresh code
the updated version resides in a green
environment while the older one remains
in a blue environment after modifying
the existing version a new instance is
generated from the old one to execute
the updating instance ensuring a
smoother transition so the main focus of
this approach is smooth deployment
all right so this was question number
15. now moving on to the question number
16 which is what is continuous testing
so continuous testing involves the
automated execution of tests within the
software delivery pipeline
offerings immediate insights into
potential business rates within the
latest release
by seamlessly integrating testing into
every stage of the software develop pill
isifit I repeat by seamlessly
integrating testing into the every stage
of software delivery life cycle
continuous testing minimizes issues
during transition phases and empowers
development teams and instant feedback
this approach accelerates developer
efficiency by obtaining the need to
re-run all tests after each update and
project rebuild culminating in notable
gains in speed and productivity so this
was about continuous testing
now moving on to question number 17 what
are the benefits of automation testing
so some of the benefits of automation
testing includes
first it helps to save money and time
second is unattended execution can be
easily done third one is huge test
matrices can be easily tested the next
one is parallel execution is enabled
then there is reduced human generated
errors which results in improved
accuracy and last one is repeated test
task execution is supported
so these are the benefits of automation
testing
coming to question number 18 which is
what is a Docker file used for
so a Docker file is used for creating
Docker images using the build command
with a Docker image any user can run the
code to create Docker containers once
Docker image is built it's uploaded in
Docker registry
from the docker registry users can get
the docker image and build new
containers whenever they want so this is
what Docker file is used for
coming to question number 19 which is
what is the process for reverting a
commit that has already been pushed and
made public so there are two ways to do
that to revert a comment so first would
be remove or fix the bad file in a new
commit and push it to the remote
Repository
then commit it to the remote repository
using get commit minus M commit message
again git commit minus M commit message
so this is the first way then second is
create a new commit that undoes all the
changes that were made in the bad
comment
and use the command get reward
commit ID as you can see in the screen
First Command also on the second command
also the second command says get revert
commit ID commit ID will have to put the
commit ID all right so question number
19 this was now moving on to the last
question which is question number 20 how
do you find a list of files that have
been changed in a particular comment
so the answer would be the command to
get a list of files that have been
changed in a particular comment is
git diff 3 minus r then there is commit
hash so this command as you can see on
the screen get div 3 minus r commit hash
you can put it then example is also
there like
commit hash
87e6735
f21b so this example as you can see on
the screen then there is minus r flag
instruct so the command to list
individual files and commit hash will
list all the files that were changed or
added in that command so there's telling
about the as you can see minus our
functionality minus r is a flag that
instructs the command to list individual
files and commit hash will list all the
files that were changed or added in that
comment
so these were the top 20 devops
interview questions that you must
understand if you are planning to give a
devops interview
so in this video we have explored key
Concepts methodologies and best
practices that are crucial in fostering
collaboration between development and
operation streams by understanding the
principles discussed here you are well
equipped to navigate the dynamic
landscape of devops and drive successful
software delivery that's it for our
devops fundamentals for beginners in
2023 video we have covered essential
Concepts like agile versus devops CI CD
get and GitHub selenium Docker and
kubernetes in a beginner friendly way we
hope you feel more confident about how
devops can improve software development
remember practice makes perfect so keep
exploring and experimenting with these
Concepts if you have any questions or
topics you would like us to cover in
future videos please leave them in the
comment section below thanks for
watching and we will see you in the next
one don't forget to like subscribe and
share with others interesting in devops
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
foreign