hello Tech Enthusiast welcome to an
exciting journey into the realm of Devo
you are tuned into simply lears YouTube
channel your goto destination for
cutting Ed Tech education in this full
course for 2024 we will delve into
devops principles Automation
containerization and more whether you're
a beginner or a season professional this
course is tailored to equip you with the
latest skills ready to stay ahead in the
dynamic world of software development
and it operations hit that subscribe
button give this video a thumbs up and
let's embark on the devops adventure
together if you want to upskill yourself
master devops skills and land your dream
job or grow in your career then you must
explore simply learns cohort of various
devops programs simply learn offers
various certification and post-graduate
programs in collaboration with some of
the world's leading universities like
Caltech and Google Cloud through our
courses you will gain knowledge and work
ready expertise in in skills like devops
methodology deployment automation cicd
Pipeline and over a dozen others that's
not all you'll also get the opportunity
to work on multiple projects and learn
from industry experts working in Top
Tire product companies and academian
from top universities after completing
these courses thousands of Learners have
transitioned into devops role as a
fresher or moved on to a higher paying
job role and profile so if you are
passionate about making your career in
this field then make sure to check out
the link in the pin comments and
description box to find devops program
that fits your expertise and areas of
interest and today we're going to go
through and introduce you to devops
we're going to go through a number of
key elements today the first two will be
reviewing models that you're already
probably using for delivering Solutions
into your company and the most popular
one is waterfall followed by agile then
we'll look at devops and how devops
differs from the two models and how it
also borrows and leverages the best of
those models we'll go go through each of
the phases that are used in typical
devops delivery and then the tools used
within those phases to really improve
the efficiencies within devops finally
we'll summarize the advantages that
devops brings to you and your teams so
let's go through waterfall so waterfall
is a traditional delivery model that's
been used for many decades for
delivering Solutions not just IT
solutions and digital Solutions but even
way before that it has its history it
goes back to World War II so waterfall
is a model that is used to capture
requirements and then Cascade each key
deliverable through a series of
different stage Gates that is used for
building out the solution so let's take
you through each of those stage Gates
the first that you may have done is
requirements analysis and this is where
you sit down with the actual client and
you understand specifically what they
actually do and what they're looking for
in the software that you're going to
build and then from that requirements
analysis you'll build out a project
planning so you have an understanding of
what the level of work is needed to be
able to be successful in delivering the
solution after that you got your plan
then you start doing the development and
that means that the programmers start
coding out their solution they build out
their applications to build out the
websites and this can take weeks or even
months to actually do all the work when
you've done your coding and development
then you send it to another group that
does testing and they'll do full
regression testing of your application
against the systems and databases that
integrate with your application you'll
test it against the actual code you'll
do manual testing you do UI testing and
then after you've delivered the solution
you go into maintenance mode which is
just kind of making sure that the
application keeps working there's any
security risks that you address those
security risks now the problem you have
though is that there are some challenges
however that you have with the waterfall
model the cascading deliveries and those
complete and separated stage Gates means
that it's very difficult for any new
requirements from the client to be
integrated into the project so if a
client comes back and it's the project
has been running for six months and
they've gone hey we need to change
something that means that we have to
almost restart the whole project it's
very expensive and it's very time cons
assuming also if you spend weeks and
months away from your client and you
deliver a solution that they are only
just going to see after you spend a lot
of time working on it they could be
pointing out things that are in the
actual final application that they don't
want or are not implemented correctly or
lead to just general unhappiness the
challenge you then have is if you want
to add back in the client's feedback to
restart the whole waterfall cycle again
so the client will come back back to you
with a list of changes and then you go
back and you have to start your
programming and you have to then start
your testing process again and just
you're really adding in lots of
additional time into the project so
using the waterall model companies have
soon come to realize that you know the
clients just aren't able to get their
feedback in quickly effectively it's
very expensive to make changes once the
teams have started working and the
requirement in today's digital world is
that Solutions simply must be delivered
faster and this has led for a specific
change in agile and we start
implementing the agile model so the
agile model allows programmers to create
prototypes and get those prototypes to
the client with the requirements faster
and the client is able to then send the
requirements back to the programmer with
feedback this allows us to create what
we call a feedback loop where we're able
to get information to the client and the
client can get back to the development
team much faster typically when we're
actually going through this process
we're looking at the engagement cycle
being about 2 weeks and so it's much
faster than the traditional waterfall
approach and so we can look at each
feedback loop as comprising of four key
elements we have the planning where we
actually sit down with the client and
understand what they're looking for we
then have coding and testing that is
building out the code and the solution
that is needed for the client and then
we review with the client the changes
that have happened but we do all of this
in a much tighter cycle that we call a
Sprint and that typically a Sprint will
last for about two weeks some companies
run sprints every week some run every
four weeks it's up to you as a team to
decide how long you want to actually run
a Sprint but typically it's two weeks
and so every two weeks the client is
able to provide feedback into that Loop
and so you were able to move quickly
through iterations and so if we get to
the end of Sprint 2 and the client says
hey you know what we need to make a
change you can make those changes
quickly and effectively for Sprint 3
what we have here is a breakdown of the
ceremonies and the approach that you
bring to Agile so typically what will
happen is that a product leader will
build out a backlog of products and what
we call a product backlog and this will
be just a whole bunch of different
features and they may be small features
or bug fixes all the way up to large
features that may actually span over
multiple Sprints but when you go through
the Sprint planning you want to actually
break out the work that you're doing so
the team has a mixture of small medium
and large solutions that they can
actually Implement successfully into
their Sprint plan and then once you
actually start running your Sprint again
it's a two-e activity you meet every
single day to with the actual Sprint
team to ensure that everybody is staying
on track and if there's any blockers
that those blockers are being being
addressed effectively and immediately
the goal at the end of the two weeks is
to have a deliverable product that you
can put in front of the customer and the
customer can then do a review the key
advantages you have of running a Sprint
with agile is that the client
requirements are better understood
because the client is really integrated
into the scrum team they're there all
the time and the product is delivered
much faster than with a traditional
waterfall model you're delivering
features at the end of each Sprint
versus waiting weeks months or in some
cases years for a wful project to be
completed however there are also some
distinct disadvantages the product
itself really doesn't get tested in a
production environment it's only being
tested on the developer computers and
it's really hard when you're actually
running agile for the Sprint team to
actually build out a solution easily and
effectively on their computers to mimic
the production environment M and the
developers and the operations team are
running in separate silos so you have
your development team running their
Sprint and actually working to build out
the features but then when they're done
at the end of their Sprint and they want
to do a release they kind of fling it
over the wall at the operations team and
then it's the operations team job to
actually install the software and make
sure that the environment is running in
a stable fashion that is really
difficult to do when you have the two
teams really not working together so
here we have is a breakdown of that
process with the developers submitting
their work to the operations team for
deployment and then the operations team
may submit their work to the production
servers but what if there is an error
what if there was a setup configuration
error with the developer test
environment that doesn't match the
production environment there may be a
dependency that isn't there there may be
a link to an API that doesn't exist in
production and so you have these
challenges that the operations team are
constantly faced with and their
challenge is that they don't know how
the code works so this is where devops
really comes in and let's dig into how
devops which is developers and operators
working together is the key for
successful continuous delivery so devops
is as an evolution of the agile model
the agile model really is great for
Gathering requirements and for
developing testing out your Solutions
and what we want to be able to do is
kind of address that challenge and that
gap between the Ops Team and the dev
team and so with Dev Ops what we're
doing is bringing together the
operations team and the development team
into a single team and they are able to
then work more seamlessly together
because they are integrated to be able
to build out solutions that are being
tested in a production like environment
so that when we actually deploy we know
that the code itself will work the
operations team is then able to focus on
what they're really good at which is
analyzing the production environment and
being able to provide feedback to the
developers on what is being successful
so we're able to make adjustments in our
code that is based on data so let's step
through the different phases of a devops
team so typically you'll see that the
devops team will actually have eight
phases now this is somewhat similar to
Agile and what I'd like to point out at
time is that again agile and devops are
very closely related that agile and
devops are closely related delivery
models that you can use with devops it's
really just extending that model with
the key phases that we have here so
let's step through each of these key
phases so the first phase is planning
and this is where we actually sit down
with a business team and we go through
and understand what their goals are the
second stage is as you can imagine and
this is where it's all very similar to
Agile is that the code is actually start
coding but they typically they'll start
using tools such as git which is a
distributed Version Control software it
makes it easier for developers to all be
working on the same code base rather
than bits of the code that is rather
than them working on bits of the code
that they are responsible for so the
goal with using tools such as git is
that each developer always has the
current and latest version of the code
you then use tools such as mavin and
gradal as a way to consistently build
out your environment and then we also
use tools to actually automate our
testing now what's interesting is when
we use tools like selenium and junit is
that we're moving into a world where our
testing is scripted the same as our
build environment and the same as using
our G environment we can start scripting
out these environments and so we
actually have scripted production
environments that we're moving towards
Jenkins is is the integration phase that
we use for our tools and another Point
here is that the tools that we're
listing here these are all open-source
tools these are tools that any team can
start using we want to have tools that
control and manage the deployment of
code into the production environments
and then finally tools such as anible
and Chef will actually operate and
manage those production environments so
that when code comes to them that that
code is compliant with the production
environment so that when the code is
then deployed to the many different
production servers that the expected
results of those servers which is you
want them to continue running is
received and then finally you monitor
the entire environment so you can zero
in on spikes and issues that are
relevant to either the code or changing
consumer habits on the site so let's
step through some of those tools that we
have in the devops environment so here
we have is a breakdown of of the devops
tools that we have and again one of the
things I want to point out is that these
tools are open-source tools there are
also many other tools this is just
really a selection of some of the more
popular tools that are being used but
it's quite likely that you're already
using some of these tools today you may
already be using Jenkins you may already
be using git but some of the other tools
really help you create a fully
scriptable environment so that you can
actually start scripting out your entire
ire Dev Ops tool set this really helps
when it comes to speeding up your
delivery because the more you can
actually script out of the work that
you're doing the more effective you can
be at running automation against those
scripts and the more effective you can
be at having a consistent experience so
let's step through this devops process
so we go through and we have our
continuous delivery which is a plan code
build and test environment so what
happens if if you want to make a release
well the first thing you want to do is
send out your files to the build
environment and you want to be able to
test the code that you've been created
because we're scripting everything in
our code from the actual unit testing
being done to the all the way through to
the production environment because we're
testing all of that we can very quickly
identify whether or not there are any
defects within the code if there are
defects we can send that code right back
to the developer with a message Mage
saying what the defect is and the
developer can then fix that with
information that is real on the either
the code or the production environment
if however your code passes the the
scripting test it can then be deployed
and once it's out to deployment you can
then start monitoring that environment
what this provides you is the
opportunity to speed up your delivery so
you go from the waterfall model which is
weak months or even years between
releases to Agile which is 2 weeks or 4
weeks depending on your Sprint Cadence
to where you are today with devops where
you can actually be doing multiple
releases every single day so there are
some significant advantages and there
are companies out there that are really
zeroing in on those advantages if we
take any one of these companies such as
Google Google any given day will
actually process 50 to 100 new releases
on their website through their Dev Ops
teams in fact they have some great
videos on YouTube that you can find out
on how their devop teams work Netflix is
also a similar environment now what's
interesting with Netflix is that Netflix
have really fully embraced Dev Ops
within their development team and so
they have a devops team and Netflix is a
completely digital company so they have
software on phones on Smart TVs on
computers and on websites interestingly
though the devops team for Netflix is
only 70 people and when you consider
that a third of all internet traffic on
any given day is from Netflix it's
really a reflection on how effective
devops can be when you can actually
manage that entire business with just 70
people so there are some key advantages
that devops has it's the actual time to
create and deler deliver software is
dramatically reduced particularly
compared to Waterfall complexity of
maintenance is also reduced because
you're automating and scripting out your
entire environment uh you're improving
the communication between all your teams
so teams don't feel like they're in
separate silos but that are actually
working cohesively together and that
there is continuous integration and
continuous delivery so that your
consumer your customer is constantly
being delighted Welcome to the Ultimate
Guide to the future of tech in the
fast-paced world of devops staying ahead
is the game changer join us as we unlock
the top devop skills needed in 2024 from
mastering Cloud architectures to
building security fortresses we are
delving into the vital skills shaping
the tech landscape get ready to untravel
the road map to develop success and set
your sites on the tech Horizon let's get
started number one continuous
integration and continuous deployment
cic CD cicd the backbone of modern
software delivery makes integrating code
changes and deploying them smooth and
fast tools like Jenkins and gitlab take
care of testing Version Control and
deployment cutting down manual work
learning these tools might take a bit of
time focusing on Version Control
scripting and how systems run to get
better at cicd trying Hands-On projects
like setting up pipelines for web apps
or automating testing can be a game
changer number two Cloud architecture
and kubernetes knowing about Cloud
architecture and mastering kubernetes is
a big deal today companies are all about
cloud services and using kubernetes to
manage apps stored in containers
learning these involves understanding
various cloud services and how to use
them to build strong and flexible
applications it also means knowing how
to set up and manage containers in the
cloud environment getting good at this
might take some effort especially
learning about networks containers and
cloud computing Hands-On practice like
deploying small apps with kubernetes or
automating deployments can be a solid
way to level up number three
infrastructure as code IAC with
terraform terraform is a start in
managing infrastructure by writing
scripts it helps set up and manage
things like servers or databases without
manual configurations mastering it means
understanding terraforms language and
managing resources across different
Cloud providers getting good at
terraform might not be too hard if you
get the basics of cloud architecture
doing projects like automating Cloud
setups or managing resources across
different Cloud platforms can boost your
skills in this area number four security
Automation and devc Ops keeping systems
secure is top priority and that's where
devc Ops shines it's about integrating
security into every step of the
development process this needs
understanding security principles
spotting threats and using tools within
the development cycle to stay secure
getting skilled at this might take some
time focusing on security practices and
how they fit into development trying out
projects like setting up Security checks
in your development process or making
sure apps are encrypted can sharpen
these skills number five data Ops and AI
ml integration data Ops mixed with AI
and ml is the new thing for smarter
decision making it's about making data
Rel it work smooth and automated and
then mixing that data with AI and ml to
make awesome decisions learning this
might need digging into data processing
machine learning and programming
languages like python r or Scala
projects like building models or setting
up data pipelines can give hands-on
experience in this Fusion of data and
smart Tech number six monitoring and
observability tools monitoring tools
keep systems healthy by finding problems
before they cause trouble tools like
Prometheus or graph help keep an eye on
system performance and solve issues
quickly learning these tools might need
some time especially getting used to
metrics and logs projects like setting
up performance dashboards or digging
into system logs can really polish these
skills number seven microservices
architecture
breaking down big applications into
smaller parts is what microservices are
about it helps in better scalability and
flexibility getting good at this might
take a bit of understanding how these
smaller parts talk to each other and
using languages like Java or python
trying projects like breaking down big
apps or putting these small Services
into containers can make you a
microservices pro number eight
containerization beyond kubernetes
beyond kubernetes there are other Cool
Tools like Docker or pman that help
manage containers making life easier
learning these tools might need a basic
understanding of system administration
and containers working on projects like
creating custom container images or
managing multicontainer apps can really
amp up your container game number nine
serverless Computing and F
serverless platforms like AWS Lambda or
Azure functions let developers focus on
writing code without handling the
backend stuff mastering this might need
getting familiar with serverless
architecture and programming in
languages like node.js python or Java
doing projects like building serverless
apps or automating tasks with serverless
functions can level up your serverless
skills now number 10 collaboration and
soft
skills apart from the tech staff being a
team player and communicating well is
super important working on open-source
projects or joining diverse teams can
really boost these skills projects like
leading teams to devops changes or
driving cultural shifts in an
organization can improve these skills in
a big way before we conclude this
exhilarating expedition in to the top 10
devop skills for 2024 Envision this the
future is a canvas waiting for your
Innovation and expertise to paint upon
these skills aren't just a checklist
they are your toolkit for crafting the
technological future embrace them
immerse yourself in their practice and
let them be the fuel propelling your
journey toward Mastery in this rapid
evolving Tech realm remember it's not
just about knowing it's about about
doing dive into project experiments
fearlessly and let these skills be the
guiding Stars illuminating your path to
success thank you for joining us on this
adventure make sure to like this video
and share it with your friends do check
out the link in the description and Pinn
comment if you are interested in making
a career in Devo welcome to Simply learn
starting on the AWS devops journey is
like getting sale on a Hightech
Adventure in this tutorial will be your
Navigators through the vast Seas of
Amazon web services helping you to
harness the power of devops to
streamline your software delivery and
infrastructure management from
understanding devops principles to
mastering aw Services we will guide you
through the transformative Voyage
whether you're a seasoned sailor or an
nose Explorer our road map will unveil
the treasures of continuous integration
containerization Automation and Beyond
so for the Doos flag and get ready to
chart a course towards efficiency
collaboration and innovation in the AWS
ecosystem that said if these are the
type of videos you'd like to watch then
hit that subscribe button and the bell
icon to get notified as we speak you
might be wondering how to become a
certified professional and back your
dream job in this domain if you are a
professional with minimum one year of
experience and an aspiring devops
engineer looking for online training and
certification from the prestigious
universities and in collaboration with
leading experts then search no more
simply learns postgraduate program in
devops from Caltech University in
collaboration with IBM should be your
right choice for more details head
straight to our homepage and search for
postgraduate program and devops from
Caltech University or simply click on
the link in the description box below
now without further delay over to our
training so without further delay let's
get started with the agenda for today's
session first we will understand who
exactly is an AWS devops engineer then
the skills is required to become an AWS
devops engineer followed by that the
important roles and responsibilities and
now the most important point of your
discussion that is the road map or how
to become an AWS devops engineer
followed by that we will also discuss
the salary compensation being offered to
a professional AWS devops engineer and
lastly we will discuss the important
companies hiring AWS stops Engineers so
I hope I made myself clear with the
agenda now let's get started with the
first subheading that is who exactly is
an AWS devops engineer
the answer for this question is an AWS
devops engineer is a professional who
combines expertise in AWS that is Amazon
web services with devops principles to
streamline software development and
infrastructure management they design
Implement and maintain cloudbased
Solutions leveraging AWS services like
ac2 S3 and RDS devops Engineers automate
processes using tools such as AWS cloud
formation and facilitate continuous
integration and deployment pipelines
they are role focuses on improving
collaboration between development and
operations teams ensuring efficient
reliable and secure software delivery
with skills in infrastructure such as
IAC or infrastructure as code
containerization scripting and
continuous integration AWS devops
Engineers play a critical role in
optimizing cloud-based applications and
services and that's exactly an AWS
devops engineer now moving ahead we will
discuss the important skills required to
become an AWS devops engineer the role
of an AWS devops engineer requires a
combination of Technical and
non-technical skills here are the top
five skills that are crucial for an AWS
devops engineer starting with the first
one AWS expertise Proficiency in AWS is
fundamental devops Engineers should have
a deep understanding of AWS services
including ac2 S3 RDS WPC and much more
they should be able to design Implement
and manage Cloud infrastructure
efficient L the next one is IAC or
infrastructure as code IAC tools like
AWS cloud formation or terraform are
essential for automating the
provisioning and management of
infrastructure devops engineer should be
skilled in writing infrastructure code
and templates to maintain consistency
and reliability third one is scripting
and programming knowledge of scripting
languages example python bash and
programming languages is important for
Automation and custom scripting python
in particular is widely used for tasks
like creating deployment scripts
automating AWS tasks and developing
custom
Solutions next one is containerization
and orchestration skills in
containerization Technologies such as
Doc and container orchestr platforms
like Amazon ECS or Amazon eks are vital
devops Engineers should be able to build
deploy and manage containerized
applications now the fifth one is cicd
Pipelines or continuous integration and
continuous deployment Proficiency in
setting up and maintaining cic pipelines
using tools like AWS code pipeline
genkins or GitHub cicd is crucial devops
Engineers should understand the
principles of automated testing
integration and continuous deployment to
streamline software delivery effective
communication and collaboration skills
are essential as devops Engineers work
closely with devops development and
operations teams to bridge the gap
between them and Ensure smooth software
delivery and infrastructure management
problem solving skills the ability to
troubleshoot issues and a strong
understanding of security best practices
are also important for this rule devops
Engineers need to be adaptable and keep
up with the evolving AWS ecosystem and
devop practices to remain effective in
their role moving ahead we will discuss
the roles and responsibilities of an AWS
devop engineer the roles and
responsibilities of an awss engineer
typically revolve around managing and op
optimizing the infrastructure and
development pipelines to ensure
efficient reliable and scalable
operations here are the top five roles
and responsibilities of an AWS devops
engineer starting with the first one
that is IAC management devops Engineers
are responsible for defining and
managing infrastructure using IAC tools
like AWS cloud formation or terraform
they create and maintain templates to
provision and configure AWS resources
ensuring consistency and
repeatability next one is continuous
integration and deployment continuous
integration and continuous deployment
are also known as CD is very critical
devops Engineers establish and maintain
cicd pipelines automating the build test
and deployment processes they use AWS
code pipeline genkin or similar tools to
streamline the delivery of software and
updates to production environment next
is server and containerization
management devops Engineers work with
AWS ec2 instances ECS eks and other
services to manage servers and
containers they monitor resource
utilization configure Autos scaling and
Ensure High availability and fall
tolerance managing and logging is the
fourth one monitoring is a critical
responsibility devops Engineers set up
monitoring and alerting systems using
AWS cloudwatch analyze logs and respond
to incidents promptly they aim to
maintain High system availability and
performance security and compliance is
the fifth one so security is a priority
devops Engineers Implement and maintain
security best practices manage AWS
identity and access management that is
IM am policies and ensure compliance
with regulatory requirements they often
work with AWS services like AWS security
Hub and AWS config to assess and improve
security AWS devops Engineers are
involved in optimizing costs ensuring
disaster recovery and backup strategies
and collaborating with development and
operations teams to enhance
communication and collaboration they may
also assist in automating routine tasks
and prompting a culture of continuous
Improvement and Innovation within the
organization now the most important
aspect of today's session that is how to
become or the road map to become an AWS
devops engineer the AWS devops road map
provides a high level guide for
individuals or teams looking to adopt
devops practices in the context of
Amazon web services
devops is a set of practices that
combine software development Dev and it
operations Ops to enhance collaboration
and automate the process of software
delivery and infrastructure management
AWS offers a range of services and tools
to support AWS practices here is a road
map to help you get started with AWS and
devops creating a road map for AWS
devops in 10 steps can help you guide
your journey towards implementing devop
practices on the Amazon website Services
platform the first one is understand
devops principles start by gaining a
solid understanding of devop principles
and practices devops is about
collaboration between development and
operations team to automate and
streamline the software delivery process
second one is learn AWS
fundamentals get acquainted with AWS
services and understand the basics of
cloud computing including compute
storage and networking Services AWS
offers a wide range of services that can
be elated
in your devops processes third one is
set up your AWS account sign up for an
AWS account and configure billing and
security settings you may also want to
consider using AWS organizations for
managing multiple accounts and AWS
identity and access management for user
access control fourth step is source
code management Implement source code
management using a tool like git and
host your code repositories on a
platform like AWS code commit or or
GitHub learn about Version Control best
practices the fifth step is continuous
integration set up our cicd pipeline
using services like AWS code pipeline
AWS code build or genkins automate
building testing and deployment of your
code sixth one being infrastructure as
code or IAC Embrace IAC principles to
manage your AWS resources use tools like
AWS cloud formation terraform or AWS cdk
to Define and Pro Vision infrastructure
as code seventh step being deployment
and orchestr use AWS services like AWS
elastic bin stock AWS elastic container
service or ECS or cubes on AWS also
known as eks for deploying and managing
your applications orchestrate these
deployments using AWS step functions or
other automation tools now the eighth
step is monitoring and logging Implement
robust monitoring and logging services
using services like Amazon cloudwatch
and AWS cloud trail create dashboards
set up alarms and analyze logs to gain
insights into your applications
performance and security Now the ninth
One Security and compliance focus on
security by following AWS best practices
using AWS identity and access management
I am effectively and automating Security
checks for AWS conflict and AWS security
Hub ensure your infrastructure and
applications are compli with industry
standards now the last continuous
learning and Improvement devops is an
ongoing journey of improvement
continuously Monitor and optimize your
devops pipeline incorporate feedback and
stay updated on new AWS services and
best practices ensure a culture of
learning and Innovation within your team
remember that this road map is a high
level guide and the specific tools and
services you choose may vary based on
your projects requirements devop is your
culture of collaboration and automation
so adapt your devops practice to best
suit your team's needs and the AWS
services that you use now moving ahead
we will discuss the salary compensations
being offered to an AWS devops
engineer now if you are in India and a
beginner in AWS devops domain you can
expect salaries ranging from 3 lakhs to
6 lakhs per an if you're an intermediate
candidate with minimum two years of
experience then you can expect salaries
ranging from 6 lakhs to 12 lakhs per an
if you are an experienced candidate with
more than four years of experience the
minimum salary you can expect is 12
lakhs and it can go all the way up to 20
or more based on the project you're
working with company you're working with
and the location now if you are in
America and if you are a beginner in AWS
devops domain then you can expect an
average salary of $80,000 to $120,000
per anom and if you are an intermediate
candidate with minimum two years of
experience then you can expect a
salaries ranging from $120,000 to
$150,000 per an if you are a highly
experienced candidate maybe with before
or more than that you can expect
salaries ranging from $150,000 to
$200,000 per and again it might also go
up based on Project you're working with
based on the company you're working with
and in the location now moving ahead we
will discuss the next important and also
the last important topic of today's
discussion that is the company's hiring
AWS Toops engineer
there are a lot of companies hiring awss
Engineers but the prominent players in
this particular field is Amazon web
services Google Microsoft IBM Oracle
Netflix Adobe Cisco slack Salesforce
deloit and much more talking about the
salary figures of a senior devops
engineer according to glaso a senior
devops engineer working in the United
States earns a whooping salary of
$178,300
the same senior devops engineer in India
earns 18 lakh rupees
annually to sum it up as you progress
from entry level to mid- level and
eventually to experience devop engineer
your roles and responsibilities evolve
significantly each level presents unique
challenges and opportunities for growth
all contributing to your journey as a
successful devops professional so
excited about the opportunities devops
offers great now let's talk about the
skills you will need to become a
successful devops enger
coding and scripting strong knowledge of
programming languages like python Ruby
or JavaScript and scripting skills are
essential for Automation and Tool
development system administration
familiarity with Linux Unix and Windows
systems including configuration and
troubleshooting cloud computing
Proficiency in Cloud platforms like AWS
Azure or Google Cloud to deploy and
manage applications in the cloud
containerization and
orchestration
understanding container Technologies
like Docker and container orchestration
tools like kubernetes is a must
continuous integration or deployment
experienced with cicd tools such as
Jenkins gitlab Ci or Circle CI to
automate the development workflow
infrastructure as code knowledge of IAC
tools like terraform or insensible to
manage infrastructure programmatically
monitoring and logging familiarity with
monitoring tools like promas grafana and
logging Solutions like elk stack
acquiring these skills will not only
make you a valuable devops engineer but
will also open doors to exciting job
opportunities so to enroll in the
postgraduate program in devops today
click the link mentioned in the
description box below don't miss this
fantastic opportunity to invest in your
future so let's take a minute to hear it
out from our Learners who have
experienced massive success in their
career through a post-graduate program
in devopment so what are we going to
cover today so we're going to introduce
to the concept of Version Control that
you will use within your Dev Ops
environment then we'll talk about the
different tools are available in a
distributed Version Control System we'll
highlight a product called git which is
typically used for Version Control today
and you'll also go through what are the
differences between git and GitHub you
may have used GitHub in the past or
other products like gitlab and we'll
explain what are the differences between
git and git and services such as GitHub
and gitlab will break out the
architecture of what a get process looks
like um how do you go through and create
forks and clones how do you have
collaborators being added into your
projects how do you go through the
process of branching merging and
rebasing your project and what are the
list of commands that are available to
you in G finally I'll take you through a
demo on how you can actually run git
yourself and in this instance use the
software of git against a public service
such as GitHub all right let's talk a
little bit about version Control Systems
so you may have already been using a
virion control system within your
environment today you may have used
tools such as Microsoft team Foundation
services but essentially the use of a
virsion control system allows people to
be able to have files that are all
stored in a single repository so if
you're working on developing a new
program such as a website or an
application uh you would store all of
your Version Control software in a
single repos repository now what happens
is that if somebody wants to make
changes to the code they would check out
all of the code in the repository to
make the changes and then there would be
an addendum added to that so um there
will be the the version one changes that
you had then the person would then later
on check out that code and then be a
version two um added to that um code and
so you keep adding on versions of that
code the bottom line is that eventually
you'll have people being able to use
your your code and that your code will
be um stored in a centralized location
however the challenge you're running is
that it's very difficult for large
groups to work simultaneously within a
project the benefits of a VCS system a
Version Control System demonstrates that
you're able to store multiple versions
of a solution in a single repository now
let's take a step at some of the
challenges that you have with
traditional Version Control Systems and
see how they can be addressed with
distributed Version Control so in a
distributed Version Control environment
what we're looking at is being able to
have the code shared across a team of
developers so if there are two or more
people working on a software package
they need to be able to effectively uh
share that code amongst themselves so
that they constantly are working on the
latest um piece of code so a key part of
a distributed Version Control System
that's different to just just a
traditional version control system is
that all developers have the entire code
on their local systems and they try and
keep it updated all the time it is the
role of the uh distributed VCS server to
ensure that each client and we have a
developer here and developer here and
developer here and each of those are
clients have the latest version of the
software and then that each person can
then share the software in a
peer-to-peer like approach so that as
changes are being made into the server
of changes to the code then those
changes are then being redistributed to
all of the development team the tool to
be able to do an effective distributed
VCS environment is G now you may
remember that we actually covered git in
a previous video and we'll reference
that video for you so we start off with
our remote git repository and people are
making updates to the copy of their code
into a local environment that local
environment can be updated manually and
then periodically pushed out to the git
repository so you're always pushing out
the latest code that youve code changes
you made into the repository and then
from the repository you're able to pull
back the latest updates and so your G
repository becomes the kind of the
center of the universe for you and then
updates are able to be pushed up and
pulled back from there what this allows
you to be able to accompl accomplish is
that each person will always have the
latest version of the code so what is
git git is a distributed Version Control
tool used for source code management so
GitHub is the remote server for that
source code management and your
development team can connect their get
client to that some remote Hub server uh
git is used to track the changes of the
source code and allows large team seems
to work simultaneously with each other
it supports a nonlinear development
because of thousands of parallel
branches and has the ability to handle
large projects efficiently so let's talk
a little bit about git versus GitHub so
git is a software tool whereas GitHub is
a service and I'll show you how those
two look in a moment you install the
software tool for get locally on your
system whereas GitHub because is a
service it's actually hosted on a
website git is actually the software
that used to manage different versions
of source code whereas GitHub is used to
have a copy of the local repository
stored on the service on the website
itself git provides command line tools
that allow you to interact with your
files whereas GI help has a graphical
interface that allows you to check in
and check out files so let me just show
you the two tools here so here I am at
the git website and this is a website
you would go to to download the latest
version of git and again git is a
software package that you install on
your computer that allows you to be able
to do Version Control in a peer-to-peer
environment for that peer-to-peer
environment to be successful however you
need to be able to store your files in a
server somewhere and typically a lot of
companies will use a service such as
GitHub as a way to be able to store your
files so git can communicate effectively
with GitHub there are actually many
different companies that provide similar
service to GitHub gitlab is another
popular service but you also find that
development tools such as Microsoft
Visual Studio are also incorporating git
commands into their tools so the latest
version of Visual Studio team ser
Services also provides this same ability
but GitHub it has to be remembered is a
place where we actually store our files
and can very easily create public and
sharable is a place where we can store
our files and create public sharable
projects you can come to GitHub and you
can do a search on projects you can see
at the moment I'm doing a lot of work on
blockchain but you can actually search
on the many hundreds of projects here in
fact I think there's something like over
a 100,000 projects being managed on
GitHub at the moment that number is
probably actually much larger than that
and so if you are working on a project I
would certainly encourage you to start
at GitHub to see if somebody's already
maybe done a prototype that they're
sharing or they have an open-source
project that they want to share that's
already available um in GitHub certainly
if you're doing anything with um Azure
you'll find that there are thousands
45,000 Azure projects currently being
worked on interestingly enough GitHub
was recently acquired by Microsoft and
Microsoft is fully embracing open-source
Technologies so that's essentially the
difference between get and GitHub one is
a piece of software and that's git and
one is a service that supports the
ability of using the software and that's
GitHub so let's dig deeper into the
actual git architecture itself so the
working directory is the folder where
you are currently working on your git
project and we'll do a demo later on
where you can actually see how we can
actually simulate each of these steps so
you start off with your working
directory where you store your files and
then you add your files to a staging
area where you are getting ready to
commit your files back to the main
branch on your git project you will want
to push out all of your changes to a
local repository after you've made your
changes and these will commit those
files and get them ready for
synchronization with the service and
we'll then push your services out to the
remote repository an example of a remote
repository would be GitHub later when
you want to update your code before you
write any more code you would pull the
latest changes from the remote
repository so that your copy of your
local software is always the latest
version of the software that the rest of
the team is working on one of the things
that you can do is as you're working on
new features within your project you can
create branches you can merge your
branches with the mainline code you can
do lots of really creative things that
Ure the that a the code remains at very
high quality and B that you're able to
seamlessly add in new features without
breaking the core code so let's step
through some of the concepts that we
have available in G so let's talk about
forking and cloning in kit so both of
these terms are quite old terms when it
comes to development but forking is
certainly a term that goes way way way
back long before uh we had distributed
CVS systems such as the ones that we're
using with Git to Fork a piece of
software is a particular open source and
project you would take the project and
create a copy of that project and but
then you would then associate a new team
and new people around that project so it
becomes a separate project in entirety a
clone and this is important when it
comes to working with g a clone is
identical with the same teams and same
structuring as the main project itself
so when you download the code you're
downloading an exact copy of that code
with all the same security and access
rights as the main code and then you can
then check that code back in and
potentially your code because it is
identical could potentially become the
mainline code uh in the future now that
typically doesn't happen your changes
are the ones that merged into to the
main branch but also but you do have
that potential where your code could
become the main code with Git You can
also add collaborators that can work on
the project which is essential for
projects where particularly where you
have large teams this works really well
when you have product teams where the
teams themselves are self-empowered you
can do a concept what's called branching
in git and so say for instance you are
working on a new feature that new
feature and the main version of the
project have to still work
simultaneously so what you can do is you
can create a branch of your code so you
can actually work on the new feature
whereas the rest of the team continue to
work on the main branch of the the
project itself and then later you can
merge the two together pull from remote
is the concept of being able to pull in
Services software the team is working on
from a remote server and get rebase is
the concept of being able to take a
project project and reestablish a new
start from the project so you may be
working a project where there have been
many branches and the team has been
working for quite some time on different
areas and maybe you kind of losing
control of what the true main branch is
you may choose to rebase your project
and what that means though is that
anybody that's working on a separate
Branch will not be able to Branch their
code back into the mainline Branch so
going through the process of a get
rebase essentially allows you to create
a new start for where you're working on
your project so let's go through forks
and clones so you want to go through the
process so you want to go ahead and Fork
the code that you're working on so this
use this scenario that one of your team
wants to go ahead and add a new change
to the project the team member may say
yeah go ahead and you know create a
separate Fork of the actual project so
what was that look like so when you
actually go ahead and create a fork of
the Repository you actually go and you
can take the version of the mainline
Branch but then you take it completely
offline into a local repository for you
to be able to work from and you can take
the mainline code and you can then work
on a local version of the code separate
from the mainland Branch it's now a
separate Fork collaborators is the
ability to have team members working on
a project together so if you know
someone is working on a piece of code
and they see some errors in the code
that you've created
none of us are perfect at writing code I
know I've suddenly made errors in my
code it's great to have other team
members that have your bag and can come
in and check and see what they can do to
improve the code so to do that you have
to then add them as a collaborator now
you would do that uh in GitHub you can
give them permission within GitHub
itself it's really easy to do super
visual um interface that allows you to
do the work quickly and easily and
depending on the type of permissions you
want to give them sometimes it could be
very limited permissions it may be uh
just to be able to read the files
sometimes it's being able to go in and
make all the changes you can go through
all the different permission settings on
GitHub to actually see what you can do
but you'll be able to make changes so
that people can actually have access to
your repository and then you as a team
can then start working together on the
same code let's step through branching
in G so suppose you're working on an
application but you want to add in a new
feature and this is very typical within
a Dev Ops environment so to do that you
can create a new branch and build a new
feature on that Branch so here you have
your main application on what's known as
the master branch and then you can then
create a sub branch that runs in
parallel which has your feature you can
then develop your feature and then merge
it back into the master Branch at a
later point in time now the benefit you
have here is that by default we're all
working on the master Branch so we
always have the latest code the the
circles that we have here on the screen
show various different commits that have
been made so that we can keep track of
the master branch and then the branches
that have come off which have the new
features and there can be many branches
in git so git keeps you the new features
you're working on in separate branches
until you're ready to merge them back in
with the main branch so let's talk a
little bit about that merge process so
you're starting with the master branch
which is the blue line here and then
here we have a separate parallel branch
which has the new features so if we to
look at this process the base commit of
feature B is the branch f is what's
going to merge back into the master
branch and it has to be said there can
be so many Divergent branches but
eventually you want to have everything
merge back into the master Branch let's
step through git rebase so again we have
a similar situation where we have a
branch that's being worked in parallel
to the master branch and we want to do a
get rebase so we're at stage C and what
we've decided is that we want to reset
the project so that everything from here
on out with along the master branch is
the standard product however this means
that any work that's been done in
parallel as a separate Branch will be
adding in new features along this new
rebased environment now the benefit you
have by going through the rebase process
is that you're reducing the amount of
storage space that's required for when
you have so many branches it's a great
way to just reduce your total footprint
for your entire project so get rebase is
the process of combining a sequence of
commits to form a new base commit and
the primary reason for rebasing is to
maintain a linear project history when
you replace you unplug a branch and
replug it in on the tip of another
branch and usually you do that on the
master branch and that will then become
the new Master Branch the goal of
rebasing is to take all the commits from
a feature branch and put it together in
a single Master Branch it makes it the
project itself much easier to manage
let's talk a little bit about pull from
remote Suppose there are two developers
working together on application the
concept of having a remote repository
allows the code to the two developers
will be actually then checking in their
code into a remote repository that
becomes a centralized location for them
to be able to store their code it
enables them to stay updated on the
recent changes to the repository because
they'll be able to pull the latest
changes from that remote repository so
that they are ensuring that as
developers they're always working on the
latest code so you can pull any changes
that you have made to your fault remote
repository to your local reposit
repository the command to be able to do
that is written here and we'll go
through a demo of how to actually do
that command in a little bit good news
is if there are no changes you'll get a
notification saying that you're already
up to date and if there is a change it
will merge those changes to your local
repository and you get a list of the
changes that have been made remotely so
let's step through some of the commands
that we have in git so git in it
initializes a local git repository on
your hard drive get add adds one or more
files to your staging are area get
commit DM commit message is a commit
changes the git command commits changes
to head up so the get command commits
changes to your local staging area get
status checks the status of your current
repository and lists the files you have
changed get log provides a list of all
the commits made on your current Branch
get diff views the changes that you've
made to the file so you can actually
have files next to each other you can
actually see the differences between the
two files uh get push origin Branch name
so the name of your branch command will
push the branch to the remote repository
so that others can use it and this is
what you would do at the end of your
project get config Das Global username
we'll tell get Who You Are by
configuring the author name and we'll go
through that in a moment get config
Global user email will tell get the
author of by the email ID get clone
creates a get repository copy from a
remote Source get remote ad origin
server connects the local repository to
the remote server and adds the server to
be able to push to it get branch and
then the branch name will create a new
Branch for you to create a new feature
that you may be working on uh get
checkout and then the branch name will
allow you to switch from one branch to
another Branch get merge Branch name
Will merge a branch into the active
Branch so if you're working on a new
feature you going to merge that into the
main branch a get rebase will reapply
commits on top of another base tip and
get rebase will reapply commits on top
of another base tip and these are just
some of the popular git commands there
are some more but you can certainly dig
into those as you're working through
using git so let's go ahead and run a
demo using git so now we are going to do
a demo using get on our local machine
and GitHub as the remote Repository for
this to work I'm going to be using a
couple of tools first I'll have the deck
open as we've been using up to this
point uh the second is I'm going to have
my terminal window also available and
let me bring that over so you can
actually see this and the terminal
window is actually running git bash as
the software in the background which
you'll need to download and install you
can also run git bash locally on your
Windows computer as well and in addition
I'll also have the GitHub repository
that we're using simply learn uh already
set up and ready ready to go all right
so let's get started so the first thing
we want to do is create a local
repository so let's go ahead and do
exactly that so the local repository is
going to reside in my development folder
uh that I have on my local computer and
for me to be able to do that I need to
create a drive in that folder so I'm
going to go ahead and change the
directory so I'm actually going to be in
that folder before I actually create
make the new folder so I'm going to go
ahead and change
directory and now I'm in the development
directory I'm going to go ahead and
create a new
folder and that's going to ahead and
created a new folder called hello
world I'm going to move my cursor so
that I'm actually in the hello world
folder and now that I'm in the hello
world folder I can now initialize this
folder as a git repository so I'm going
to use the git command in it to
initialize and let's go ahead and
initialize that folder so let's see
what's happened so here I have my hello
allall folder that I've created and
you'll now see that we have a hidden
folder in there which is called doget
and if we expand that we can actually
see all of the different subfolders that
g repository will create so let's just
move that over a little bit so that we
can see the rest of the work and now if
we check on our folder here we actually
see this is users math uh development
hello world doget and that matches up
with hidden folder
here so we're going to go ahead and
create a file called readme.txt in our
folder so here is our hello world folder
and I'm going to go ahead and using my
text editor which happens to be
Sublime I'm going to create a file and
it's going to have in there the text
hello world and I'm going to call this
one readme do
txt if I go to my Hello World folder
you'll see that we have the readme.txt
file actually in the folder what's
interesting is if I select the get
status command what it'll actually show
me is that this file has not yet been
added to the commits yet for this
project so even though the file is
actually in the folder it doesn't mean
that it's actually part of the project
for us to do that we actually have to go
and select
for us to actually commit the file we
have to go into our terminal window and
we can use the git status to actually
read the files that we have there so
let's go ahead and use the git status
command and it's going to tell us that
this file has not been committed you can
use this with any folder to see which
files and subfolders haven't been
committed and what we can now do is we
can go and actually add the readme file
so let's go ahead we just going to S git
add so the git command is ADD
readme.txt so that then adds that file
into our main um project and we want to
then commit those files into the main
repositories history and so that do that
we'll hit the the get command commit and
we'll do a message in that commit and
this one will
be first
commit and it has committed that project
what's interesting is we can now go back
into readme file and I can change this
so we can go hello git git is a very
popular version control
solution and
we'll we'll save that now what we can do
is we can actually go and see if we have
made differences to the read me text so
to do that we'll use the diff command
for get so we do get
diff and it gives us two um releases the
first is what the original text was
which is hello world and then what we
have afterwards is what is now the new
text in green which has replaced the
original
text so what we're going to do now is
you want to go ahead and create an
account on GitHub we already have one
and so what we're going to do is we're
going to match the account from GitHub
with our local account so to do that
we're going to go ahead and set get
config and we're going to do Dash and
it's going to be be a global user.name
and we going to put in our username that
we use for GitHub and this instance
we're using the simply
[Music]
learn Das GitHub account
name and under the GitHub account you
can go ahead and create a new repository
name in this instance we called the
repository
hello-world and and what we want to do
is connect the local GitHub account with
the remote hello world.it account and we
do that by using this command uh from
git which is our remote connection and
so let's go ahead and type that in open
this up so we can see the whole thing so
we're going to type in get remote add
origin
https back SL back
slash GitHub
hub.com
SL simply
learn D GitHub and you have to get this
typed in correctly when you're typing in
the location
hello-world
doget that creates the connection to
your hello world account and now we want
to do is we want to push the files to
the remote location using the git push
command commit get push
origin
master so we're going to go ahead and
connect to our local remote GitHub so
I'm just going to bring up my terminal
window again and so let's select git
remote add
origin and we'll connect to the the
remote
location
github.com
SLS simply
learn Das GitHub
slash hello dwld
dog oh we actually have already
connected so we're connected to that
successfully and now we're going to push
the master Gish so get push origin
master and everything is connected and
successful and if we go out to GitHub
now we can actually see that our file
was updated just a few minutes
ago so what we can actually do now is we
can go and Fork a project from GitHub
and clone it locally so we're going to
use the um fork tool that's actually
available on GitHub let me show you
where that is located and here is our
branching tool it's actually changed
more recently with a new UI
interface and once complete we'll be
able to then pull a copy of that to our
account using the forks new HTTP URL
address so let's go ahead and do
that so we're going to go ahead and
create a fork of our project now to do
that you would normally go in when you
go into your project you'll see that
there are Fork options in the top right
hand corner of the screen now right now
I'm actually logged in with the default
primary count for this project so I
can't actually Fork the project as I'm
working on the main branch however if I
come in with a separate ID and here I am
I have a different ID and so I'm
actually pretending I'm somebody else I
can actually come in and select the fork
option and create a fork of this project
and this will take just a few seconds to
actually create the
fork and there we are we have gone ahead
and uh created the
fork so you want to S clone or download
with this and so this is the I select
they'll actually give me the web address
I can actually show you what that looks
like I'll open up my text editor
that's not
correct I guess that is correct so I'm
going to copy
that and I can Fork the project Lo Al
and clone it locally I can change the
directory so I can create a new
directory that I'm going to put my files
in and then post in that content into
that file so I can now actually have
multiple versions of the same code
running on my
computer I can then go into the for
content and use the patchwork command to
actually so I can create a copy of that
code that we've just created and we call
it that's a a clone and we can create a
new folder that we're actually putting
the work in and we could for whatever
reason we wanted to we could call this
folder Patchwork and that would be maybe
a new feature and then we can then paste
in the URL of the new uh directory that
would has the fork work in it and now at
this point we've now pulled in and
created a clone of the original
content and so this allows us to go
ahead and Fork out all of the work for
our project onto our computer so we can
then devel our work
separately so now what we can actually
do is we can actually create a branch of
the fork that we've actually pulled in
onto our computer so we can actually
then create our own code that runs in
that separate branch and so we want to
check out um the uh the branch and then
push the origin Branch uh down to our
computer
this will give us the opportunity to
then add our collaborators so we can
actually then go over to GitHub and we
can actually come in and add in our
collaborators and we'll do that under
settings and select collaborators and
here we can actually see we have
different collaborators that have been
added into the project and you can
actually then request people to be added
via their GitHub name or by email
address or by their full
name one of the things that you want to
be able to do is ensure that you're
always keeping the code that you're
working on fully up to date by pulling
in all the changes from your
collaborators you can create a new
branch and then make changes and merge
it into the master Branch now to do that
you would create a folder and then that
folder in this instance would be called
test we would then move our cursor into
the folder called test and then
initialize that folder so let's go ahead
and do that so let's call um create a
new folder and we're going to first of
all change our root folder and we're
going to go
to
development and we're going to create a
new
folder call it test and we're going to
move into the test folder and we will
initialize that
folder and we're going to move move some
files into that test
folder
call this one test
one and then we're going to do file save
as and this one's going to be test
two and now we're going to commit those
files kit
add kit add and then we'll use the dot
to pull in all
files and then get commit
DM
files
committed make sure I'm in the right
folder here I don't think I
was and now that I'm in the correct
folder let's go ahead and
and and get
commit and it's going ahead and added
those files and so we can see the two
files that were created have been added
into the
master and we can now go ahead and
create a new Branch we call this one git
Branch
testore branch
and let's go ahead and create a third
file to go into that
folder this
is file three and do file save as we'll
call this one test 3.
text and we'll go ahead and
[Music]
add that file and do get ADD test 3
.txt and we're going to move from the
master Branch to the test
Branch get check out
testore
branch and it's switched to the test
branch and we'll be able to list out all
of the files that are in the that branch
brch
now and we want to go through and merge
the files into one area so let's go
ahead and we'll do git merge
testore
branch and it's well we've already
updated everything so that's good
otherwise it would tell us what we would
be
merging and now all the files are merged
successfully into the master
Branch there we go all merge together
fantastic and so what we're going to do
now is move from Master Branch to test
Branch so get check
out
testore
branch and we can modify the files the
test three file that we took out and
pull that file
up
and we
can now
modified and we can
then
commit that
file
back in and we've actually been able to
then commit the file with one changes
and and see it's the text re change that
was
made and we can now go through the
process of checking the file back in
switching back to the master branch and
ensuring that everything is in sync
correctly we may at one point want to
rebase all of the work is kind of a hard
thing you want to do but it will allow
you to allow for managing for changes in
the future future so let's switch to it
back to our test branch which I think
we're actually on we're going to create
two more
files let's go to our folder here and
let's go copy
those and that's
created we'll rename those tests
four
and
five and so we now have additional
files
and we're going to add those into our
branch that we're working on so we're
going to go in and select G add-
A and we're going to commit those files
get commit D
a-m
adding two new
files and it's added in the two new
files
so we have all of our files now we can
actually list them out and we have all
the files that are in the
branch and we'll switch then to our
Master Branch we want to rebase the
master so we do get
rebase
master and that will then give us the
command that everything is now
completely up to dat and we can go get
checkout Master to switch to the master
account this will allow us to then um
continue through and rebase the test
branch and then list all the files so
they're all in the same area so let's go
get
rebase
testore
branch and now we can list and there we
have all of our files listed in
correctly
if we talk in the literal sense Maven
means accumulator of knowledge Maven is
a very powerful project management tool
or we can call it a build tool that
helps building documenting and managing
a project but before we move forward and
dive deep into the basics of Maven let's
understand what is meant by the term
build
tool a build tool takes care of
everything for building a project
it generates a source code generates
documentation from a source code it even
compiles the source code and packages
the compiled codes into jar of zip files
along with that the build tool also
installs the packaged code in local
repository server repository or Central
repository coming back to Maven it is
written in Java or C and it is based on
Project object model or p M again let's
have a pause and understand what is
meant by this term project object
model a project object model or p is a
building block in Maven it is an XML
file that contains information about the
project and configuration details used
by Maven to build a project this file
resides in the base directory of the
project as pom.xml file the pum contains
information about the project and
various configuration details it also
includes the goals and plugins used by
Maven in a
project Maven looks for the p in the
current directory while executing a task
or a goal it reads the P gets the needed
configuration information and then runs
the
goal coming back MAV is used to building
and managing any Java based
project it simplifies the day-to-day
work of Java developers and helps them
in their
projects now when we know the basics of
Maven let's have a look at some reasons
to know why is Maven so popular and why
are we even talking about it so let's
have a look at the need for
Maven Maven as by now we know is
popularly used for Java based projects
it helps in downloading libraries or ja
files used in the
project to understand the part of why do
we use Maven or the need of Maven let's
have a look at some problems that Maven
solved the first problem is getting the
jar files in a project getting the right
jar files is a difficult task where
there could be conflicts in the versions
of the two separate packages however it
makes sure all the jar files are present
in its repositories and avoid any such
conflicting
scenarios the next problem it sorted was
downloading
dependencies we needed to visit the
official website of different software
which could be a TDS task now instead of
visiting individual websites we could
visit mvn repository. comom which is a
central repository of the jar
files then Maven plays a vital role in
the creation of the right project
structure in servlets struts Etc
otherwise it won't be
executed then Maven also helps to build
and deploy the project so that it may
work
properly so the next point is what
exactly M
does it makes the building of the
project easy the task of downloading the
dependencies and jar files that were to
be done manually can now be done
automatically
all the information that is required to
build the project is readily available
now finally M helps manage all the
processes such as building documenting
releasing and other methods that play an
integral part in managing a
project now when we know everything
about Maven let's look at some companies
that use
Maven there are over 2,000 companies
that use Maven to
the companies that use Maven are mostly
located in the United States and in the
computer science Industry Maven is also
used in Industries other than computer
science like information technology and
services financial service banking
hospital and care and much more some of
the biggest corporations that use Maven
are as
follows first we have viaro then comes
Accenture followed by JP Morgan Chas in
company then comes craft base and
finally we have red hat first
understanding that what exactly is in
gridle all about now griddle is an kind
of a build tool which can be used for
the uh build automation performance and
uh it can be used for various
programming languages primary it's being
used for the uh Java base applications
it's an kind of build tool which can
help you to see that how exactly
automatically you can prepare the builds
you can perform the automations earlier
we used to do the build activity from
the eclipse and uh we used to do it
manually right but with the help of this
build tool we are going to do it like
automatically without any manual efforts
as such here there are like lot of
activities which we will be doing during
the build process primary there are
different activities like compilations
linkage packaging these are the
different uh tasks which we perform
during the build process so that we can
understand that how the build can be
done and we can perform the
automations uh this uh process also it's
kind of uh standardized because again if
you want to automate something standards
or a standard process is something which
we require for that before being going
ahead with that part so that's the
reason why we are getting this build
tool because this pill tool helps us to
do an standardization process to see
that how the standards can be met and
how we can proceed further with that
part also it's something which can be
used with variety of languages
programming languages Java is the
primary language for which we use the
Gradle but again other languages like
Scala Android cc++ gruy these are some
of the languages for which we can use
the same tool now it's actually using
like it's referring to as an gry based
domain specific language rather than XML
because ant and MAV these are the XML
based build tools but this one is not
that uh dependent on XML it's using the
gry based domain specific language DSL
language is being used here right now um
again uh it's something which can be
used to do the build uh it can further
on used to perform the test cases
automotions also there and then forther
on you can deploy to the artifactry also
that okay I want to push the artifa to
the artifactry so that also that part
also you can get it done over here so
primary this tool is known for doing the
build automations for the big and large
projects the projects in which the
source code the amount of source code
and uh the uh efforts is more so in that
case this particular tool makes sense
now gridle includes both the pros of M
and uh ant but it removes the drawbacks
or whatever the uh issues which we face
face during these two build tools so
it's helping us to remove all the cons
which we face during the implementation
of ant and Maven and again again all the
pros of ADD and Maven is implemented
with this griddle tool now let's see
that why exactly this griddle is used
because that's a very valid question
that what is the activity like what is
the reason why we use the gradel because
um the first one is that it resolves
issues faced on other build tools that's
a primary reason because we all already
having the tools like Maven and andt
which is a aailable there but primary
this gridle tool is something which is
removing all the issues which we are
facing with the implementation of other
tools so these issues are getting uh
removed as such second one is that it
focuses on maintainability performance
and uh flexibility so it's giving the
focus on that how exactly we can manage
the big large projects and uh we can
have flexibility that what different
kind of a projects I want to build today
I want to build in different ways
tomorrow the source code modifies gets
added up so I have the flexibility that
I can change this build scripts I can
perform the automations so a lot of
flexibility is available which is being
supported by this tool and then the last
one is like uh it provides a lot of
features a lot of plugins now this is
one of the uh benefit which we get in
the case of mayavan also that we get lot
of features but again when we talk about
cradle then it provides a lot of plugins
like let's say that normally in a build
process we do the compilation of the
source code but sometimes let's say that
we want to build an angular or a nodejs
application now in that case we may be
involved in running some command line
executions some command line commands
just to make sure that yes we are
running the commands and we are getting
the output so there are a lot of
features which we can use like uh there
are a lot of plugins which is available
there and we will be using those uh
plugins in order to go ahead and in
order to execute those build process and
doing the automations now let's talk
about the crle and M because again when
we talk about Mayan like it was like
something which was primary used for the
Java but again when we are talking about
cradle so again it's just uh being used
primary for the Java here but what is
the reason that we prefer Gradle over
the CR uh mavan so what are the
different uh reason for that let's talk
about that part because this is very
important we need to understand that
what is the reason that Gradle is
preferred as an better tool for the Java
as compared to MAV when we talk about
for the build automation here now the
first one is that the uh gridle using
the gry DSL language domain specific
Lang language whereas the maven is
considered as in project management tool
which is uh creating the pals or XML
format files so it's being used for the
Java project but XML format is being
used here and on the other hand griddle
is something which is not using the XML
formats and uh whatever the build
scripts you are creating that is
something which is there in the groupy
based uh DSL language and on the other
hand in the pal we have to create the
xmls dependencies whatever the
attributes you're putting up in the May
one that's something which is available
there in the format of XML the overall
goal of the griddle is to add
functionality to a project whereas the
goal of uh the maven is to you know to
complete a project phase like to work on
different different project phase like
compilation test executions uh then uh
packaging so uh then deploying to artifa
so these are all different phases which
is available there into the MAV but on
the other hand gridle is all about
adding the functionality that how you
want to have some particular features
added up into the build scripes in
gridle there are like we usually specify
that what are the different tasks we
want to manage so different different
tasks we can add up into the case of
griddle and we can override those tasks
also in case of Maven it's all about the
different phases which is being
happening over here and it's in sequence
manner so these phases happens in the
sequence order that how exactly you can
uh build up the sequence there but in
case of gridle you can have your own
tasks custom tasks also and you can
disrup the sequence and you can see that
how the different steps can be executed
in a different order so Maven is
something which is a phase mechanism
there but gridle is something which is
according to the features or the
flexibilities now griddle works on the
tasks whatever the task you want to
perform you uh it works directly on
those task there on the other hand uh
May is something does not have any kind
of inbuilt cach so every time you
running the build so separate uh things
or the plugins and all these information
gets loaded up which takes definitely a
lot of time on the the other hand gdle
is something which is uh using its own
internal cache so that it can make the
uh builds a little bit faster because
it's not something which is doing the
things from the scratch whatever the uh
things is already being available in the
cash so it's just pick that part and
from there it will proceed further on
the build Automation and that's the
reason why Gradle performance is much
faster as compared to Maven because it
uses as some kind of a cach in there and
then helps to improve the overall
performance now let's talk about the
gridle installation because this is an
very important aspect to be done because
when we are doing the installation we
have to download the Cradle executables
right so let's see that what are the
different steps is involved in the
process of the Cradle installation so
when we talk about the gridle
installation so there are primary four
steps which is available the very first
one is that you have to check if the
Java is installed now if the uh Java is
not installed so you can go to the open
jdk uh or you can go for the Oracle Java
so you can do the inst installation of
the jdk on your system so jdk8 is
something you can uh most commonly used
nowadays so you can install that once
the Java is downloaded and installed
then you have to do the Gradle uh
download Gradle there now once the
Gradle binaries are executable or or the
Z file gets downloaded so you can add
the environment variables and then you
can validate if the Cradle installation
is working fine as expected not so we
will be doing the grel installation into
our local systems and uh into the
windows platform and we'll see that how
exactly we can go for the installation
of cradle and we'll see that what are
the different version we are going to
install here so let's go back to the
system and see that how we can go for
the Gradle installation so this is the
website of the jdk of java orle Java now
here you have different jdk so from
there you can do whatever the uh option
you want to select you can go with that
so jdk8 is something which is most
commonly used nowadays like its most
comfortable or compatible version which
is available so um in case you want to
see the if the jdk is installed into
your system all you have to do is that
you have to just say like Java hone
version and that will give you the uh
output that whether the Java is
installed into your system or not so in
case my system the Java is installed but
if you really want to do the
installation you have to download the
jdk installer from this website from
this article website and then you can
proceed further on that part now once
the jdk is installed so you have to go
for the Cradle installation because
cradle is something the which will be
performing the build automations and all
that stuff
so you have to download the boundaries
like uh the Z file probably in which we
have the executables and all and then we
have to have have some particular
environment variables configured so that
we will be able to have the System
modified over there so right now we have
got like the prequests as an Java
version installed now the next thing is
that we have to install or download the
execute tables so uh in order to
download the latest gradel distribution
so you have to click on this one right
now over here there are different
options like uh you want to go for 6.7
now it's having like binary only or
complete we'll go for the binary only
because we don't want to have the source
we just want the binaries and
executables now it's getting downloaded
it's around close to 100 MB of the
installer which is there now we have to
just extract into a directory and then
the same uh path we need to configure
into the environment variable so that in
that way we will be able to see that how
the uh gridle executables will be
running and uh it will give the uh
complete output to us over here in this
case so it may take some time and once
the the uh particular modifications and
the download is done then we have to
extract it and once the extraction is
done so we will be able to go back and
uh have some particular uh version or
have the configurations established over
there so let's just wait for some time
and then we will be continuing with the
environment variables like this one so
once the installation and the extraction
is done now we just have to go to the
downloads where this one is downloaded
we have to extract it now extraction is
required so that we can have the setup
like like we can set up this path into
our environment variables and once the
path is configured and established we
will be able to start further on that
path on the execution so meanwhile these
files are getting extracted let's see so
we already got the folder structure over
here and uh we will see like we will
give this path here there is two
environment variables we have to
configure one is the Cradle underscore
home and one is the um in the path
variable so we'll copy this path here so
meanwhile this is getting uh extracted
we can see save our time and we can go
to the environment variable so we can
right click on this one
properties in there we have to go for
the advanced systems settings then
environment
variables now here we have to give it
like Gradle and go home now in this one
we will not be going giving it till the
bin directory so that only needs to be
there where the gridle is extracted so
we'll say okay and uh then we have to go
for the path variable where we will be
adding up a new entry in this one we
will be putting up till the pin
directory here because the crle
executable should be there when I'm
running the crle command so these two
variables I have to configure then okay
okay and okay so this one is done so now
you have to just open the command prompt
and see that whether the execution or
the uh commands which you're running is
is completely successful or not so
meanwhile it's extracting all the
executables and all those things it will
help us to understand that how the whole
build process or how the build tools can
be integrated over there now once the
extraction is done so you have to run
like
CMD Java iph version to check the
version of the Java and then the
Cradle underscore version is what you're
going to see check the version of the
Cradle which is installed and now you
can see that say that 6.7 version is
being installed over here in this case
so that's a way that how we are going to
have the crle installation performed
into our particular system so let's go
back to the content let's talk about the
crle Core Concepts here now in this one
we are going to talk about what are the
different Core Concepts of CR are all
about the very first one is the projects
here now a project uh represents a item
to be performed over here to be done
like uh deploying an application to a
staging environment performing some
build so gridle is something which is
required uh the projects um the gdle
project which you prepare is not having
multiple tasks which is available there
which is configured and all these task
all these different tasks needs to be
executed into a sequence now sequence is
again is a very important part because
again if the sequence is not me properly
then the uh execution will not be done
in a proper order so that's the very
important aspect here tasks is the one
in which is a a kind of an entity in
which we will be performing a series of
steps these tasks may be like
compilation of a source code preparing a
jar file preparing a web application
archive file or ER file also so we can
have like in some task we can even
publish our artifacts to the artifactry
so that we can store those artifacts
into a shade location so there are
different ways in which we can have
these uh particular tasks
executed now build scripts is the one in
which we will be storing all this
information what are the dependencies
what are the different task we want to
refer it's all going to be present in
the build. Gradle file there build.
Gradle file will be having the
information related to what are the
different dependencies you want to
download and you want to store there so
all these L will be a part of the build
scripts now let's talk about the
features of cradle what are the
different features which we can uh use
in case of cradle
here there are different type of uh
features which is available there so
let's talk about them one by one so the
very first one over here is the high
performance then um high performance is
something which we can see that we
already discussed that in case you are
using in large projects so griddle is
something which is in better approach as
compared to Maven because of the high
per performance which we are getting it
uses an internal cache which makes sure
that you are using like you are doing
the builds faster and that can give you
a higher performance over there second
one is the support it provides the
support so it yes definitely provides a
lot of uh support on how you can perform
the builds and it's being a latest tool
which is available there so the support
is also quite good in terms of how you
want to prepare the build how you want
to download the plugins different plugin
supports and the dependencies uh
information also there next one is multi
project build software so using this one
you can have multiple projects in case
in your repository you have multiple
projects here so all of them can be
easily built up with the help of this
particular tool so it supports multiple
project to be built up using the same
Gradle project and uh gridle scripts so
that support is also available with this
Gradle build Tool uh incremental builds
are also something which you can do with
the help of cradle so if you have uh
done only the incremental changes and
you want to perform only the increment
Al build so that can also be possible
with the help of a griddle here the uh
build scans so we can also perform the
build scans so we can use some uh
Integrations with sonar and all where we
can have the uh scans done to the source
code on understand on how the build
happens or how the source code really
happens on there so that code scan or
the build scans can also be performed
with this one and then uh it's a
similarity with Java so for Java it's
something which is uh considered as in
by default not even Java in fact Android
which is also using the Java programming
language is using the uh particular
cradle over here so that the build can
be done and it can gain uh benefits out
of that so in in all the manners in all
the different ways it's basically
helping us to see that how uh we can
make sure that this tool can help us in
providing a lot of features and that can
help us to make a reliable build tool
for our Java based projects or any other
programming based project here right now
let's see that how we can un a Java
project with with the Gradle here and uh
for that we have to go back and Gradle
is something which is already installed
we just have to create a directory where
we can have like how we can perform some
executions we can prepare some build
scripts and we can have a particular
execution of a gdle build happened over
there so let's go back to the machine
okay so we are going to open the
terminal here and we'll see that how we
can create it so first of all I have to
create a directory structure let's say
that we'll say like
griddle hyphen project
now once the project is created so we
can go inside this directory so to uh
create some uh cradle related projects
and preparing the files now uh in this
one we let's first create a particular
one so we will be saying like VI
build. cradle so in this one we are
going to put like uh two plugins we are
going to use so we are going to say like
apply
plin Java and uh then we are going to
say like
apply
plug-in
application so these two plugins we are
going to use and when we got this file
over here in this one so it shows like
build. gridle which is available there
in this case so two these files are
available now if you want to learn like
you know what are the different tasks
you can run like griddle tasks command
over there so gridle task will help you
know that what are the different tasks
which is available over here by
processing the build SCP scripts and all
so um this will definitely help you to
understand on giving you the output so
here all the different tasks are being
given and it will help you to understand
that what are different tasks you can
configure and you can work over here
just like jar files clean and all that
stuff build compile then uh init is
there then all these different uh
executions assemble then Java dog then
build then check test all these
different tasks are there and if you
really want to run the Gradle build so
you can can run like griddle clean to
perform the clean activity because right
now you are doing like if a build so
before that you can have a clean and
then you can run a a specific command or
you can run The Griddle clean build
which will perform the cleanup also and
it will at the same time we'll have the
build process also performed over there
so build and clean up both will be
executed over here and what is the
status whether it's a success or a
failure that will be given back to you
now in this case in the previous one if
you see that when you run the clean the
Gradle clean it was only running one
task but when you go for the uh build uh
process when you run the Gradle clean
build it's going to give you much more
information in fact you can also give me
uh further information like you can have
the hyph I info flag also there so that
if you want to get the details about the
uh different uh tasks which we which is
being executed over here so that also
you're going to get over here in this
one so you just have to put like hyph
iPhone Info and then all the these steps
will be given back to you that how these
uh tasks will be executed and the
response will be there so that's a way
that how you can create a pretty much
simple straightforward project in form
of cradle which can definitely help you
to run some couple of cradle commands
and then you can understand that what
are the basic commands you can run and
how the configurations really works on
there right let's go back to the main
content right now let's move on to the
next one so in the next one we are going
to see that how we can prepare a griddle
build project in case of eclipse now we
are not using the local system we are
not directly creating the folders and
the files here we are actually using the
eclipse for performing the creating a
new credle project over here so let's
move on that part okay so now the
eclipse is open and uh I have opened in
this one the very first thing is that we
have to do the gdle plug-in installation
so that we can create new projects on
cradle and uh then we have to uh
configure the path that how the Cradle
plug-in can be configured on the pref uh
preferences and all that stuff and then
we will be doing the build process so
the very first thing is that we have to
go to the eclipse
Marketplace in there we have to search
for
griddle so once the search is
done it will show us the plugins related
to Gradle so we have to go for build
ship Gradle integration so we'll click
on the install it will proceed with
installation it will download it in some
cases maybe it's part of the eclipse as
in uh in the ID so you can go to the
installed Tab and you can see that also
that if this plug-in is already
installed or not but in this case we are
installing it and uh once the
installation is done we just have to
restart the uh specific uh once we have
to restart this uh Eclipse so that the
changes can be
reflected so it's
downloading it's downloading the Cradle
here and once that that is installed we
will be able to use it over here in this
case in this scenario so we have to just
wait for that part so still downloading
the jar files so once the jar file is
done it's over the areas and download it
so after that we will be able to proceed
further on that download aspect so it's
going to take some time to download it
and once it's done we will be able to
proceed further now once the progress is
done so it's asking us for the restart
now so uh before that uh we just have to
click on restart now and then the
Eclipse will be restarted all together
again here so you can do it manually or
you can go for that options it just
require a restart so that the new
changes can be reflected over here so
the plugins can be activated and can be
referenced here now we have to just uh
put up like the you know the
configuration where we can have the
system so we can go for the gridle
configuration so we can go for Windows
and then
preferences now in this case we have to
go for the uh for the ones in which the
Cradle option is available there so
Gradle is what we are going to select
now user home the Gradle user home is
what we need to use right so you want to
go for the gridle you want to go for
local installation so so all these
options you can use you can if if you go
for the gridle rapper then it will be
downloading the gridle locally and it is
going to use the gridle W or gridle W.B
file but if you already have an
installation locally so you can pray for
that also right now uh in the previous
demo we have already got the uh griddle
uht so we just have to go for the
downloads in the downloads already gdle
is available so we are going to select
that part here so this is what we are
going to
select right so this represents that
this is the directory structure in which
we are having the uh mechanism so you
can either go for the build scan so you
can select the build scan also so once
this uh is enabled then all the projects
will be scanned and will be you know
published and uh it's in kind of
additional option which is available if
you really want to disable it you can
disable it also and you can go with this
configuration also so uh this is where
the uh particular gridle folder is being
put over here in this case and uh then
we have to just click on
apply and we just have to click on apply
and close so with this one the
particular execution is done now we will
be going for the project creation so you
can right click over here or you can go
to the file also so here we going to go
for the CH project and in this we are
going to have a Gradle project so Gradle
project is what we are going to create
here and
next so we are going to say like
Gradle
project and then
next so once that is done so
finish so uh with this one when you
create the project so what will happen
that uh automatically there will be a
folder structure will be available there
right and uh there are some uh Gradle
scripts which will also be created there
so we will be doing the modifications
there and we'll see that how the uh
particular Gradle build script looks
like and how we can we will be adding
some couple of uh selenium related
dependencies and we'll see that how we
can have more and more dependencies
added and what will be the impact of
those dependencies on the overall
project so that also it's very important
aspect to be considered so let this
processing be happened over there it
just creating and uh some plugins and
boundaries are getting installed and
getting downloaded so we'll see that
once the project is uh imported
completely executed over here and got
created we can extract that now if you
see here the particular option is
available about The Griddle tasks so you
can extract it also and you will be able
to know that what are the different
tasks which is available there let's see
that in the build they are running like
build these are the different tasks
which is happening inside the build
process so g g executions will be also
available over here in this case and
gridle tasks will be different will be
represented over here in this one so you
just have to extract on the Gradle
project okay this is the library which
is available now uh what happens that uh
you will be able to have like settings.
Gradle in this one you will be able to
have like okay Gradle hyphen project is
something which is available there in
this one so that's what being at
refering then we have over here as in
these folder structures which is created
like Source main Java this is the one
source test Java is the one which is
available as in the folder structure and
Source test resources are also available
here so the main source main resources
are also available now in this case what
happens that these are the dependencies
project and external these are the
different dependencies are available
there so let's see let's add an
dependency over here in this one in The
bail. Griddle script and see that how we
can do that if we open build. gdle file
so you can see that these dependencies
are there like test implementation junit
is available there right and then we
have a implementations of this one which
is available now these jav files when
you put up it will automatically be
added up as in part of this one as in
part of the uh particular uh
dependencies over here and uh which
means that you don't have to store them
as an within the repository and
automatically they can be happened over
there so let's open a dependency page so
we will be going to MN repository where
we will be opening a dependency link so
this is the dependency link here so slum
hyphone Java is available and it can
give you the uh dependency for all the
different options now we have for Maven
this is the one and for GLE this is the
one here so we have to just copy this
one and uh we have to use it as a
dependency so this is the group and this
is the name and the version which we are
using here now we have copied this one
so we will go back to the eclipse so
here we have to just put that
dependency and uh we have to just save
it so uh this is something which is
providing like selenium dependencies
which is available so now we have to
just refresh the project so right click
over here then you will be able to see
the options in the Cradle saying that
refresh cral project now once the M you
do that so you will be able to do like
for the first time maybe it will take
some time to download all the
dependencies which is related to
selenium but after that you will be able
to see like the dependencies will be
simply added up over here in this case
so you can see that all the selenium
related dependencies are added up for
any reason if you comment these ones and
you say like
synchronize
again so you will see that all the
dependencies which you are adding up
from this selenium repres uh from the
selenium perspective will be gone back
again so this is the way that how you
can keep on adding the dependencies
which is required for preparing your
build for your source code and from
there you will be able to proceed
further on the execution part so that's
the best part about this uh cradle here
so that's a way that how we are going to
prepare a gridle project within the
eclipse and now you can keep on adding
like the source code code in this one
and that's a way that how the code base
will be added up over here so when we
talk about the gridle installation so
there are primary four steps which is
available the very first one is that you
have to check if the Java is installed
now if the uh Java is not installed so
you can go to the open jdk uh or you can
go for the Oracle Java so you can do the
installation of the jdk on your system
so jdk8 uh is something you can uh most
commonly used nowadays so you can
install that once the Java is downloaded
and install then you have to do the
Gradle uh download Gradle there now once
the Gradle boundaries are executable uh
or the user file gets downloaded so you
can add the environment variables and
then you can validate if the Gradle
installation is working fine as expected
not so we will be doing the Gradle
installation into our local systems and
uh into the windows platform and we'll
see that how exactly we can go for the
installation of cradle and we'll see
that what are the different version we
are going to install here so let's go
back to the system and see that how we
can go for the Gradle installation so
this is the website of the jdk of java
orle Java now here you have different
jdk so from there you can do whatever
the uh option you want to select you can
go with that so jdk8 is something which
is most commonly used nowadays like it's
most comfortable or compatible version
which is available so um in case you
want to see that if the jdk is installed
into your system all you have to do is
that you have to just say like Java
hyphone version and that will give you
the uh output at whether the Java is
installed into your system or not so in
case my system the Java is installed but
if you really want to do the
installation you have to download the
jdk installer from this website from
this article website and then you can
proceed further on that part now once
the jdk is installed so you have to go
for the Cradle installation because
cradle is something the which will be
performing the build automations and all
that stuff so you have to download the
bindar like uh the Z file probably in
which we have the executables and all
and then we have to have have some
particular environment variables
configured so that we will be able to
have the System modified over there so
right now we have got like the prequests
as in Java version installed now the
next thing is that we have to install or
download the execute tables so uh in
order to download the latest gradal
distribution so you have to click on
this one right now over here there are
different options like uh you want to go
for 6.7 now it's having like binary only
or complete we'll go for the binary only
because we don't want to have the source
we just want the binaries and the
executables now it's getting downloaded
it's around close to 100 MB of the
installer which is there now we have to
just extract into a directory and then
the same uh path we need to configure
into the environment variable so that in
that way we will be able to see that how
the uh gridle executables will be
running and uh it will give the uh
complete output to us over here in this
case so it may take some time and once
the uh particular modifications and the
download is done then we have to extract
it and once the extraction is done so we
will be able to go back and uh have some
particular version or have the
configurations established over there so
let let's just wait for some time and
then we will be continuing with the
environment variables like this one so
once the installation and the extraction
is done now we just have to go to the
downloads where this one is downloaded
we have to extract it now extraction is
required so that we can have the setup
like we can set up this path into our
environment variables and once the path
is configured and established we will be
able to start further on that part on
the execution so meanwhile these files
are getting extracted let's see so we
already got the folder structure over
here and uh we will see like we will
give this path here there is two
environment variables we have to
configure one is the cre Le disc go home
and one is the um in the path variable
so we'll copy this path here so
meanwhile this is getting uh extracted
we can save our time and we can go to
the environment variable so we can right
click on this one
properties in there we have to go for
the advanced systems settings then in
environment
variables now here we have to give it
like
Gradle underscore home now in this one
we will not be going giving it till the
bin directory so that only needs to be
there where the gdle is extracted so
we'll say okay and uh then we have to go
for the path variable where we will be
adding up a new entry in this one we
will be putting up till the pin
directory here because the gdle
executable should be there when I'm
running the gdle command so these two
variables I have to configure then okay
okay and okay so this one is done so now
you have to just open the command prompt
and see that whether the execution or
the uh commands which you're running is
is completely successful or not so
meanwhile it's extracting all the
executables and all those things it will
help us to understand that how the whole
build process or how the build tools can
be integrated over there now once the
extraction is done so you have to run
like
CMD Java iph version to check the
version of the Java and then the
Cradle underscore version is what you're
going to see see check the version of
the Cradle which is installed and now
you can see that it shows that 6.7
version is being installed over here in
this case so that's a way that how we
are going to have the crle installation
performed into our particular system the
birth of
selenium selenium was primarily created
by Jason Huggins in 2004 Jason an
engineer at thought work was working on
a web application that needed to be
tested frequently he realized that the
repeated manual testing of the
application is becoming inefficient so
he created a JavaScript language that
automatically controlled the browser's
actions this program was named
JavaScript test Runner after he realized
that his idea of automating the web
applications has a lot of potential he
made the JavaScript test Runner open
source and it was later renamed as
selenium
core so we know that Jason was a person
who initially created selenium but then
we must also know that selenium is a
collection of different tools and since
there are different tools there will be
several developers too to be exact about
the number of different tools there are
four different tools that have their own
creators let's have a look at all of
them
the first tool is the selenium remote
control or the selenium RC that was
created by Paul
Hammond then comes selenium grid that
was developed by Patrick
lightbody the third tool is the selenium
IDE that was created by shinya katani
and the fourth tool that is the selenium
web driver was created by Simon
stward we shall be learning about all
these tools in great detail as we move
forth in our video first let's have a
look at what is selenium selenium is a
very popular open- Source tool that is
used for automating the test carried on
the web browsers there may be various
programming languages like Java C python
Etc that could be used to create
selenium test
scripts this testing that is done using
selenium tool is referred to as selenium
testing we must understand that selenium
allows the testing of web applications
only we can neither test any computer
software nor any mobile application
using
selenium selenium is composed of several
tools with each having its own specific
role and serving its own testing
needs moving forth let's have a look at
the features of selenium which will help
us understand the reason behind its
widespread popularity so so here we have
a set of features of
selenium selenium is an open source and
portable framework that has a playback
and record feature it is one of the best
cloud-based testing platform and
supports various OS and languages it can
be integrated with several testing
Frameworks and supports parallel test
execution we will be talking about all
these features in detail as we move
forward so here the first feature that
we have is open- source and portable
framework this feature states that
selenium is an open source and portable
framework for testing web applications
in addition to that selenium commands
are categorized in terms of different
classes which make it easier to
understand and
implement the second feature is the
playback and record feature the feature
states that the tests can be authorized
without learning a test scripting
language language with the help of
playback and record
features the next feature says that
selenium is a cloud-based testing
platform selenium is a leading
cloud-based testing platform that allows
testers to record their actions and
Export them as a reusable script with a
simple to understand and easy to use
interface moving forth the next feature
states that selenium supports various
operating systems browsers and
programming languages the tool supports
programming languages such as cop Java
python PHP Ruby Pearl and JavaScript if
we talk about the operating systems then
selenium supports operating systems like
Android iOS Windows Linux Mac and
Solaris and the tool also supports
various browsers like Google Chrome
Mozilla Firefox Internet Explorer Edge
Opera
Safari
Etc then the next feature we have is the
integration with testing
Frameworks selenium can be well
integrated with testing Frameworks like
test NG for application testing and
generating reports and also selenium can
be integrated with Frameworks like ant
and Maven for source code compilation
the last feature in our list is the
parallel test execution selenium enables
parall testing which reduces time and
increases the efficiency of the test
selenium requires fewer resources as
compared to other automation testing
tools now let's move further and get to
know different selenium tools by now we
know that there are four different tools
that come under the selenium suit the
four tools are selenium remote control
selenium grid selenium IDE and selenium
web
driver we we shall have a look at these
four Tools in detail one after the
another beginning with selenium remote
control selenium remote control enables
the writing of automated web
applications in languages such as Java C
Pearl Python and PHP to build complex
tests such as reading and writing files
quering a database and emailing the test
results selenium RC was a sustainable
project for a very long time before
selenium web driver came into existence
and hence selenium RC is hardly in use
today as the web driver offers more
powerful
functionalities the second tool we shall
see is the selenium
grid selenium grid is a smart proxy
server that enables the running of test
in parallel on several
machines this is made possible when the
commands are rooted to the remote web
browser instances and one server acts as
as the Hub The Hub here is responsible
for conducting several test on multiple
machines selenium grid makes cross
browser testing easy as a single test
can be carried on multiple machines and
browsers all together making it easy to
analyze and compare the
results here there are two main
components of selenium grid Hub and the
node Hub is a server that accepts the
access request from from the web driver
client rooting the Json test commands to
the remote drivers on nodes and here the
node refers to the Remote device that
consist of a native OS and a remote web
driver it receives the request from the
Hub in the form of a Json test commands
and executes them using the web driver
moving forth the third tool is the
selenium IDE if you want to begin with
the selenium IDE it needs no additional
setup except installing the extension of
your
browser it provides an easy to use tool
that gives instant feedbacks selenium
IDE records multiple locators for each
element it interacts with if one locator
fails during the playback the others
will be tried until one is successful
the IDE makes the test debugging easier
with features like setting break points
and pausing exceptions
through the use of the Run command you
can reuse one test case inside one
another and also selenium IDE can be
extended through the use of plugins they
can introduce new commands to the IDE or
integrate with a third party service the
last and the fourth tool in the selenium
suit is the selenium web driver selenium
web driver is the most critical
component of the selenium tools suit
that allows cross browser compatibility
testing the web driver supports various
operating systems like Windows Mac Linux
Unix Etc it provides compatibility with
a range of languages including python
Java and pearl along with it it provides
supports different browsers like Chrome
Firefox Opera Safari and Internet
Explorer the selenium web driver
completes the execution of test Scripts
faster when compared to other tools and
also it provides compatibility with
iPhone driver HTML unit driver and
Android driver now after you know about
the tools the question arises which tool
to choose so in the next topic we shall
see different factors on the basis of
which we may decide which tool would be
more suitable for us here we shall have
a look at the reasons why one should
choose that particular tool tool we
shall begin with selenium remote control
selenium remote control or selenium RC
should be chosen to design a test using
a more expressive language than
Selen Selen is a set of selenium
commands that are used to test our web
applications then the selenium RC might
be chosen to run tests against different
browsers on different operating systems
the RC may be chosen to deploy tests
across multiple environments using
selenium grid it helps in testing the
applications against a new browser that
supports JavaScript and web applications
with complex Ajax based
scenarios now the second tool for which
we shall see the reasons are selenium
grid selenium grid as we learned in the
reasons for selenium RC is used to run
selenium RC scripts in multiple browsers
and operating systems
simultaneously the grid is used to run a
huge test suit that needs to be
completed at the earliest possible time
now comes the third tool the third tool
is selenium IDE selenium ID is used to
learn about Concepts on automated
testing and selenium these Concepts
include selenous commands such as type
Open click and wait assert verify Etc
the concepts also include locators such
as ID name xath CSS selector
Etc selenium IDE enables the execution
of customized JavaScript code using
runscript and exporting test cases in
several different formats the IDE is
used to create tests with a limited
amount of knowledge in programming and
these test cases and test suits can be
exported later to RC or web driver now
finally let's have a look at the reasons
to choose a last tool that is selenium
web driver selenium web driver uses a
certain programming language in
designing a test case the web driver is
used to test applications that are rich
in Ajax based functionalities execute
tests on HTML unit browser and create
customized test results so this is what
we're going to be covering in this
session we're going to cover what life
is like for using Jenkins and the issues
that Jenkins specifically addresses then
we'll get into what Jenkins is about and
how it applies to continuous integration
and the other continuous integration
tools that you need in your devops team
then specifically we'll Deep dive into
features of Jenkins and the Jenkins
architecture and we'll give you a case
study of a company that's using Jenkins
today to actually transform how their it
organization is operating so let's talk
a little bit about about life before
Jenkins let's see this scenario I think
it's something that maybe all of you can
relate to as developers we all write
code and we all submit that code into a
code repository and we all keep working
away writing our unit tests and
hopefully we're running our unit tests
but the problem is that the actual
commits that actually get sent to the
code repository aren't consistent you as
developer may be based in India you may
have another developer that's based in
the Philippines and you may have another
team Le that's based in the UK and
another development team that's based in
North America so you're all working at
different times and you have different
amounts of code going into the code
repository There's issues with the
integration and you're kind of running
into a situation that we like to call
development hell where things just
aren't working out and there's just lots
of delays being added into the project
and the bugs just keep mounting up the
bottom line is the project is is delayed
and in the past what we would have to do
is we'd have to wait until the entire
software code was built and tested
before we could even begin checking for
errors and this just really kind of
increased the amount of problems that
you'd have in your project the actual
process of delivering software was slow
there was no way that you could actually
iterate on your software and you just
ended up with just a big headache with
teams pointing fingers at each other and
blaming each other so let's jump into
Jenkins and see what Jenkins is and how
it can address these problems so Jenkins
is a product that comes out of the
concept of continuous integration that
you may have heard of as power
developers where you'd have two
developers sitting next to each other
coding against the same piece of
information what they were able to do is
to continuously develop their code and
test their code and move on to new
sections of code Jenkins is a product
that allows you to expand on that
capacity to your entire team so you're
able to submit your codes consistently
into a source code environment so there
are two ways in which you can do
continuous delivery one is through
nightly builds and one is through
continuous so the approach that you can
look at continuous delivery is modifying
the Legacy approach to building out
Solutions so what we used to do is we
would wait for nightly builds and the
way that our nightly builds would work
and operate is that as Cod dib we would
all run and have a cut of time at the
end of the day um that was consistent
around the world that we would put our
codes into a single repository and at
night all of that code would be run and
operated and tested to see if there were
any changes and a new build would be
created and that would be referred to as
the nightly build with continuous
integration we're able to go one step
further we're able to not only commit
our changes into our source code but we
can actually do this continuously
there's no need to race and have a team
get all of their code in at arbitrary
time you can actually do a continuous
release because what you're doing is
you're putting your tests and your
verification Services into the build
environment so you're always running
Cycles to test against your code this is
the power that Jenkins provides in
continuous integration so let's dig
deeper into continuous integration so
the concept of continuous integration is
that as a developer you're able to pull
from a repository the code that you're
working on and then you'll be able to
then at any time submit the code that
you're working on into a continuous
integration server and the goal of that
continuous integration server is that it
actually goes ahead and validates and
passes any tests that a tester may have
created now if on the continuous
integration server a test isn't passed
then that code gets sent back to the
developer and the developer can then
make the changes it allows the developer
to to actually do a couple of things it
allows the developer not to break the
build and we all don't want to break the
builds that are being created but it
also allows the developer not to
actually have to run all the tests
locally on their computer running tests
particularly if you have a large number
of tests can take up a lot of time so if
you can push that service off to another
environment like a continuous
integration server it really improves
the productivity of your developer
what's also good is that if there are
any code errors that have come up that
may be Beyond just the standard CI test
so maybe there's a Code the way that you
write your code isn't consistent those
errors can then be passed on easily from
the tester back to the developer too the
goal from doing all this testing is that
you're able to release and deploy and
your customer is able to get new code
faster and when they get that code it
simply just works so let's talk a little
bit about some of the the tools that you
may have in your continuous integration
environment so the cool thing with
working with continuous integration
tools is that they are all open source
at least the ones that we have listed
here are open source there are some that
are private but typically you'll get
started with open-source tools and it
gives you the opportunity to understand
how you can accelerate your environment
quickly so bamboo is a continuous
integration tool that specifically runs
multiple builds in par for faster
compilation so if you have multiple
versions of your software that runs on
multiple platforms this is a tool that
really allows you to get that up and
running super fast so that your teams
can actually test how those different
builds would work for different
environments and this has integration
with an and mavin and other similar
tools so one of the tools you're going
to need is a tool that allows you to
automate the software build test and
release process and buildbot is that
open-source product for you again it's
an open-source tool so there's no
license associated with this so you can
actually go in and you can actually get
the environment up and running and you
can then test for and build your
environment and create releases very
quickly so buildbot is also written in
Python and it does support power
execution jobs across multiple platforms
if you're working specifically on Java
projects that need to be built and test
then Apache Gump is the tool for you it
makes all of those projects really
really easy it makes all the Java
projects easier for you to be able to
test with API level and functionality
level testing so one of the popular
places to actually store code and create
a versioning of your code is GitHub and
it's a service that's available on the
web just recently acquired by Microsoft
if you are storing your projects in
GitHub then you'll be able to use Travis
continuous integration or Travis CI and
it's a tool designed specifically for
hosted GitHub projects and so finally
we're covering Jenkins and Jenkins is a
central tool for automation for all of
your projects now when you're working
with Jenkins sometimes you'll find
there's documentation that refers to a
product called Hudson Hudson is actually
the original version of the product that
finally became Jenkins and it was
acquired by Oracle when that acquisition
happened the team behind um Hudson was a
little concerned about the direction
that Oracle May potentially go with
Hudson and so they created a hard Fork
of Hudson that they renamed Jenkins and
Jenkins has now become that open-source
project it is one of the most popular
and continuously contributed projects
that's available as open source so
you're always getting new features being
added to it it's a tool that really
becomes the center for your CI
environment so let's jump into some of
those really great features that are
available in Jenkins so Jenkins itself
is really comprised of five key areas
around easy installation easy
configuration plugins extensibility and
distribution so as I mentioned for the
easy installation Jenkins is a
self-contained Java program and that
allows it to run on most popular
operating systems including Windows Mac
OS and Unix you can even run it on Linux
it really isn't too bad to set up it
used to be much harder than it is to day
the setup process has really improved
the web interface makes it really easy
for you to uh check for any errors in
addition you have great built-in help
one of the things that makes tools like
Jenkins really powerful for developers
and continuous integration teams and
your devops teams as a whole when you
have plugins that you can then add in to
extend the base functionality of the
product Jenkins has hundreds of plugins
and you can go and visit the update
Center and see which other plugins that
would be good for your Dev Ops
environment suddenly check it out
there's just lots of stuff out there in
addition to the plug-in architecture
Jenkins is also extremely extensible the
opportunity for you to be able to
configure Jenkins to fit in your
environment it's almost endless now it's
really important to remember that you
are extending Jenkins not creating a
custom version of Jenkins and that's a
great differentiation because the core
found found ation remains as the core
Jenkins product the extensibility can
then be continued with newer releases of
Jenkins so you're always having the
latest version of Jenkins and your
extensions mature with those core
foundations the distribution and the
nature of Jenkins makes it really easy
for you to be able to have it available
across your entire network it really
will become the center of your CI
environment and it's certainly one of
the easier tools and more effective
tools for devops so let's jump into the
standard Jenkins pipeline so when you're
doing development you start off and
you're coding away on your computer the
first thing you have to do when you're
working in the Jenkins pipeline is to
actually commit your code now as a
developer this is something that you're
already doing or at least you should be
doing you're committing your code to a
git server um or to an SVN server or
similar type of service so in this
instance you'll be using Jenkins as the
place for you to commit your code
Jenkins will then create a build of your
code and part of that build process is
actually going through and running
through tests and again as a developer
you're already comfortable with running
unit tests and writing those tests to
validate your code but there may be
additional tests that Jenkins is running
so for instance as a team you may have a
standard set of tests for how you
actually write out your code so that
each team member can understand the code
that's been written and those tests can
also be included in the testing process
within the Jenkins environment assuming
everything pass the the tests you can
then get everything placed in a stage
and release ready environment within
Jenkins and finally you can ready to
deploy or deliver your code to a
production environment Jenkins is going
to be the tool that helps you with your
server environment to be able to deploy
your code to the production environment
and the result is that you're able to
move from a developer to production code
really quickly this whole process can be
automated rather than having to wait for
people to actually test your codes or
going through a nightly build you're
looking at being able to commit your
code and go through this testing process
and release process continuously as an
example companies Etsy will release up
to 50 different versions of their
website every single day so let's talk
about the architecture within Jenkin
allows you to be so effective at
applying a continuous delivery devops
environment so the server architecture
really is broken up into two sections on
the left hand side of section you have
the code the developers are doing and
submitting that code to a source code
repository and then from then Jenkins is
your continuous integration server and
it will then pull any code that's been
sent to the source code repository and
will run tests against it it will use a
build server such as mavin to actually
then build the code and every single
stage that we have that Jenkins manages
there are constant tests so for instance
if a build fails that feedback is sent
right back to the developers so that
they can then change their code so that
the build environment can run
effectively the final stage is to
actually execute specific test scripts
and these test scripts can be written in
selenium so it's probably good to
mention here that both mavin and
selenium are are plugins that run in the
Jenkins environment so before we were
talking about how Jenkins can be
extended with plugins mavin and selenium
are just two very popular examples of
how you can extend the Jenkins
environment the goal to go through this
whole process again it's an automated
process is to get your code from the
developer to the production server as
quickly as possible have it fully tested
and have no errors so it's probably
important at this point to mention uh
one one piece of information around the
Jenkins environment that if you have
different code builds that need to be
managed and distributed this will
require that you need to have multiple
builds being managed Jenkins itself
doesn't allow for multiple files and
builds to be executed on a single server
you need to have a multiple server
environment with running different
versions of Jenkins for that to be able
to happen so let's talk a little bit
about the Master Slave architecture
within Jenkins so what we have here is
an overview of the Master Slave
architecture within Jenkins on the left
hand side is the remote source code
repository and that remote source code
repository could be GitHub or it could
be uh Team Foundation services or the
new Azure Dev Ops Code repository or it
could be your own git repository the
Jenkins server acts as the master
environment on the left hand side and
that Master environment can then push
out to multiple other Jenkin slave
environments to distribute the workload
so it allows you to run multiple builds
and tests and production environments
simultaneously across your entire
architecture so Jenkins slaves can be
running the different build versions of
the code for different operating systems
and the server Master is controlling how
each of those builds operate so let's
step into a quick story of a company
that has used Jenkins very successfully
so here's a use case scenario um over
the last 10 or 15 years there has been a
significant shift within the automotive
industry where manufacturers have
shifted from creating complex Hardware
to actually creating software we've seen
that with companies such as Tesla where
they are creating uh software to manage
their cars we see the same thing with
companies such as General Motors with
their OnStar program and Ford just
recently have rebranded themselves as a
technology company rather than just a
automotive company what this means
though is that the software within these
cars is becoming more complex and
requires more testing to allow more
capabilities enhancements to be added to
the core software so Bosch is a company
that specifically ran into this problem
and their challenge was that they wanted
to be able to streamline the
increasingly complex Automotive software
by adopting continuous integration and
continuous delivery best practices with
the goal of being able to delight and
exceed the customer expectations of the
end user so Bosch has actually used
Cloud bees which is the Enterprise
Jenkins environment so to be able to
reduce them the number of manual steps
such as building deploying and testing
Bosch has introduced the use of
cloudbees from Jenkins and this is part
of the Enterprise Jenkins platform it
has significantly helped improve the
efficiencies throughout the whole
software development cycle from
automation stability and transparency
because Jenkins becomes a self- auditing
environment now the results have been t
tangible previously it took 3 days
before a bill process could be done and
now it's taken that same 3-day process
and reduced it to Less Than 3 hours that
is significant large scale deployments
are now kept on track and have expert
support and there is clear visibility
and transparency across the whole
operations through using the Jenkins
tools let me start the session with this
scenario let's imagine how life would
have been without Spotify for those who
are hearing about Spotify for the first
time uh Spotify is an online music
service offering and it offers instant
access to over 16 million licensed songs
Spotify now uses AWS Cloud to store the
data and share it with their customers
but prior to AWS they had some issues
imagine using spotify before AWS let's
talk about that back then users were
often getting errors because Spotify
could not keep up with the increased
demand for storage in every new day and
that led to users getting upset and
users canceling the subscription the
problem Spotify was facing at that time
was their users were present globally
and were accessing it from everywhere
and uh they had different latency in
their applications and Spotify had a
demanding situation where they need to
frequently catalog the songs released
yesterday today and in the future and
this was changing every new day and the
songs coming in rate was about 20,000 a
day and back then they could not keep up
with this requirement and needless to
say they were badly looking for way to
solve this problem and that's when they
got introduced to AWS and it was a
perfect fit and match for their problem
AWS offered a dynamically increasing
storage and that's what they needed AWS
also offered tools and techniques like
storage life cycle management and
trusted advisor to properly utilize the
resource so we always get the best out
of the resource used AWS addressed their
concerns about easily being able to
scale yes you can scale the AWS
environment very easily how easily one
might ask it's just a few button clicks
and AWS solved spotify's problem let's
talk about how it can help you with your
organizations problem let's talk about
what is AWS first and then let's bleed
into how AWS became so successful and
the different types of services that AWS
provides and what's the future of cloud
and AWS in specific let's talk about
that and finally we'll talk about a use
case where you will see how easy it is
to create a web application with AWS all
right let's talk about what is AWS AWS
or Amazon web services is a secure cloud
service platform it is also pay as youo
type billing model where there is no
upfront or Capital cost we'll talk about
how soon the service will be available
well the servers will be available in a
matter of seconds with AWS you can also
do identic and access management that is
authenticating and authorizing a user or
a program on the flight and almost all
the services are available on demand and
most of them are available
instantaneously and as we speak Amazon
offers 100 plus services and this list
is growing every new week now that would
make you wonder how AWS became so
successful of course it's their
customers let's talk about the list of
well-known companies that has their it
environment in AWS Adobe Adobe uses a s
to provide multi-terabyte operating
environments for its customers by
integrating its system with AWS Cloud
adob can focus on deploying and
operating its own software instead of
trying to you know deploy and manage the
infrastructure Airbnb is another company
it's an Community Marketplace that
allows property owners and travelers to
connect each other for the purpose of
renting unique vacation spaces around
the world and the rbnb community users
activities are connected on the website
and through iPhones and Android
applications Airbnb has a huge
infrastructure in AWS and they're almost
using all the services in AWS and are
getting benefited from it another
example would be Autodesk Autodesk
develops software for engineering
designing and entertainment Industries
using services like Amazon RDS or
rational database service and Amazon S3
or Amazon simple storage servers
Autodesk can focus on deploying or
developing its machine learning tools
instead of spending that time on
managing the infrastructure AOL or
American online uses AWS and using AWS
they have been able to close data
centers and decommission about 14,000
in-house and collocated servers and move
Mission critical workload to the cloud
and extend its Global reach and save
millions of dollars on energy resources
Bit Defender is an internet security
software firm and their portfolio of
softwares include antivirus and
anti-spyware products Bit Defender uses
ec2 and they're currently running few
hundred instances that handle about 5
tabt of data and they also use elastic
load balancer to load balance the
connection coming in to those instances
across availability zones and they
provide seamless Global delivery of
service because of that the BMW group it
uses AWS for its new connected Car
application that collects sensor data
from BMW 7 series cards to give drivers
dynamically updated map information
canons offers Imaging products division
benefits from faster deployment times
lower cost and Global reach by using AWS
to deliver cloud-based services such as
mobile print the office Imaging products
division uses AWS such as Amazon S3 and
Amazon Route 53 Amazon cloudfront and
Amazon IM for their testing development
and Production Services Comcast it's the
world's largest cable company and the
leading provider of internet service in
the United States Comcast uses AWS in a
hybrid environment out of all the other
Cloud providers Comcast chose AWS for
its flexibility and scalable hybrid
infrastructure Docker is a company
that's helping redefine the way
developers build ship and run
applications this company focuses on
making use of containers for this
purpose and in AWS the service called
the Amazon ec2 container service is
helping them achieve it the esa or
European Space Agency although much of
esa's work is done by satellites some of
the programs data storage and Computing
infrastructure is built on Amazon web
services Esh chose AWS because of its
economical pay as EO system as well as
its quick startup time the Guardian
newspaper uses AWS and it uses a wide
range of AWS services including Amazon
Kinesis Amazon red shift that power an
analytic Dash dashboard which editors
use to see how stories are trending in
real time Financial Times ft is one of
the world's largest leading business
news organization and they used Amazon
red shift to perform their analysis A
Funny Thing Happened Amazon red shift
performed so quickly that some
analysists thought it was malfunctioning
they were used to running queries
overnight and they found that the
results were indeed correct just as much
faster by using Amazon Redi FD is
supporting the same business function
with costs that are 80% lower than what
was before general electric GE is at the
moment as we speak migrating more than
9,000 workloads including 300 desperate
Erp systems to AWS while reducing its
data center footprint from 34 to 4 over
the next 3 years similarly Howard
Medical School HTC IMDb McDonald's NASA
Kelloggs and lot more are using the
services Amazon provides and are getting
benefited from it and this huge success
and customer portfolio is just the tip
of the Ice book and if we think why so
many adapt AWS and if we let AWS answer
that question this is what AWS would say
people are adapting AWS because of the
security and durability of the data and
endtoend privacy and encryption of the
data and storage experience we can also
rely on aw's way of doing things by
using the AWS tools and techniques and
suggested best practices built upon the
the years of experience it has gained
flexibility there is a greater
flexibility in AWS that allows us to
select the OS language and database easy
to use swiftness in deploying we can
host our applications quickly in AWS be
it a new application or migrating an
existing application into AWS
scalability the application can be
easily scaled up or scaled down
depending on the user requirement cost
saving we only pay for the compute power
storage and other resources you use and
that to without any long-term
commitments now let's talk about the
different types of services that AWS
provides the services that we talk about
fall in any of the following categories
you see like you know compute storage
database Security customer engagement
desktop and streaming Mission learning
developers tools stuff like that and if
you do not see the service that you're
looking for it's probably is because AWS
is creating it as we speak now let's
look at some of them that are very
commonly used within Computer Service we
have Amazon ec2 Amazon elastic beant
stock Amazon light sale and Amazon
Lambda Amazon ec2 provides compute
capacity in the cloud now this capacity
is secure and it is resizable based on
the user's requirement now look at this
the requirement for the web traffic
keeps changing and behind the scenes in
the cloud ec2 can expand its environment
to three instances and during no load it
can shrink its environment to just one
resource elastic beant stock it helps us
to scale and deploy web applications and
it's made with a number of programming
languages elastic beanock is also an
easy to use service for deploying and
scaling web applications and services
deployed be it in Java net PHP nodejs
python Ruby doer and lot other familiar
services such as Apache passenger and
IIs we can simply upload our code and
elastic beanock automatically handles
the deployment from capacity
provisioning to load balancing to
autoscaling to application Health
monitoring and Amazon lightell is a
virtual private server which is easy to
launch and easy to manage Amazon
lightell is the easiest way to get
started with AWS for developers who just
need a virtual private server lightell
includes everything you need to launch
your project quickly on a virtual
machine like SSD based storage a virtual
machine tools for data transfer DNS
management and a static IP and that to
for a very low and predictable price AWS
Lambda has taken cloud Computing
Services to a whole new level it allows
us to pay only for the compute time no
need for provisioning and managing
servers and AWS Lambda is a compute
service that lets us run code without
provisioning or managing servers Lambda
executes your code only when needed and
scales automatically from few requests
per day to thousands per second you pay
only for the compute time you consume
there is no charge when your code is not
running let's look at some storage
services that Amazon provides like
Amazon S3 Amazon glitch sh Amazon abs
and Amazon elastic file system Amazon S3
is an object storage that can store and
retrive data from anywhere websites
mobile apps iot sensors and so on can
easily use Amazon S3 to store and
retrive data it's an object storage
built to store and drive any amount of
data from anywhere with its features
like flexibility in managing data and
the durability it provides and the
security that it provides Amazon simple
storage service or S3 is a storage for
the internet and Glacier Glacier is a
cloud storage service that's used for
archiving data and long-term backups and
this Glacier is an secure durable and
extremely lowcost cloud storage service
for data archiving and long-term backups
Amazon EBS Amazon elastic Block store
provides Block store volumes for the
instances of ec2 and this elastic Block
store is highly available and a reliable
storage volume that can be attached to
any running instance that is in the same
availability Zone ABS volumes that are
attached to the E intenses are exposed
as storage volumes that persistent
independently from the lifetime of the
instance an Amazon elastic file system
or EFS provides an elastic file storage
which can be used with AWS cloud service
and resources that are on premises an
Amazon elastic file system it's an
simple it's scalable it's an elastic
file storage for use with Amazon cloud
services and for on premises resources
it's easy to use and offers a a simple
interface that allows you to create and
configure file systems quickly and
easily Amazon file system is built to
elastically scale on demand without
disturbing the application growing and
shrinking automatically as you add and
remove files so your application have
the storage they need and when they need
it now let's talk about databases the
two major database flavors are Amazon
auds and Amazon red shift Amazon auds it
really eases the process involved in
setting up operating and scaling a
rational database in the cloud Amazon
audius provides cost efficient and
resizable capacity while automating time
consuming administrative tasks such as
Hardware provisioning database setup
patching and backups it sort of frees us
from managing the hardware and sort of
helps us to focus on the application
it's also cost effective and resizable
and it's also optimized for memory
performance and input and output
operations not only that it also
automates most of the services like
taking backups you know monitoring stuff
like that it automates most of those
Services Amazon redshift Amazon red
shift is a data warehousing service that
enables users to analyze the data using
SQL and other business intelligent tools
Amazon R shift is an fast and fully
managed data warehouse that makes it
simple and cost effective analyze all
your data using standard SQL and your
existing business intelligent tools it
also allows you to run complex analytic
queries against terabyte or structured
data using sophisticated query
optimizations and most of the results
they generally come back in seconds all
right let's quickly talk about some more
services that AWS offers there are a lot
more services that AWS provides but
we're going to look at some more
services that are widely used AWS
application Discovery Services help
Enterprise customers plan migration
projects by gathering information about
their on premises data centers you a
planning a data center migration can
involve thousands of workloads they are
often deeply interdependent server
utilization data and dependency mapping
are important early first step in
migration process and this AWS
application Discovery service collects
and presents configuration usage and
behavior data from your servers to help
you better understand your workloads Rod
53 it's a network and content delivery
service it's an highly available and
scalable Cloud domain name system or DNS
service and Amazon Route 53 is fully
compliant with IPv6 as well elastic load
balancing it's also a network and
content delivery service elastic load
balancing automatically distributes
incoming application traffic across
multiple targets such as Amazon ec2
instance containers and IP addresses it
can handle the varing load of your
application traffic in a single
availability zones and also across
availability zones a is auto scaling it
monitors your application and
automatically adjusts the capacity to
maintain steady and predictable
performance at a lowest possible cost
using AWS Autos scaling it's easy to set
up application scaling for multiple
resources across multiple service
services in minutes autoscaling can be
applied to web services and also for DB
Services AWS identity and access
management it enables you to manage
access to AWS services and resources
securely using IM you can create and
manage AWS users and groups and use
permissions to allow and deny their
access to AWS resources and moreover
it's a free service now let's talk about
the future of AWS well let me tell you
something cloud is here to stay here's
what in store for AWS in the future
future as years pass by we're going to
have variety of cloud applications Bor
like iot artificial intelligence
business intelligence serverless
Computing and so on cloud will also
expand into other markets like
healthcare banking space automated cars
and so on as I was mentioning some time
back lot or greater Focus will be given
to artificial intelligence and
eventually because of the flexibility
and advantage that cloud provides we're
going to see a lot of companies moving
into the cloud all right right let's now
talk about how easy it is to deploy an
web application in the cloud so the
scenario here is that our users like a
product and we need to have a mechanism
to receive input from them about their
likes and dislikes and uh you know give
them the appropriate product as per
their need all right though the setup
and the environment it sort of looks
complicated we don't have to worry
because AWS has tools and Technologies
which can help us to achieve it now
we're going to use services like Route
50 three services like Cloud watch ec2
S3 and lot more and all these put
together are going to give an
application that's fully functionable
and an application that's going to
receive the information uh like using
the services like Route 53 Cloud watch
ec2 and S3 we going to create an
application and that's going to meet our
need so back to our original requirement
all I want is to deploy a web
application for a product that keeps our
users updated about the happenings and
the new comings in the market and to
fulfill this requirement here is all all
the services we would need ec2 here is
used for provisioning the computational
power needed for this application and
ec2 has a vast variety of family and
types that we can pick from for the
types of workloads and also for the
intents of the workloads we're also
going to use S3 for storage and S3
provides any additional storage
requirement for the resources or any
additional storage requirement for the
web applications and we are also going
to use cloud watch for monitoring the
environment and cloudwatch monitors the
application and the environment and it
uh provides trigger for scaling in and
scaling out the infrastructure and we're
also going to use Route 53 for DNS and
Route 53 helps us to register the domain
name for our web application and with
all the tools and Technologies together
all of them put together we're going to
make an application a perfect
application that CS our need all right
so I'm going to use elastic beant stock
for this project and the name of the
application is going to be as as you see
GSG signup and the environment name is
GSG signup environment 1 let me also
pick a name let me see if this name is
available yes that's available that's
the domain name so let me pick that and
the application that I have is going to
run on node.js so let me pick that
platform and launch now as you see
elastic bean stock this is going to
launch an instance it's going to launch
the monitoring setup or the monitoring
environment it's going to create a load
balancer as well and it's going to take
care of all the security features needed
for this
application all right look at that I was
able to go to that URL which is what we
gave and it's now having an default page
shown up meaning all the dependencies
for the software is installed and it's
just waiting for me to upload the code
or in specific the page required so
let's do
that let me upload the code I already
have the code saved
here that's my code and that's going to
take some time all right it has done its
thing and now if I go to the same URL
look at that I'm being thrown an
advertisement page all right so if I
sign up with my name email and stuff
like that you know it's going to receive
the information and it's going to send
an email to the owner saying that
somebody had subscribed to your service
that's the default feature of this app
look at that email to the owner saying
that somebody had subscribed to your app
and this is their email address stuff
like that not only that it's also going
to create an entry in the database and
Dynamo DB is the service that this
application uses to store data there's
my Dynamo DB and if I go to tables right
and go to items I'm going to see that a
user with name Samuel and email address
so and so has said okay or has shown
interest in the preview of my site or
product so this is where or this is how
I collect those information right and
some more things about the
infrastructure itself is it is running
behind an load balancer look at that it
had created a load balancer it had also
created an autoscaling group now that's
the feature of elastic load balancer
that we have chosen it has created an
Autos Skilling group and now let's put
this URL you see this it's it's not a
fancy URL right it's an Amazon given URL
a dynamic URL so let's put this URL
behind our DNS let's do that so go to
Services go to R
53 go to hostage Zone and there we can
find the DNS name right so that's a DNS
name all
right all right let's create an
entry and map that URL to our load
balancer right and create now
technically if I go to this URL it
should take me to that application all
right look at that I went to my custom
URL and now that's pointed to my
application previously my application
was having a random URL and now it's
having a custom URL welcome to the realm
of cutting a technology have you ever
wondered what it would like to harness
the immense power of Google's Computing
infrastructure right at your fingertips
look no further for in digital r that
there exist a phenomenon known as Google
collab an interesting Fusion of cloud
computing and collaborative progress
imagine a world where you can
effortlessly explore the Limitless
possibilities of artificial intelligence
machine learning and data analysis
without the need for expensive Hardware
or complex setups Google collab emerges
as Celestial Century offering you a
virtual laboratory to experiment create
and collaborate with like-minded
enthusiasts from every corner of the
globe in this exciting video we will
understand what what is Google collab
and then we will have a complete
overview of it so let's start with the
first topic what is Google
collab so Google collab is a free online
tool that lets you do coding and data
related work on the Internet it's like
having a virtual notebook where you can
write and run python code without
needing to install anything on your
computer with collab you can also get
data from different places and share
your work with others easily another
cool thing is that it gives you access
to powerful processes like gpus and tpus
which make tasks like machine learning
and data science faster and more
efficient so it's a great tool for
Learning and working on coding and data
projects all right now let's have a look
at Google collab let's have a complete
overview of
it so first of all Google collab
requires a Google account for access to
begin visit the
link welcome to
collaboratory write
welcome to collabor as you can see we'll
click on
this then we'll click on this open this
one and sign in using our Google account
or credentials if you have already have
a Google account then you can just sign
in with
that all right now let's start with the
Google collab we'll simply
write Google
collab we'll click on this
all right so this is the window that
appears when we click on that first of
all here we have is example tab so the
example tab offers some initial examples
to help you begin using collab for a
comprehensive understanding of
collaborative features and efficient
usage of collab documents refer to the
document title overview of collaboratory
features it covers the fundamental
aspects to get you started
efficiently then there is recent tab all
right so the recent app displays a
collection of all the latest documents
you have been working on all right now
next is our Google Drive tab the Google
Drive tab features allows you to import
any of your previous notebooks directly
from your Google Drive all right then
next is GitHub tab so with the GitHub
tab you have the option to import
notebooks from your own GitHub
repository or any public repository
simply provide the GitHub URL and you
can easily import notebooks from the
desired public repository then we have
is upload tab so the upload feature
allows you to bring and work on your own
jut notebooks that you have created on
your computer you can easily upload any
file from your local Machine by
selecting the choose file option here it
is choose file option now next is our
creating a notebook as you can see
here's a new notebook option and there's
a cancel option so creating a notebook
so to create a a new notebook you have
the option to use the new notebook
button located at the bottom by clicking
on this button a new Untitled notebook
will be generated to give it a specific
name and simply click on the current
untitle name if we want to name it so as
we have clicked on it yeah to give it a
specific name as we just said simply
click on the current untitle name on The
Notebook and make the desired changes so
let's say we want to make
it SP
1 let's just say it a name all right so
next is our
cell here we can see this is a cell in a
notebook cell serve as the fundamental
building blocks encompassing everything
within there are two types of cells
actually one is code cell as we can see
it contains executable code and has a
run button on the left side to execute
its
content this one
all right the output is displayed its
output is displayed below the code cell
after running right then the next one is
other one is text cell so a text cell
can include text images links and more
you can edit its content by double
clicking it the text cell supports
markdown markup language but you can
also use the provided options on top on
the top for formatting the right half of
the cell shows you how your edit text
will appear
all right now the next point would be
adding a new cell like how to add a new
cell to add a new code or text cell use
the respective option at the top of the
work area as you can see these code or
text by clicking on these we can add
code cell and text cell so clicking on
any of these button creates a new cell
right below the current one you can like
let's say we want to add this so now we
want to add this we can add this also
now you can rearrange the cell order
using the arrow options located at the
top right corner of each cell as you can
see by using this we can do
that yeah the up Arrow moves the cell
one position up while the down arrow
moves it down by one
position as we can see now adding and
running a code let's talk about this so
by default Google collab provides One
initial code cell which can be
supplemented with additional cells as
desired these cells serve as a platform
for writing and executing python code
for instance let's consider the
following code let's say first of all I
have to let us delete all
these this one
also yeah so first of of all we'll try
one code let's see how it goes so we'll
simply write the code
print yeah
print my favorite
color is yellow all right so now let's
try to execute it we can run it by
clicking on this it is running
executing so as you can see my favorite
color is yellow is being printed so you
can either press on this play button to
run the code or you can press control+
enter to run the code as we can see
control+ enter to run the cell now next
is changing the order of the cell so in
Google collab you have multiple options
for rearranging cells you can utilize
the up cell and down cell buttons
located in the toolbar above of the
notebook as we discussed previously
alternatively you can employ keyboard
shortcuts such as control shift up or
control shift down to move the chosen
cell up or down Additionally you have
the flexibility to reorder cells by
clicking on the cell number in the left
Gutter and dra dragging it to a
different position all right now
deleting the current tab so to delete
the current tab in Google collab you can
use the delete icon available over the
top of the cell this icon looks like a
trash can and it will remove the
selected cell from The Notebook as we
can see this is the trash can and this
is just at the right above corner of the
cell by clicking on this we can delete
the tab alternately you can also use the
keyboard shortcut control+ m and d to
delete the current cell you can also
right click on the cell and select
delete cells from the context menu as we
can see delete cell all right now we
will understand how to add data sets so
to add data sets from your local device
go to the left corner and click on the
folder icon as we can see this is the
left corner and this is a folder icon
we'll click on that now click on the
upload icon and upload the desired file
this is the upload icon we'll click on
that now we can upload the desired file
whichever we
want all right you can also upload files
by writing a particular command let's
try writing that command okay
so we'll write
from
Google
collab import files let's import files
right after that we'll
write upload
equals files
do
upload after that we'll put on
brackets all right right now we'll press
control and
enter and here we can see choose files
now from here you can choose files so
both the ways you can upload files just
telling now let's understand how to like
import libraries in Google
collab so first of all we have to cancel
we click on this all right so to import
it we'll
write inut
penders as
PD all right after
that we'll write pd.
read so here we are importing the CSV
files actually so we have to write like
xd. rore
CSV and inside this we'll write a file
let's say I have to check what what all
files like this is the sample data files
right so you have to write the file name
actually which you want to import that
CSV file name I'm writing sample _ data
all
right test the location you have to
put do
CSV so this is the
location all right so if we want to
display 10 rows we'll
write DF do
head write 10 we'll press control and
enter and we can see 10 rows are being
displayed so this is how you can import
and like showcase the data and
all so this was about the Google collab
we have covered major things in that
many important things in that there are
there must be many more things in that
that you will explore when you will
start using it so after understanding
about Google collab and hearing about
various Cloud Technologies so let's take
a little scenario of a developer and a
tester before you had the world of
Docker a developer would actually build
their code and then they send it to the
tester but then the code wouldn't work
on their system the code doesn't work on
the other system due to the differences
in computer environments so what could
be the solution to this well you could
go ahead and create a virtual machine to
be the same of the solution in both
areas
we think Docker is an even better
solution so let's kind of break out what
the main big differences are between
Docker and virtual machines as you can
see between the left and the right hand
side both look to be very similar what
you'll see however is that on the docker
side what you'll see as a big difference
is that the guest OS for each container
has been eliminated Docker is inherently
more lightweight but provides the same
functional ity as a virtual machine so
let's step through some of the pros and
cons of a virtual machine versus Docker
so first of all a virtual machine
occupies a lot more memory space on the
host machine in contrast Docker occupies
significantly less memory space the boot
up time between both is very different
Docker just boots up faster the
performance of the docker environment is
actually better and more consistent than
the virtual machine Docker is also very
easy to set up and very easy to scale
the efficiencies therefore are much
higher with a Docker environment versus
a virtual machine environment and you'll
find it is easier to Port Docker across
multiple platforms than a virtual
machine finally the space allocation
between Docker and a virtual machine is
significant when you don't have to
include the guest OS you're eliminating
a significant amount of space and the
docker environment is just inherently
smaller so after Docker as a developer
you can build out your solution and send
it to a tester and as long as we're all
running in the docker environment
everything will work just great so let's
step through what we're going to cover
in this presentation we're going to look
at the devops tools and where Docker
fits within that space we'll exam exine
what Docker actually is and how Docker
works and then finally we'll step
through the different components of the
docker environment so what is devops
devops is a collaboration between the
development team the operation team
allowing you to continuously deliver
Solutions and applications and services
that both delight and improve the
efficiency of your customers if you look
at the vend diagram that we have here on
the left hand side we have development
on the right hand side we have operation
and and then there's a crossover in the
middle and that's where the devops team
sits if we look at the areas of
integration between both groups
developers are really interested in
planning code building and testing and
operations want to be able to
efficiently deploy operate and monitor
when you can have both groups
interacting with each other on these
seven key elements then you can have the
efficiencies of an excellent devops team
so planning and c-base we tools like Jet
and geara for building we use gradal and
Maven testing we use selenium the
integration between Dev and Ops is
through tools such as Jenkins and then
the deployment operation is done with
tools such as Docker and Chef finally
nagas is used to monitor the entire
environment so let's step deeper into
what Docker actually is so Docker is a
tool which is used to automate the
deployment of app applications in a
lightweight container so the application
can work efficiently in different
environments now it's important to note
that the container is actually a
software package that consists of all
the dependencies required to run the
application so multiple containers can
run on the same Hardware the containers
are maintained in isolated environments
they're highly productive and they're
quick and easy to configure so let's
take an example of what dog is by using
a house that may be rented for someone
using Airbnb so in the house there are
three rooms and only one cupboard and
kitchen and the problem we have is that
none of the guests are really ready to
share the cupboard and kitchen because
every individual has a different
preference when it comes to how the
cupboard should be stocked and how the
kitchen should be used this is very
similar to how we run software
applications today each of the
applications could end up using
different Frameworks so you may have a
framework such as rails perfect and
flask and you may want to have them
running for different applications for
different situations this is where
Docker will help you run the
applications with the suitable
Frameworks so let's go back to our airnb
example so we have three rooms and a
kitchen and cupboard how do we resolve
this issue well we put a kitchen and
cupboard in each room we can do the same
thing for computers Docker provides the
suitable Frameworks for each different
application and since every application
has a framework with a suitable version
this space could also then be utilized
for putting in software and applications
that alone and since every application
has its own framework and suitable
version the area that we had previously
stored for a framework can be used for
something else now we can create a new
application in this instance a fourth
application that uses its own resources
you know what with these kinds of
abilities to be able to free up space on
the computer it's no wonder Docker is
the right choice so let's take a closer
look to how Docker actually works so
when we look at Docker and we call
something Docker we're actually
referring to the base engine which
actually is installed on the host
machine that has all the different
components that run your Docker
environment and if we look at the image
on the left hand side of the screen
you'll see that Docker has a client
server relationship there is a client
installed on the hardware there is a
client that contains the docker product
and then there is a server which
controls how that Docker client is
created the communication that goes back
and forth to be able to share the
knowledge on that Docker client
relationship is done through a rest API
and this is fantastic news because that
means that you can actually interface
and program that API so we look here in
the animation we see that the docker
client is constantly communicating back
to the server information about the
infrastructure and it's using this rest
API as that Communication channel the
docker server then will check out the
requests and the interaction necessary
for it to be the docker demon which runs
on the server itself will then check out
the interaction and the necessary
operating system pieces needed to be
able to run the container okay so that's
just an overview of the docker engine
which is probably where you're going to
spend most of your time but there are
some other components that form the
infrastructure for Dockers let's dig
into those a little bit deeper as well
so what we're going to do now is break
out the four main components that
comprise of the docker environment the
four components are as follows the
docker client and server which we've
already done a deeper dive on Docker
images Docker containers and the docker
registry so if we look at the structure
rure that we have here on the left hand
side you see the relationship between
the docker client and the docker server
and then we have the rest API in between
now if we start digging into that rest
API particularly the relationship with
the docker Damon on the server we
actually have our other elements that
form the different components of the
docker ecosystem so the docker client is
accessed from your terminal window so if
you are using Windows it's going to be
Powershell on Mac it's going to be your
terminal window and it allows you to run
the dock demon and the registry service
when you have your terminal window open
so you can actually use your terminal
window to create instructions on how to
build and run your Docker images and
containers if we look at the images part
of our registry here we actually see
that the image is really just a template
with the instructions used for creating
the containers which you use within
Docker the docker image is built using a
file called the docker file and then
once you've created that Docker file
you'll store that image in the docker
Hub or R stre and that allows other
people to be able to access the same
structure of a Docker environment that
you've created the syntax of creating
the image is fairly simple it's
something that you'll be able to get
your arms around very quickly and
essentially what you're doing is you're
creating the option of a new container
you're identifying what the image will
look like what are the commands that are
needed and the arguments for within
those commands and once you've done that
you have a definition for what your
image will look like so if we look here
at what the container itself looks like
is that the container is a standalone
executable package which includes
applications and their dependencies it's
the instructions for what your
environment will look like so you can be
consistent in how that environment is
shared between multiple developers
testing units and other people within
your devops team now the thing that's
great about working with Docker is that
it's so lightweight that you can
actually run multiple Docker containers
in the same infrastructure and share the
same operating system this is its
strength it allows you to be able to
create those multiple environments that
you need for multiple projects that
you're working on interestingly though
within each container that container
creates an isolated area for the
applications to run so while you can run
multiple containers in an infrastructure
each of those containers are completely
isolated they're protected so that you
can actually control how your Solutions
work there now as a team you may start
off with one or two developers on your
team but when a project starts becoming
more important and you start adding in
more people to your team you may have 15
people that are offshore you may have 10
people that are local you may have 15
Consultants that are working on your
project you have a need for each of
those developers or each each person on
your team to have access to that Docker
image and to get access to that image we
use the docker registry which is an Open
Source serviz service for hosting and
distributing the images that you have
defined you can also use Docker itself
as its own default registry and Docker
Hub now something it has to be bear in
mind though is that for publicly shared
images you may want to have your own
private images in which case you would
do that through your own registry so
once again public repositories can be
used to host the docker images which can
be accessed by anyone and I really
encourage you to go out to Docker and
see the other docket images that have
been created because there may be tools
there that you can use to speed up your
own development environments now you
will also get to a point where you start
creating environments that are very
specific to the solutions that you are
building and when you get to that point
you'll likely want to create a private
repository so you're not sharing that
knowledge with the world in general now
now the way in which you connect with
the docker registry is through simple
pull and push commands that you run
through terminal window to be able to
get the latest information so if you
want to be able to build your own
container what you'll start doing is
using the pull commands to actually pull
the image from the docker repository and
the command line for that is fairly
simple in terminal window you would
write Docker pull and then you put in
the image name and any tags associated
with that image and use the command
pools so in your terminal window you
would actually use a simple line of
command once you've actually connected
to your Docker enironment and that
command will be Docker pull with the
image name and any Associated tags
around that image what that will then do
is pull the image from the docker
repository whether that's a public
repository or a private one now in
Reverse if you want to be able to update
the docker image with a new information
you do a push command where you take the
script that you've written about the
docker container that you've defined and
push it to the repository and as you can
imagine the commands for that are also
fairly simple in terminal window you
would write Docker push the image name
any Associated tags and then that would
then push that image to the docker
repository again either a public or a
private repository so if we recap the
docker file creates a Docker image
that's using the build commands a Docker
image then contains all the inform
necessary for you to be able to execute
the project using the Dogg image any
user can run the code in order to create
a Docker container and once a Docker
image is built it's uploaded to a
registry or to a Docker Hub where it can
be shared across your entire team and
from the docker Hub users can get access
to the docker image and build their own
new containers so let's have a look at
what we have in our current environment
so today when you actually have your
standard machine you have the
infrastructure structure you have the
host operating system and you have your
applications and then when you create a
virtual environment what you're actually
doing is you're actually creating
virtual machines but those virtual
machines actually are now sitting within
a hypervisor solution that sits still on
top of your host operating system and
infrastructure and with a Docker engine
what we're able to do is we're able to
actually reduce
significantly the different elements
that you would normally have within a
virtualized environment so we're able to
get rid of the the bins and the so we're
able to get rid of the guest OS and
we're able to eliminate the hypervisor
environment and this is really important
as we actually start working and
creating environments that are
consistent because we want to be able to
make it so it's really easy and stable
for the environment that you have within
your uh Dev and Ops environment now
critical is getting rid of that
hypervisor element it's just a lot of
overhead so let's have a look at a
container as an example so here we
actually have a couple of examples on
the right hand side we have different
containers we have one container is
running Apache Tomcat in a with Java a
second container is running SQL server
in a microsoft.net environment third
containers running python with mySQL
these are all running just fine within
the docker engine and sitting on top of
a host OS which could be Linux it really
could be any host OS within a consistent
infrastructure and you're able to have a
solution that can be shared easily
amongst your teams so let's have a look
at an example that you'd have today if a
company is doing a traditional Java
application so you have your developers
working in JBoss on his system and he's
coding away and he has to get that code
over to a tester now what will happen is
that tester will then typically in your
traditional environment then have to
install JBoss on their machine and get
everything running and and hopefully set
up identically to the developer but
chances are they probably won't have it
exactly the same but they'll try to get
as close as possible and then at some
point you want to be able to test this
within your production environment so
you send it over to a system
administrator who would then also have
to install JBoss on their environment as
well yeah this just seems to be a whole
lot of duplication so why go through the
problem of installing JBoss three times
and this is where it things get really
interesting because the challenge you
have today is that it's very difficult
to almost impossible to have identical
environments if you're just installing
software locally on devices the
developers probably got a whole bunch of
development software that could be
conflicting with the Jos environment the
tester has similar testing software but
probably doesn't have all the
development software and certainly the
system administrator won't have all the
tools that the developer and tester have
their own tools and so what you want to
be able to do is kind of get away from
The Challenge you have of having to do
local installations on three different
computers computers and in addition what
you see is that this uses up a lot of
effort because when you're having to
install software over and over again you
just keep repeating doing really basic
foundational challenges so this is where
Docker comes in and Docker is the tool
that allows you to be able to share
environments from one group to another
group without having to install software
locally on a device you install all of
the code into your Docker container and
simply share the container so so in this
presentation we're going to go through a
few things we're going to cover what
Docker actually is and then we're going
to dig into the actual architecture of
Docker and kind of go through what
Docker container is and how to create a
Docker container and then we'll go at
through the benefits of using Docker
containers and then the commands and
finalize everything out with a brief
demo so what is darker so darker is as
you'd expect because all the software
that we cover in this series is an
open-source solution and and is a
container solution that allows you to be
able to containerize all of the
necessary files and applications needed
to run the solution that you're building
so you can share it from different
people in your team whether it's a
developer tester or system administrator
and this allows you to have a consistent
environment from one group to the next
so let's kind of dig into the
architecture so you understand why
Docker runs effectively so the docker
architecture itself is built up of uh
two key elements there is the docker
client and then there is a rest API
connection to a Docker demon which
actually hosts the entire environment
within the docker host and the docker
demon you have your different containers
and each one has a link to a Docker
registry the docker client itself is a
rest service so as you'd expect a rest
API U that sends command line to the
docker Damon through a terminal window
or command command line interface window
and we'll go through some of these demos
later on so you can actually see how you
can actually interact with Docker the
docker Damon then checks the request
against um the docket components and
then performs the service that you're
requesting now the docker image itself
all it really is a collection of
instructions used to create a container
and again this is consistent with all
the devops tools that we have the devops
tools that we're looking to use
throughout this series of videos are all
Environ that can be scripted and this is
really important because it allows you
to be able to duplicate and scale the
environments that you want to be able to
build very quickly and effectively the
actual container itself has all of the
applications and the dependencies of
those applications in one package you
kind of think of it as a really
effective and efficient zip file it's a
little bit more than that but it's one
file that actually has everything you
need to be able to run all of your
Solutions the actual Docker rry itself
is an environment for being able to host
and distribute different Docker images
among your team so say for instance you
had a team of developers that were
working on multiple different solutions
so say you have a team of developers and
you have 50 developers and they're
working on five different applications
you can actually have the applications
themselves the containers shared in the
docker registry so each of those teams
at any time check out and have the
latest container of that latest image of
the code that you're working on so let's
dig into what actually is in a container
so the important part of a duckin
container is that it has everything you
need to be able to run the application
it's like a virtualized environment it
has all your Frameworks and your
libraries and it allows the teams to be
able to build out and run exactly the
right environment that the developer
intended what's interesting though is
the actual applications then will run in
isolation so they're not impacting other
applications that's using dependencies
on other libraries or files outside of
the container because of the
architecture it really uses a lot less
space and because it's using less space
it's a much more lightweight
architecture so the files the actual
folder itself is much smaller it's very
secure highly portable and the boot up
time is incredibly fast so let's
actually get into to how you would
actually create a Docker container so
the docker container itself is actually
built through command line and it's
built of a file and Docker image so the
actual um Docker file is a text file
that contains all the instructions that
you would need to be able to create that
Docker image and then we'll actually
then create all of the project code with
inside of that image then the image
becomes the item that you would share
through the docker registry you would
then use the command line and we'll do
this later on select docket run and then
the name of the image to be able to
easily and effectively run that image
locally and again once you've created
the docker image you can store that in
the docker registry making it available
to anybody within your network so
something to bear in mind is that Docker
itself has its own registry called
dockerhub and that is a public registry
so you can actually go out and see other
dock images that have been created and
access those images as your own company
you may want to have your own own
private um repository so you want to be
able to go ahead and either do that
locally through your own repository or
you can actually get a licensed version
of dock Hub where you can actually then
share those files now something that's
also very interesting to know is that
you can have multiple versions of a
Docker image so if you have a different
version control different release
versions and you want to be able to test
and write code for those different
release versions because you may have
different setups you can certainly do
that within your Docker registry
environment okay so let's go ahead and
we're going to create a Docker image
using some of our basic Docker commands
and so there are essentially really you
know kind of just two commands that
you're going to be looking for one is
the build command and another one is to
actually put it into your registry which
is a push command so if you want to get
a image from a Docker registry then you
want to use the pull command and a pull
command simply pulls the image from the
um in this example using NG as our
registry and we can actually then pull
the image down to our test environment
on our local machine so we're actually
running the container within our Docker
application on our local machine we're
able to then have the image run exactly
as it would in production and then you
can actually use the Run command to
actually use the docker image on your
local machine so just a you know a few
interesting tidbits about the docking
container once the container is created
a new new layers formed on top of the
dock image layers called the container
layer each container has a separate read
write container layer and any changes
made in that docking container is then
reflected upon that particular container
layer and if you want to delete the
container layer the container layer also
gets deleted as well so you know why
would using Docker and containers be of
benefit to you well you know some of the
things that are useful is that
containers have no external depend IES
for the applications they run once you
actually have the container running
locally it has everything it needs to be
able to run the application so there's
no having to install additional pieces
of software such as the example we gave
with Jake boss at the beginning of the
presentation now the containers are
really lightweight so it makes it very
easy to share the containers amongst
your teams whether it's a developer
whether it's a tester whether it's
somebody on your operations environment
it's really easy to share those
containers amongst your entire team
different data volumes can be easily
reused and shared among multiple
containers and again this is another big
benefit and this is a reflection of the
lightweight nature of your containers
the container itself also runs in
isolation uh which means that it is not
impacted by any dependencies you may
have on your own local environment so
it's a completely sandboxed environment
so some of the questions you might ask
us you know can you run multiple
containers together without the need to
start each one individually and you know
what yes you can with Docker compose
Docker compose allows you to run
multiple containers in a single service
and again this is a reflection on the
lightweight nature of containers within
the docker environment so we're going to
end our presentation by looking at some
of the basic commands that you'd have
within Docker so we have here on the
left hand side we have a Docker
container and then the command for each
item we're actually going to go ahead
and use some of these commands in the
demo that we're going to do after this
presentation you'll see that in a moment
but just you know some of the basic
commands we have are committing the
docket image into the Container kill is
a you know standard kill command to you
know terminate one or more of the
running containers so they stop working
then restart those containers but
suddenly you can look at all the image
all of the commands here and try them
out for yourself so we're going to go
ahead and start a demo of how to use the
basic commands to run Docker so to do
this we're going to open up terminal
window or command line depending whether
you're running Linux PC or Mac and we're
going to go ahead and the first thing we
want to do is see what our dogger image
lists are so we can go pseudo Docker
images and this will give us well first
we're entering our password so let's go
enter that in and this will now give us
a list of our Docker images and here are
the docker images have already been
created in the system and we can
actually go ahead and actually see the
processes that are actually running so
I'm going to go ahead and open up this
window a little bit more but this will
show you the actual processes and the
containers that we actually have and so
on the far left hand side you see under
names we have learn simply learn bore
cool these are all just different ones
that we've been working on so let's go
ahead and create a docket image so we're
going to do pseudo
Docker
run- D Das P
0.0.0.0
colon 80 callon
80 obuntu and this will allow us to go
ahead and run an obuntu image and this
will run the latest image and what we
have here is a hash number and this hash
number is a unique name that defined the
container that we've just created and we
can go ahead and we can check to make
sure that the container actually is
present so we're going to do so pseudo
doer. PS and this actually show us on
there so it's not in a running state
right now but that doesn't mean that we
don't have it so let's list out all the
containers that are both running and in
the exit state so let's do sucker ps- a
and this lists all the containers that I
have running on my
machine and this shows all the ones that
have been in the running State and in
the exit State and here we see one that
we just created about a minute ago and
it's called
learn and these are all running Ubuntu
and this is the one that we had created
just a few seconds
ago let's open it up and there we go so
let's change that to that new Doc
container to a running state so scroll
down and we're going to type suit
[Music]
Docker
run
dashit Das
Das name
my um so this is going to be the new
container name it's going to be my
Docker so this is how we name our Docker
environment and we'll put in the image
name which is
Ubuntu
and dash bin Dash
bash and it's now in our route and we'll
exit out of
that so now we're going to go ahead and
start the new my Docker container so
sudo
Docker
start
my and we'll get the container image
which will be my Docker my
Docker return and that's started that
Docker image and let's go ahead and
check against the other running Docker
images to make sure it's running
correctly so pseudo Docker
PS and there we are underneath name on
the right hand side you actually see my
Docker along with the other docket
images that we created and it's been
running for 13 seconds quite fast so we
want to rename the container let's use
the command pseudo
Docker rename we can take another docket
image this grab this one and we'll put
it in rename and we'll rename and we'll
put in the old container name which is
image and then we'll put in the new
container name and let's call
it
purple so now the container image that
had previously been called image is now
called Purple so we do pseudo Docker
PS to list all of our Docker images
and if we scroll up and there there we
go purple how easy is that to rename an
image and we can go ahead and use this
command if we want to stop a container
so we're going to write pseudo
Docker
stop and then we have to put it in the
container
name and we'll put in my Docker the
container that we originally
created and that image has now
stopped and let's go ahead prove that
we're going to list out all the docket
images and what you see is that it's not
listed in the active images it's uh not
on the list on the far right hand side
but if we go ahead and we can list out
all of the dock images so you actually
see it's still there as an image is just
not in an active State it's what's known
as in an exit state so here we
go and there's my Docker it's in an
exited state so that happened 27 seconds
ago
SC so if we want to to remove a
container we can use the following
command so pseudo
Docker
RM we
remove my Docker and that will remove it
from the exited
State and we're going to go ahead and we
going to double check
that and
yep yep it's not not listed there under
exit State
anymore it's
gone and there we go there work that's
where it used to be all right let's go
back so if we want to exit a container
in the running state so we do pseudo
kill and then the name of the
container I think one of them is called
yellow let's just check and see if
that's going to kill
it oops no I guess we don't have one
called yellow so let's find out the name
of container that we actually have so
pseudo Docker kill oh we're going to
list out the ones that are running oh
okay there we go yellow isn't in that
list so let's take I know let's take
simply learn and so we can actually go
ahead and let's write PSE sudo Docker
kill simply learn and that will actually
kill an active Docker
container boom there we
go and we list out all the active
containers you can actually see now
that's the simply learn container is not
active
anymore and these are all the basic
commands for Docker container break up
this presentation into four key areas
we're going to talk about life before
kubernetes which some of you are
probably experiencing right now what is
kubernetes the benefits that kubernetes
brings to you particularly if you are
using containers in a Dev Ops
environment and then finally we're going
to break down the architecture and
working infrastructure for kubernetes so
you understand what's happening and why
the actions are happening the way that
they are so let's jump into our first
section of life before kubernetes so the
way that you have done work in the past
or you may be doing work right now is
really building out and deploying
Solutions into two distinct areas one is
a traditional deployment where you're
pushing out code to physical servers in
a Data Center and you're managing the
operating system and the code that's
actually running on each of those
servers another environment that you may
potentially be using is deploying code
out to Virtual machines so let's go
through and look at the two different
types of deployment that you may be
experiencing when you have applications
running on multiple machines you run
into the potential risk that the setup
and configuration of each of those
machines isn't going to be consistent
and your code isn't going to work
effectively and there may be issues with
uptime and errors within the
infrastructure of your entire
environment there's going to be problems
with resource allocation and you're
going to have issues where applications
may be running effectively and not not
effectively and not load balance um
effectively across the environment the
problem that you have with this kind of
infrastructure is that it gets very
expensive uh you can only install one
piece of software one service on one
piece of Hardware so your Hardware is
being massively underutilized this is
where virtual machines have become
really popular with a virtual machine
you're able to have better resource
utilization and scalability at much less
cost and this allows you to be able to
run multiple virtual machines on a
single piece of Hardware the problem is
is that VMS or for virtual machines are
not perfect either some of the
challenges you run with VMS is that the
actual hardware and software need needed
to manage the VM environment can be
expensive there are security risks with
virtual with VMS there are security
risks with VMS there have been data
breaches recorded about solutions that
run in virtualized environments you also
run into an issue of availability and
this is largely because you can only
have a finite number of virtual machines
running on a piece of hardware and this
results in limitations and restrictions
in the types of environment you want to
be running and then finally setting up
and managing a virtualized environment
is time consuming uh it can take a lot
of time and it can also get very
expensive so how about kubernetes well
kubernetes is a tool that allows you to
manage containerized deployment of
solutions and inherently kubernetes is a
tool that is really a Next Level
maturity of deployment so if you can
think of your maturity curve as
deploying code in directly to Hardware
in a Data Center and then deploying your
solutions to Virtual machines the next
evolution of that deployment is to use
containers and kubernetes so let's kind
of go through and look at the
differences between a virtual machine
and kubernetes and we've got a few here
that we want to highlight and you'll get
an understanding of what the differences
are between the two so first of all with
virtual machines there is inherently
security risks and what you'll find as
we get dig through the architecture
later in the presentation is that
kubernetes is inherently secure um and
this is largely because of the Legacy
code uh the legacy of kubernetes and
where it came from we will talk about
that in just a moment but kubernetes is
inherently secure uh virtual machines
are not easily portable now with that
said they they are technically portable
they're just not very easily portable
whereas with kubernetes it's working
with darker containers Solutions it is
extremely portable that means that you
can actually spin up and spin down and
manage your infrastructure exactly the
way that you want it to be managed and
scale it on the demands of the customers
as they're coming in to use the solution
from a time consuming point of view
kubernetes is much less time consuming
than with a virtual machine a few other
areas that we want to kind of um
highlight from differences virtual
machines use much less isolation when
than building out the uh encapsulated
environment than kubernetes does uh for
instance with a virtual machine you have
to run hypervisor on top of the OS and
hardware and then inside of the virtual
machine you also have to have the
operating system as well whereas in
contrast on a kubernetes environment
because it's leveraging a darker
container and or container like
Technologies it only has to have the OS
and the hardware and then inside of each
container doesn't need to have that
additional OS layer it's able to inherit
what it needs to be able to run the
application this makes the whole
solution much more flexible and allows
you to run many more containers on a
piece of Hardware than versus running
virtual machines on a single piece of H
grare so as we um highlighted here VMS
are not as portable as kubernetes and
kubernetes is portable directly related
to the use of containerization and
because kubernetes is built on top of
containers it is much less time
consuming because you can actually
script and automatically allocate
resource to nodes within your kubernetes
environment this allows the
infrastructure to run much more
effectively and much more efficiently so
this is why if we look at our evolution
of the land of time before kubernetes
why we are running into a solution where
kubernetes had to come about because the
demand for having more highly scalable
solutions that are more efficient was
just really a natural evolution of this
software deployment model that started
with pushing out code to physical
hardware and then pushing code out to
Virtual machines and then needing to
have a solution much more sophisticated
kubernetes would have come about at some
point in time I'm just really glad it
came back when it did so what is
kubernetes let's let's dig into the
history of kubernetes and how it came
about so in essence kubernetes is an
open-source platform that allows you to
manage and deploy and maintain groups of
containers and a container is something
like doer and if you're developing code
you're probably already using Docker
today consider kubernetes as the tool
that manages multiple Docker
environments together now we talk a lot
about Docker and as a container solution
with kubernetes the reality is is that
kubernetes can actually use other
container tools out there but Docker
just simply is the most popular
container out there both these tools are
open source that's why they're so
popular and they just allow you to be
able to have flexibility in being able
to scale up your Solutions and they were
designed designed for the postd digital
world that we live and exist in today so
a little bit of background a little bit
of trivia around uh kubernetes so
kubernetes was originally a successor to
a project at Google and the original
project was Google Bor um Google Bor it
does exactly what kubernetes done does
today but kubernetes was Rewritten from
the ground up and then released as an
open-source project in 2014 so that
people outside of Google could take take
advantage of the power of kubernetes
containerization management tools and
today it is managed by the cloud native
Computing foundation and there are many
many companies that support and manage
kubernetes so for instance if you're
signing up for Microsoft Azure AWS
Google Cloud all of them will leverage
kubernetes and it's just become the the
de facto tool from managing large groups
of containers so let's kind of Step
through some of the key benefits that
You' experience from kubernetes and so
we have nine key benefits and the first
it is highly portable and is 100%
open-source code and this means that you
can actually go ahead and contribute to
this code project if you want to through
GitHub uh the ability to scale up the
solution is incredible um what's um the
the history of kubernetes being part of
a Google project for managing the Google
network and infrastructure uh kind of
really sets the groundwork for having a
solution that that is highly scalable
the out of the high scalability also
comes the need for high availability and
this is the desire to be able to have a
highly efficient and highly energized
environment that also you can really
rely on so if you're building out a
kubernetes management um environment you
know that it's going to be um available
for the solutions that you're
maintaining and it's really designed for
deployment so you can script out the
environment and actually have it as part
of your Dev model so you can scale up
and meet the demands of your customer
then what you'll find is that the um
load balancing is extremely efficient
and it allows you to distribute the load
efficiently across your entire network
so your network remains stable and then
also the tool um allows you to U manage
the orchestration of your storage so you
can have local storage such as an SSD on
the hardware that the kubernetes is M
maintaining or if the kubernetes
environment is is pulling storage from a
public Cloud such as Azure or AWS you
can actually go ahead and make that
available to your entire system and you
can inherit the security that goes back
and forth between the cloud environments
and one of the things you'll find
consistent with kubernetes is that it is
designed for a cloud first environment
um kubernetes as well is that it's it's
really a self-healing environment so if
something happens or something fails uh
kubernetes will detect that failure and
then either restart the process kill the
process or replace it and then because
of that you also have automated roll
outs and roll backs uh in case you need
to be able to manage the state of the
environment and then finally uh you have
automatic bin packaging so you can
actually specify the compute power
that's being used from CPU and RAM for
each container so let's dig into the
final area which is the actual
kubernetes architecture and we're going
to cover this at a high level there's
actually another video uh that you can
that simply learn has developed which
digs deeper into the kubernetes
architecture and so the kubernetes
architecture is a cluster based
architecture and it's really about two
key areas you have the kubernetes master
which actually controls um um all of the
activities within your entire kubernetes
infrastructure and then you have nodes
um that actually are running on Linux
machines um out that are controlled by
the master so let's kind of go through
some of these um areas so if we look at
the kubernetes master uh to begin with
um then we'll start with etet this is a
tool that allows for the configuration
of information and the management of
nodes within your cluster and one of the
key features that you'll find with all
of the tools that are managed within
either a the master environment or
within a node is that they are all
accessible via the API server um and
what's interesting about the API server
is that it's a restful based
infrastructure which means that you can
actually secure each connection with SSL
um and other um security models to
ensure that your entire infrastructure
and the communication going back and
forth across your infrastructure is
tightly secured scheduler goes ahead and
actually as you'd expect actually um
manages the schedule of activities
within the actual cluster and then you
have the controller and the controller
is a Demon server that actually manages
and pushes out the instructions to all
of your nodes so uh the other tools
really are the uh the infrastructure and
you can consider them the administration
side um of the master whereas controller
is the management it actually pushes out
all of the controls via the API server
so let's actually dig into um one of the
actual nodes themselves and there are
three key areas of the noes one is the
do environment which actually helps and
manage and maintain the container that's
actually inside um of the node and then
you have the kuet which is responsible
for information that goes back and forth
and it's going to do most of the
conversation with the API server on the
actual health of that node and then you
have the actual kubernetes proxy which
actually runs the services actually
inside of the node so as you see all of
these infrastructures are extremely
lightweight and designed to be very
efficient and very available for your
infrastructure and so here's a quick
recap of the different tools that are
available and it really breaks down into
two key areas you have your kubernetes
mask and the kubernetes node the
kubernetes master has the instructions
of what's going to happen within your
kubernetes infrastructure and then it's
going to push out those instructure to
an indefinite number of nodes that will
allow you to be able to scale up and
scale down your solution in a dynamic
way so let's have an overview of the
kubernetes architecture so kubernetes is
really broken up into three key areas so
you have your workstation where you
develop your commands and you push out
those commands to your master and the
master is comprised of um four key areas
um which essentially control all of your
nodes and and the node contains multiple
pods and each pod has your Docker
container built into it so consider that
you could have really almost an infinite
number of PODS sorry infinite number of
nodes U being managed um by the master
environment so you have your cluster
which is a collection of servers that um
maintain the Ava availability and the
compute power such as RAM CPU and disk
utilization um you have the master which
is really components that control and
schedule uh the activities of your
network and then you have the node which
actually hosts the actual Docker virtual
machine itself um and be able to
actually control and communicate back to
the master the health of that pod and
we'll get into more detail on the
architecture later in the presentation
so you know you keep hearing me talk
about um containers but they really are
the center of the work that you're doing
with kubernetes and and con the concept
around kubernetes and containers is
really just a natural evolution of where
we've been with internet and digital
Technologies over the last uh 10 15
years so before kubernetes um you had
tools where you're either running
virtual machines or you're running uh
data centers that had to maintain and
and manage and notify of any
interruptions in your network kubernetes
is the tool that actually comes in and
helps address those interrup options and
manages them for you so the solution to
this is the use of containers so uh you
can think of containers as that Natural
Evolution from you know uh 15 20 years
ago you would have written your code and
posted it to a data center uh more
recently you probably posted your code
to a virtual machine and then move the
virtual machine and now you actually
just work directly into a container and
everything is self-contained and can be
pushed out to your um environment and
the thing that's great about containers
they they isolated environments very
easy for developers to work in them but
it's also really easy for uh operations
teams to be able to move a container
into production so let's kind of step
and back and look at a competing product
to kubernetes which is Docker swarm now
one of the things that we have to um
remember is that Docker um containers
which are extremely popular um built by
the company Docker and made open source
and Docker actually has other products
one of those other products is Docker
swarm and Docker swarm is a tool that
allows you to be able to manage multiple
containers uh so if we look at some of
the uh the benefits of using Docker
swarm versus kubernetes um one of the
things that you'll find is that both
tools have strength and weaknesses but
it's really good that they're both out
there U because it helps keep it really
kind of justifies uh the importance of
having these kind of tools so kubernetes
was designed originally from the ground
up to be autoscaling whereas Docker
swarm isn't the load balancing is
automatic on Docker swarm whereas with
kubernetes you have to manually
configure load balancing across your
nodes the installation for Docker swarm
is really fast and easy I mean you can
be up and running within minutes a
kubernetes takes a little bit more time
is a little bit more complicated
eventually you'll get there um I mean
it's not like it's going to take you
days and weeks but it's it is a tool
that's a when you compare the two doctor
swarm's much easier to get up and
running now what's interesting is that
kubernetes is incredibly scalable and
it's you know that's its real strength
is its ability to have strong clusters
whereas with Docker swarm it's cluster
strength isn't um as strong when
compared to kubernetes now you compare
it to anything else on the market it's
really good um so this is kind of a
splitting hairs kind of comparison um
but kubernetes really does have the
advantage here if you're looking at the
two compared to each other for
scalability I mean kubernetes was
designed for by Google to to scale up
and support uh Google Cloud Network
infrastructure they uh both allow you to
be able to share um uh storage volumes
with Docker you can actually do it with
um any container U with u that is
managed by the docker swamp whereas with
kubernetes it manages the storage with
the pods and a pod can have multiple
containers within it but you can't take
it down to the level of the container
interestingly uh kubernetes does have a
graphical user interface um for being
able to control and manage uh the
environment the reality however is that
you're likely to be using terminal uh to
actually make the controls and Commands
to control your um either dock swarm or
kubernetes environment um it's great
that it has a a goey and to get you
started but once you're up and running
you're going to be using terminal window
for those fast quick administrative
controls that you need to make so let's
look at the hardware components for
kubernetes so what's interesting is that
kubernetes is extremely l light of all
the systems that we're looking at it's
extremely lightweight um it's allows you
to have if you compare it to like a
virtual machine which is very heavy you
know um cumes is extremely lightweight
and hardly uses any resources at all
interesting enough though is that if you
are looking at the usage of CPU it's
it's better to actually take it for um
uh the cluster as a whole rather than
individual nodes uh because uh the nodes
will actually combin together to give
you that whole compute power again this
is why kubernetes works really well in
the cloud where you can do that kind of
activity rather than if you're running
in your own data center um so you can
have persistent volumes um such as a
local SSD um or you can actually attach
to a cloud data storage again kubernetes
is really designed for the cloud I would
encourage you to use cloud storage
whereever a possible rather than relying
on physical storage uh the reason being
is that if you connect to cloud storage
and you need to flex your your storage
the cloud will do that for you I mean
that's just an inherent part of why You'
have cloud storage whereas if you're
connecting to physical storage you're
always restricted to the limitations of
the physical Hardware so let's um kind
of pivot and look at the software
components as compared to the hardware
components so the main part of the um
components is the actual container and
all of the software running in the
container runs on Linux so if you um
have doc stalled as a developer on your
machine it's actually running inside of
Linux um that's what makes it so
lightweight and really one of the things
that you'll find is that most data
centers and Cloud providers now are
running predominantly on Linux inside of
the um the the container itself is then
managed inside of a part and a part is
really just a group of containers
bundled together and um the kubernetes
scheduler and proxy Ser then actually
manage what um how the pods are actually
pushed out uh into your kubernetes andv
enironment the the pods themselves can
actually then share resources both
networking and storage so pods aren't um
pushed out manually they're actually U
managed through a layer of abstraction
and part of their deployment and and
this is the strength of kubernetes you
use use um Define your um infrastructure
and then kubernetes will then manage it
for you and there isn't that problem of
um manual management of PODS uh if you
have to manage the deployment of them
and you that's simply taken away and
it's completely automated and the the
final area of software Services is on
Ingress and this is really the secure
way of being able to have communication
from outside of the cluster and passing
of information into that cluster um and
again this is done securely through SSL
uh layers and allows you to ensure that
security is at the center of the work
that you have within your kubernetes
environment so let's dive now into the
actual architecture before we start
looking at a use case of how kubernetes
is being imped de loyed so kubernetes
again is um we looked at this uh diagram
at the beginning of the presentation and
there are really three key areas there's
the workstation where you develop your
commands and then you have your master
environment which uh controls the uh
scheduling the communication um and the
actual um commands that you have created
and pushes those out and manages the
health of your entire node Network and
each node has uh various pods so we like
break this down uh so the master node is
the most vital component um with the
master uh you have four key controls you
have ET controller manager schedule and
API server the cluster store Etc this
actually manages the details and values
that you've developed on your local
workstation um and then we'll work with
the out the control schedule on API
server to communicate that out those
instructions of how your infrastructure
should look like to your entire network
uh the control manager is really an API
server and again um this is all about
security so we use restful apis um which
can be packaged in SSL to communicate
back and forth um across your uh pods
and the master and indeed the services
within each of them as well uh so um at
every single layer of extraction uh the
communication is secure uh the schedule
as the name would imply really schedules
um when tasks get sent out to the actual
nodes themselves the nodes elves are are
dumb nodes they just have uh the
applications um running on them the
master in the is really doing all of the
work uh to make sure that your entire
network is running um efficiently and
then you have the API server which has
your rest commands and the communication
um back and forth across your network
that is secure and efficient so your
node environment is where all the work
that you do with your containers gets
pushed out too so um a a work is really
a it's a combination of containers and
each container coner will then logically
run together on that node so you'd have
a collection of containers on a node
that all make logical sense to have
together uh within each node um you have
a Docker and this is your isolated
environment for running your container
uh you have your cubet which is a
service for conveying information back
and forth um to the service about the
actual health of the kubernetes node
itself and then finally you have the
proxy server and the proxy server is
able to manage the nodes the volume the
the creation of new containers um and
actually helps pass the communic the the
health of the container back up to the
master to see whether or not the
container should be either killed stop
started or um updated so finally let's
look at see where uh kubernetes is being
used by other companies so you know
kubernetes is being used by a lot of
companies and they're really using it to
help manage complex existing systems so
that they can have uh greater
performance and with the end goal of
being able to Delight the customer
increase value to the customer and hence
increase value and revenue into the
organization so an example of this is a
company called Black Rock uh where they
actually went through the process of
implementing kubernetes uh so they could
so Black Rock had a challenge where they
needed to be able to have much more
Dynamic access to their resources uh
they were running complex installations
on people's desktops and it was just
really really difficult to be able to
manage their entire infrastructure
structure and so they actually then um
pivoted to using cetes and this allowed
them to be able to be much more scalable
and expansive in the the management of
their uh infrastructure and as you can
imagine kubernetes was then hooked into
their entire existing system and has
really become a key part of the success
that Black Rock is now experiencing of a
very stable infrastructure um and the
bottom line is that black rock is now
able to have confidence in their
infrastructure structure and be able to
give that confidence um as back to their
customers through the implementation and
more rapid deployment of additional
features and services so now what is
azure what's the big cloud service
provider all about so Azure is a cloud
computing platform provided by Microsoft
now it's basically an online portal
through which you can access and manage
resources and services now resources and
services are nothing but you know you
can store your data and you can
transform the data using services that
Microsoft provides again all you need is
the internet and being able to connect
to the Azure portal then you get access
to all of the resources and their
services in case you want to know more
about how it's different from its rival
which is AWS I suggest you click on the
top right corner and watch the AWS
versus AO video so that you can clearly
tell how both these cloud service
providers are different from each other
now here are some things that you need
to know about aure it was launched in
February 1st 2010 which is significantly
late later than when AWS was launched
it's free to start and has a pay per use
model which means like I said before you
need to pay for the services you use
through Azure and one of the most
important selling points is that 80% of
Fortune 500 companies use Azure Services
which means that most of the bigger
companies of the world actually
recommend using AZ and then azir
supports a wide variety of programming
languages the cop nodejs Java and so
much more another very important selling
point of aure is the amount of data
centers it has across the world now it's
important for a cloud service provider
to have many data centers around the
world because it means that they can
provide their services to a wider
audience now Azor has 42 which is more
than any cloud service provider has at
the moment it expects to have 12 more in
a period of time which brings its total
number of regions it covers to 54 now
let's talk about Azor Services now Azor
Services have 18 categories and more
than 200 services so we clearly can't go
through all of them it has services that
cover compute a machine learning
integration management tools identity
devops web and so much more you're going
to have a hard time trying to find a
domain that Azure doesn't cover and if
it doesn't cover it now you can be
certain they're working on it as we
speak so first let's start with the
compute Services first virtual machine
with this service what you're getting to
do is to create a virtual machine of
Linux or Windows operating system it's
easily configurable you can add RAM you
can decrease RAM you can add storage
remove it all of it is possible in a
matter of seconds now let's talk about
the second service cloud service now
with this you can create a application
within the cloud and all of the work
after you deploy it deploying the
application that is is taken care of by
aard which includes you know
provisioning the application load
balancing ensuring that the application
is in good health and all all of the
other things are handled by aor next up
let's talk about service fabric now with
service fabric the process of developing
a microservice is greatly simplified so
you might be wondering what exactly is a
microservice now a microservice is
basically an application that consists
of smaller applications coupled together
next up functions now with functions you
can create applications in any
programming language that you want
another very important part is that you
don't have to worry about any Hardware
components you don't have to worry what
Ram you require or how much storage you
require all of that is taken care of by
AER all you need is to provide the code
to Azure and it'll execute it and you
don't have to worry about anything else
now let's talk about some networking
Services first up we have Azor CDN or
the content delivery Network now the
Azor CDN service is basically for
delivering web content to users now this
content is of high bandwidth and can be
transferred or can be delivered to any
person across the world now these are
actually a network of servers that are
placed in strategic positions across the
world so that the customers can obtain
this data as fast as possible next up we
have expess R now with this you can
actually connect your on promise Network
onto the Microsoft cloud or any of the
services that you want through a private
connection so the only communication
that happens is between your on promise
Network and the service that you want
then you have virtual Network now with
virtual Network you can have any of the
Azure Services communicate with each
other in a secure manner in a private
manner next we have Azure DNS so Azure
DNS is a hosting service which allows
you to host their DNS or domain name
system domains in Azure so you can host
your application using a your DNS now
for the storage Services first up we
have dis storage with dis storage you're
given a cost effective option of
choosing HDD or solid state drives to go
along with your virtual machine machines
based on your requirements then you have
blob storage now this is actually
optimized to ensure that they can store
massive amounts of unstructured data
which can include Text data or even
binary data next you have file storage
which is a managed file storage and can
be accessible via the SMB protocol or
the server message block protocol and
finally you have q storage now with Q
storage you can provide durable message
queuing for an extremely large workload
and the most important part is that this
can be accessed from anywhere in the
world now let's talk about how a can be
used firstly for application development
it could be any application mostly web
applications then you can test the
application see how well it works you
can host the application on the internet
you can create virtual machines like I
mentioned before with the service you
can create these virtual Machines of any
size or Ram that you want you can
integrate and Sync features you can
collect and store matrices for example
how the data Works how the current data
is how you can improve upon it all of
that is possible with these services and
you have virtual hard drives which is an
extension of the virtual machines where
these services are able to provide you a
large amount of storage where data can
be let's talk about Azure Services as I
told you Azia provides services for a
wide range of domains now let's have a
look at some of these domains there's Ai
and machine learning compute containers
database identity management tools
networking Security Storage and so much
more now let's have a look at some of
the individual services within these
domains firstly you have AZ virtual
machines with Az virtual machines what
you get is the opportunity to create
Windows or Linux virtual machines now
all of this is possible in a matter of
seconds with a large amount of
customization now let's have a look at
some of its features firstly you can
choose from a wide variety of virtual
machine options then you have a large
amount of optimization available to you
for example what size of operating
system do you want how much size do you
want allocated to it what version of the
system is it and so much more then it
provides low cost and per minute billing
now AZ provides you per minute billing
which means that you're only charged for
how much time you use the service and
finally you have enhanced security and
protection for your virtual machines
next we have service fabric now with
service fabric you have a platform which
enables you to create microservices now
this also makes the process of
application life cycle management a
whole lot easier as a direct result you
can create applications with a faster
time to Market it supports Windows Linux
on promises or other clouds and it
enables you to do a tremendous amount of
scaling up depending on your requirement
and finally we have functions now with
functions you can build applications
with the help of serverless computing
here the users only pay for the amount
of resources that they've used you can
create applications in any language that
you want and the only thing you need to
worry about is the code of the
application everything other than that
that is the hardware requirements are
taken care of by your now let's have a
look at the networking Services firstly
we have the Azor CDN or the content
delivery network with Azor CDN what you
get is the ability to deliver your
content with reduced load times fast
responsiveness and less bandwidth now
CDN can be integrated with several other
AZ services so that the process can move
at a FAS rate it can handle heavy loads
and traffic spikes with these it also
provides a robust security system now
with the content that's delivered you
can get Advanced analytic data with
which you can understand how customers
are using your content next we have
express route with express route you can
connect your on premisis network to aure
through a private Network Now by default
this lowest latency it increases the
emphasis on reliability and speed and it
can be of great use when you have to
transfer large amounts of data between
networks now another way this can be
useful is if it's used to add compute or
storage capacity to Data Centers next to
have Azure DNS domain name service or
Azure DNS can be used to host your
domains on Azure this provides High
availability and great performance it
provides fast responses to DNS queries
by taking advantage of Microsoft's
Global Network it also provides High
availability next we have virtual
Network Azo virtual Network allows the
Azo resources to communicate with each
other or other on promise networks via
the Internet and all of this is kept
extremely secure now with this users can
create their own private Network for
communication it provides users with an
isolated and extremely secure
environment for their applications to
run now all of the traffic stays
entirely within the Azure Network and it
also allows users to design their own
networks next we have traffic manager
now with traffic manager you can route
incoming traffic to improve your
performance and availability now one
thing it provides is multiple failover
options so if a particular situation
goes wrong there's always an option to
consider to salvage the situation it
helps reduce application runtime and
enables the distribution of user traffic
across multiple locasion it also helps
the people who are using it to know
where the customers connecting from
across the world next we have load
balancer with this you have provided the
ability to instantly scale applications
at the same time providing High
availability and improved Network
performance for users applications it
can be integrated into virtual machines
and cloud services it provides highly
reli able applications it also allows
users to secure and integrate security
groups finally we have aure VPN Gateway
now this allows users to connect their
on promise networks to aure using a
sight tosite VPN now this allows users
to connect their virtual machine to
anywhere in the world through a pointto
site VPN and also it's very easy to
manage and is highly available now let's
talk about the storage Services first we
have data Lake storage now with this
what you get is a scalable data storage
storage with an emphasis on cost
Effectiveness and scalability now it
comes of Maximum use when you've
integrated with other services so that
you can get analytics on how the data is
being used it is also integrated with
other services like the Azo blob storage
now it is also optimized for Big Data
analysis tools like Pache spark and
Hadoop next up we have blob storage The
Blob storage provides a storage capacity
for data now depending on how often a
particular data is used it is classified
into different tiers now all the data
that is within the blob storage is
unstructured data now it has a way of
ensuring that the data Integrity is
maintained every time a particular
object is being changed or the data is
being accessed and it also helps improve
app performance and reduces bandwidth
consumption next we have q storage now
with this you have a message queuing
system for large workloads this allows
users to build flexible applications and
separate functions not to men mention
with this you can be sure that your
individual components will not fail it
also makes sure that your application is
scalable Q storage provides Q monitoring
which helps ensure that the customer
demands are met then we have file
storage now with file storage you can
perform file sharing with the help of
the SMB protocol or the server message
block protocol now this data is
protected by SMB 3.0 and the https
protocol in this CL like we mentioned in
functions a your takes care of all the
hardware needs and the operating system
deployments on its own it also improves
on premises performance and other
capabilities lastly we have table
storage with table storage you can
deploy semi-structured data sets and
osql key value store now this is used
for creating applications which have a
flexible data schema and also
considering how it has a very strong
consistency model it's mainly aimed for
enterprices next let's have a look at
some web and mobile services first we
have the the AZ search now with Azure
search you get a cloud search service
which is powered by artificial
intelligence with this you can develop
web applications as well as mobile
applications now one big Advantage is
that you don't have to set up or manage
your search indices as your takes care
of that and by extension it increases
your development speed the artificial
intelligence also will provide insights
and structured information that you can
use to improve the search as structured
information next we have logic apps now
with this you can create integration
Solutions which can connect applications
that are important to your business now
with this you can visually create
business processes and workflows you can
integrate SAS or software as a service
applications and Enterprise applications
and more importantly it allows you to
unlock data within the firewall and
securely connect to services next we
have web applications now with web apps
you can create deploy and scale web
applications according to business
requirements now it supports both
windows and Linux platforms and it helps
with continuous integration or
deployment abilities another very
important aspect of this is that the
data can be deployed and hosted across
multiple locations in the world and
finally we have mobile apps with mobile
apps you can create applications for iOS
Android and Windows platforms one
advantage is that it automatically
scales Up and Down based on your
requirements now in situations where you
have network issues offline data syncing
ensures that your applications work
anyway and you can create crossplatform
applications or native applications for
iOS Android and Windows next let's have
a look at some container services first
let's talk about ACS or AO container
services it is also known as the Azia
kubernets Services as it's a fully
managed kubernets container
orchestration service now what this
means is that it eases the process of
container integration and deployment it
also can be used with other resources
from security like virtual networks
cryptographic keys and so much more to
ensure that your container is kept
secure next we have container instances
now this is similar to functions in a
way just that in this we're using
containers without having to manage
servers now applications can be
developed here without managing virtual
machines or learning new tools all that
is as yours problem to take care of and
it enables building applications without
having to manage the infrastructure that
is all you need to worry about is
running the container next let's have a
look at some database Services first we
have the SQL database now with SQL
database what you get is a relational
Cloud database service now this means
that it helps accelerate your app
development and makes it easier for you
to maintain your application now SQL
database is also used extensively in
migrating workloads to the cloud and
hence saves time and cost it also helps
improve your performance by integrating
machine machine learning and Adaptive
Technologies into your database next we
have Azure Cosmos DB now this is a
globally distributed multimodel database
service now what this means is that with
this you can create application with
support nosql it provides a high grade
security system has high availability
and low latency now this is usually used
in situations where you have a diverse
and highly unpredictable workload now
let's have a look at some security and
identity Services firstly we have the
Azor active directory now if you want to
know more about Azure active directory I
suggest you click on the top right
corner and watch our video on the Azure
active directory this is just an
introduction so with this you can manage
user identities and you can make sure
the resources are kept safe with the
help of access policies most of these
are intelligence-driven now one of the
main features is that you can have
access to your applications from any
location or device it helps increase
your efficiency and helps down cutting
cost when it comes to having a help desk
it can also help improve security and
can respond to Advanced threats in real
time next you have a your active
directory b2c it helps provide customer
identity and access management in the
cloud now protecting customer identity
is extremely important for an
organization and that's what aure ad b2c
does now it also enables the application
to be scaled to great amounts even
billions of customers next we have the
Azure security Center this is basically
like a command post with which you get a
complete view of the security across
users on your on premises and Cloud
workloads so with this you're given
threat protection method that adapts to
situations and helps reduce exposing you
to threats it also has rapid threat
response and makes the process of
finding and fixing vulnerabilities a
whole lot easier next up let's talk
about monitoring and Management Services
so first let's have a look at aure
advisor advisor now Azure advisor is
basically a guide for the best practices
when it comes to Azure now when you
follow these it improves performance
security cost and increases availability
now it also learns from how you use the
services on your configuration and usage
pattern and the adjustments that it
suggests can be implemented very quickly
and easily next we have Network Watcher
now with this you can monitor diagnose
and understand the working of your
network now you can monitor your network
without actually having to log in to
your virtual machine now you can also
use something known as network security
flow logs to understand the traffic
pattern how much traffic is coming
towards you how much you're giving and
so much more it also helps diagnose VPN
problems that you might have with detail
logs and finally you have the Azure
resource manager now with this you can
ensure that the resources that you have
are managed and deployed at a consistent
rate now this makes it extremely easy
for you to manage and visualize your
resources that are used in your
applications or some other requirements
and you can control who can access your
resources as well as perform actions on
it going to be six things that we're
going to step through the first is
really looking at what is the world that
many of you already work in today how do
you administer your networks and your
systems today then we're going to look
at how Chef can be the tool that really
helps improve the efficiencies of
creating a consistent operations en
environment we're going to look at the
tools and the components and the
architecture that takes to construct
chef and then finally we're going to
take a use case of one of the many
companies that is using Chef today to
improve the efficiency of their
operations environment so let's take a
step back at what life was like before
Chef this may well be the life that
you're experiencing right now so as a
system administrator typically the way
that you work is that you are resp
responsible for the uptime of all of the
systems within your network and you may
have many many systems that you are
responsible for now if one system goes
down that's not a problem because as a
system administrator you can get
notifications and you can jump right
onto the problem and solve it when
things start getting really difficult
however is when you have multiple
systems and you are not able as a single
person to get to all those systems and
Sol them as quickly as possible possible
the problem that you start having is
that your environments are not in sync
if only this could all be automated well
this is where Chef comes to the rescue
for you so let me take some time and
introduce you to the concepts behind
chef and why it's a tool that you would
probably want to be using in your
environment so what actually is Chef so
Chef is a way in which you can use code
to fix your physical Hardware
environment and so what does that look
like well the way that we write out
scripts in Chef is that you can actually
code your entire environment so you
don't have to actually be managing your
environment on a hardware by Hardware
basis but actually use scripts called
recipes that will actually manage the
environment for you so Chef will
actually ensure that every system is
automatically set to the right states
that meet the requirements that you have
defined in your code so you don't have
any ISS is where Hardware starts to fail
because it references the code to reset
the state within that environment and
that makes you a happy system
administrator so let's go through the
mechanics of how this actually works
with Chef so Chef is ultimately an
automation tool that converts
infrastructure to code and you'll often
hear the term infrastructure as code or
I a and this really starts as a output
of the work that Chef has done cuz you
take the policies that you as an
organization have created and you
convert that into a scripted language
that can then be implemented
automatically this enables shift to be
able to manage multiple systems very
very easily and considering how broad
and deep our systems become it's very
easy to see how this can help you as a
system administrator the code is then
tested continuously and deployed by Chef
so that you're able to guarantee that
your standards and appropriate States
for your hardware and operating
environment are always maintained so
let's step through the different
components that are available in Chef so
the first one is actually creating the
actual recipes for Chef so you actually
work on these on a workstation and
you'll write out your code which is
referred to as a recipe and that recipe
will be written using a language called
Ruby good thing for you is that there
are lots of examples of Ruby recipes
that have been created by the
open-source Community a lot of examples
knife is the command tool that you use
that communicates between the recipes
and the server so your recipe is the
instruction and knife is the tool that
makes that instruction work and sets the
appropriate State on the server
environment when you create more than
one recipe you start creating a
collection those collections are
referred to as cookbooks and you can
make take an environment such as a large
Network where you have multiple CES
potentially hundreds of servers that
have to all have a consistent and equal
State environment and you would put that
cookbook and use that cookbook to ensure
with Chef that the state of each of
those codes is consistent and then ohigh
fetures the current state of your nodes
and then the chef client configures the
nodes as defined within your cookbook so
let's step through the architecture and
working environment of Chef so for an
administrator a systems administrator
you only have to allow work from your
workstation and from that workstation
you can then configure all the nodes of
your environment you can use uh
different recipes that you create and
those recipes can be compiled into a
cookbook that can be then applied to a
node the thing that of value is that as
you change the environment As you move
and mature as an organization and your
nodes need to mature consistently and
quickly with your organization all you
have to do is roll out a different
recipe to your nodes and then the nodes
will use the tools within Chef to make
the updates so you can create a second
recipe or even a third recipe to be
deployed out to your server environment
knife is going to be the tool that does
the hard work of doing the updates on
the server with the server you have your
collection of recipes in a single
cookbook however understanding the state
of the individual nodes is important to
ensure that the information is broadcast
from the server with the instructions on
how to set up the network within those
nodes in this instance we use ohigh and
ohigh will fetch the state of the nodes
and send that back to the server and get
the information from The cookbook using
the chef client in your Chef node
Network there is the potential at times
that maybe one of your node fails if
there are two failed nodes then a
request is sent back to the server and
information on the latest node structure
that is identified and defined in the
cookbook will be sent to the failed
nodes if that fails to reset the nodes
then the workstation is notified and you
as the administrator will receive a
notification to manually reset those
nodes it should be noted that this is a
very rare occasion within the chef
network setup so let's step through a
use case and this instance we're going
to look at holum and how they used Chef
so um holum is a bank and it's the
largest bank in Israel and they have a
mixture of Linux and windows servers
that they have to maintain and as you
can imagine this requires a lot of
constant configuration and work and this
has led to issues in the past so the
challenge that the team apum faced was
creating and hardening software that ran
the major tasks and was repeatable on
those servers that it didn't matter who
the people were forming the jobs that
there was consistency in the work that
was being done and address the problem
that the servers including the hardware
on the servers was not cons consistent
throughout the organization Chef
addresses those specific problems and
became the go-to product for holum they
were able to write the recipes and the
cookbooks that could be deployed out to
the network effectively and it didn't
matter what the system was the recipes
reflected the standards that the system
administration team put together and
they were able to ensure and test each
script and use standard testing tools
for those scripts
it didn't matter what kind of
environment that holum had if certain
ports need to be closed or if a f wall
needs to be uh installed or modified or
if custom strong passwords had to be
created all of this could be done using
the recipes that Chef offers and once
those recipes had been finalized they
could be packaged into a common cookbook
that could then be deployed to the
entire network ensuring a consistent see
in results for the company and also it
didn't matter whether the cookbooks were
being deployed to Linux or Windows
machine because the scripts were being
put together in the recipes and were
written in Ruby you could actually go
through an update and modify those
scripts depending on the environment so
if you needed to make a modification for
Linux you could make those modifications
so they were consistent across all the
Linux servers or all the windows servers
and this really drove in the ability to
harden and secure the environment in the
entire network within a matter of
minutes using Chef versus the days and
weeks that it previously took so if we
take a before an after scenario using
Chef before Chef tasks were repeated
multiple times and it was just hard to
keep track of all the people doing the
work and the reality is because so many
people were touching all the different
systems specific standards the banks had
established were simply not being met
after Chef all of the tasks could be
scripted using Ruby and using the recipe
and cookbook model that Chef has created
the chef tool was able to deploy out to
the entire network specific cookbooks
that provided a consistent experience
and consistent setup and operation
environment for all the hardware the end
result is is that the system
administration team could guarantee that
the standards of the bank were being met
consistently we're going to focus
specifically on puppet so in this
session what we're going to do is we're
going to cover what and why you would
use puppet what are the different
elements and components of puppet and
how does it actually work and then we'll
look into the companies that are
adopting puppet and what are the
advantages that they have now received
by having puppet within their
organization and finally we'll wrap
things up by reviewing how you can
actually write a manifest in puppet so
let's get started so why puppet so here
is a scenario that as an administrator
you may already be familiar with you as
an administrator have multiple servers
that you have to work with and manage so
what happens when a server goes down
it's not a problem you can jump onto
that server and you can fix it but what
if the scenario changes and you have
multiple servers going down so here is
where PU shows its strap with puppets
all you have to do is write a simple
script that can be written with Ruby and
write out and deploy to the servers your
settings for each of those servers the
code gets pushed out and to the servers
that are in having problems and then you
can choose to either roll back to those
servers to their previous working States
or set them to a new state and do all of
this in a matter of seconds and it
doesn't matter how large your server
environment is is you can reach to all
of these servers your environment is
secure you're able to deploy your
software and you're able to do this all
through infrastructure as code which is
the advanced devops model for building
out Solutions so let's dig deeper into
what puppet actually is so puppet is a
configuration management tool maybe
similar tools like Chef that you may
already be familiar with it ensures that
all your systems are configured to a
desired and predictable State pu can
also be used as a deployment tool for
software automatically you can deploy
your software to all of your systems or
to specific systems and this is all done
with code this means you can test the
environment and you can have a guarantee
that the environment you want is written
and deployed accurately so let's go
through those components of puppet so
here we have a breakdown of the puppet
environment and on the top we have the
main server Environ enironment and then
below that we have the client
environment that would be installed on
each of the servers that would be
running within your network so if we
look at the top part of the screen we
have here our puppet master store which
has and contains our main configuration
files and those are comprise of
manifests that are actual codes for
configuring the clients we have
templates that combine our codes
together to render a final document and
you have files that will be deployed as
content that could be potentially
downloaded by the clients wrapping this
all together is a module of manifest
templates and files you would apply a
certificate authority to sign the actual
documents so that the clients actually
know that they're receiving the
appropriate and authorized modules
outside of the master server where You'
create your manifest templates and files
you would have public client is a piece
of software that is used to configure a
specific machine there are two parts to
the one is the agent that constantly
interacts with the master server to
ensure that the certificates are being
updated appropriately and then you have
the fact of that the current state of
the client that is used and communicated
back to through the agent so let's step
through the workings of puppet so the
puppet environment is a Master Slave
architecture the clients themselves are
distributed across your network and they
are constantly communicating back to a
Master server environment where you have
your puppet module
the client agent sends a certificate
with the ID of that server back to the
master and then the master will then
sign that certificate and send it back
to the client and this authentication
allows for a secure and verifiable
communication between client and master
the factor then collects the state of
the client and sends that to the master
based on the facts sent back the master
then compiles manifests into the
cataloges and those cataloges are sent
back to the clients and an agent on the
client will then initiate the catalog a
report is generated by the client that
describes any changes that have been
made and sends that back to the master
with the goal here of ensuring that the
master has full understanding of the
hardware running software in your
network this process is repeated at
regular interos ensuring all client
systems are up to date so let's have a
look at companies that are using puppet
today there are a number of companies
that have adopted puppet as a way to
manage their infrastructure so companies
that are using puppet today include
Spotify Google AT&T so why are these
companies choosing to use puppet as
their main configuration management tool
the answer can be seen if we look at a
specific company Staples so Staples
chose to take and use puppet for their
configuration management tour and use it
within their own private Cloud the
results were dramatic the amount of time
that the it organized ation was able to
save in deploying and managing their
infrastructure through using puppet
enabled them to open up time to allow
them to experiment with other and new
projects and assignments a real tangible
benefit to a company so let's look at
how you write a manifest in puppet so so
manifests are designed for writing out
in code how you would configure a
specific node in your server environment
the manifestor compiled into catalogs
which are then executed on the client
each of the manifests are written in the
language of Ruby with a PP extension and
if we step through the five key steps
for writing a manifest they are one
create your manifest and that is written
by the system administrator two compile
your manifest and it's compiled into a
catalog three deploy the catalog is then
deployed onto the clients for execute
the catalogs are run on the the client
by the agent and then five end clients
are configured to a specific and desired
state if we actually look into how
manifest is written it's written with a
very common syntax if you've done any
work with Ruby or really configuration
of systems in the past this may look
very familiar to you so we break out um
the work that we have here you start off
with a package file or service as your
resource type and then you give it a
name and then you look at the features
that need to be set such as IP ke
address then you're actually looking to
have a command written such as present
or start the Manifest can contain
multiple resource types if we continue
to write our manifest and puppet the
default keyword applies a manifest to
all clients so an example would be to
create a file path that creates a folder
called sample in a main folder called
Etc the specified content is written
into a file that is then posted into
that folder and then we're going to say
we want to be able to trigger an Apache
service and then ensure that that Apache
service is installed on a node so we
write the Manifest and we deploy it to a
client machine on that client machine a
new folder will be created with a file
in that folder and an Apache server will
be installed you can do this to any
machine and you'll have exactly the same
results on those machines so first we'll
see the career with Cloud associate
Cloud engineer leave the server X behind
and Venture into the dynamic world of
cloud Computing the perfect moment has
arrived and now let's talk about what
this certification means for you your
associate Cloud engineer certification
is like a bon signaling to prospective
employers your progress in Google Cloud
as an associate Cloud engineer you will
be adapt at deploying and securing
applications and infrastructure you will
maintain Enterprise solutions to meet
performance standards and keep a
watchful eye on the operations of
multiple Cloud projects but that's not
all associate Cloud Engineers also prove
their Mastery in using the Google Cloud
console and command line interface you
will effortlessly manage and scale Cloud
Solutions leveraging Google managed or
self-managed services on Google cloud
and many of our peers once dwell in the
own premises realm meticulously setting
up servers now they're prepared to level
up their skills for the cloud era the
associate Cloud engineer certification
is your stepping stone to a flourishing
it career so picture yourself as a cloud
developer architect security engineer
systems engineer or network engineer the
possibilities are endless so let's get
started with the benefits of doing gcp
AC so the first benefit you can gain by
acing this certification is you will
have the membership to Google credential
holder portal so I will show you so this
is the Google Cloud certified directory
here you can find all the people who
have cleared this certification and you
can just click on the LinkedIn profile
and see you what skills they process and
you have other filters also that is you
can search by the credentials locations
or skills so this is the first benefit
of doing gcp AC now moving back the
second benefit is exposure to all gcp
services while preparation will boost
your confidence and then this will set a
good foundation for professional
architect exam and Google certification
and badge to be used in your profile for
visibility that will uphold your profile
and will gain you many opportunities and
now we'll see steps what you can follow
to Ace the gcp AC certification exam so
before we dive in it's essential to
grasp the exam structure the Google
associate Cloud engineer certification
exam consists of 50 multiple choice
questions and you have a total of two
hours to complete it a passing score
requires at least 70% correctness this
exam evaluates your knowledge of Google
Cloud platform Services basic networking
Concepts and cloud computing
fundamentals and the recommended
experience for this certification is 6
Plus months and the prerequisites are
none for this certification exam and the
link to register for this exam I will
mention that link in the description box
below and I will show you the link also
so this is the link here you have to go
and make your profile if you don't have
an account and after that you can log in
so this is here where you will register
for the exam and moving back now we'll
see the step two that is getting
familiar with gcp so to succeed you need
to become best friends with Google Cloud
platform luckily Google offers a free
tire that allows you to explore gcps
product and services without cost and
don't forget about the comprehensive gcp
documentation it's your treasure throve
of insights into various services and
functionalities so you can just go to
the documentation and read it and the
third is enroll in a course or training
program so education is the key here
enroll in a course or training program
tailored to the Google associate Cloud
engineer exam the simply learn CCH cloud
computing boot camp course lets you
master key architectural principles and
develop the skills needed to become a
cloud expert and provides in-depth
coverage of exam topics or if you prefer
hands-on experience the associate Cloud
engineer exam prep Quest from quickls is
a great choice and I will mention the
link for the boot camp in the
description box you can go there and you
can enroll in a course or training
program to Ace the preparation and now
the step four take practice test sharpen
your skills with practice test Google
Cloud offers official practice test for
purchase which give you a real feel for
the exam environment Cloud Guru and Wiz
laabs also provide valuable practice
exams and if you're budget conscious
exam Labs offer free practice test and
now moving on to the step five review
exam objectives and resources so stay
focused by reviewing the exam objectives
outlined by by Google Cloud these guides
serve as your road map to success dive
deep into Google Cloud's documentation
and white papers for additional study
materials don't forget to engage with
the Google Cloud community on Reddit for
valuable insights and tips and now
talking about the cabus I will move to
the website directly only so this is the
cabus for Google Cloud architect so here
you will learn and you should know the
setting up a Cloud solution environment
for setting up organization resources
that is then you should have a knowledge
on planning and configuring a Cloud
solution deploying an implementing a
Cloud solution and ensuring successful
operation of a Cloud solution so these
all are the things that are covered in
the cabus so you should have a hands on
that as I told you about the boot camp
so all these labus topics are covered in
that boot camp or the postgraduate
program that I mentioned in the starting
so if you want to a that program you can
just under role in the course or
training program and now moving back now
we'll just have a look at the tips to a
this exam so the first tiep is book the
exam in advance before starting
preparation so you don't miss out and
the second tip is practice practice and
practice that will help you clear that
certification and the third tip is start
the exam with relaxed mind and take
poses after five questions each time
that really helps and then we have the
four tip that is understand intention of
question look for keywords in questions
and answers and the tip five while
choosing options focus on available
option do not worry about perfect answer
and there you have a tech explorers your
gateway to a cloud powered future awaits
the cloud engineer learning path crowned
by the revamp preparing for the
associate Cloud engineer certification
course is a compass to steer your career
toward New Horizons certifications are
the passport to a successful career in
the ever evolving world world of
technology they validate your expertise
in essential devop practices tools and
principles these certifications not only
boost your credibility but also open
doors to a creative job opportunities
with salaries reaching up to
$18,000 per year in the United States in
this video we will be discussing top
four devop certifications that you must
enroll if you actually want to gain
knowledge and make a successful career
in the devops field the topics that we
will be covering are are first why
devops then skills of devops
professionals and then the top
certifications for devops stay tuned
till the end of this video if you want
to have a look at those certifications
also before we begin make sure to
subscribe to our YouTube channel and
press the Bell icon to never miss an
update from Simply learn now let's start
with the first topic which is why devops
so in 2024 devops is more important than
ever because it helps teams work
together smoothly making software
develop velopment and it operations
faster and more efficient think of it as
teamwork for technology with the world
relying on Digital Services we need to
deliver software quickly and reliably
devops make this happen by bringing
development and ID operation closer they
communicate better share ideas and use
tools to automate tasks this means fewer
errors faster fixes and happier
customers learning devops in 2024 is
necessary because it offers numerous
benefits in the rapidly evolving Tech
landscape firstly devops practices
enable organization to deliver software
faster ensuring they stay competitive in
the market secondly devops Fosters
collaboration and communication between
development and operation teams leading
to improved efficiency and reduced
errors thirdly automation plays a
crucial role in devops allowing teams to
streamline process and focus on
Innovation fourth devop skills are
highly sought after by employers
providing excellent career opportunities
all right so this was about why devops
now let's have a look at the skills of
devops professional so being a devops
professional requires a diverse set of
technical skills to bridge the gap
between development and operations here
are some of the skills of devops
professional first containerization
Mastery of containerization Technologies
such as Docker is essential containers
make it easy to package and deploy
applications consistently across
different environments next is orchest
isation tools proficiency in container
or clusterization tools like kubernetes
is important these tools manage the
deployment scaling and maintenance of
containerized applications next is
scripting and coding skills Proficiency
in scripting languages like python Ruby
or shell scripting is crucial devops
professionals often write scripts to
automate tasks and processes Version
Control understanding and using Version
Control Systems like git is essential it
helps track changes in code and
collaborate effectively with team
members next is continuous integration
and continuous deployment that is cicd
knowledge of cicd tools like Jenkins
Travis Ci or Circle CI is key these
tools automate buildings testing and
deploying software ensuring rapid and
reliable releases next is cloud
platforms expertise in Cloud platforms
like AWS Azure or Google cloud is
essential for scalable and flexible
infrastructure management so this was
about the skills of a devops
professional now let's discuss the
certifications that is the top
certifications so the first
certification on the list is
professional certificate program in
cloud computing and devops the
professional certificate program in
cloud computing and devops offered by
simply learn is an exciting opportunity
for anyone looking to gain valuable
skills and tools in the world of cloud
computing and devops this course is in
partnership with E and ICT Academy I
guati this comprehensive program covers
a wide range of technical skills and
tools that are in high demand in the IT
industry it combines the power of cloud
computing with efficiency of devops to
create a well-rounded skill set in this
course you will learn about Cloud
platforms like AWS and Azure which are
the backbone of Mod 19 infrastructure
you will discover how to deploy and
manage applications on these platforms
ensuring scalability and cost efficiency
this is AK to mastering the tools that
Architects use to construct sturdy and
adaptable buildings the devops component
of the program equips you with essential
skills such as continuous integration
and continuous deployment cicd
automation containerization with Docker
and configuration management with tools
like anible it's like having a toolkit
that streamlines the software
development and deployment process
making it faster and more reliable by
completing this program you will be well
prepared to excel in rules where cloud
computing and devops expertise are
essential it's a pathway to promising
career and a chance to become a
proficient in the skills and tools that
are shaping the future of the IT
industry all right so this was about
professional certificate program in
cloud computing in devops now coming to
the next one that is devops engineer
Masters program so introducing our
devops engineer training Masters program
your gateway into a thriving career in
devops Technologies this comprehensive
course equips you with the technical
skills and tools essential for deploying
managing and monitoring Cloud
applications in this devops engineer
course you will delve into the
deployment methodologies cicd pipelines
all while mastering a robust arsenal of
devops tools such as git Docker Jenkins
and more think of it as a toolbox packed
with the latest most powerful equipment
for your Tech Journey our program offers
120 plus live classes instructor-led
online classes ensuring you learn
directly from the industry experts
additionally you will gain access to 90
Plus hours of high quality e-learning
real life case studies chapter and
quizzes and simulation exams making your
learning experience rich and handsome
but it doesn't stop there you'll also
become a part of Vibrant Community
moderated by devops expert where you can
share insights and grow together so if
you are ready to embark on a career in
devops do enroll for this program so
this was about devops engineers Masters
program now coming to the next
certification that is postgraduate
program in devops so this postgraduate
program in devops is a dynamic
collaboration with celtech ctme designed
to elevate your skills and align you
with the industry benchmarks this
program is your gateway to a world of
opportunities where you will gain
expertise through our Innovative Blended
learning approach this course offers
live online devop certification classes
that are complemented by immersive
interactive Labs these Hands-On Labs
provide you with the The Real World
Experience setting you on the path to
earning a prestigious Caltech ctme
postgraduate certificate enrolling in
simply learns devops course also grants
you access to our exclusive job assist
program facilitating your transition
into the professional World Additionally
you will work on 20 plus life I repeat
Additionally you will work on 20 plus
real life projects in integrated labs
and culminate your journey with a
Capstone project in three crucial
domains this course offers essential
skills including devops methodology
continuous integration and continuous
delivery configuration management and
containerization the best part is prior
work experience is not mandatory if you
hold a bachelor degree with an average
of 50% or higher marks whether from a
programming or non-programming
background you're eligible to join the
transformative learning experience so
this was about this post-graduate
program in devops now let's move on and
have a look at the next certification of
devops that is aure devops Solutions
expert embark on a transformative
Journey towards becoming an industry
ready professional with the Azure devop
Solutions expert Masters program this
comprehensive course is your gateway to
a flourishing career in the dynamic
cloud and devops Industry in this
program or in this certification you
will Master the art of working smarter
collaborating efficiently and delivering
faster results using The Cutting Edge
array of modern development services you
will be armed with the Knowledge and
Skills that employers in the industry
truly value what sets this program apart
is the industry recognized certification
you will earn upon successful completion
ATT Testament to your expertise so for
all the courses that we have now
discussed you will get a certification
for that all right now what makes this
learning experience exceptional is the
opportunity to learn from leading
industry experts who have created the
course content to ensure a directly
applicability in the real world you'll
not only gain knowledge but also work on
the the Practical projects backed by our
network of Industry partners and their
Career Services job assistance Services
personalized CV building career Affairs
and interview preparation make your
transition into roles like Azure
pipeline Azure container Azure storage
smoother whether you're a fresh graduate
eager to make your mark or a
professional looking to Pivot into the
exciting field of cloud security this
program open doors to a world of
possibilities in the cloud and devops
Landscape so this was about the Azure
devop Solutions exper as you explore the
vast landscape of devops and cloud
computing certifications remember that
your journey towards expertise is a
professional one each certifications
like I gu professional certificate in
cloud computing and devops devops
engineer Masters program postgraduate
programming devops and Azure devop
solution expert offers a unique path to
earning your skills whichever you choose
know that your commitment to learning
and growth will open doors to a world of
opportunities in this exciting and
evolving field the industry so in this
explanation of the top 10 devops
projects we'll delve into the innovative
solutions and tools that are catalyzing
progress from Automation and
containerization to continuous
integration and deployment these
projects not only facilitate agility but
also Drive excellence in software
delivery ensuring that devops remains at
the Forefront of modern technology so
join us as we embark on a journey
through the most influential Dev
initiatives offer time with that said if
these are the types of videos you would
like to watch then hit that subscribe
button and the bell icon to get notifi
and if you are a professional with
minimum one year of experience and an
aspiring devops engineer looking for
online training and certifications from
prestigious universities and in
collaboration with leading experts then
search no simply learns postgraduate
program in devops from CCH University in
collaboration with the IBM should be
your right choice for more details use
the link mentioned in the description
box below so let's start with why are
devop skills
crucial understanding devops is vital
for optimizing the software development
life cycle devops Engineers need to
master several key skills lenux
proficiency many firms prefer Linux for
hosting apps and managing configuration
systems it's essential for devops
engineers to be well wored in Linux as
it's the foundation of tools like chef
anible and puppet continuous integration
and continuous delivery CI ensures teams
collaborate using a single version
control system while CD automates design
testing and release improving efficiency
and reducing errors number three
infrastructure as code automation
scripts provide Swift access to
necessary infrastructure a critical
aspect with containerization and Cloud
Technologies ISC manages configuration
executes commands and swiftly deploys
application infrastructure configuration
management tracking software and
operating system configurations ensures
consistency across servers tools like
anible chef and puet simplify this
process making it efficient at number
five we have automation devops aims for
minimal human intervention maximizing
efficiency familiarity with automation
tools like dreel git jenin and Docker is
essential for devops and genius so these
tools streamline development processes
and enhance productivity moving on to
the first project of the day we have
unlocking efficient of Java application
with griddle Meet gridle The Versatile
build automation tool transcending
platforms and languages this project
helps you start on a journey of Java
application creation breaking it into
modular sub projects and more the main
aim of this project is to help you
master project initiation as a Java
application adaptly build it and
generate meticulous test reports you
will be well versed in running Java
applications crafting archives and
elevating your Java development Pro
so dive in to transform your coding
skills with grle the source code for
this project is linked in the
description box below moving on to
project number two unlock robust
applications with Docker for web servers
Docker the go-to container technology
revolutionizes services and app hosting
by virtualizing operating systems and
crafting Nimble containers this project
focuses on creating a universal base
image and helping you collaborate with
fellow developers in diverse production
Landscapes you will be dealing with
taking web apps foundations in Python
Ruby and Meo so Master this project and
you will yield Docker file efficiency
like a pro slashing build times and
simplifying setups so say goodbye to
lendy docka file creation and resource
heavy downloads the source code for this
project is also mentioned in the
description box below so don't forget to
check out moving on to project number
three we have Master cicd pipelines
using Azure in this Azure project we
harness Azure devops to create efficient
cicd pipelines this project mainly
focuses on leveraging Azure devops
project we deploy applications
seamlessly across Azure services like
app service virtual machines and Azure
kuber service or AKs utilizing azure's
devop stter we set up asp.net sample
code explore pre-configured cicd
pipelines commit code changes and
initiate cicd work flows additionally we
F tune monitoring with aure application
insight for enhanced performance
insights the source code for this
project is also mentioned in the
description box below moving on to the
next project elevating Jenkins
communication the remoting project the
Jenkins remoting project is all about
enhancing Jenkins communication
capabilities it's an Endeavor to bolster
the Jenkins remoting Library creating a
robust communication layer this project
incorporates a spectrum of features from
TCP protocols to efficient data
streaming and procedure calls as a part
of this project you will start on the
exciting journey of making Jenkins
remoting compatible with bus
Technologies like active mq and rabbit
mq to succeed in this project a strong
grasp of networking fundamentals Java
and message cues is your Arsen div in
and join us in elevating the way Jenkins
communicates with the world check out
the link mentioned in the description
box below for the source code moving on
to project number five automated web
application deployment with AWS your CD
pipeline project in this project you
will create a seamless continuous
delivery pipeline for a compact web
application your journey begins with the
source code management through a Version
Control System next discover the art of
configuring a CED pipeline enabling
automatic web application deployment
whenever your source code under goes
changes embracing the power of GitHub
AWS elastic be stock AWS code build and
AWS code pipeline this project is your
gateway to streamline efficient software
delivery the source code for this
project is linked in the description box
below moving on to the next project
containerized web app deployment on gke
scaling with Docker this project will
help you discover the power of
containerization with this project you
will learn how to package a web
application as a Docker container image
and deploy it on a Google kubernetes
engine or gke cluster you can watch your
app scale effortlessly to meet user
demands this Hands-On projects cover
packaging your web app into a Docker
image uploading it to effect effect
registry creating a gke cluster managing
autoscaling exposing your app to the
world and seamlessly deploying newer
versions you get to unlock the world of
efficient scalable web app deployment on
gke the source code for this project is
linked in the description box below
moving on to project number seven
mastering Version Control with kit in a
world of software development mastering
a version control system is Paramount
version controlling enables you for code
tracking version comp comparison
seamless switching between versions and
collaborating among developers your
journey in this project will begin with
the fundamental art of saving code in a
VCS taking the scenic route to set up a
repository you can then start on a quest
through code history and reving the
mysteries of version navigation
navigating through branching a
deceptively intricate task is next on
your path by the end of this project you
will be fully equipped to conquer git
one of the most powerful version control
system tools in the Developers Arsenal
the source code for this project is
mentioned in the description box below
moving on to the next project effortless
deployment running applications with
kubernetes the major focus of this
project is to help you harness a
straightforward web service that handles
user messages EK to a voicemail system
for leaving messages your mission you
ask you get to deploy this application
seamlessly with kubernetes then
dockerize it by mastering this
fundamental step you will unlock the
power to run your application in Docker
containers simplifying the deployment
process the source code for this project
is mentioned in the description box
below so don't forget to check it out
moving on to the project number nine
mastering terraform project structure
this project will help you maintain and
extend the efficiency of terraform
projects in everyday operations a well
structured approach is essential this
project unveils the art of organizing
terraform projects based on their
purpose and complexity so harness the
power of key terraform features
including ining variables data sources
provisionals and locals to craft a
streamline project structure by the end
your project will effortlessly deploy an
Ubuntu 20.04 server on digital ocean
configure an Apache web server and
seamlessly Point your domain to it level
up your tform game with proper project
structuring and practical application
check out the link mentioned in the
description box below for this s code
moving on to the last project of the day
we have efficient selenium project
development and and execution in the
world of test automation selenium
projects play a pivotal role they enable
seamless test execution report analysis
and Bug reporting this proficiency not
only accelerates product delivery but
also elevates client Satisfaction by the
end of this project you will Master the
art of building selenium projects
whether through a Java project or a
maven project showcasing your ability to
deliver high quality results efficiently
check out the link mentioned in the
description box below for the source
code through this project understand
what is devops devops is like a teamwork
approach for making computer programs
better and faster it combines the work
of software developers and operation
team the goal is to help them work
together and use tools that speed up the
process and make fewer mistakes they
also keep an eye on the programs and fix
problems early This Way businesses can
release programs faster with few errors
and everyone works better together if
you want to learn more about this then
check postgraduate program in devops to
understand from the basics to advanced
concepts this post-graduate program in
devops is crafted in partnership with
Caltech ctme this comprehensive course
aligns your expertise with industry
benchmarks experience are Innovative
Blended learning merging live online
daop certification sessions with
immersive labs for practical Mastery
Advance your career with Hands-On
training that meets industry demands all
right now let's move on to the first
question of devops interview question
which is what do you know about devops
so think of a devops like teamwork in
the IT world it's become really
important because it helps teams work
together smoothly to make computer
programs faster and with few mistakes
imagine a soccer team everyone works
together to win the game devops is
similar it's when computer developers
and operations people team up to make
software better they start working
together from the beginning when they
plan what the software will be like
until they finish and put it out for
people to use this Teamwork Makes sure
things go well and the software works
great so this was about devops now
moving on to the second question which
is how is devops different from agile
methodology devops is like a teamwork
culture where the people who create the
software and the people who make it work
smoothly join forces this helps in
always improving and updating the
software without any big breaks agile is
a way of making softwares that's like
taking small steps towards instead of
big jumps it's about releasing small
parts of the software quickly and
getting feedback from the users this
helps in solving any issues or
differences between what users want and
what developers are making so you can
answer this question in this way all
right moving on to the third question
which is what are the ways in which a
build can be scheduled SL run in genkins
so as you can see there are four ways by
source code management commits second is
after the completion of other builds
third is schedule to run at a specified
time and fourth one is manual belt
requests so if the interviewer asks then
you can answer these four ways in which
a Bel can be scheduled in Jenkins all
right now the fourth question is what
are the different phases in devops so
the various phases of devops life cycle
are as follows so as you can see first
is plan so initially there should be a
plan for the type of application that
needs to be developed getting of picture
of the development process is always a
good idea then code the application is
coded as per the end user requirements
then there is build build the
application by integrating various codes
formed in previous steps after that
there is test this is the most crucial
step of the application development test
the application and rebuild if necessary
then there is integrate so multiple
codes from different programmers are
integrated into one after integrate
there is is deploy so code is deployed
into a cloud environment for further
usage it is ensured that any new changes
do not affect the functioning of a hight
traffic website after that there is
operate so operations are performed on
the code if required then there is
Monitor application performance is
monitored changes are made to meet the
end user requirements so these all were
the different phases of devops and here
we have explained each one of these you
can go through it and if the interviewer
have asked you this question you can
answer it in a similar way all right now
moving on to the next question which is
mention some of the core benefits of
devops so the core benefits of devops
are as follows first of all we'll say
technical benefits first technical
benefit of devops is continuous software
delivery then second is less complex
problems to manage third is early
detection and faster correction of
defects then here comes the business man
benefits the first benefit of devops
that is business benefit of devops is
faster delivery of features so it allows
faster delivery of features then there
is stable operation environment then
third one is improved communication and
collaboration between the teams so these
were the business benefits so here we
have discussed both technical benefits
and business benefits all right now the
next question is how will you approach a
project that needs to implement devops
so here are the simpler terms
here's how we can bring devops into a
specific project using these Steps step
one would be first we look at how things
are currently done and figure out where
we can make them better this makes about
2 to 3 weeks then we can plan for what
changes to make then step two would be
we create a small test to show that our
plan works when everyone agrees with it
we can start making the real changes and
put the plan into action then third step
would be we are all all set to actually
use devops we do things like keeping
track of different versions of our work
putting everything together testing it
and making it available to the users we
also watch How It's Working to make sure
everything goes smoothly by doing these
steps right the keeping track of changes
putting everything together testing and
watching how it's going we are all set
to use devops in our project so this is
how you can approach a project that
needs to implement in devops
and this is how you can answer this
question all right moving on to the
question number seven which is what is
the difference between continuous
delivery and continuous deployment all
right so first would be continuous
delivery so it ensures code can be
safely deployed on production whereas
continuous deployment here every change
that passes the automated test is
deployed to production automatically
then in continuous delivery it ensures
business applications and services
functions as expected whereas in
continuous deployment it makes software
development and the release process
faster and more robust in continuous
delivery delivers every change to a
production like environment through
rigorous and automa testing whereas in
continuous deployment there is no
explicit approval from a developer and
requires a developed culture of
monitoring so these were the three
points that you can highlight while
differentiating between continuous
delivery and continuous deploy reyment
all right so this was the question
number seven now moving on to the
question number eight which is name
three security mechanisms of Jenkins
uses to authenticate users so here we
have to name three security mechanisms
first one would be genkins uses an
internal database to store user data and
credentials second is genkins can use
the lightweight directory access
protocol that is ldap server to
authenticate users third one is genin
can be configured to employ the
authentication mechanism that the
deployed application server uses so
these were the three mechanisms that
genen uses to authenticate users all
right now moving on to the question
number nine which is how does continuous
monitoring help you maintain the entire
architecture of the system so continuous
monitoring within devops involves the
ongoing identification detection and
reporting of any anomalies or security
risk across the entire system
infrastructure it guarantees the proper
functioning of services applications and
resources on servers by overseeing
server statuses it accesses the accuracy
of application operations this practice
also facilitates uninterrupted audits
transaction scrutiny and regulated
surveillance so this was about the
question number nine that was how does
continuous monitoring help you maintain
the entire architecture of the system
now moving on to the question number 10
which is what is the role of AWS in
devops so in the realm of devops AWS
assumes several rules first one would be
adaptable services so it offers
adaptable pre-configured Services
eliminating the necessity for software
installation or configuration second one
is designed for expansion whether
managing one instance or expanding to
th000 AWS Services accommodate seamless
scalability then there is automated
operations AWS owers task and process
automation freeing up valuable time for
inventing Pursuits the next one is
enhanced security AWS identity and
accesses management that is IM am allows
precise user permissions and policy
establishment then the last one is
extensive partner Network so AWS Fosters
a vast partner Network that integrates
with and enhances its service offerings
so these were the role of awf in the
realm of devops all right now moving on
to the question number 11 name three
important devops kpis the three
important important kpis are as follows
first one is meantime to failure
recovery this is the average time taken
to recover from a failure second K is
deployment frequency the frequency in
which the deployment occurs is called
deployment frequency third one is
percentage of failed deployments the
number of times the deployment fails is
called percentage of failed deployments
so these were the three important devops
kpis this question can also be asked all
right move moving on to question number
12 which is how is IAC implemented using
AWS so commence by discussing
traditional methods involving scripting
commands into files followed by testing
in isolated environments prior to
deployment notice how this practice is
giving way to infrastructure as code IAC
comparable to code for various Services
IAC AED by AWS empowers developers to
craft accesses and manage infrastructure
components Des descriptively utilizing
formats like Json or yl this Fosters
streamlined development and expeditious
implementation of alterations in
infrastructure all right now moving on
to question number 13 which is describe
the branching strategies you have used
they'll ask you this question so to test
our knowledge the purpose of branching
and our experience of branching at a
past job this question is usually asked
so here we will discuss topics that can
help you answering this devops interview
question
so release branching we can clone the
develop Branch to create a release
Branch once it has enough functionality
for a release this Branch kicks off the
next release cycle th no new features
can be contributed Beyond this point the
things that can be contributed are
documentation generation bug fixing and
other release related tasks the release
is merged into master and given a
version number once it is ready to ship
it should also be merged back into the
development branch which may have
evolved since the initial release so
this was the first one then there is
feature branching this branching model
maintains all modifications for a
specific feature contained within a
branch the branch gets merged into
Master once the feature has been
completely tested and approved by using
test that are automated then the third
branching is Task branching in this
branching model every task is
implemented in its respective Branch the
task key is mentioned in the branch name
we need to Simply look at the task key
in the branch name to discover which
code implements which tasks so these
were the branching strategies that you
can say that you have used or if you
have really used it otherwise you can
say something else all right moving on
to question number 14 which is can you
explain the shift left to reduce failure
Concept in devops so shifting left
within the devops framework is a concept
aimed to enhance security for
performance and related aspect effects
to illustrate consider the entity of
divers processes currently security
valuations occur before the deployment
stage by employing the left ship
approach we can introduce security
measures during the earlier development
phase denoted as the left this
integration spans on multiple phases and
compens pre-development and testing not
confined to development alone this
holistic integration is likely to
elevate security measures by identifying
vulnerabilities at initial stage leading
to a more fortified overall process now
moving on to the question number 15
which is what is blue/ te deployment
pattern so this approach involves
seamless deployment aimed at minimizing
downtime it entails shifting traffic
from one instance to another requiring
the replacement of outdating code and a
new version to integrate fresh code the
updated version resides in a green
environment while the older one remains
in a blue environment after modifying
the existing version a new instance is
generated from the old one to execute
the updated instance ensuring a smoother
transition so the main focus of this
approach is smooth
deployment all right so this was
question number 15 now moving on to the
question number 16 which is what is
continuous testing so continuous testing
involves the automated execution of
tests within the software delivery
pipeline offerings immediate insights
into potential business risk within the
latest release by seamlessly integrating
testing into every stage of the software
develop Li I repeat by seamlessly
integrating testing into the every stage
of software delivery life cycle
continuous testing minimizes issues
during transition phases and empowers
development teams and instant
feedback this approach accelerates
developer efficiency by obtaining the
need to rerun all test after each update
and project rebuild culminating in
notable gains in speed and productivity
so this was about continuous
testing now moving on to question number
17 what are the benefits of automation
testing so some of the benefits of
automation testing includes first it
helps to save money and time second is
unattended execution can be easily done
third one is huge test matrices can be
easily tested the next one is parallel
execution is enabled then there is
reduced human generated errors which
results in improved accuracy and last
one is repeated test task execution is
supported so these are the benefits of
automation
testing coming to question number 18
which is what is a Docker file used for
so a Docker file is used for creating
Docker images using the build command
with a Docker image any user can can run
the code to create Docker containers
once Docker image is built it's uploaded
in Docker registry from the docker
registry users can get the docker image
and build new containers whenever they
want so this is what Docker file is used
for coming to question number 19 which
is what is the process for reverting a
commit that has already been pushed and
made public so there are two ways to do
that to revert a commit so first what be
remove or fix the bad file in a new
commit and push it to the remote
repository then commit it to the remote
repository using get commit minus M
commit
message again get commit minus M commit
message so this is the first way then
second is create a new commit that undos
all the changes that were made in the
bad comment then use the command get
rever commit ID as you can see in the
screen First Command also and the second
command also the second command says get
revert commit ID commit ID you'll have
to put the commit ID all right so
question number 19 this was now moving
on to the last question which is
question number 20 how do you find a
list of files that have been changed in
a particular comment so the answer would
be the command to get a list of files
that have been changed in a particular
comit is
get diff tree minus r then there is
commit hash so this command as you can
see on the screen get diff 3 minus r
commit hash you can put it then example
is also there like commit hash 87 e
6735 F2 1B so this is example as you can
see on the screen then there is minus r
flag instruct so the command to list
individual files and commit hash will
list list all the files that were
changed to or added in that commit so
there's telling about the as we can see
minus r functionality minus r is a flag
that instructs the command to list
individual files and commit hash will
list all the files that were changed or
added in that comment so these are the
top 20 devops interview questions that
you must understand if you're planning
to give a devops interview so in this
video we have explored key Concepts
methodologies and best practices that
are crucial in fostering collaboration
between development and operations teams
by understanding the principles
discussed here you are well equipped to
navigate the dynamic landscape of devops
and drive successful software delivery
as we wrap up this devops full course
for 2024 we hope you have gain valuable
insights and skills to thrive in today's
Tech landscape you have found this
content helpful don't forget to like
share and subscribe for more enriching
Tech tutorials your feedback is crucial
so drop your thoughts in the comment
section below stay connected for the
latest updates and remember the future
of tech starts with continuous learning
thanks for joining us on this journey
and we look forward to seeing you in the
next video staying ahead in your career
requires continuous learning and
upskilling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in cuttingedge
domains and including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click
here