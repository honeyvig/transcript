hey everyone welcome to another video by
simply learn in this video we will teach
you everything that you need to know
about keras in machine learning and data
analysis to begin we will explain the
concept of keras to you we will answer
the question what is keras and how it is
used in deep learning then
we will list out some common differences
between keras tensorflow and pi torch so
that you can understand why keras is the
superior framework after this we will
dive deep into keras and see how you can
use keras to implement deep learning
models with hands-on tutorials
and finally we will take a look at keras
sequential models a type of model in
keras which makes implementing neural
networks extremely easy
so
let's get started the first question
that arises in our mind is what is deep
learning deep learning is generally
considered to be a subset of machine
learning which is nothing but a subset
of artificial intelligence what is
artificial intelligence artificial
intelligence is the intelligence which
is demonstrated by machines unlike the
natural intelligence which is displayed
by humans and animals which also
involves consciousness and emotionality
meanwhile machine learning is an
application of artificial intelligence
that provides systems the ability to
automatically learn and improve from
experience without being programmed to
specifically now deep learning as i said
is a class of machine learning
algorithms that uses multiple layers to
progressively extract higher level
features from raw input for example if
we have a system which we are using for
image processing lower layers in the
system may be used to detect the
outlines of images while higher layers
may identify the concepts which are
relevant
to the image in case of the picture of a
human it might recognize features such
as digits or letters of face now in deep
learning we use neural networks to use
multiple mathematical operations to
break down a complex problem into
smaller paths which are solved
individually each mathematical
expression is called a neuron
deep learning models are trained using
large sets of labeled data and neural
network architectures that learn
features directly from the data without
the need for manual feature extraction
one of the most popular types of deep
neural networks is known as a
convolution neural network a convolution
neural network convolves learned
features with input data and uses 2d
convolutional layers making this
architecture well suited processing 2d
data such as images
now in deep learning neural networks
will play a very important part what
exactly are neural networks
neural networks are modeled from the way
the human brain works in our brain we
have neurons which carry electrical
impulses from our brain to different
parts of our body and help in
transmitting instructions from the brain
all over our body in neural networks a
neuron acts as nothing more but a
mathematical operation
multiple neurons are required to break
down a complex problem into various
paths to make it easier to solve now a
neural network contains layers of
interconnected nodes each node in a
neural network is a perceptron and is
similar to a multiple linear regression
the perceptron feeds the signals which
are produced by multiple linear
regressions into an activation function
that may be non-linear hidden layers
will fine-tune the input weightings
until neural network's margin of error
is minimal and we get the optimal output
now what is keras
before we move on to keras let's take a
look at tensorflow tensorflow is a
software library which was created by
google to implement large-scale machine
learning models and to help solve
complex numerical problems tensorflow is
an end-to-end open source machine
learning platform
which you can think of as an
infrastructure layer for differentiable
programming it combines four key
abilities
first efficiently executing low level
tensor operations on cpu and gpu or tpu
computing gradient of arbitrary
differential expressions scaling
computation to many devices and
exporting programs to external runtimes
such as servers browsers or mobiles
but what does keras have to do with all
of this keras is nothing more than a
high level deep learning api which is
written in python for neural networks it
has multiple backend neural network
computations and it makes implementing
neural networks easy
as we know keras is a powerful and easy
to use free open source python library
for developing and evaluating deep
learning models
keras is a high level api of tensorflow
2
an approachable highly productive
interface for solving machine learning
problems with a focus on modern deep
learning it provides essential
abstractions and building blocks for
developing and shipping machine learning
solutions with high iteration velocity
keras empowers engineers and researchers
to take full advantage of the
scalability and cross-platform
capabilities of tensorflow 2. you can
run keras on tpu or on large classes of
gpu and you can export your keras model
to run in the browser or on a mobile
device keras provides a python interface
for artificial neural networks keras
acts as an interface for tensorflow
library and it helps in implementation
of commonly used neural network building
block layers such as objectives
activation functions and optimizers and
a host of tools to make working with
images and text data easier to simplify
the coding necessary for writing deep
neural network code
basically it wraps efficient numerical
computation libraries such as theano and
tensorflow and allows you to define and
change neural network models in just a
few lines of code in fact tensorflow and
keras are so tight together that even in
tensorflow the official get started with
tensorflow tutorial we use high level
keras apis embedded in tensorflow
instead of just using tensorflow by
itself
some of the important features of keras
include keras being a high level
interface which uses theano or
tensorflow for its back end
keras can run smoothly on both cpu and
gpu
it supports almost all the models
required for you to create a complete
neural network
and it is extremely modular in nature
which means it is incredibly expressive
flexible and apt for innovative research
so why do we need keras
first of all keras is an api which is
designed for human beings not for
machines which means that it follows
best practices for reducing cognitive
load it offers consistent and simple
apis it minimizes the number of users
action required for common use cases and
it provides clear and actionable
feedback upon
user error it is extremely powerful and
easy to use and it also wraps numerical
computation libraries such as theano in
tensorflow and allows you to train and
define neural networks in a few lines of
code
this makes keras easy to learn and use
as a keras user you can be more
productive allowing you to try more
ideas than your competition faster which
in turn will help you win machine
learning competition this ease of use
does not come at the cost of reduced
flexibility because keras integrates
deeply with low level tensorflow
functionality it enables you to develop
highly hackable workflows where any
piece of functionality can be customized
or reused easily
building a model in keras let's take a
look at the basic pipeline which you
will have to follow when you are using
keras to define a neural network first
of all you will have to define a neural
network you will have to select the
activation functions and the various
mathematical operations that are gonna
be taking place in your neural network
along with the different weightages
next you will use keras to compile your
network which is basically you will use
keras to build it
after this you will fit to your network
you will fit all the training data that
you have to your network
and use your testing data to evaluate
how the network works
finally you will use this network to
make predictions
this entire five-step pipeline can be
executed in keras within two to three
lines of code neural networks which
would have taken people days or even
weeks to make can be executed in keras
in a couple of hours
now uses of keras
the first use of keras is to productize
deep models on smartphones
deep models require a lot of computation
power to run but with the help of keras
we can literally
make deep models a product which can be
sold and executed on smartphones
the next use of keras is in distributed
training of deep learning models
distributed training means that we can
split our deep learning model into
different paths and train it on systems
all across the globe this makes training
of our deep learning model extremely
fast along with saving time we are also
saving on the computational power of our
system as it is not only our system
which has to run such a heavy program by
distributing it across various systems
all the resources required to train a
deep learning model go down
significantly before we can see the
differences between these platforms we
first need to know what exactly each of
these platforms are let's start with
tensorflow
what is tensorflow tensorflow is a low
level software library which is created
by google to help implement machine
learning models and to solve complex
numerical problems
tensorflow is nothing but a free and
open source software library for machine
learning it can be used across a range
of tasks but has a particular focus on
training and inference of deep neural
networks
tensorflow is a symbolic math library
based on data flow and differential
programming it is used for both research
and production at google
what do you mean by data flow it
basically means that we perform
calculations by converting every element
into graphical form the variables of the
graph are called tensors and
mathematical operations are called
operators
here in the computational graph shown
you can see that x y and 2
are the variables they will also be
called tensors and division
multiplication and addition are the
operators
this graph basically shows us the
calculation that is going to occur in a
machine learning model
where x and y are gonna be divided and y
and two are gonna be multiplied the
results of these two calculations are
then gonna be added to give us the final
output i told you that x y and 2 are
also called tensors
what exactly are tensors tensors are
multi-dimensional arrays with a uniform
type all tensors are immutable like
python numbers and strings which means
that you cannot update the contents of a
tensor you can only create a new tensor
next let's look at the api keras
what exactly is keras keras is a high
level deep learning api written in
python for easy implementation and
computation of neural networks keras is
an open source software library that
provides a tensorflow interface for
artificial neural networks
keras acts as an interface for the
tensorflow library which means that it
runs on top of tensorflow
up until version 2.3 keras supported
multiple back-ends
including tensorflow microsoft cognitive
toolkit
or cntk
r theano and played ml as of version 2.4
only tensorflow is supported
as of version 2.4 only tensorflow is
supported designed to help enable fast
experimentation with deep neural
networks in it focuses on being user
friendly modular and extensible
keras is a high level api of tensorflow
an approachable highly productive
interface for solving machine learning
problems with the focus on modern deep
learning it provides essential
abstractions and building blocks for
developing
and shipping machine learning solutions
with high iteration velocity keras does
not perform its own low level operations
such as tensor products and convolution
it relies on back end engines for that
even though keras supports multiple back
end engines its primary backend engine
is tensorflow and its primary supporter
is google which means that keras acts as
nothing more but a wrapper class around
tensorflow theano cntk blade ml or mxnet
which are low level apis next we will
look at pi torch what exactly is pytoch
pytorch is a low level api which is
developed by facebook for natural
language processing and computer vision
it is a more powerful version of numpy
it is an open source machine learning
library based on the torch library used
for applications such as computer vision
and natural language processing
primarily developed by facebook's ai
research lab it is free and open source
software released under the modified bsd
license although the python interface is
a more polished and the primary focus of
development pytorch also has a c plus
plus interface python is a widely liked
language because it is easy to
understand and write pytorch emphasizes
flexibility and allows deep learning
models to be expressed in basic python
pytorch is mainly used for natural
language processing
and for computer vision
now let's move on to the differences
between tensorflow keras and pytorch
the first difference that we'll be
looking at is called level of api there
are two main types of apis a low-level
api and a high-level api api stands for
application programming interface
a low-level application programming
interface is generally more detailed and
allows you to have
more detailed control to manipulate
functions within them on how to use and
implement them
while a high level api is more generic
and simple and provides more
functionality with one command
statements than a lower level api
high level interfaces are comparatively
easier to learn and to implement the
models using them
they allow you to write code in a
shorter amount of time and to be less
involved with the details
in this case tensorflow is a high and
low level api
pure tensorflow is a low level api while
tensorflow wrapped in keras is a high
level api
keras in itself is a high level api
which uses multiple low-level apis as a
back-end and simplifies the operation of
these low-level apis
pi torch is a low-level api
the next criteria that we'll be looking
at is speed
tensorflow is very fast and is used for
high performances
keras is slower as it works on top of
tensorflow not only does it have to wait
for tensorflow to finish implementation
it then starts its own implementation
meanwhile pi torch works at the same
speed as tensorflow as both of them are
both low level apis
now keras is a wrapper class for
tensorflow and has added abstraction
functionalities on top of tensorflow
which make it slower than tensorflow and
pi torch in computation speed both
tensorflow and pi torch are almost equal
and in development speed keras is faster
as it has built-in functionalities which
can significantly reduce your
development time the next difference is
on the architecture
tensorflow is not very easy to use and
even though it provides keras as a
framework that makes it work easier
tensorflow still has a very complex
architecture which is hard to use
meanwhile keras has a simpler
architecture and is easier to use it
provides a high level of abstraction
which makes implementation of programs
in keras significantly easier pie touch
on the other hand also has a complex
architecture and the readability is less
when compared to keras tensorflow uses
computational graphs which makes it very
complex and hard to interpret but it has
amazing computational ability across
platforms pytorch is a little hard for
beginners but is really good for
computer vision and deep learning
purposes
data sets and debugging
tensorflow works with large data sets
due to its high execution speed and
debugging is really hard in tensorflow
due to its complex nature
meanwhile keras only works with very
small data sets as its speed of
execution is low
programs do not require frequent
debugging in keras as they are
relatively simpler and pi torch can
manage high level tasks in higher
dimension data sets and is easier to
debug than both qrs and tensorflow
next we'll be looking at ease of
development
as we said before tensorflow works with
many
hard concepts such as computational
graphs and tensors which means that
writing code in tensorflow is very hard
it is generally used by people when they
are doing research work and really need
very specific functionalities
heroes on the other hand provides a high
level of abstraction which makes it very
easy to use it is best for people who
are just starting out with python and
machine learning pytorch is easier than
tensorflow but is still comparatively
hard than keras it is not very easy to
learn for beginners but is significantly
more powerful than just plain keras ease
of deployment tensorflow is very easy to
deploy as it uses tensorflow serving
tensorflow serving is a flexible high
performance serving system for machine
learning models designed for production
environments tensorflow serving makes it
easy to deploy new algorithms and
experiments while keeping the same
server architecture and apis
tensorflow serving provides
out-of-the-box integration with
tensorflow models but can be easily
extended to serve other types of models
and data
in keras model deployment can be done
with either tensorflow serving or flask
which makes it relatively easy but not
as easy as you as it would be with
tensorflow and pytorch
pytorch uses i torch mobile which makes
deployment easy but again for tensorflow
deployment is way easier as tensorflow
serving can update your machine learning
backend on the fly without
the user even realizing there's a
growing need to execute ml models on
edge devices to reduce latency preserve
privacy and enable new interactive use
cases in the past engineers used to
train models separately they would then
go through a multi-step error-prone and
often complex process to train the
models for execution on a mobile device
the mobile runtime was often
significantly different from the
operations available during training
leading to inconsistent developer and
eventually user experience
all of these frictions have been removed
by pytorch mobile by allowing a seamless
process to go from training to
deployment by staying entirely within
the pytorch ecosystem it provides an
end-to-end workflow that simplifies the
research to production environment for
mobile devices
in addition it paves the way for privacy
preserving features via federated
learning techniques
at the end of the day the question that
really matters is which framework should
you use keras tensorflow or pytoch
now tensorflow has implemented various
levels of abstraction to make
implementation of deep learning and
neural networks easy this has also made
debugging easier keras is simple and
easy but not as fast as tensorflow it is
more user-friendly than any other deep
learning api however and is easier to
learn for beginners
pytorch on the other hand is the
preferred deep learning api for teachers
but it is not as widely used in
production as tensorflow is it is faster
but it has lower gpu utilization
at the end of the day
the framework that we would suggest that
you use is tensorflow
why
while pi torch may have been the
preferred deep learning library for
researchers tensorflow is much more
widely used in day-to-day production
pytorch is ease of use combined with the
default ego execution mode for easier
debugging predestines it to be used for
fast hacky solutions and smaller scale
models
but tensorflow's extensions for
deployment on both servers and mobile
devices combined with the lack of python
overhead makes it the preferred option
for companies that work with deep
learning models
in addition the tensorflow board
visualization features offers a nice way
of showing the inner workings of your
model to say your customers
meanwhile between tensorflow and keras
the main difference isn't in performance
tensorflow is a bit faster due to less
overhead but also the level of control
you would like keras is much easier to
start with than plain tensorflow but if
you want to do something with keras that
doesn't come out of the box it will be
harder to implement that tensorflow on
the other hand allows you to create any
arbitrary computational graph providing
much more flexibility so if you're doing
more research type of work tensorflow is
the sure route to go due to the
flexibility that it provides if getting
your learning started is half the battle
what if you could do that for free
visit skill up by simply learn click on
the link in the description to know more
so we're going to dive right into what
is cross we'll also go all the way
through this into a couple of tutorials
because that's where you really learn a
lot is when you roll up your sleeves so
we talk about what is cross cross is a
high level deep learning api written in
python for easy implement implementation
of neural networks it uses deep learning
frameworks such as tensorflow pytorch
etc is back in to make computation
faster
and this is really nice because as a
programmer there is so much stuff out
there and it's evolving so fast
it can get confusing and having some
kind of high level order in there we can
actually view it and easily program
these different
neural networks
is really powerful it's really powerful
to
have something out really quick
and also be able to start testing your
models and seeing where you're going
so crossworks by using complex deep
learning frameworks such as tensorflow
pytorch
ml played etc as a backend for fast
computation
while providing a user-friendly and easy
to learn front-end
and you can see here we have the cross
api specifications and under that you'd
have like tf cross for tensorflow thano
cross and so on and then you have your
tensorflow workflow that this is all
sitting on top of
and this is like i said it organizes
everything the heavy lifting is still
done by tensorflow or whatever you know
underlying package you put in there and
this is really nice because you don't
have to
dig as deeply into the heavy end stuff
while still having a very robust package
you can get up and running rather
quickly
and it doesn't distract from the
processing time because all the heavy
lifting is done by packages like
tensorflow this is the organization on
top of it
so the working principle of cross
uh the working principle of keras is
cross uses computational graphs to
express and evaluate mathematical
expressions
you see here we put them in blue they
have the expression
expressing complex problems as a
combination of simple mathematical
operators
where we have like the percentage or in
this case in python that's usually your
left your remainder or multiplication
you might have the operator of x to the
power of 0.3 and it uses useful for
calculating derivatives by using
back propagation so if we're doing with
neural networks we send the error back
up to figure out how to change it
this makes it really easy to do that
without really having not banging your
head and having to hand write everything
it's easier to implement distributed
computation
and for solving complex problems uh
specify input and outputs and make sure
all nodes are connected
and so this is really nice as you come
in through is that as your layers are
going in there you can get some very
complicated
different setups nowadays which we'll
look at in just a second
and this just makes it really easy to
start spinning this stuff up and trying
out the different models
so we look across models across model we
have a sequential model
sequential model is a linear stack of
layers where the previous layer leads
into the next layer
and this if you've done anything else
even like the sk learn with their neural
networks and propagation and any of
these setups this should look familiar
you should have your input layer it goes
into your layer 1 layer 2 and then to
the output layer and it's useful for
simple classifier decoder models
and you can see down here we have the
model equals across sequential this is
the actual code you can see how easy it
is
we have a layer that's dense your layer
one
as an activation they're using the relu
in this particular example and then you
have your name layer one layer dense
relu name layer two and so forth
and they just feed right into each other
so it's really easy just to stack them
as you can see here and it automatically
takes care of everything else for you
and then there's a functional model and
this is really where things are at this
is new make sure you update your cross
or you'll find yourself running this
doing the functional model you'll run
into an error code because this is a
fairly new release
and he uses multi-input and multi-output
model the complex model which forks into
two or more branches
and you can see here we have our image
inputs equals your cross input shape
equals 32 by 32 by 3.
you have your dense layers dense 64
activation relu
this should look similar to what you
already saw before
but if you look at the graph on the
right it's going to be a lot easier to
see what's going on
you have two different inputs
and one way you could think of this is
maybe one of those is a small image and
one of those is a full-sized image
and
that feedback goes into you might feed
both of them into one note because it's
looking for one thing
and then only into one note for the
other one and so you can start to get
kind of an idea that there's a lot of
use
for this kind of split and this kind of
setup we have multiple information
coming in but the information is very
different even though it overlaps and
you don't want to send it through the
same neural network
and they're finding that this trains
faster and is also has a better result
depending on how you split the data and
how you fork the models coming down
and so in here we do have the two
complex models coming in
we have our image inputs which is a 32
by 32 by 3 or 3 channels or four if
you're having an alpha channel
you have your dense your layers dense is
64 activation using the relu very common
x equals dense inputs x layers dense x64
activation equals relu x outputs equals
layers dense 10
x model equals cross model inputs equals
inputs outputs equals outputs name
equals minced model
uh so we add a little name on there and
again this is this kind of split here
this is setting us up to
have the input go into different areas
so if you're already looking at crust
you probably already have this answer
what are neural networks uh but it's
always good to get on the same page and
for those people who don't fully
understand neural networks to dive into
them a little bit or do a quick overview
neural networks are deep learning
algorithms modeled after the human brain
they use multiple neurons which are
mathematical operations to break down
and solve complex mathical problems
and so just like the neuron one neuron
fires in and it fires out to all these
other neurons or nodes as we call them
and eventually they all come down to
your output layer
and you can see here we have the really
standard graph input layer a hidden
layer and an output layer
one of the biggest parts of any data
processing is your data pre-processing
so we always have to touch base on that
with a neural network like many of these
models
they're kind of uh when you first start
using them they're like a black box you
put your data in
you train it and you test it and see how
good it was and you have to pre-process
that data because bad data in is
bad outputs so in data pre-processing we
will create our own data examples set
with keras the data consists of a
clinical trial conducted on 2100
patients ranging from ages 13 to 100
with a the patients under 65 and the
other half over 65 years of age
we want to find the possibility of a
patient experiencing side effects due to
their age and you can think of this in
today's world with covid
what's going to happen on there and
we're going to go ahead and do an
example of that in our
live hands on like i said most of this
you really need to have hands on to
understand so let's go ahead and bring
up our anaconda and
open that up and open up a jupiter
notebook for doing the python code in
now if you're not familiar with those
you can use pretty much any of your
setups i just like those for doing demos
and uh showing people especially
shareholders it really helps because
it's a nice visual so let me go and flip
over to our anaconda and the anaconda
has a lot of cool two tools they just
added data lore and ibm watson studio
cloud into the anaconda framework
but we'll be in the jupiter lab or
jupiter notebook i'm going to do jupiter
notebook for this because i use the lab
for like large projects with multiple
pieces because it has multiple tabs or
the notebook will work fine for what
we're doing
and this opens up in our browser window
because that's how jupiter helps our
jupiter
notebook is set to run
and we'll go under new
create a new python 3 and it creates an
untitled python we'll go ahead and give
this a title and we'll just call this a
cross
tutorial
and let's change that to capital there
we go i'm going to just rename that
and the first thing we want to go ahead
and do
is uh
get some pre-processing tools involved
and so we need to go ahead and import
some stuff for that like our numpy do
some random number generation
i mentioned sklearn or your site kit if
you're installing sk learn the sk learn
stuff it's a
scikit you want to look up
that should be a tool of anybody who is
doing data science if you if you're not
if you're not familiar with the sklearn
toolkit
it's huge but there's so many things in
there that we always go back to
and we want to go ahead and create some
train labels and train samples for
training our data
and then
just a note of what we're we're actually
doing in here let me go ahead and change
this this is kind of a fun thing you can
do we can change the code to markdown
and then markdown code is nice for doing
examples once you've already built this
our example data we're going to do
experimental
there we go
experimental drug was tested on 2100
individuals between 13 to 100 years of
age
half the participants are under 65 and
95 of participants are under 65
experienced no side effects while 95
percent of participants over 65 percent
experience side effects
so that's kind of where we're starting
at and this is just a real quick example
because we're going to do another one
with a little bit more complicated
information
and so we want to go ahead and generate
our setup
uh so we want to do for i and range and
we want to go ahead and create if you
look here we have random integers
train the labels append so we're just
creating some random data
uh let me go ahead and just run that
and so once we've created our random
data and if you if i mean you can
certainly ask for a copy of the code
from simplylearn they'll send you a copy
of this or you can zoom in on the video
and see how we went ahead and did our
train samples a pin
and we're just using this i do this kind
of stuff all the time i was running a
thing on that had to do with errors
following a bell-shaped curve on
a standard distribution error
and so what do i do i generate the data
on a standard distribution error to see
what it looks like and how my code
processes it since that was the baseline
i was looking for in this we're just
doing uh generating random data for our
setup on here
and we could actually go in
print some of the data up let's just do
this print
we'll do train
samples
and we'll just do the first
five pieces of data in there to see what
that looks like
and you can see the first five pieces of
data in our train samples is 49 85 41 68
19
just random numbers generated in there
that's all that is and we generated
significantly more than that
let's see 50 up here 1 000 yeah so
there's 1 000 here 1 000 numbers we
generated and we could also if we wanted
to find that out we could do a quick
print the length of it
and
so or you could do a shape kind of thing
and if you're using numpy
although the link for this is just fine
and there we go it's actually 2100 like
we said in the demo setup in there
and then we want to go ahead and take
our labels oh that was our train labels
we also did samples didn't we
so we could also print
do the same thing
labels
and let's change this to
labels
and labels
and run that just to double check and
sure enough we have 2100 and they're
labeled one zero one zero one zero
i guess that's if they have symptoms or
not one symptoms zero none and so we
wanna go ahead and take our train labels
and we'll convert it into a numpy array
and the same thing with our samples and
let's go ahead and run that
and we also shuffle
this is just a neat feature you can do
in numpy right here
put my drawing thing on which i didn't
have on earlier
i can take the data and i can shuffle it
uh so we have our so it's it just
randomizes it that's all that's doing
we've already randomized it so it's kind
of an overkill it's not really necessary
but if you're doing a larger package
where the data is coming in and a lot of
times it's organized somehow and you
want to randomize it just to make sure
that that you know the input doesn't
follow a certain pattern
that might create a bias in your model
and we go ahead and create a scalar uh
the scalar range minimum max scalar
feature range zero to one
uh then we go ahead and scale the uh
scaled train samples so we're gonna go
ahead and fit and transform the data so
it's nice and scaled
and that is the age
so you can see up here we have 49 85 41.
we're just moving that so it's going to
be between 0 and 1. and so this is true
with any of your neural networks
you really want to convert the data
to zero and one otherwise you create a
bias
so if you have like 100 creates a bias
versus
the math behind it gets really
complicated
if you actually start multiplying stuff
there's a lot of multiplication addition
going on in there
that higher end value will eventually
multiply down and it will have a huge
bias as to how the model
fits it and then it will not fit as well
and then one of the fun things we can do
in jupiter notebook is that if you have
a variable you're not doing anything
with it it's the last one on the line
it will automatically print
and we're just going to look at the
first five samples on here it's just
going to print the first five samples
and you can see here we go 0.9195
0.791 so everything's between 0 and 1.
and that just shows us that we scaled it
properly and it looks good
it really helps a lot to do these kind
of print ups halfway through you never
know what's going to go on there
i don't know how many times i've gotten
down and found out that the data sent to
me that i thought was scaled was not
and then i have to go back and track it
down and figure it out on there
so let's go ahead and create our
artificial neural network
and for doing that this is where we
start diving into tensorflow and cross
tensorflow
if you don't know the history of
tensorflow
it helps to jump into we'll just use
wikipedia
be careful don't quote wikipedia on
these things because you get in trouble
but it's a good place to start back in
2011 google brain built disbelief as a
proprietary machine learning setup
tensorflow became the open source for it
so tensorflow was a google product
and then it became
open sourced and now it's just become
probably one of the defactos when it
comes for neural networks as far as
where we're at
so when you see the tensorflow setup
it it's got like a huge following there
are some other setups like
the scikit under the sk learn has their
own little neural network but the
tensorflow is the most robust one out
there right now
and cross sitting on top of it makes it
a very powerful tool so we can leverage
both the cross
easiness in which we can build a
sequential setup on top of tensorflow
and so in here we're going to go ahead
and do our input of tensorflow
uh and then we have the rest of this is
all cross here from number two down
uh
we're gonna import from tensorflow the
cross connection
and then you have your tensorflow cross
models import sequential it's a specific
kind of model we'll look at that in just
a second if you remember
from the files that means it goes from
one layer to the next layer to the next
layer there's no funky splits or
anything like that
uh and then we have from tensorflow
cross layers we're going to import our
activation and our dense layer
and we have our optimizer atom
this is a big thing to be aware of how
you optimize
your data when you first do it atoms as
good as any atom is usually there's a
number of optimizer out there there's
about there's a couple of main ones but
atom is usually assigned to bigger data
it works fine usually the lower data
does it just fine but atom is probably
the mostly used but there are some more
out there and depending on what you're
doing with your layers your different
layers might have different activations
on them and then finally down here
you'll see
our setup where we want to go ahead and
use the metrics
and we're going to use the tensorflow
cross metrics
for categorical cross entropy
so we can see how everything performs
when we're done that's all that is a lot
of times you'll see us
go back and forth between tensorflow and
then scikit has a lot of really good
metrics also for measuring these things
again it's the end of the day at the end
of the story how good does your model do
and we'll go ahead and load all that and
then comes the fun part
i actually like to spend hours messing
with these things
and
four lines of code you're like oh you're
gonna spend hours on four lines of code
um no we don't spend hours on four lines
of code that's not what we're talking
about when i say spend hours on four
lines of code uh what we have here i'm
going to explain that in just a second
we have a model and it's a sequential
model if you remember correctly we
mentioned the sequential up here where
it goes from one layer to the next
and our first layer is going to be your
input
it's going to be what they call dense
which is
usually just dense and then you have
your input and your activation
how many units are coming in we have 16
what's the shape what's the activation
and this is where it gets interesting
because we have in here
uh relu
on two of these and softmax activation
on one of these there are so many
different options
for
what these mean
and how they function how does the relu
how does the soft max function
and they do a lot of different things
we're not going to go into the
activations in here that is what really
you spend hours doing is looking at
these different activations
and just
some of it is just
almost like you're playing with it like
an artist you start getting a fill for
like a
inverse tangent activation or the 10h
activation
takes up a huge processing amount
so you don't see it a lot
yet
it comes up with a better solution
especially when you're doing uh when
you're analyzing word documents and
you're tokenizing the words
and so you'll see this shift from one to
the other because you're both trying to
build a better model and if you're
working on a huge data set
it'll crash the system it'll just take
too long to process
and then you see things like soft max
softmax
generates an interesting um
setup
where a lot of these when you talk about
rayleigh oops let me do this
rayleigh there we go relu has
a setup where if it's less than zero
it's zero and then it goes up
and then you might have what they call
lazy
setup where it has a slight negative to
it so that the errors can translate
better same thing with softmax it has a
slight laziness to it so that errors
translate better
all these little details
make a huge difference on your model
so one of the really cool things about
data science that i like
is you build
your
what they call you build to fail and
it's an interesting
design setup oops i forgot the end of my
code here
the concept of builder fail is you want
the model as a whole to work so you can
test your model out
so that you can do
uh
you can get to the end and you can do
your let's see where was it
over shot down here
you can test your test out how the
quality of your setup on there and
see where did i do my tensorflow oh here
we go it was right above me there we go
we start doing your cross entropy and
stuff like that is you need a full
functional set of code so that when you
run
it you can then test your model out and
say hey it's either this model works
better than this model and this is why
and then you can start swapping in these
models and so when i say spend a huge
amount of time on pre-processing data is
probably 80 percent of your programming
time
well between those two it's like 80 20.
you'll spend a lot of time on the models
once you get the model down once you get
the whole code and the flow down
set
depending on your data your models get
more and more robust as you start
experimenting with different inputs
different data streams and all kinds of
things and we can do a simple model
summary here
here's our sequential here's a layer
output a parameter
this is one of the nice things about
cross is you just you can see right here
here's our sequential one model boom
boom boom everything's set and clear and
easy to read
so once we have our model built the next
thing we're going to want to do is we're
going to go ahead
and train that model
and so the next step is of course model
training
and when we come in here
this a lot of times it's just paired
with the model because it's so
straightforward
it's nice to
print out the model setup so you can
have a tracking
but here's our model
the keyword in cross is compile
optimizer atom learning rate another
term right there that we're just
skipping right over
that really becomes the meat of
[Music]
the setup is your learning rate so
whoops i forgot that i had an arrow but
i'll just underline it
a lot of times the learning rate set to
0.00
set to 0.01
depending on what you're doing
this learning rate
can overfit and underfit
so you'd want to look up
i know we have a number of tutorials out
on overfitting and underfitting that are
really worth reading once you get to
that point and understanding and we have
our loss
sparse categorical cross entropy so this
is going to tell keras how far to go
until it stops
and then we're looking for metrics of
accuracy so we'll go ahead and run that
and now that we've compiled our model
we want to go ahead and
run it fit it so here's our model fit
we have our scaled train samples
our train labels our validation split
in this case we're going to use 10
percent of the data for validation
uh batch size another number you kind of
play with not a huge difference as far
as how it works
but it does affect how long it takes to
run and it can also affect the bias a
little bit most of the time though a
batch size is between 10 to 100
depending on just how much data you're
processing in there we want to go ahead
and shuffle it we're going to go through
30 epochs
and
put a verb rose of two let me just go
ahead and run this
and you can see right here here's our
epic here's our training
um here's our loss now if you remember
correctly up here we set the loss let's
see where was it
compiled our data
there we go loss uh so it's looking at
the sparse categorical cross entropy
this tells us that as it goes how how
how much
how how much does the
error go down
is the best way to look at that and you
can see here the lower the number the
better it just keeps going down
and vice versa accuracy we want let's
see where's my accuracy
value accuracy at the end
and you can see 0.619 0.69 0.74 it's
going up we want the accuracy would be
ideal if it made it all the way to one
but we also the loss is more important
because
it's a balance
you can have 100 accuracy and your model
doesn't work because it's over fitted
again you won't look up
overfitting and under fitting models
and we went ahead and went through uh 30
epics it's always fun to kind of watch
your code going um to be honest i
usually uh uh the first time i run it
i'm like oh that's cool i get to see
what it does and after the second time
of running it i'm like i'd like to just
not see that and you can repress those
of course in your code
repress the warnings in the printing
and so the next step is going to be
building a test set and predicting it
now
so here we go we want to go ahead and
build our test set
and we have just like we did our
training set
a lot of times you just split your your
initial set up
but we'll go ahead and do a separate set
on here
and this is just what we did above
there's no difference as far as
the randomness that we're using to build
this set on here
the only difference is that
we already
did our scalar up here well it doesn't
matter because the the data is going to
be across the same thing but this should
just be just transformed down here
instead of fit transform
because you don't want to refit your
data
on your
testing data
there we go now we're just transforming
it because you never want to transform
the test data
easy to mistake to make especially on an
example like this where we're not doing
um you know we're randomizing the data
anyway so it doesn't matter too much
because we're not expecting something
weird
and then we went ahead and do our
predictions the whole reason we built
the model is we take our model we
predict
and we're going to do here's our x scale
data batch size 10 verbose
and now we have our predictions in here
and we could go ahead and do a
we'll print
predictions
and then i guess i could just put down
predictions in five so we can look at
the first five of the predictions
and what we have here is we have our
age
and uh the prediction on this age versus
on what is what we think it's going to
be but what we think is going to they
are going to have
symptoms or not
and the first thing we notice is that's
hard to read because we really want a
yes no answer
so we'll go ahead and just
round off the predictions
using the arg max
the numpy arg max for prediction so it
just goes to 0 1
and
if you remember this is a jupiter
notebook so i don't have to put the
print i can just put in
rounded predictions and we'll just do
the first five
and you can see here zero one zero zero
zero so that's what the predictions are
that we have coming out of this
is no symptoms symptoms no symptoms
symptoms no symptoms
and just as we were talking about at the
beginning we want to go ahead and
take a look at this there we go
confusion matrix is for accuracy check
most important part
when you get down to the end of the
story how accurate is your model before
you go and play with the model and see
if you can get a better accuracy out of
it and for this we'll go ahead and use
the scikit the sk learn metrics
site kit being where that comes from
import confusion matrix
some iteration tools and of course a
nice matte plot library
that makes a big difference so it's
always nice to
have a nice graph to look at a
picture's worth a thousand words
um and then we'll go ahead and call it
cm for confusion matrix y true equals
test labels why predict rounded
predictions
we'll go ahead and load in our cm
and i'm not going to spend too much time
on the plotting
going over the different plotting code
you can spend
like whole we have whole tutorials on
how to do your different plotting on
there uh but we do have here is we can
do a plot confusion matrix there's our
cm our classes normalize false title
confusion matrix
c map is going to be in blues
and you can see here we have uh to the
nearest c map titles all the different
pieces whether you put tick marks or not
the marks the classes the color bar
so a lot of different information on
here as far as how we're doing the
printing of the of the confusion matrix
you can also just dump the confusion
matrix into a seaborn and real quick get
an output
it's worth knowing how to do all this
when you're doing a presentation to the
shareholders
you don't want to do this on the fly you
want to take the time to make it look
really nice like our guys in the back
did and let's go ahead and do this
forgot to put together our cm plot
labels
we'll go ahead and run that
and then we'll go ahead and call the
little the
definition
for our mapping
and you can see here plot confusion
matrix that's our the little script we
just wrote and we're going to dump our
data into it
so our confusion matrix
our classes
title confusion matrix and let's just go
ahead and run that
and you can see here we have our basic
setup
no side effects 195
had side effects 200
no side effects that had side effects so
we predicted the 10 of them
who actually had side effects and that's
pretty good i mean i i don't know about
you but you know that's five percent
error on this and this is because
there's 200 here that's where i get five
percent is uh divide these both by by
two and you get five out of a hundred uh
you can do the same kind of math up here
not as quick on the fly because it's 15
and 195 not an easily rounded number but
you can see here where they have 15
people
who predicted to have no
with the no side effects but had side
effects
kind of set up on there and these
confusion matrix are so important at the
end of the day
this is really where where you show
whatever you're working on comes up and
you can actually show them hey this is
how good we are or not how messed up it
is
so this was a uh i spent a lot of time
on some of the parts but you can see
here is really simple
we did the random generation of data but
when we actually built the model coming
up here
here's our model summary
and we just have the layers on here that
we built with our model on this and then
we went ahead and trained it and ran the
prediction
now we get a lot more complicated let me
flip back on over here because we're
going to do another uh demo
so that was our basic introduction to it
we talked about the uh oops there we go
okay so implementing a neural network
with keras after creating our samples
and labels we need to create our cross
neural network model we will be working
with a sequential model which has three
layers and this is what we did we had
our input layer our hidden layers and
our output layers and you can see the
input layer coming in
was the age factor we had our hidden
layer and then we had the output are you
going to have symptoms or not
so we're going to go ahead and go with
something a little bit more complicated
training our model is a two-step process
we first compile our model and then we
train it in our training data set
so we have compiling compiling converts
the code into a form of understandable
by machine
we use the atom in the last example a
gradient descent algorithm to optimize a
model and then we trained our model
which means it let it
learn on training data
and i actually had a little backwards
there but this is what we just did is we
if you remember from our code we just
had let me go back here
here's our model that we created
summarized
we come down here and we compile it
so it tells it hey we're ready to build
this model and use it
uh and then we train it this is the part
where we go ahead and fit our model and
and put that information in here and it
goes through the training on there
and of course we scaled the data which
was really important to do and then you
saw we did the creating a confusion
matrix with keras
as we are performing classifications on
our data we need a confusion matrix to
check the results a confusion matrix
breaks down the various
misclassifications as well as correct
classifications to get the accuracy
and so you can see here this is what we
did with the true positive false
positive true negative false negative
and that is what we went over let me
just scroll down here
on the end we printed it out and you can
see we have a nice printout of our
confusion matrix
with the true positive false positive
false negative true negative
so the blue ones
we want those to be the biggest numbers
because those are the better side and
then
we have our false predictions on here
as far as this one so had no side
effects but we predicted
let's see no side effects predicting
side effects and vice versa
if getting your learning started is half
the battle what if you could do that for
free visit scale up by simply learn
click on the link in the description to
know more
now
saving and loading models with cross
we're going to dive into a more
complicated demo
and you're going to say oh that was a
lot of complication before well if you
broke it down we randomized some data we
created the
kara setup we compiled it we trained it
we predicted and we ran our matrix
so we're going to dive into something a
lot a little bit more fun is we're going
to do a face mask detection with keras
so we're going to build a cross model to
check if a person is wearing a mask or
not in real time and this might be
important if you're at the front of a
store this is something today which is
might be very useful as far as some of
our you know making sure people are safe
and so we're going to look at mask and
no mask and let's start with a little
bit on the data
and so in my data i have with a mask you
can see they just have a number of
images showing the people in masks and
again
if you want some of this information
contact simply learn and they can send
you some of the information as far as
people with and without masks so you can
try out on your own and this is just
such a wonderful example of this setup
on here
so before i dive into the mass detection
talk about being in the current with a
covid and seeing if people are wearing
masks
this particular example i had to go
ahead and update to a python 3.8 version
it might run in a 3.7 i'm not sure i
haven't i kind of skipped three seven
and installed three eight
uh so i'll be running in a three python
38
and then you also want to make sure your
tensorflow is up to date
because the
they call functional
layers with that's where they split if
you remember correctly from back
let's take a look at this
remember from here the functional model
and a functional layer allows us to feed
in the different layers into different
you know different nodes into different
layers and split them a very powerful
tool
very popular right now in the edge of
where things are with neural networks
and creating a better model so i've
upgraded to python 3.8 and let's go
ahead and open that up and
go through
our next example which includes
multiple layers
programming it to recognize whether
someone wears a mask or not and then
saving that model so we can use it in
real time so we're actually almost a
full
end-to-end development of a product here
of course this is a very simplified
version and it'd be a lot more to it
you'd also have to do like recognizing
whether it's someone's face or not all
kinds of other things go into this
so let's go ahead and jump into that
code and we'll open up a new python
three oops python three
it's working on it there we go
and then we want to go ahead and train
our mask we'll just call this train mask
and we want to go ahead and train mask
and save it uh so it's it's uh it's safe
mask train mask detection
not to be confused with masking data a
little bit different we're actually
talking about a physical mask on your
face
and then from the cross standpoint we
got a lot of imports to do here
and i'm not going to dig too deep on the
imports
we're just going to go ahead and notice
a few of them
so we have in here
alt d there we go
i have something to draw with a little
bit here
we have our
image processing
and the image processing right here let
me underline that uh deals with how do
we bring images in because most images
are like a
square grid and then each value in there
has three values for the three different
colors
cross and tensorflow do a really good
job of
working with that so you don't have to
do all the heavy listening and figuring
out what's going to go on
and we have the mobile net average
pooling 2d
this again is
how do we deal with the images and
pulling them uh dropout's a cool thing
worth looking up
if you haven't when you as you get more
and more into cross and tensorflow
uh it'll auto drop out certain nodes
that way you'll get a better
the notes just kind of die and they find
that they actually create more of a bias
and help
and they also add processing time so
they remove them
and then we have our flatten that's
where you take that huge array with the
three different colors and you find a
way to flatten it so it's just a one
dimensional array instead of a two by
two by three
dense input we did that in the other one
so that should look a little familiar
oops there we go our input
our model again these are things we had
on the last one
here's our optimizer with our atom
we have some pre-processing on the input
that goes along with bringing the data
in
more pre-processing with image to array
loading the image
this stuff is so nice it looks like a
lot of works you have to import all
these different modules in here but the
truth is is it does everything for you
you're not doing a lot of pre-processing
you're letting the software do the
pre-processing
and we're going to be working with the
setting something to categorical
again that's just a conversion from a
number to a category 0 1 doesn't really
mean anything it's like true false
label binarizer the same thing
we're changing our labels around and
then there's our train test split
classification report
our
im utilities let me just go ahead and
scroll down here a notch for these
this is something a little different
going on down here this is not part of
the
tensorflow or the sklearn this is the
site kit set up and tensorflow above
the path this is part of
opencv
and we'll actually have another tutorial
going out with the opencv so if you want
to know more about opencv you'll get a
glance on it in
this software especially the
second piece when we reload up the data
and hook it up to a video camera we're
going to do that on this round
but this is part of the opencv thing
you'll see cv2 is usually how that's
referenced
but the im utilities has to do with how
do you rotate pictures around and stuff
like that
and resize them and then the matte plot
library for plotting because it's nice
to have a graph tells us how good we're
doing and then of course our numpy
numbers array and just a straight os
access wow so that was a lot of imports
uh like i said i'm not going to spend i
spent a little time going through them
but we didn't want to go too much into
them
and then i'm going to create
some variables that we need to go ahead
and initialize
we have the learning rate number of
epics to train for and the batch size
and if you remember correctly we talked
about the learning rate uh to the
negative 4.0001
a lot of times it's 0.001 or 0.001
usually it's in that
variation depending on what you're doing
and how many epochs and they kind of
play with the epics
the epics is how many times we're going
to go through all the data
now i have it as two
the actual setup is for 20 and 20 works
great the reason i have it for two is it
takes a long time to process
one of the downsides of jupiter
is that jupiter isolates it to a single
kernel so even though i'm on an eight
core processor
with 16 dedicated threads only one
thread is running on this no matter what
so it doesn't matter
so it takes a lot longer to run even
though
tensorflow really scales up nicely and
the batch size is how many pictures do
we load at once in process
again those are numbers you have to
learn to play with depending on your
data and what's coming in and the last
thing we want to go ahead and do is
there's a directory with a data set
we're going to run
and this just has images of mass and not
masks
and if we go in here you'll see data set
and then you have pictures with mass
they're just images of people with mass
on their face
uh and then we have the opposite let me
go back up here
without masks so it's pretty
straightforward they look kind of askew
because they tried to format them into
very similar uh setup on there so
they're they're
mostly squares you'll see some that are
slightly different on here
and that's kind of important thing to do
on a lot of these data sets
get them as close as you can to each
other and we'll we actually will run in
the in this processing of images up here
and the cross layers and importing and
dealing with images it does such a
wonderful job of converting these that a
lot of it we don't have to do a whole
lot with
so you have a couple things going on
there
and so we're now going to be this is now
loading the
images and let me see
and we'll go ahead and create data and
labels here's our
here's the features going in which is
going to be our pictures and our labels
going out
and then for categories in our list
directory directory and if you remember
i just flashed that at you it had
face mask or or no face mask those are
the two options
and we're just going to load into that
we're going to append the image itself
and the labels so we'll just create a
huge array
and you can see right now this could be
an issue if you had more data at some
point
thankfully i have a 32 gig hard drive or
ram
even that
doesn't you could do with a lot less of
that probably under 16 or even 8 gigs
would easily load all this stuff
and there's a conversion going on in
here i told you about how we are going
to convert
the size of the image so it resizes all
the images that way our data is all
identical the way it comes in
and you can see here with our labels we
have without mask without mask without
mask
the other one would be with mask those
are the two that we have going in there
and then we need to change it to the one
not hot encoding
and this is going to take our
up here we had was it labels and data
we want the labels to be categorical so
we're going to take labels and change it
to categorical and our labels then equal
a categorical list
we'll run that and again if we do uh
labels and we just do the last or the
first 10. let's do the last 10 just
because
minus 10 to the end there we go
just so we can see where the other side
looks like we now have one that means
they have a mask one zero one zero so on
uh one being they have a mask and zero
no mask
and if we did this in reverse
i just realized that this might not make
sense if you've never done this before
let me run this
zero one
so zero
is uh do they have a mask on zero do
they not have a mask on one so this is
the same as what we saw up here without
mask one equals
um the second value is without mass so
with mass without mask
and that's just a with any of your data
processing
we can't really zero if you have a zero
one output
uh
it causes issues as far as training and
setting it up so we always want to use a
one hot encoder if the values are not
actual
linear value or regression values
they're not actual numbers
if they represent a thing
and so now we need to go ahead and do
our train x test x train y test y
train split test data
and we'll go ahead and make sure it's
going to be random and we'll take 20 of
it for testing and the rest for
setting it up as far as training their
model
this is something that's become so cool
when they're training these set
they realize we can augment the data
what does augment mean well if i rotate
the data around and i zoom in i zoom out
i rotate it
share it a little bit flip it
horizontally
fill mode as they do all these different
things to the data it
is able to it's kind of like increasing
the number of samples i have
so if i have all these perfect samples
what happens when we only have part of
the face or the face is tilted sideways
or
all those little shifts cause a problem
if you're doing just a standard set of
data so we're going to create an augment
in our image data generator
which is going to rotate zoom and do all
kinds of cool thing and this is worth
looking up this image data generator and
all the different features it has
a lot of times i'll the first time
through my models i'll leave that out
because i want to make sure there's a
thing we call build to fail which is
just cool to know
you build the whole process and then you
start adding these different things in
so that you can better train your model
and so we go and run this and then we're
going to load
and then we need to go ahead and you
probably would have got an error if you
hadn't put this piece in right here um i
haven't run it myself because the guys
in the back did this
we take our base model and one of the
things we want to do is we want to do a
mobile net v2
and this we this is a big thing right
here include the top equals false
a lot of data comes in with a label on
the top row
so we want to make sure that that is not
the case
and then the construction of the head of
the model that will be placed on the top
of the base model
we want to go ahead and set that up
and you'll see a warning here i'm kind
of ignoring the warning because it has
to do with the
uh size of the pictures and the weights
for input shape
so they'll it'll switch things to
defaults to saying hey we're going to
auto shape some of this stuff for you
you should be aware that with this kind
of imagery we're already augmenting it
by moving it around and flipping it and
doing all kinds of things to it
so that's not a bad thing in this but
another data it might be if you're
working in a different domain
and so we're going to go back here and
we're going to have we have our base
model we're going to do our head model
equals our base model output
and what we got here is we have an
average pooling 2d pool size 77 head
model
head model flattened so we're flattening
the data
uh so this is all
processing and flattening the images and
the pooling has to do
with some of the ways it can process
some of the data we'll look at that a
little bit when we get down to the lower
levels processing it
and then we have our dents we've already
talked a little bit about a dense just
what you think about and then the head
model has a drop out
of 0.5
what we can do with a drop out
the dropout says that we're going to
drop out a certain amount of nodes while
training
so when you actually use the model
it will use all the nodes but this drops
certain ones out and it helps
stop biases from performing so it's
really a cool feature in here they
discovered this a while back
we have another dense mode and this time
we're using soft max activation
lots of different activation options
here soft max is a real popular one for
a lot of things
and so was the relu
and you know there's we could do a whole
talk on activation formulas
and why what their different uses are
and how they work
when you first start out you'll you'll
use mostly the relu and the softmax for
a lot of them
just because they're some of the basic
setups it's a good place to start
and then we have our model equals model
inputs equals base model dot input
outputs equals head model so again we're
still building our model here we'll go
ahead and run that
and then we're going to loop over all
the layers in the base model and freeze
them so they will not be updated during
the first training process
so for layer and base model layers
layers dot trainable equals false
a lot of times when you go through your
data
you want to kind of jump in part way
through
i
i'm not sure why in the back they did
this for this particular example
but i do this a lot when i'm working
with series and
specifically in stock data i wanted to
iterate through the first set of 30 data
before it does anything
i would have to look deeper to see why
they froze it on this particular one
and then we're going to compile our
model
so compiling the model atom
init layer decay
initial learning rate over epics
and we go ahead and compile our loss is
going to be the binary cross entropy
which we'll have that print out
optimizer for opt metrics is accuracy
same thing we had before not a huge jump
as far as
the previous code
and then we go ahead and we've gone
through all this and now we need to go
ahead and fit our model so train the
head of the network print info training
head run
now i skipped a little time because it
you'll see the run time here is at 80
seconds per epic takes a couple minutes
for it to get through on a single kernel
one of the things i want you to notice
on here while we're while it's finishing
the processing
is that we have up here our augment
going on so
anytime the train x and trading y go in
there's some randomness going on there
and is jiggling it around what's going
into our setup
uh of course we're batch sizing it so
it's going through whatever we set for
the batch values how many we process at
a time and then we have the steps per
epic
the train x the batch size validation
data here's our
test x and test y where we're sending
that in
and this again it's validation
one of the important things to know
about validation is
our
when both our training data and our test
data have about the same accuracy that's
when you want to stop that means that
our model isn't biased
if you have a higher accuracy on your
testing
you know you've trained it and your
accuracy is higher on your actual test
data then something in there is probably
has a bias and it's overfitted
so that's what this is really about
right here with the validation data and
validation steps
so it looks like it's let me go ahead
and see if it's done processing looks
like we've gone ahead and gone through
two epics again you could run this
through about 20
with this amount of data and it would
give you a nice refined model at the end
we're going to stop at 2 because i
really don't want to sit around all
afternoon and i'm running this on a
single thread
so now that we've done this we're going
to need to evaluate
our model and see how good it is and to
do that we need to go ahead and make our
predictions
these are predictions on our test x
to see what it thinks are going to be
so now it's going to be evaluating the
network and then we'll go ahead and go
down here
and we will need to
turn the index in because remember it's
either 0 or 1 it's a
0 1 0 1 so you have two outputs not
wearing wearing a mask not wearing a
mask and so we need to go ahead and take
that argument at the end and change
those predictions to a
zero or one coming out
and then
to finish that off we want to go ahead
and let me just put this right in here
and do it all in one shot we want to
show a nicely formatted classification
report so we can see what that looks
like on here
and there we have it we have our
precision uh it's 97 with a mask
there's our f1 score support without a
mass 97 percent
um so that's pretty high
set up on there you know you three
people are going to sneak into the store
who
are without a mask and
thinks they have a mask and there's
going to be three people with a mask
that's going to flag the person at the
front to go oh hey look at this person
you might not have a mask
that's if i guess it's just set up in
front of a store
so there you have it and of course one
of the other cool things about this is
if someone's walking in to the store and
you take multiple pictures of them
um
you know this is just an it would be a
way of flagging and then you can take
that average of those pictures and make
sure they match or don't match if you're
on the back end and this is an important
step because we're gonna this is just
cool i love doing this stuff uh so we're
gonna go ahead and take our model and
we're gonna save it
so model save mass detector.model going
to give it a name
we're going to save the format
in this case we're going to use the h5
format
and so this model we just programmed has
just been saved
so now i can load it up into say another
program what's cool about this is let's
say i want to have somebody work on the
other part of the program well i just
save the model they upload the model now
they can
use it for whatever and then if i get
more information
and we start working with that at some
point
i might want to update this model and
make a better model and this is true of
so many things where i take this model
and maybe i'm running a prediction on
making money for a company
and as my model gets better
i want to keep updating it and then it's
really easy just to push that out to the
actual end user
uh here we have a nice graph you can see
the training loss and accuracy as we go
through the epics
we only did the you know only shows just
the one epic coming in here but you can
see right here as the
value loss train accuracy and value
accuracy
starts switching and they start
converging and you'll hear converging
this is a convergence they're talking
about when they say you're you're
i know when i work in the
psy kit with sk learn neural networks
this is what they're talking about a
convergence is our
loss and our accuracy come together and
also up here and this is why i'd run it
more than just two epics as you can see
they still haven't converged all the way
so that would be a cue for me to keep
going
but what we want to do is we want to go
ahead and create a new
python 3
program
and we just did our train mask so now
we're going to go ahead and import that
and use it and show you in a live action
get a view of both myself in the
afternoon along with my background of an
office which is in the middle still of
reconstruction for another month
and we'll call this a mask
detector
and then we're going to grab a bunch of
a few items coming in
uh we have our
mobile net v2 import pre-processing
input so we're still going to need that
we still have our tensorflow image to
array we have our load model that's
where most of stuff's going on
this is our cv2 or opencv again i'm not
going to dig too deep into that we're
going to flash a little opencv code at
you
and we actually have a tutorial on that
coming out
our numpy array our im utilities which
is part of
the opencv or cv2 setup
and then we have of course time and just
our operating system so those are the
things we're going to go ahead and set
up on here and then we're going to
create
this takes just a moment
our module here which is going to do all
the heavy lifting
so we're going to detect and predict the
mask we have frame face net mass net
these are going to be generated by our
open cv we have our frame coming in and
then we want to go ahead and create a
mask around the face it's going to try
to detect the face and then set that up
so we know what we're going to be
processing through our model
and then there's a frame shape here this
is just our height versus width that's
all hw stands for
they've called it blob which is a cv2
dnn blob form image frame
so this is reformatting this frame
that's going to be coming in literally
from my camera and we'll show you that
in a minute that little piece of code
that shoots that in here
and we're going to pass the blob through
the network and obtain the face
detections
so face net dot set import blob
detections face net forward
print detections shape
so these is this is what's going on here
this is that model we just created we're
going to send that in there and i'll
show you in a second where that is but
it's going to be under face net
and then we go ahead and initialize our
list of faces their corresponding
locations and the list of predictions
from our face mask network
we're going to loop over the detections
and this is a little bit more work than
you think
as far as looking for different faces
whatever the view of a crowd of faces
so we're looping through the detections
and the shapes going through here
and probability associated with the
detection here's our confidence of
detections
we're going to filter out weak detection
by ensuring the confidence is greater
than the minimum confidence
so we've said it remember 0 to 1 so 0.5
would be our minimal confidence probably
is pretty good
[Music]
and then we're going to put in compute
bounding boxes for the object if i'm
zipping through this it's because we're
going to do an open cv and i really want
to stick to just the cross part
and so i'm just kind of jumping through
all this code you can get a copy of this
code from simplylearn and take it apart
or look for the opencv coming out
and we'll create a box the box sets it
around the image
ensure the bounding boxes fall within
the dimensions of the frame
as we create a box around what's going
to be hope is going to be the face
extract the face roi convert it from bgr
to rgb channel
again this is an open cv issue not
really an issue but it has to do with
the order i don't know how many times
i've forgotten to check the order colors
working with opencv
because there's all kinds of fun things
when red becomes blue
and blue becomes red uh and we're gonna
go ahead and resize it process it frame
it uh face frame
setup again the face the cbt color we're
gonna convert it
we're gonna resize it
image to array pre-process the input
pin the face locate face start x dot y
and x
boy that was just a huge amount and i
skipped over a ton of it but the bottom
line is we're building a box around the
face and that box because the opencv
does a decent job of finding the face
and that box is going to go in there and
see hey does this person have a mask on
it
and so that's what that's what all this
is doing on here and then finally we get
down to this where it says predictions
equals mass net dot predict faces batch
size 32
so these different images of where we're
guessing where the faces are then going
to go through and
generate an array of faces if you will
and we're going to look through and say
does this face have a mask on it and
that's what's going right here is our
prediction that's the big thing that
we're working for
and then we return the locations and the
predictions the location just tells
where on the picture it is
and then the
prediction tells us what it is is it a
mask or is it not a mask
all right so we've loaded that all up
so we're going to load our serialized
face detector model from disk
and we have our the path that it was
saved in obviously you're going to put
it in a different
path depending on where you have it or
however you want to do it and how you
saved it on the last one where we
trained it
and then we have our weights path
and so finally our face net here it is
equals
cb2.dnn dot read net
prototext path weights path and we're
going to load that up on here so let me
go ahead and run that
and then we also need to i'll just put
it right down here i always hate
separating these things in there
um and then we're going to load the
actual mass detector model from disk
this is the the model that we saved so
let's go ahead and run that on there
also so this is pulling on all the
different pieces we need for our model
and then the next part
is we're going to create open up our
video
and this is just kind of fun because
it's all part of the opencv
the video set up
and we just put this all in as one
there we go
so we're going to go ahead and open up
our video we're going to start it and
we're going to run it until we're done
and this is where we get some real like
kind of live action stuff which is fun
this is what i like working about with
images and videos
is that when you start working with
images and videos it's all like right
there in front of you it's visual and
you can see what's going on
so we're going to start our video
streaming this is grabbing our video
stream source zero start
uh that means it's grabbing my main
camera i have hooked up um
and then you know starting video you're
gonna print it out here's our video
source equals zero start
loop over the frames from the video
stream
oops a little redundancy there
let me close
i'll just leave it that's how they
headed in the code so uh so while true
we're going to grab the frame from the
threaded video stream and resize it to
have the maximum width of 400 pixels so
here's our frame we're going to read it
from our visual
stream
we're going to resize it
and then we have we're returning
remember we returned from the our
procedure the location and the
prediction so detect and predict mask
we're sending it the frame
we're sending it the face net and the
mast net so we're sending all the
different pieces that say this is what's
going through on here
and then it returns our location and
predictions and then for our box
and predictions in the location and
predictions
and the boxes is again this is an open
cv set that says hey this is a box
coming in from the location
because you have the two different
points on there
and then we're going to unpack the box
in predictions and we're going to go
ahead and do mask without a mask equals
prediction
we're going to create our label
no mask and create color if the label
equals mask l0 225 and you know it's
going to make a lot more sense when i
hit the run button here
but we have the probability of the label
we're going to display the label and
bounding box rectangle on the output
frame
and then we're going to go ahead and
show the output from the frame cv2 i am
show frame frame and then the key equals
cp2 weight key one we're just going to
wait till the next one comes through
from our feed
and we're going to do this until
we hit the stop button pretty much
so you're ready for this to see if it
works we've distributed our
our model we've loaded it up into our
distributed
code here we've got it hooked into our
camera and we're going to go ahead and
run it
and there it goes it's going to be
running and we can see the data coming
down here and we're waiting for the
pop-up
and there i am in my office with my
funky headset on
uh
and you can see in the background my
unfinished wall and it says up here no
mask oh no i don't have a mask on
i wonder if i cover my mouth
what would happen
uh you can see my no mask
goes down a little bit i wish i brought
a mask into my office it's up at the
house but you can see here that this
says you know there's a 95 98
chance that i don't have a mask on and
it's true i don't have a mask on right
now and this could be distributed this
is actually an excellent little piece of
script that you can start you know you
install somewhere on a video feed on a
on a security camera or something and
then you'd have this really neat uh
setup saying hey do you have a mask on
when you enter a store or a public
transportation or whatever it is where
they're required to wear a mask
let me go ahead and stop that
now if you want a copy of this
code definitely give us a hauler we will
be going into opencv in another one so i
skipped a lot of the opencv
code in here as far as going into detail
really focusing on the cross
saving the model uploading the model and
then processing a streaming video
through it so you can see it that the
model works we actually have this
working model that hooks into
the video camera
which is just pretty cool and a lot of
fun
so i told you we're going to dive in and
really roll up our sleeve and do a lot
of coding today we did the basic uh
demo up above for just pulling in across
and then we went into a cross model
where we pulled in data to see whether
someone was wearing a mask or not so
very useful in today's world as far as a
fully running application sequential
models in cross
so what is caras
cross is a high level python deep
learning api
which is used for easy implementation of
neural networks it has multiple
low-level back-ends like tensorflow
fano
pi torch etc which are used for fast
computation
so you can think of this as cross being
almost its own little programming
language
and then it sits on neural networks in
this case uh the ones listed where
tensorflow thano and pi torch which can
all integrate with the cross model
this makes it very diverse and also
makes it very easy to use and switch
around with different things
cross is very
user friendly as far as neural network
software goes as a high level api
computational graphs
so computational graphs are really the
heart and soul of neural networks
we talk about a computational graph
they are a visual representation of
expressing and evaluating mathematical
equations
the nodes and data flow in a graph
correspond to mathematical operations
and variables
you'll hear a lot
some of the terms you might hear are
node and edge the edge being the data
flow in this case
it could also represent an actual value
they have oh i think in spark they have
a graph x which works just on computing
edges there's all kinds of stuff that
has evolved from computational graphs
we're focusing just on keras and on
neural networks so we're not going to go
into great detail on everything a
computational graph does it is a core
component of a neural network is what's
important to know on this
so cross offers a python user-friendly
front-end while maintaining a strong
computation power by using a low level
api
like tensorflow pi torch etc which use
computational graphs as a back end
so one this allows for abstraction of
complex problems while specifying
control flow
if you've ever looked at some of the
backend or the original versions of
tensorflow
it's really a nightmare you have all
these different settings you have to put
in there and create
it's a lot a lot of back-end programming
this is like the old computers when you
had to
tell it how to dispose of a variable and
how to properly re-allocate the memory
for use
all that is covered nowadays in our
higher level programming well this is
the same thing with cross is it covers a
lot of this stuff and does things for
you that you would spend hours on just
trying to figure out
it's useful for calculating derivatives
by using back propagation
we're definitely not going to teach a
class on derivatives
in this little video
but understanding
a derivative is the rate of change so if
you have a particular function you're
using in your neural network
a lot of them is just simple
y equals mx plus b
your euclidean geometry where you just
have a simple slope times the intercept
and they get very complicated they have
the inverse tangent function for
activation as opposed to just a linear
euclidean model and you can think about
this as you have your data coming in and
you have to alter it somehow
well you alter it going down to get an
answer you end up with an error and that
error goes back up and you have to have
that back propagation with the
derivative you want to know how it
changed so that you can figure out how
to adjust it for the error
a lot of that's hidden so you don't even
have to worry about it with cross and in
today's cross you'll even if you create
your own
formula for computing an answer it will
automatically compute the back prop the
the derivative for you in a lot of cases
it's easier to implement distributed
computation
so cross is really nice way to package
it and get it off on different computers
and share it and it allows parallelism
which means that two operations can run
simultaneously
so as we start developing these back
ends it can do all kinds of cool things
and utilize multiple cores gpus on a
computer
to get that parallel processing up
what are neural networks
well like i said there are already we've
talked about in computational edges you
have a node and you have a connection or
your edge so neural networks are
algorithms fashioned after the human
brain which contain multiple layers each
layer contains a node called a neuron
which performs a mathematical operation
they break down complex problems into
simple operations
so one an input layer takes in our data
and pre-processes it
when we talk about pre-processing when
you're dealing with neural networks uh
you usually have to pre-process your
data so that it's between
minus one and one or zero and 1
into some kind of value that's usable
that occurs before it gets to the neural
network
in fact 80 of data science is usually in
prepping that data and getting it ready
for your different models
two you have hidden layer performs a
non-linear transformation of input
now it can do a hidden a linear
transformation it can use just a basic
euclidean geometry and you could think
of a node adding all the different
connections coming in
so each connection would have a weight
and then it would add to that weight
plus an intercept in the note itself so
you can actually use euclidean geometry
but a lot of these get really
complicated they have all these
different formulas
and they're really cool to look at but
when you start looking at them look at
how they work
you really don't need to know the high
math behind it
to figure them out and figure out what
they're doing
which is really cool that means a lot of
people can use this without having to go
get a phd in mathematics
number three the output layer takes the
results from hidden layer transform them
and gives a final output
so sequential models
so what makes this a sequential model
sequential models are linear stacks of
layers where one layer leads to the next
it is simple and easy to implement and
you just have to make sure that the
previous layer is the input to the next
layer
so you have used for plain stack of
layers where each layer has one input
and one output
tensor and this is what tensorflow is
named after is each one of these layers
is like a tensor
each node is a tensor and the layer is
also considered a tensor of values
and it's used for simple classifier
declassifier models you can it's also
used for regression models too so it's
not just about
this is something this is a teapot this
is a cat this is a dog
it's also used for generating um uh
regret the actual values you know this
is worth ten dollars that's worth thirty
dollars uh the weather is going to be 90
degrees out or whatever it is so you can
use it for both classifier and
declassifier models
and one more note when we talk about
sequential models the term sequential is
used a lot and it's used in different
areas in different notations when you're
in data science so when we talk about
time series we'll talk about sequential
that is something very different
sequential in this case means it goes
from the input to layer 1 to layer 2 to
the output so it's very directional it's
important to note this because if you
have a sequential model can you have a
non-sequential model and the answer is
yes
if you master the basics of a sequential
model you can just as easily have
another model that shares layers
you can have another model where you
have an input coming in and it splits
and then you have one set that's doing
one set of nodes maybe they're doing a
yes no kind of node where it's either
putting out a zero or a one a classifier
and the other one might be regression
it's just processing numbers and then
you recombine them for the output
that's what they call across the cross
api
so there's a lot of different
availabilities in here and all kinds of
cool things you can do as far as
encoding and decoding and all kinds of
things and you can share layers and
things like that
we're just focusing on the basic cross
model with the sequential model
so let's dive into the meat of the
matter let's do and do a demo on here
uh today's demo in this demo we'll be
performing flower classification using
sequential model and cross and we'll use
our model to classify between five
different types of flowers
now for this demo and you can do this
demo whatever platform you want or
whatever
user interface for developing python
i'm actually using anaconda and then i'm
using jupyter notebooks to develop in
and if you're not familiar with this you
can go under environment once you've
created environment you can come in here
to open a terminal window
and if you don't have the different
modules in here you can do your conda
install whatever module it is
just happened that this particular setup
didn't have a seaborn in it which i
already installed uh
so here's our anaconda and then i'm
going to go back
and start up my jupiter notebook
where i already created a
new
python project python 3 i'm in python
3.8 on this particular one
sequential model for flowers
so lots of fun there
so we're going to jump right into this
the first thing is to make sure you have
all your modules installed
so if you don't have numpy pandas
matplot library and seaborn in the cross
an sklearn or sitekit it's not actually
sklearn you'll need to go ahead and
install all of those
now having done this for years and
having switched environments and doing
different things
i get all my imports done and then we
just run it and if we get an error we
know we have to go back and install
something
right off the bat though we have numpy
pandas matplot library seaborn these are
built on top of each other panda's a
data frame and built on top of numpy the
data array
and then we bring in our sklearn or
scikit this is the site kit setup sci
kit
even though you use sklearn to bring it
in it's a scikit and then our cross we
have our pre-processing the images image
data generator
our model this is our basic model or
sequential model
and then we bring in from cross layers
import dents
optimizers
these optimizers a lot of them already
come in these are your different
optimizers and it's almost a lot of this
is so automatic now
atom
is the a lot of times the default
because you're dealing with a large data
and then we get our sgd which is smaller
data does better on smaller pieces of
data
and i'm not going to go into all of
these uh different optimizers we didn't
even use these in the actual demo you
just have to be aware that they are
different optimizers and the digger the
more you dig into these models
you'll hit a point where you do need to
play with these a little bit but for the
most part leave it at the default when
you're first starting out
and we're doing just the sequential
you'll see here layers dense
and then if we come down a little bit
more when they put this together and
they're running the dense layers you'll
also see they have dropout they have
flattened they have activation
they have the convolutional
layer 2d max pooling 2d
batch normalization
what are all these layers and when we
get to the model we're going to talk
about them a lot of times when you're
just starting you can just
import cross dot layers and then you
have your dropout your flattened
your convolutional
neural network 2d
and we'll we'll cover what these do
in the actual example when we get down
there
what i want you to take from here though
is you need to run your imports
and load your different aspects of this
and of course your tensorflow tf because
this is all built on tensorflow
and then finally import random is rn
just for random generation
and then we get down here we have our
cv2
that is your
open image or your open cv they call it
for processing images that's what the
cvd 2 is
we have our tqdm
the tqdm is for um is a progress bar
just a fancy way
of adding when you're running a process
you can view the bar going across in the
jupiter
setup not really necessary but it's kind
of fun to have
we want to shuffle some files
again these are all different things
pill is another
image processor it goes with the cv2 a
lot of times you'll see both of those
and so we run those we've got to bring
them all in
and the next thing is to set up our
directories
and so we come into the directories
there's an important thing to note on
here other than we're looking at a lot
of flowers which is fun
uh
as we get down here we have our
directory archive flowers that just
happens to be where the different
files for different flowers are put in
we're denoting an x and a z
and the x is the data of the image and
the z is the tag for it what kind of
flower is this
and the image size is really important
because we have to re-size everything
if you have a neural network and if you
remember from our neural networks let me
flip back to that slide
we look at this light we have two input
nodes here uh with an image you have an
input
node depending on how you set it up for
each pixel and that pixel has three
different color schemes usually in it
sometimes four so if you have a picture
that's 150 by 150
you multiply 150 times 150 times three
that's how many nodes input layers
coming in i mean so this is a massive
input a lot of times you think oh yeah
it's just a small amount of data or
something like that
no it's a full image coming in and then
you have your hidden layers a lot of
times they match what the image size is
coming in so each one of those is also
just as big and then we get down to just
a single output
so that's kind of a thing to note in
here what's going on behind the scenes
and of course each one of these layers
has a lot of processes and stuff going
on
and then we have our different
directories on here let me go and run
that so i'm just setting the directories
that's all this is
archive flowers daisy sunflower tulip
dandelion rose
just our different directories that
we're going to be looking at
and then we want to go ahead and we need
to assign labels remember we defined x
in z
so we're just going to create a
definition here
and the first thing is return flower
type okay
just returns it what kind of flower it
is i guess a sign label to it uh but
we're going to go ahead and make our
train data
and when you look at this there's a
couple things to take away from here
the first one is we're just appending
right onto our numpy array the image
we're gonna let numpy handle all that
different
aspects as far as 150 by 150 by three
we just dump it right into the numpy
which makes it really easy we don't have
to do anything funky on the processing
and we want to leave it like that and
i'm going to talk about that in a minute
and then of course we have to have the
string a pin the label on there and i
want you to notice right here
we're going to read the image in
and then we're going to size it and this
is important because we're just changing
this to 150 by 150 we're resizing the
image so it's uniform every image comes
in identical to the other ones this is
something that's so important is
when you're resizing or reformatting
your data you really have to be aware of
what's going on
with
images it's not a big deal because with
an image you just resize it so it looks
squishy or spread out or stretched
the neural network picks up on that and
it doesn't really change how it
processes it
so let's go ahead and run that
and now we've got our definition set up
on there
and then we want to go ahead and make
our
training data
so make the train data
daisy flower daisy directory
print length of x so here we go let's go
ahead and run that
and we're just loading up the flower
daisy so this is going all in there and
it's setting
it's adding it in to the our setup on
there
to our x and z set up
and we see we have 769
um and then of course you can see this
nice bar here this is the bar going
across is that little added uh code in
there that just makes it really cool for
doing demos uh not necessarily when
you're building your own model or
something like that but if you're going
to display this to other people adding
that little what was it called
tqdm i can never remember that but the
tqdm module in there is really nice and
we'll go ahead and do sunflowers and of
course you could have just
created an array of these
but this has an interesting problem
that's going to come up and i want to
show you something
it doesn't matter how good the people in
the back are how good you are
programming
errors are going to come up and you got
to figure out how to handle them
and so when we get all the way down to
the
where is it dandelion here's our
dandelion directory we're going to build
um jupiter has some cool things it does
which makes this really easy to deal
with
but at the same time you want to go back
in there depending on how many times you
rerun this how many times you pull this
so when you're finding
errors going in here there's a couple
things you can do and we're just going
to um oh it wasn't there it is there's
our error i knew there was an error
this processed
1062 out of
1065. now i can do a couple things one i
could go back into our definition
and i can just put in here try and so if
it has a bad conversion this is where
the error is coming from
uh just skip it that's one way to do it
when you're doing a lot of work in data
science and you look at something like
this where you're losing three points of
data at the end
you just say okay i lost three points
who cares
or you can go in there and try to delete
it
it really doesn't matter for this
particular demo
and so we're just going to leave that
error right alone and skip over because
it's already added all the other files
in there and this is a wonderful thing
about jupyter notebook
is that i can just continue on there and
the x and z which we're creating
is still
running and we'll just go right into the
next flower rose so all these flowers
are in there
that's just a cool thing about jupiter
notebook
uh and then we can go ahead and just
take a quick look and see
what we're dealing with and this is of
course really when you're dealing with
the other people and showing them stuff
this is just kind of fun where we can
display it on the plot library here
and we're just going to go through and
see what we got here
uh looks like
we're going to do like five of each of
them i think
is that how they set this up um
plot library five by two okay oh i see
how they did it okay so two each so we
have five by two set up on our axes and
we're just going to go in and look at a
couple of these flowers
it's always a good thing to look at some
of your data uh no matter what you're
doing
we've reformatted this to 150 by 150
you can see how it really blurs this one
up here on the tulip
that is that resize to 150 150x150
and these are what's actually going in
these are all 150 by 150 images you can
check the dimensions on the side
and you can see just a quick sampling of
the flowers we're actually going to
process on here
and again like i said at the beginning
most of your work in data science is
reprocessing
this different information so we need to
go ahead and take our labels
uh and run a label encoder on there and
then we're just going to le is a label
encoder one of the things we imported
and then we always use the fit
to categorical y comma 5 x here's our
array
x so if you look at this here's our fit
we're going to transform z
that's our z array we created
um and then we have y which equals that
and then we go ahead and do uh to
categorical we want five different
categories
and then we create our x uh np array of
x x equals x over 255.
so what's going on here there's two
different transformations one we've
turned our categories into
zero one two three four five as the
output
and we have taken our x array
and remember the x array is three values
of your different colors
this is so important to understand when
we do this across a numpy array this
takes every one of those three colors so
we have 150 by 150 pixels
out of those 150 by 150 pixels they each
have three
color arrays and those color arrays
range from zero to 250. so when we take
the x equals x over 255
i'm sorry range from 0 to 255.
this converts all those pixels to a
number between 0 and 1.
and you really want to do that when
you're working with neural networks
now if you do a linear regression model
it doesn't affect it as much and so you
don't have to do that conversion if
you're doing straight numbers but when
you're running neural networks if you
don't do this you're going to create a
huge bias
and that means they'll do really good on
predicting one or two things and they'll
just totally die on a lot of other
predictions
so now we have our
x and y values
x being the data n y being our known
output
and with any good setup we want to
divide this data into our training
so we have x train
we have our x test this is the data
we're not going to program the model
with
and of course your y train corresponds
to your x train and your y test
corresponds to your x test the outputs
and this is uh when we do the train test
split this was from the site kit sklearn
we imported train test split and we're
just going to go ahead and do the test
size at about a quarter of the data 0.25
and of course random is always good
this is such a good tool i mean
certainly you can do your own division
um
you know you can just take the first you
know 0.25 of the data or whatever do the
length of the data not real hard to do
but this is randomized so if you're
running this test a few times you can
kind of get an idea whether it's going
to work or not
sometimes what i will do
is i'll just split the data into three
parts
and then i'll test it on two with one
being the uh or train it on two of those
parts with one being the test and i
rotate it so i come up with three
different answers which is a good way of
finding out just how good your model is
but for setting up let's stick with the
x train x test and the sk learn package
and then we're going to go ahead and do
a random seed
now a lot of times the cross actually
does this automatically but we're going
to go ahead and set it up on here and
you can see we did an np random seed
from 42 and we get a nice rn number
and then we do tf random we set the seed
so you can set your randomness at the
beginning of your tensorflow and that's
what the
tf.random.set is
so that's a lot of prep
all this prep and then we finally get to
the exciting part
this is where you probably spend once
you have the data prepped and you have
your pipeline going
and you have everything set up on there
this is the part that's exciting is
building these models
and so we look at this model one we're
going to designate a sequential um they
have the api which is across the cross
tensorflow api versus sequential
sequential means we're going one layer
to the next so we're not going to split
the layer and bring it back together
it looks almost the same with the
exception of
bringing it back together so it's not a
huge step to go from this to an api
and the first thing we're going to look
at is
our convolutional neural network in 2d
so what's going on here there's a lot of
stuff that's going on here
the default for well let's start with
the beginning what is a convolutional 2d
network
well convolutional 2d network creates a
number of small windows and those small
windows float over the picture and each
one of them is their own neural network
and this basically
becomes like a
categorization and then it looks at that
and it says oh if we add these numbers
up a certain way
we can find out whether this is the
right flower based on this this little
window floating around which looks at
different things
and we have filters 32 so this is
actually creating 32 windows is what
that's doing
and the kernel size is five by five so
we're looking at a five by five square
remember it's 150 by 150 so this narrows
it down to a five by five
it's a two d so it has your x y
coordinates
um and
we look at this five by five remember
each one of these is is actually looking
at five by five by three
so we're actually looking at 15 by 15
different
pixels
and padding is just
usually just ignore that
activation by default is relu we went
ahead and put the relu in there
there's a lot of different activations
relu is for your smaller uh when you
remember i mentioned atom when you have
a lot of datum data use an atom kind of
activation or using atom processing
we're using the relu here um it kind of
gives you a yes or no but it it doesn't
give you a full yes or no it has a
zero and then it kind of shoots off at
an angle
very common it's the most common one and
then of course here's our input shape
150 by 150 by three pixels
and then we have to pull it so whenever
you have a two convolutional 2d um
layer
we have to bring this back together and
pull this into a neural network and then
we're going to go ahead and repeat this
so we're going to add another network
here
one of the cool things if you look at
this is that it as it comes in it just
kind of automatically assumes you're
going down to the next layer
and so we have another convolutional
null network 2d
here's our max pooling again we're going
to do that again max pooling
and we're just going to filter on down
now one of the things they did on this
one is they changed the kernel size they
changed the number of filters
and so each one of these steps
kind of looks at the data a little bit
differently
and that's kind of cool because then you
get a little added filtering on there
this is where you start playing with the
model you might be looking at a
convolutional neural network which is
great for image classifications
when we get down to here one of the
things we see is flattened so we add we
just flatten it remember this is 150 by
150 by 3.
well and actually
the pool size changes so it's actually
smaller than that flattened just puts
that into a 1d array
so instead of being
you know a tensor of this really
complexity with the pixels and
everything is just flat
and then the dense
is just another activation on there
by default it is probably relu
as far as its activation
and then oh yeah here we go in
sequential they actually added the
activation as relu so this just because
this is sequential this activation is
attached to the dents
uh and there's a lot of different
activations but relu is the most common
one and then we also see a soft max
softmax is similar
but it has its own kind of variation
and one of the cool things you know what
let me bring this up because if we if
you don't know about these activations
this doesn't make sense
and i just did a quick google search on
images of tensorflow activations um i
should probably look at which set
website this is
but this is the output of the values uh
so as your x as it adds in all those
weighted x values
going into the node
it's going to activate it a certain way
that's a sigmoid activation and you can
see it goes between zero and one and has
a nice curve there this also shows the
derivatives
and if we come down the seven popular
activation functions non-linear
activations there's a lot of different
options on this let me see if i can find
the
[Music]
let's make sure we can find the specific
to relu
so this is a leaky relu
and you can see instead of it just being
zero and then a value between uh going
up it has a little leaky there otherwise
your relu loses some notes they just
become inactive
but you can see there's a lot of
different options here here's a good one
right here with the relu you can see the
rayleigh function on the upper on the
upper left here and then the leaky
rayleigh over here on the right which is
very commonly used also
one of the things i use with processing
language is the
sig is the exponential one
or the tangent h the hyperbolic tangent
because they have that nice uh funky
curve that comes in that has a whole
different meaning and captures word use
better
again these are very specific to domain
and you can spend a lot of time
playing with different models
for our basic model we'll stick to the
relu and the softmax on here and we'll
go ahead and run and build this model
so now that we've had fun playing with
all these different models that we can
add in there
we need to go ahead and have a batch
size on here
128
epix 10
this means that we're going to send 128
rows of data or flowers at a time to be
processed
and the epics 10 that's how many times
we're going to loop through all the data
and then there's all kinds of stuff you
can do again this is now built into a
lot of cross models already by default
so there's different ways to reduce
the values and
verbose verbose equals one means that
we're going to show what's going on
um
value the monitor what we're monitoring
we'll see that as we actually train the
model this is what's what's going to
come out of there if you set the verbose
equal to zero
you don't have to watch it train the
model
although
it is kind of nice to actually know
what's going on sometimes
and since we're still working on
bringing the data in here's our batch
side here's our epics we need to go
ahead and create a data generator
this is our image data generator
and it has all the different settings in
here almost all of these are defaults so
if you're looking at this going oh my
gosh this is confusing
most of the time you can actually just
ignore most of this
vertical flip so you can randomly flip
pictures you can randomly horizontally
flick them
you can shift the picture around this
kind of helps gives you multiple data
off of them zooming rotation there's all
kinds of different things you can do
with images
most of these we're just going to leave
as false we don't really need to do all
that
setup because we already have a huge
amount of data
if you're short data you can start
flipping like a horizontal picture and
it will generate it's like doubling your
data almost
um so the upside is you double your data
the downside is that if you already have
a bias in your data you already have
[Music]
5 000 sunflowers and only two roses
that's a huge bias it's also going to
double that bias that is the downside of
that
and so we have our model compile and
this you're going to see in all the
cross we're going to take this model
here
we're going to take all this information
as far as how we want it to go and we're
going to compile it
this actually builds the model and so
we're going to run that and i want you
to notice
learning rate
very important this is the default zero
zero one
there's there you really don't this is
how
slowly it adjusts to find the right
answer
and the more data you have you might
actually make this a smaller number
with larger with you have a very small
sample of data you might go even larger
than that
and then we're going to look at the loss
categorically categorical cross entropy
most commonly used
and this is uh how how much it improves
the model is improving is what this
number means or yeah that's that's
important on there and then the accuracy
we want to know just how good our model
is on the
accuracy and then
one of the cool things to do is if
you're in a group of people who are
studying the model if you're in
shareholders you don't want to do this
is you can run the model summary
i do this by default and you can see the
different layers that you built into
this model just a quick summary on there
so we went ahead and we're going to go
ahead and create a
we'll call it history but we want to do
a model fit generator
and so what this history is doing is
this is tracking what's going on as
while it fits
the model
now there's a lot of new setups in here
where they just use fit and then you put
the generator in here
we're going to leave it like this even
though the new default
is a little different on that
it doesn't really matter it does the
same thing and we'll go ahead and just
run that
and you can see while it's running right
here uh we're going through the epics we
have one of ten now we're going through
six to 25.
here's our loss we're printing that out
so you can see how it's improving and
our accuracy the accuracy gets better
and better and this is 6 out of 25. this
is going to take a couple minutes to
process
because we are training 150 by 150 by 3
pixels across
six layers or eight layers whatever it
was
that is a huge amount of processing so
this will take a few minutes to process
this is when we talk about the hardware
and the problems that come up in data
science and why it's only now just
exploding being able to do neural
networks this is why this process takes
a long time
now you should have seen a jump on the
screen here because i did
pause the recorder to let this go ahead
and run all the way through its epics
let's go and take a look and see what
these epics are and if you set the
verbose to
zero instead of one it won't show what's
going on in the behind the scenes as
it's training it
so we look at this epic 10 epic so we
went through all the data 10 times if i
remember correctly there's roughly a gig
of data there that's a lot of data
the first thing you're going to notice
is the 270 seconds
that's how much each of those epics took
to run
and so if you divide 60 in there you
roughly get about five minutes worth of
each epic so if i have 10 epics that's
50 minutes almost an hour of run time
that's a big deal we talk about
processing uh in on this particular
computer
i actually have what is it uh
eight cores with 16 dedicated threads so
it runs like a 16 core computer it
alternates the threads going in
and it still takes it five minutes for
each one of these epics so you start to
see that if you have a lot of data
this is going to be a problem if you
have a number of models you want to find
out how good the models are doing what
model to use
and so each of those models could take
all night to run in fact i have a model
i'm running now that takes over
takes about a day and a half to test
each model
it takes four days to do with the whole
data
so what i do is i actually take a small
piece of the data
test it out to find out uh get an idea
of how the different setups are going to
do
and then i increase that size of the
data and then increase it again and i
can just take that that curve and kind
of say okay if the data is doing this
then i need to add in more dense layers
or whatever
so you can do a small chunks of data
then figure out what it costs to do a
large set of data and what kind of model
you want
the loss as we see here continues to go
down
this is the error this is how much error
is in there it really isn't a
user-friendly number other than
the more it trends down the better and
so if you continue to see the loss going
down eventually get to the point where
it stops going down and it goes up and
down and kind of wavers a little bit at
that point you know you've run too many
epics you're starting to get a bias in
there and it's not going to give you a
good model fit
the accuracy just turns us into
something that
we can use
and so the accuracy is what percentage
of guesses in this case is categorical
so this is the percentage of guesses are
correct
um value loss is similar you know it's a
minus a value loss
and then you have the value accuracy and
you'll see the value accuracy is pretty
similar to the accuracy um just browns
it off basically and so a lot of times
you come down here and you go okay we're
doing 0.5.6
0.7 and that is seventy percent accuracy
or in this case sixty eight point uh
five nine percent accuracy and that's a
very usable number and it's very
important to have
if you're identifying flowers that's
probably good enough if you can get
within a close distance and knowing what
flower you're identifying
if you're trying to figure out whether
someone's going to die from a heart
attack or not you might want to rethink
it a little bit or re-key how you're
building your model
so
if i'm working with a
group of clients
shareholders in a company or something
like that
you don't really want to show them this
you don't want to show them hey you know
this is what's going on with the
accuracy these are just numbers and so
we want to go and put the finishing
touches just like when you are building
a house and you put in the frame and the
trim on the house it's nice to have
something a nice view of what's going on
and so we'll go ahead and do a pie plot
and we'll just plot the history of the
loss
the history of the
value loss
over here
epics train and test and so we're just
going to compute these
this is really important and what i want
you to notice right here is when we get
to about oh
five epics a little more than five six
epics you see a cross over here
and it starts crossing as far as the
value loss and what's going on here is
you have the loss in your actual model
and your actual data and you have the
value loss where it's testing it against
the the test data the the data wasn't
used to program your model wasn't used
to train your model on
and so when we see this crossing over
this is where the bias is coming in this
is becoming over fitted and so when you
put these two together
right around five and six you start to
see how it does this this switch over
here and that's really where you need to
stop right around five yeah six
it's always hard to guess because at
this point the model is kind of a black
box
see but you know that right around here
if you're saving your model after each
run you want to use the one that's right
around five epics because that's the one
that's going to have the least amount of
bias so this is really important as far
as guessing what's going on with your
model and its accuracy and when to stop
it also is you know i don't show people
this mess up here um i show somebody
this kind of model and i say this is
where the training and the testing comes
in on this model
it just makes it easier to see and
people can understand what's going on
so that completes our demo and you can
see we did what we were set out to do we
took our flowers and we were able to
classify them uh within about you know
68 70
accuracy whether it's going to be a
dahlia sunflower cherry blossom rose
a lot of other things you can do with
your output as far as a
different tables to see where the errors
are coming from and what problems are
coming up image classification using
cross
and we're going to take a look at image
classification using cross and the basic
setup and we'll actually look at two
different demos on here
what's in it for you today
what is image classification
intel image classification data
creating neural networks with keras and
the vgg16 model
what is image classification
the process of image classification
refers to assigning classes to an entire
image images can be classified based on
different categories like weather it is
a nighttime or daytime shot what the
image represents etc you can see here we
have mountains looking for mountains
well i should be doing some
pictures of scenery and stuff like that
in deep learning we perform image
classification by using neural networks
to extract features from images and
classifies them based on these features
and you can see here where it says like
what computer sees and it says oh yeah
we see mostly forests maybe a little bit
of mountains because the way the image
is
and this is really where
one of the areas that neural networks
really shines
if you try to run this stuff through
more like a linear regression model
you'll still get results but the results
kind of miss a lot of things as the
neural networks get better and better at
what they do with different tools we
have out there
so intel image classification data
the data being used as the intel image
classification data set would consist of
images of six types of land areas
and so we have forest building glaciers
and mountains sea and street
and you can see here there's a couple of
the images out of there as a setup in
the
intel image classification data that
they use
and then we're going to go into creating
a neural networks with cross
the convolutional neural network that we
are creating from scratch looks
as shown below
you'll see here we have our input layer
um
they haven't listed max pooling uh so
you have
as you're coming in with the input layer
the input layer is actually
before this but the first layer that
it's going to go into is going to be a
convolutional neural network
uh then you have a max pooling that
pulls those the the convolutional neural
networks returns
in this case they have two of those that
is very standard with convolutional
neural networks uh one of the ones that
i was looking at earlier that was
standard being used by um
i want to one of the larger companies
that came for which one for doing a
large amount of identification had two
convolutional neural networks each with
their max pooling and then about 17
dense layers after it we're not going to
do that heavy duty of a of a code but
we'll get you head in the right
direction and that gives you an idea of
what you're actually going to be looking
at when you look at the flattened part
and then the dents we're talking like 17
dense layers afterwards
i find that a lot of the stuff i've been
working on i end up maxing it out right
around nine dense layers it really
depends on what you have going in and
what you're working with
and the vgg-16 model
vgg16 is a pre-trained cnn model which
is used for image classification it is
trained on a large varied data set and
fine-tuned to fit image classification
data sets with ease
and you can see down here we have the
input coming in
the convolutional neural network one to
one one to two and then pooling and then
we do two to one two to two
convolutional network then pooling three
to two and you can see there's just this
huge layering of convolutional neural
networks
and in this case they have five such
layers going in and then three dents
going out or
more
now when they
took this setup this actually won an
award back in 2019 for this particular
setup
and it does it does really good except
that again
we only show the three dense layers here
and as you find out
depending on your data going in what you
have set up
that really isn't enough on one of these
setups and i'm going to show you why we
restricted it because it does take up a
lot of processing power in some of these
things
so let's go ahead and roll up our
sleeves and we're going to look at both
the setups we're going to start with the
the first classification
and then we'll go into the vgg16 and
show you how that's set up
now i'm going to be using anaconda and
let me flip over to my anaconda so you
can see what that looks like
now i'm running in the anaconda here
you'll see that i've set up a main
python
3 8. i always put that in there because
this is where i'm doing like most of my
kind of playing around
this is done in python version 3.8 we're
not going to dig too much into versions
at this point you should already have
cross installed on there usually cross
takes a number of extra steps
and then our usual
setup is the numpy the pandas
your sk your site kit which is going to
be the sk learn your seaborn and i'll
show you those in just a minute
um and then i'm just going to be in the
jupiter lab where i've created a new
notebook in here
and let's flip on over there to my blank
notebook
now there's a couple of cool things to
note in here is that
one i use the the
anaconda jupiter notebook setup because
it keeps everything separate
except for cross
cross is actually running separately in
the back i believe it's a c program
what's nice about that is that it
utilizes the multi processors on the
computer and i'll mention that just in a
little bit when we actually get down to
running the code
and when we look in here a couple things
to note is here's our
oops
i thought i'd grab the other drawing
thing uh but here's our numpy and our
pandas right here and our operating
system
this is our psi kit you always import it
as sk learn for the classification
report
uh we're going to be using well usually
import like seaborne brings in all of
your pie plot library also
kind of nice to throw that in there i
can't remember if we're actually using
seabourn if they just the people in the
back just threw that together
and then we have the sklearn shuffle for
shuffling data here's our matplot
library that the seaborn is pretty much
built on
cv2
if you're not familiar with that that is
our image
module for importing the image
and then of course we have our
tensorflow down here which is what we're
really working with
and then the last thing is just for
visual effect while we're running this
if you're doing a demo and you're
working with the partners or the
shareholders
this tqdm is really kind of cool it's an
extensible progress bar for python and
i'll show you that too
remember data science is not i mean
almost this code when i'm looking
through this code i'm not going to show
half of this stuff to the shareholders
or anybody i'm working with they don't
really care about pandas and all that we
do because we want to understand how it
works
so we need to go ahead and import those
different
setup on there and then the next thing
is we're going to go ahead and set up
our classes
now we remember if we had mountain
street glacier building c and forest
those were the different images that we
have coming in
and we're going to go ahead and just do
class name labels and we're going to
kind of match that class name of i for i
class name
equals the class names so our labels are
going to match the names up here
uh and then we have the number of uh
classes
and the print the class names and the
labels and we'll go ahead and set the
image size this is important that we
resize everything because if you
remember with neural networks
they take one size data coming in and so
when you're working with images you
really want to make sure they're all
resized to the same setup it might
squish them it might stretch them that
generally does not cause a problem in
these uh in some of the other tricks you
can do with if you if you need more data
and this is one that's used regularly
we're not gonna do it in here is you can
also take these images and not only
resize them but you can tilt them one
way or the other crop parts of them
so they process slightly differently and
they'll actually increase your accuracy
of some of these predictions
and so you can see here we have
mountain equals zero that's what this
class name label is street equals one
glacier equals two buildings equals
three c four four s equals five
now we did this as an enumerator so each
one is zero through five
a lot of times we do this instead as
uh
0 1 0 1 0 1 so you have five outputs and
each one's a zero or a one coming out
so the next thing we really want to do
is we want to go ahead and load the data
up and just put a label in there loading
data
just just so you know what we're doing
i'm going to put in the loading data
down here
make sure it's well labeled
and we'll create a definition for this
and this is all part of your
pre-processing
at this point you could replace this
with all kinds of different things
depending on what you're working on
and if you once you download you can go
download this data set uh send a note to
the simply learn team here in youtube
and they'll be happy to direct you in
the right direction and make sure you
get this path here
so you have to write whatever wherever
you saved it a lot of times i'll just
abbreviate the path or put it as a sub
thing and just get rid of the directory
but again double check your paths
we're going to separate this into a
segment for training
and a segment for testing and that's
actually how it is in the folder let me
just show you what that looks like
so when i have my lengthy path here
where i keep all my programming simply
learn this particular setup we're
working on image classification
and image classification
clearly you probably wouldn't have that
lengthy of a list
and when we go in here
you'll see sequence train sequence test
they've already split this up this is
what we're going to train the data in
and again you can see buildings 4th
glacier mountain c street
and if we double click let's go under
forest you can see all these different
forest
images and there's a lot of variety here
i mean we have winter time we have
summertime
so it's kind of interesting
you know here's like a fallen tree
versus
a road going down the middle
that's really hard to train and if you
look at the buildings
a lot of these buildings you're looking
up a skyscraper we're looking down the
setup
here's some trees with one i want to
highlight this one it has trees in it
let me just open that up so you can see
it a little closer
the reason i want to highlight this is i
want you to think about this
we have trees growing is this the city
or a forest
so this kind of imagery makes it really
hard for a classifier and if you start
looking at these you'll see a lot of
these images do have trees and other
things in the foreground
weird angles really a hard thing for a
computer to
sort out and figure out whether it's
going to be a forest or a city
and so in our loading of data
one we have to have the path the
directory
we're going to come in here we have our
images and our labels
so we're going to load the images in one
section the labels in another
and if you look through here it just
goes through the different folders
in fact let me do this let me
there we go
uh as we look at this we're just going
to loop through the three six different
folders that have the different
landscapes and then we're going to go
through and pull each file out
and each label
so we set the label we set the folder
for file and list
here's our image path
join the paths this is all kind of not
general stuff
so i'm kind of skipping through it
really quick
and here's our image setup if you
remember we're talking about the images
we have our cv2 reader so it reads the
image in
it's going to go ahead and take the
image and convert it to from
blue green red to red green green blue
this is a cv2 thing almost all the time
it imports it and instead of importing
it as a standard that's used just about
everywhere it imports it with the bgr
versus rgb
rgb is pretty much a standard in here
you have to remember that with cv2
and then we're going to go ahead and
resize it this is the important part
right here we've said it we've decided
what the size is and we want to make
sure all the images have the same size
on them
and then we just take our images and
we're just going to pin the image append
the label
and then the images it's going to turn
into a numpy array this just makes it
easier to process and manipulate
and then the labels is also a numpy
array and then we just return the output
append images and labels and we return
the output down here
so we've loaded these all into memory uh
we haven't talked too much
there'd be a different setup in there
because there is ways to feed the files
directly into your cross model
but we want to go ahead and just load
them all because really
for today's processing and what our
computers can handle that's not a big
deal
and then we go ahead and set the
train images train labels test images
test labels and that's going to be
returned in our output append and you
can see here we did
images and labels set up in there and it
just loads them in there so we'll have
these four different categories let me
just go ahead and run that
so now we've gone ahead and loaded
everything on there
and then if you remember from before uh
we imported just go back up there
shuffle here we go here's our sk learn
utilities import shuffle
and so we want to take these labels and
shuffle them around a little bit
just mix them up so it's not having the
same if you run the same process over
and over
uh then you might run into some problems
on there
and just real quick let's go ahead and
do uh
um a plot so we can just you know we've
looked at them as far as from outside of
our code we pulled up the files and i
showed you
what that was going on we can go and
just display them here too
and i tell you when you're working with
different people
this should be highlighted right here um
this thing is like when i'm working on
code and i'm looking at this data and
i'm trying to figure out what i'm doing
i skip this process
the second i get into a meeting and i'm
showing what's going on to other people
i skip everything we just did
so
and go right to here where we want to go
ahead and display some images and take a
look at it
and in this display
i've taken them and i've resized the
images to 20 by 20.
that's pretty small
so we're going to lose just a massive
amount of detail
and you can see here these nice
pixelated images
i might even just stick with the folder
showing them what images were processing
uh again this is
you gotta be a little careful this
maybe resizing it was a bad idea
in fact let me try it without resizing
and see what happens oops so i took out
the image size and then we put this
straight in here
one of the things again this is um
but the d there we go one of the things
again that we want to know
whenever we're working on these things
is the cv2
there are so many different
image classification setups it's really
a powerful package when you're doing
images
but you do need to switch it around so
that it works with the pie plot and so
make sure you take your numpy array and
change it to a u integer 8 format
because it comes in as a float
otherwise you'll get some weird images
down there
and so this is just basically we've
split up our we've created a plot
we went ahead and did the plot 20 by 20
um or
the plot figure size is 20 by 20
and then we're doing 25 so a 5x5 subplot
nothing really going on here too
exciting but you can see here where we
get the images
and
really when you're showing people what's
going on this is what they want to see
so you skip over all the code and you
have your meeting you say okay here's
our images of the building
don't get caught up in how much work you
do get caught up in what they want to
see so if you want to work in data
science that's really important to know
and this is where we're going to start
having fun
here's our model this is where it gets
exciting
when you're digging into these models
and you have here
let me get
there we go
when you have here if you look here
here's our convolutional neural network
uh 2d
and
2d is an image you have two different
dimensions x y and even though there's
three colors it's still considered 2d
if you're running a video you'd be
convolutional neural network 3d
if you're doing a series going across
a time series it might be 1d
and on these you need to go ahead and
have your convolutional neural network
if you look here there's a lot of really
cool settings going on
to dig into um we have our input shape
so everything's been set to 150 by 150
and it has of course three different
color schemes in it that's important to
notice
activation
default is relu this is
small amounts of data are being
processed on a bunch of little
neural networks
and right here is the 32 that's how many
of these convolutional null networks are
being strung up on here
and then the by three
uh when it's doing its steps it's
actually looking at a little three by
three square on each image
and so that's what's going on here and
with convolutional neural networks the
window floats across
and adds up all these numbers going
across on this data and then eventually
it comes up with 30 in this case 32
different
feature options that it's looking for
and of course you can change that 32
you can change the three by three so you
might have a larger setup
you know if you're going across 150
um by 150 that's a lot of steps so we
might run this as 15 by 15. uh there's
all kinds of different things you can do
here
we're just putting this together again
that would be something you would play
with to find out which ones are going to
work better on this setup
and there's a lot of play involved
that's really where it becomes an art
form is guessing at what that's going to
be
the second part i mentioned earlier and
i can only begin to highlight this
when you get to these dense layers
one is the activation is a relu they use
a relu and a softmax here here
it's a whole whole setup just explaining
why these are different
and how they're different because
there's also an exponential there's a
tangent in fact
there's just a ton of these and you can
build your own custom activations
depending on what you're doing
a lot of different things go into these
activations
there are two or three major thoughts on
these activations and
rayleigh and softmax are well relu
you're really looking at just the number
you're adding all the numbers together
and you're looking at euclidean geometry
ax plus b
x 2 plus c x 3
plus a bias
with softmax this belongs to the party
of um
it's activated or it's not except it's
they call it softmax because when you
get the the to zero instead of it just
being zero
uh it's actually slightly
a little bit less than zero so that when
it trains it doesn't get lost
um there's a whole series of these
activations
another activation is the tangent
where it just drops off and you have
like a very narrow area where you have
from minus one to one or exponential
which is zero to one
so there's a lot of different ways to do
the activation
again we can do that would be a whole
separate lesson on here
we're looking at the convolutional
neural network um and we're doing the
two pools
this is so common you'll see two two
convolutional neural networks stacked on
top of each other each with its own max
pool underneath let's go ahead and run
that so we built our model there and
then we need to go ahead and
compile the model so let's go ahead and
do that
uh we're gonna use the atom uh optimizer
the bigger the data the atom fits better
on there there's some other optimizer
but i think atom's a default
um i don't really play with the
optimizer too much that's like the
once you get a model that works really
good you might try some different
optimizers but atoms usually the most
and then we're looking at loss
pretty standard we want to minimize our
law we want to
maximize the loss of error
and then we're going to look at accuracy
um everybody likes the accuracy i'm
going to tell you right now
i start talking to people and like okay
what's what's the loss on this and then
as a data science yeah i want to know
how the lot what's going on with that
we'll show you why in a minute
but everybody wants to see accuracy we
want to know how accurate this is
and then we're going to run the fit and
i wanted to do this just so i can show
you
even though we're in a python
setup in here where jupyter notebook is
using only a single processor i'm going
to bring over my little cpu tool
this is eight cores on 16 dedicated
threats so it shows up as 16 processors
and actually i got to run this and then
move it over
so we're going to run this
and hopefully it doesn't destroy my mic
and as it comes in you can see it's
starting to do go through the epics and
we said i set it for five epics
and then this is really nice because
cross uses all the different uh threads
available
so it does a really good job of doing
that
this is going to take a while if you
look at here it's uh
eta
two minutes and 25 seconds 24 seconds so
this is roughly two and a half minutes
per epic
and we're doing five epics
so this is going to be done in roughly
15 minutes
i don't know about you but i don't think
you want to sit here for 15 minutes
watching the green bars go across so
we'll go ahead and let that run
and
there we go uh there was our 15 minutes
it's actually less than that
uh because i did when i went in here
realized that uh
where was it
here we go here's our model compile
here's our model flip fit
and here's our epics
so i did four epics
so a little bit better a little more
like 10 to 11 minutes instead of
doing the full
15. and
when we look at this here's our model we
did we talked about the compiler uh
here's our history we're going to click
history equals the model fit
we'll go into that in just a minute
and what we're looking at is we have our
epochs
here's our validation split
so as we train it
we're weighing the accuracy versus
you kind of pull some data off to the
side
while you're training it
and the reason we do that is that
you don't want to overfit
and i'll we'll look at that chart in
just a minute
here's batch size
this is just how many images you're
sending through at a time
the larger the batch it actually
increases the processing speed um and
there's reasons to go up or down on the
batch size because of the uh the the
smaller of the batch
there's a certain point where
you get too large of a batch and it's
trying to fit everything at once
so i yeah 128 is kind of big
depends on the computer you're on what
it can handle
and then of course we have our train
images and our train labels going in
telling it what we're going to train on
and then we look at our four epics here
here's our accuracy we want the accuracy
to go up and we get all the way up to
0.83 or 83 percent now this is actual
percentage based pretty much
and we can see over here our loss we
want our loss to go down really
fluctuates 55 1.2.77
so we have a lot of things going on
there
let's go ahead and graph those
turn that up
and our team in the back did a wonderful
job of putting together um this basic
plot set up
um here's our subplot coming in we're
going to be looking at um
uh from the history we're going to send
it the accuracy and the value accuracy
labels are set up on there
and we're going to also look at loss and
value loss
so you can see what this looks like
what's really interesting about
this setup and let me let me just go
ahead and show you because
without actually seeing the plots it
doesn't make a whole lot of sense
it's just basic plotting of
the data using the pi plot library
and i want you to look at this this is
really interesting
when i ran this the first time i had
very different results
um and they they vary greatly and you
can see here our accuracy continues to
climb
um
and there's a crossover here
put it in here
right here's our crossover
and i point that out because as we get
to the right of that crossover where our
accuracy
we're like oh yeah i got .8
we're starting to get an overfit here
that's what this this switch over means
um as our value uh as a training set
versus the value
accuracy stays the same and so that this
is the one we're actually really want to
be aware of and where it crosses
is kind of where you want to stop at
and we can see that also with the train
loss versus the value loss right here we
did one epic and look how it just
flatlines right there with our loss
so really
one epic
is
probably enough
and you're going to say wow okay 0.8
percent
certainly if i was working with the
shareholders um
telling them that it has an 80 accuracy
isn't quite true and and we'll look at
that a little deeper it really comes out
here that the accuracy
of our actual values is closer to 0.41
right here
even after running it this number of
times
and so you really want to stop right
here at that crossover
one epic would have been enough
so the data is a little over fitted on
this when we do four epics
and uh oops there we are
okay
my drawing won't go away um let's see if
i can get there we go
for some reason i've killed my drawing
ability and my recorder
all right took a couple extra clicks
uh so let's go ahead and take a look at
our actual test loss
so you see where it crosses over that's
where i'm looking at
that's where we start overfitting the
model
and this is where
if we were going to go back and
continually upgrade the model
we would start taking a look at the
images and start rotating them
we might start playing with the
convolutional neural network instead of
doing the three by three window
we might expand that or you know find
different things that might make a big
difference as far as the way it
processes these things
so let's go ahead and take a look at our
test loss now remember we had our
training data
now we're going to look at our test
images and our test labels
for our test loss here and this is just
model evaluate
just like we did fit up here
where was it
one more
model fit with our training data going
in now we're going to evaluate it on the
and this data has not been touched yet
so
this model's never seen this data this
is on
completely new information as far as the
model is concerned of course we
already know what it is from the labels
we have
and this is what i was talking about
here's the actual accuracy right here
point four eight
uh or four eight four seven
so this forty nine percent of the time
guesses what the image is
uh and
i mean really that's the bottom dollar
uh does this work for what you're
needing does 49 work do we need to
upgrade the model more
um
in some cases this might be uh
oh what was i doing i was working on
stock evaluations
and
we were looking at what stocks were the
top performers
well if i get that 50 correct on top
performers
i'm good with that um that's actually
pretty good for stock evaluation
in fact the number i had for stock was
more like
30 something percent as far as being a
top performer stock
much harder to predict
but at that point you're like well
you'll make money off of that so again
this number right here depends a lot on
the domain you're working on
and then we want to go ahead and bring
this home a little bit more uh as far as
looking at the different setup in here
and one of the uh from sk learn if you
remember actually let's go back to the
top
uh we had the classification report and
this came in from our sklearn or scikit
setup
and that's right here you can see it
right here on the
see there we go
classification report right here
sklearn metrics import classification
report that's we're going to look at
next
a lot of this stuff
depends on who you're working with
so when we start looking at
precision
you know this is like for each value
i can't remember what one one one was
probably mountains so if 44
is not good enough if you if you're
doing like um you're in the medical
department and you're doing cancer is it
is this cancerous or not i'm only 44
accurate
not a good deal
you know i would not go with that um
so it depends on what you're working
with on the different labels and what
they're used for facebook
you know 44 i'm guessing the right
person i hope it does a little bit
better than that um but here's our main
accuracy this is what almost everybody
looks at they say oh 48 that's what's
important
um again it depends on what domain
you're in and what you're working with
and now we're going to do the same model
oops somehow i got my there it goes i
thought i was going to get stuck down
there again
this time we're going to be using the
vgg16
and remember this one is
all those layers
going into it so it's basically a bunch
of convolutional neural networks getting
smaller and smaller on here
and so we need to go ahead and
import all our different stuff from
cross we're importing the main one is
the v
g 16 set up on there let me just aim
that there we go
there's kind of a pre-processing images
applications pre-process input this is
all part of the vg g16 setup on there
and once we have all those we need to go
ahead
and create our model
and we're just going to create a vgg16
model in here
inputs model inputs outputs model inputs
i'm not going to spend as much time as
they did on the other one uh we're going
to go through it really quickly
one of the first things i would do is if
you remember in cross you can create
treat a model like you would a layer
and so at this point i would probably
add a lot of dense layers on after the
vgg16 model and create a new model with
all those things in there
and we'll go ahead and run this
because here's our model coming in and
our setup
and it'll take it just a moment to
compile that what's funny about this is
i'm waiting for it to download the
package since i pre-ran this
it takes it a couple minutes to download
the vgg16 model into here
and so we want to go ahead and train
features for the model we're going to
predict that we're going to predict the
train images and we're going to test
features on the predict test images on
here
and then i told you it's going to create
another model too and the people in the
back did not disappoint me they went
ahead and did just that
and this is really an important part um
this is worth stopping for i told you i
was going to go through this really
quick
so here's our uh
we we have our model 2
coming in and we've created a model up
here with the vgg16 model equals model
inputs model inputs
and so we have our vgg16 this has
already been pre-programmed
and then we come down here i want you to
notice on this
right here
layer model two layers minus four to one
x layer x
um we're basically taking this model and
we're adding stuff onto it
and so
we've taken we've just basically
duplicated this model we could have done
the same thing
by using model up here as a layer
we could have the input go to this model
and then have that go down here so we've
added on
this whole setup
this whole block of code from 13 to 17
has been added on to our vgg16 model
and we have a new model uh with the
layer ma input and x down here
let's go ahead and run that and compile
it and that was a lot to go through
right there when you're building these
models this is the part that gets so
complicated
did you get stuck playing in and yet
it's so fun
it's like a puzzle how can i loop these
models together
and in this case you can see right here
that the layers
we're just copying layers over and
adding each layer in
this is one way to build a new model
and we'll go ahead and run that
like i said the other way is you can
actually use the model as a layer i've
had a little trouble playing with it
sometimes when you're using the straight
model over
you run into issues
it seems like it's going to work and
then you mess up on the input and the
output layers there's all kinds of
things that come up
let's go ahead and do the new model
we're going to compile it again here's
our metrics accuracy sparse categorical
loss
pretty straightforward just like we did
before you got to compile the model
and just like before we're going to take
our create a history
the history is going to be
a new model fit train 128
and just like before if you remember
when we started running this stuff we're
going to have to go ahead and it's going
to light up our setup on here and this
is going to take a little bit to get us
all set up it's not going to just happen
in a couple minutes so let me go ahead
and pause the video and run it and then
we'll talk about what happened
okay now when i ran that these actually
took about six minutes each
so it's a good thing i put it on whole
we did four epics i actually had to stop
it says at 10 and switch it to forks i
didn't want to wait an hour
and you can see here our accuracy
and our loss numbers going down
and just at a glance
it actually performed if you look at the
accuracy
0.2658
so our accuracy is going down or you
know 26
um 34
35
and you can see here at some point it
just kind of kicks the bucket again this
is overfitting
that's always an issue when you're
running on
programming these different neural
networks
and then we're going to go ahead and
plot the accuracy
history we built that nice little
subroutine up above so we might as well
use it
and you can see it right here
there's that crossover again
and
if you look at this look how the how the
uh the red shifts up how the uh
our loss functions and everything
crosses over we're overfitting after one
epic um we're clearly
not helping the problem or doing better
um we're just going to it's just going
to baseline this one actually shows with
the training versus the loss
value loss maybe second epic so on here
we're now talking more between the first
and the second epic and that also shows
kind of here so
somewhere in here it starts overfitting
and right about now you should be saying
uh-oh
something went wrong there i thought
that
um we went up here and ran this look at
this we have the accuracy up here it's
hitting that 48 percent
and we're down here
you look at the score down here that
looks closer to
20
not nearly
anywhere in the ballpark of what we're
looking for
and we'll go ahead and run it through
the
the actual test features here
and there it is
we actually run this on the unseen data
and everything
point uh one eight or eighteen percent
um i don't know about you but i wouldn't
want you know at 18 this did a lot worse
than the other one
i thought this was supposed to be the
supermodel the
model that beats all models uh vgg16
that won the
awards and everything
well the reason is is that
one we're not
pre-processing the data so it needs to
be more there needs to be more
um as far as like rotating the data at
you know 45 degree angle taking partials
of it
so you can create a lot more data to go
through here
and that would actually greatly change
the outcome on here and then we went up
here we only added a couple dense layers
uh
we added a couple convolutional neural
networks
this huge pre-trained setup is looking
for a lot more information coming in
as far as how it's going to train
and so
uh this is one of those things where i
thought it would have done better and i
had to go back and research it and look
at it and say why didn't this work why
am i getting only
18 percent here instead of 44 or better
and that would be wise it doesn't have
enough training data coming in
uh and again you can make your own
training data so it's not that we have a
shortage of data it's that that some of
that has to be switched around and moved
around a little bit
and this is interesting right here too
if you look at the precision
we're getting it on number two and yet
we had zero on everything else
so for some reason it's not seeing
the different variables in here so it'd
be something else to look in and try to
track down
and that probably has to do with the
input but you can see right here we have
a really good solid 0.48 up here and
that's where i'd really go with is
starting with this model and then we
look at this model and find out why are
these numbers not coming up better is it
the data coming in
where's the setup on there
and that is the art of data science
right there is finding out which models
work better and why
and we went through the very basics of
convolutional neural networks along with
the
vgg16 import and how you can get started
with that and again the art is the data
going in and learning to play with it
find out what works
and with that we come to the end of this
video on keras
we really hope that you enjoyed this
video
if you did a thumbs up would be really
appreciated
also here's your reminder to subscribe
to our channel and click on the bell
icon for more on the latest technologies
and trends
thank you for watching and stay tuned
for more from simply learn