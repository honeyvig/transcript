foreign
how is deep learning contributing to
technological advancements and impacting
various Industries in the current year
well the importance of deep learning is
increasing rapidly as more and more
Industries and Fields adapt AI
Technologies to improve more efficiency
accuracy and decision making process
deep learning algorithms can understand
natural language and Tackle various
tasks as this algorithm becomes Smarter
with each new data set the range of
problems they can solve is expanding
rapidly so let's take a closer look at
some of the exciting real world examples
of how deep learning is helping to solve
complex problems
deep learning is a popular approach for
building chat boards and service boats
as it enables them to understand natural
language inputs recognize patterns and
generate appropriate responses deep
learning models are used in speech
recognization systems to enable virtual
assistants to recognize and interpret
spoken language inputs next neural
Mission translation system leverage deep
learning models such as Transformer
architect to facilate the translations
of text from one language to another
there are numerous career opportunities
for those skills in deep learning
including rules such as deep learning
engineer machine learning engineer data
scientist computer vision engineer and
natural language processing engineer the
Deep learning Market is expected to grow
at a compound annual rate of 38.9
percent by 2024 leading to an increasing
demand for Professionals in this build
according to Glassdoor a deep learning
average salary in the United States is
around 109 000 dollars per year however
it is 8 lakhs per annum in India after
all deep learning adapts CI technology
so if you also want to advance your
career in deep learning this master's
program in artificial intelligence
offered in collaboration with IBM can
boost your career it offers courses
relevant to the industry such as data
science using python machine learning
deep learning NLP and charitivity the
program also includes hackathons and ask
me anything session hosted by IBM you
can gain job ready skills in AI by
working on practical projects
participating in live sessions and
completing Capstone projects end all the
course now and get the special discount
by applying code ytb E15 check the
course Link in the description for more
details hey guys welcome to this deep
learning bootcamp by simply learn in
this bootcamp we will start off by
understanding the introduction to deep
learning followed by what deep learning
is next we will move on to topics like
types of neural networks and deep
learning libraries then we will look at
the Deep learning demo text
classification tensor and tensorflow 1.0
versus 2.0 we will conclude this deep
learning bootcamp by discussing topics
like types of RNN Keras computational
graphs and the essential interview
question and answers of deep learning to
help every individual crack an interview
before we begin let's take a minute to
hear from our Learners who have
experienced huge success in their career
followed by introduction to deep
learning
my name is srinath because of Simply
learn course how I got into data science
Road along with good parents I have
always been power looking person I felt
stagnant at my current job position and
realized eventually I would hit the
glass ceiling so I really felt the need
in upskilling myself to stay relevant in
the current job position initially I
thought to go for the classroom training
but I thought it is not feasible for me
to go for a classroom training and as my
office timings are from morning to
evening so then I started searching for
the online training institutions in
internet then I came to know about
simply learn and I came to know that it
is giving better training so I opted for
this Implement simply learn has very
unique feature where a person can
register for multiple batches where
different instructors will be teaching
us so this will definitely helps us to
understand the different Topics in
different perspectives I started
training in simply learned with the goal
of becoming a data scientist in my
current organization itself I have
opportunities for the data science role
and I thought this course will
definitely help me in taking up the data
science role in my organization also my
friends and my manager even my manager
also encouraged me to take a course and
to become a change the role for the
better opportunities in the future I'm
very much impressed with the trainings
offered by simply run live classes
especially the practice Labs which are
cloud-based we can access them anywhere
in the world without carrying laptop
everywhere and the trainers are very
good and even on getting very good
replies and quickly price from the
support team for any issue the mobile
app provided by simply run is very good
we can access self-paced learning videos
on the mobile app currently I'm focusing
on becoming a data science engineer
gradually I wanted to become a
full-fledged engineer sticking to only
data science may put me in the same
challenge what I'm facing today because
of Simply learn course how I got into
data science Road along with good
parents now I 100 confident that I would
become a data scientist one day I
definitely refer simply learn to my
friends simply learn has renewed my
interest to learn more and grow in my
career I don't think we can stay
relevant in these times without
upskilling and staying up to date
ever wondered how Google Translates an
entire web page to a different language
in a matter of seconds or your phone
gallery groups images based on their
location to achieve AI through
algorithms trained with data and finally
deep learning is a type of machine
learning inspired by the structure of
the human brain in terms of deep
learning this structure is called an
artificial neural network let's
understand deep learning better and how
it's different from machine learning say
we create a machine that could
differentiate between tomatoes and
cherries if done using machine learning
we'd have to tell the machine the
features based on which the two can be
differentiated these features could be
the size and the type of stem on them
with deep learning on the other hand the
features are picked out by the neural
network without human intervention of
course that kind of Independence comes
at the cost of having a much higher
volume of data to train our machine now
let's dive into the working of neural
network works here we have three
students each of them write down the
digit 9 on a piece of paper notably they
don't all write it identically the human
brain can easily recognize the digits
but what if a computer had to recognize
them that's where deep learning comes in
here's a neural network trained to
identify handwritten digits each number
is present as an image of 28 times 28
pixels now that amounts to a total of
784 pixels neurons the core entity of a
neural network is where the information
processing takes place each of the 784
pixels is fed to a neuron in the first
layer of our neural network this forms
the input layer on the other end we have
the output layer with each neuron
representing a digit with the hidden
layers existing between them the
information is transferred from one
layer to another over connecting
channels each of these has a value
attached to it and hence is called a
weighted Channel all neurons have a
unique number associated with it called
bias this bias is added to the weighted
sum of inputs reaching the neuron which
is then applied to a function known as
the activation function the result of
the activation function determines if
the neuron gets activated every
activated neuron passes on information
to the following layers this continues
up till the second last layer
the one neuron activated in the output
layer corresponds to the input digit the
weights and bias are continuously
adjusted to produce a well-trained
network so where is deep learning
applied in Customer Support when most
people converse with customer support
agents the conversation seems so real
they don't even realize that it's
actually a bot on the other side in
medical care neural networks detect
cancer cells and analyze MRI images to
give detailed results self-driving cars
with seem like science fiction is now a
reality Apple Tesla and Nissan are only
a few of the companies working on
self-driving cars so deep learning has a
vast scope but it too faces some
limitations the first as we discussed
earlier is data while deep learning is
the most efficient way to deal with
unstructured data a neural network
requires a massive volume of data to
train let's assume we always have access
to the necessary amount of data
processing this is not within the
capability of every machine and that
brings us to our second limitation
computational power training and neural
network requires graphical processing
units which have thousands of cores as
compared to CPUs and gpus are of course
more expensive and finally we come down
to training time deep neural networks
take hours or even months to train the
time increases with the amount of data
and number of layers in the network some
of the popular deep learning Frameworks
include tensorflow High torch Keras deep
learning 4J Cafe and Microsoft cognitive
toolkit considering the future
predictions for deep learning and AI we
seem to have only scratched the surface
in fact horse technology is working on a
device for the blind that uses deep
learning with computer vision to
describe the world to the users
replicating the human mind at the
entirety may be not just an episode of
Science Fiction for too long the future
is indeed full of surprises last summer
my family and I visited Russia even
though none of us could read Russian we
did not have any trouble in figuring our
way out all thanks to Google's real-time
translation of Russian boards into
English this is just one of the several
applications of neural networks neural
networks form the base of deep learning
a subfield of machine learning where the
algorithms are inspired by the structure
of the human brain neural networks take
in data train themselves to recognize
the patterns in this data and then
predict the outputs for a new set of
similar data let's understand how this
is done
let's construct a neural network that
differentiates between a square circle
and triangle neural networks are made up
of layers of neurons these neurons are
the core processing units of the network
first we have the input layer which
receives the input the output layer
predicts our final output in between
exists the hidden layers which perform
most of the computations required by our
Network here's an image of a circle this
image is composed of 28 by 28 pixels
which make up for 784 pixels each pixel
is fed as input to each neuron of the
first layer neurons of one layer are
connected to neurons of the next layer
through channels each of these channels
is assigned a numerical value known as
weight the inputs are multiplied to the
corresponding weights and their sum is
sent as input to the neurons in the
hidden layer each of these neurons is
associated with a numerical value called
the bias which is then added to the
input sum this value is then passed
through a threshold function called the
activation function the result of the
activation function determines if the
particular neuron will get activated or
not an activated neuron transmits data
to the neurons of the next layer over
the channels in this manner the data is
propagated through the network this is
called forward propagation in the output
layer the neuron with the highest value
fires and determines the output the
values are basically a probability for
example here our neuron associated with
square has the highest probability hence
that's the output predicted by the
neural network of course just by a look
at it we know our neural network has
made a wrong prediction but how does the
network figure this out note that our
network is yet to be trained during this
training process along with the input
our Network also has the output fed to
it the predicted output is compared
against the actual output to realize the
error in prediction the magnitude of the
error indicates how wrong we are and the
sign suggests if our predicted values
are higher or lower than expected the
arrows here give an indication of the
direction and magnitude of change to
reduce the error this information is
then transferred backward through our
Network this is known as back
propagation now based on this
information the weights are adjusted
this cycle of forward propagation and
back propagation is iteratively
performed with multiple inputs this
process continues until our weights are
assigned such that the network can
predict the shapes correctly in most of
the cases this brings our training
process to an end
you might wonder how long this training
process takes honestly neural networks
may take hours or even months to train
but time is a reasonable trade-off when
compared to its scope
let us look at some of the Prime
applications of neural networks facial
recognition cameras on smartphones these
days can estimate the age of the person
based on their facial features this is
neural networks at play first
differentiating the face from the
background and then correlating the
lines and spots on your face to a
possible age forecasting neural networks
are trained to understand the patterns
and detect the possibility of rainfall
or rise in stock prices with high
accuracy music composition neural
networks can even learn patterns in
music and train itself enough to compose
a fresh tune with deep learning and
neural networks we are still taking baby
steps the growth in this field has been
foreseen by the big names companies such
as Google Amazon and Nvidia have
invested in developing products such as
libraries predictive models and
intuitive gpus that support the
implementation of neural networks the
question dividing the vision Aries is on
the reach of neural networks to what
extent can we replicate the human brain
we'd have to wait a few more years to
give a definite answer
my name is Richard kirschner with the
simply learned team and today we're
going over the Deep learning tutorial
what is deep learning
deep learning is a type of machine
learning that works on the basis of
functionalities of human brain it trains
machines to work on vast volumes of
structured and unstructured data
deep learning uses the concept of neural
networks that is derived from the
structure of a human brain
and if you've ever seen images of human
brain Network you have dendrites inputs
you have cell which is a nucleus as your
nodes you have synapses which create
weights and an axon which create an
output
now this is a very simplified version of
the human brain
and there are certain sets of cells that
are strung together very similar to how
today's neural networks perform but they
still have a long ways to go and the
human brain is still significantly more
complex with hundreds of different kinds
of cells and different interactions that
we don't see yet
so what is deep learning artificial
intelligence so you have your AI is a
method of building smart machines that
are capable to think like human and
mimic their actions
be careful with that definition we're a
long way from a human cyborg coming in
and taking over when we talk about this
we're usually talking about automating a
single kind of instances or things going
on how do we automate a new process how
do we automate a small robot to do
something how do we get a drone to fly
out when it loses contact to turn around
and come back in the direction it came
from
those are very simple processes and
significantly lower than the scale of
like what human thinking does now a
subcategory of artificial intelligence
is machine learning machine learning is
an application of AI that allows
machines to automatically learn and
improve with experience
there's a lot of tools out there to use
with machine learning
and you'll see really in regression
models and things like that they're much
more common when dealing with straight
numbers a linear regression model and
there's other models that just do basic
math functions quite well
uh but then we get into one of the
subcategories deep learning deep
learning is a subfield of machine
learning that is used to extract
patterns from data using neural networks
and again we're talking about
complicated patterns we talk about image
processing and things like that they're
not as straightforward as the numbers in
stock exchange so you start looking at
another way to solve these problems and
figure out is that a raccoon in the
picture or something much more
complicated if getting your learning
started is half the battle what if you
could do that for free visit skillup by
simply learn click on the link in the
description to know more
deep learning performance so when you
have we talk about performance of deep
learning and the amount of data the
performance goes up the more data you
have the higher the performance
when we talk about a lot of machine
learning platforms they kind of peek out
at a lower level so again you can think
of this as having thousands of pictures
of raccoons like I said before versus
the numbers in a stock exchange which
are very rigid and very clear there you
have a close and an open and the stock
exchange kind of thing where you have an
image of a raccoon the color shading all
kinds of things go into trying to figure
out is it a raccoon
so we look at what is a neural network
we really look into kind of a nice image
of it today's neural networks usually
have a layer of inputs and you'll see
here we have input one two and three
with X1 X2 X3
so you have your input layer you have
your hidden layers this might have
multiple layers depending on what you're
working on
then you have your output layer which in
this case we have two outputs y1 and Y2
and you can also see the connectivity
here so everything in the first row
connects with everything in the second
row in the hidden layer and if you had
another hidden layer everything in the
first hidden layer would connect to the
second hidden layer and then everything
in the second hidden layer would connect
to each of the outputs and then
calculate calculations based on that and
this is interesting because there's so
many different aspects of neural
networks nowadays that are changing this
basic configuration
they find that if you skip a hidden
layer or two with your input going
through that that actually changes the
results and works better in some cases
there's also convolutional neural
networks which look at windows and
adding up the numbers in them there's a
lot of complexity when we start getting
into the different aspects of what you
can do and how you build neural networks
and again it's very very much in an
infant stage so this basic diagram does
a great job of capturing what it looks
like now the input layer is responsible
to accept the inputs in various formats
the hidden layers again there's that s
could be multiple layers it's
responsible for extracting features and
hidden patterns from the data and you
know a hidden patterns and features is
important we want to look at features
you usually talk about features as your
input you have input one is one feature
input two is another feature if you're
looking at the iris data it might be the
width and length of the paddle each one
of those would be a feature you have
these nodes which generate a number that
doesn't really have a specific
representation but it becomes a way of
looking at it of the adding the features
together and creating an important value
there and the output layer produces the
desired output after completing the
entire processing of the data what is a
perceptron a perceptron is a binary
classification algorithm proposed by
Frank rosenblatt and you'll see here we
have our constants come in and our
inputs we have a constant one for the
bias the bias is important you can go
back to euclidean Geometry where you
have a line Y equals MX plus b b being
the y-intercept that's what that
constant is is there needs to be some
kind of adjustment that basically is the
y-intercept
and you have your inputs you have your
weights you have your weighted sum your
activation function and an output of 0
or 1.
and so here we have we'll go ahead and
walk through these we have X1 X2 X3 are
the inputs W naught W1 W2 W3 are the
weights weights are values that
determine the strength of the connection
between two neurons
and if we go back to this slide let me
just flash back here to this slide you
can see how each one of these nodes has
multiple inputs so when you hit look at
the hidden layer of the outputs they
have multiple inputs so each one of
these nodes has these inputs they might
be the original features coming in they
might be the layer of nodes before so we
have your X1 X2 and X3 are the input
into your node your weights are the
weighted values that determine the
strength of the connection between two
neurons
the input neurons are multiplied with
the weight and a bias is added there's
that one which is weighted the
y-intercept in euclidean geometry
to give the resulted weight sum
the activation function applies a step
to check if the output of the weighting
function is greater than zero or not
there's a lot of ways to do an
activation function but this is the most
common or the most not common but the
most easy to see way of doing an
activation on here
so we look at here we go another glocken
back through this diagram and taking a
closer look at it we have the predicted
output is compared with the actual
output
so once you go through the step and it
adds everything together in there and
these are your weights are multiplied
they're just multiples so you have
feature of X1 times weight of one plus
feature of X2 times weight of two so on
plus the weight times the bias and we go
ahead and compute all those all the way
through the node comes out and we have
the actual output and then we go ahead
and have a predicted output what do we
think it's going to be
and this is on each note so keep in mind
we're 0 in and on the Node this isn't
the whole process because this actually
goes through all the notes but there's a
there's another step in here when you
get to that part
the error in the network is estimated
using the cost function and back
propagation technique is used to improve
the performance
so when we look at the uh in the back
propagation algorithm the cost function
is minimized by changing weights and
biases in the network and you can see
here we have our input layer we have our
hidden layers and we have our output
layer and so you could think of this as
we have our actual output we predict
what it's going to be in this case we
have two outputs and we might predict
that it's either nothing there
or a raccoon so one of them comes up and
says I don't see any kind of animal and
the other one says this is a raccoon so
it's either yes or no and if it gets it
wrong it says that's wrong and it sends
that error back and that error goes
through the first set of Weights which
then go to the hidden layers and their
set of Weights which goes back to B1 the
other layer and their set of weights and
it adjusts those weights as it goes
backwards and it says hey this is the
error on the output we kind of adjusted
a little bit the part that we don't
adjust in the first set of Weights we
say there's still an error so we send it
to the second set of weights and so
forth what happens is as we adjust these
weights each one of these nodes
starts creating kind of a category or
something to look for in whatever image
or data we have going in
and so for making a better predictions a
number of epics are executed where the
error is determined by the cost function
now epic is a important thing to keep
track of epics is if you have a large or
any data set and let's say you've split
out your test date and your training
data set you're running your data
through how many times do you have to
run all of that data through
before you start getting something that
is usable until the error goes down to
you can't adjust the error anymore it's
the lowest error you can get so each
time you go through a full set of data
before you repeat and go through the
same data that's called an epic
the error is backward propagated until a
sufficiently small error is achieved
and again that's what we're talking
about we're trying to minimize the error
so we get to a point where that error
really isn't changing anymore you just
have the same error coming out
and there's other ways to weight that
error too
the cost function or loss function
measures the accuracy of the network the
cost functions tries to penalize the
network when it makes errors
and you can see here we have a formula
for the cost function C equals one half
of the Y predicted minus the Y the
actual y squared and of course the
squared value removes the sign because
we don't know whether it's plus or minus
when you look at an error and then the C
this is your cost function how much is
it going to cost how much of an error do
we really have that we're sending back
and so we have with the cost function we
can look at this we can look at it as a
loss in the Epic again that's every time
we go through the data that's an epic
how many epic runs are we going to go
and so we have a nice graph here that
shows like a very high learning rate low
learning rate different data is going to
change depending on what you're working
on they put the yellow as a good
learning rate because it has a nice
slope to it and you think yeah the more
epics that go in the lower the loss so
that means you're going in a good
direction there
and as you curves down it gets to the
the best answer has the lowest loss on
there
so uh looking at types of neural
networks we have the Gan we have the dbn
the rbfn the auto encoder the RNN the
lstm this is a flash of some of the main
ones that are out there there are now so
many variations that there's variations
on the variations
so just quickly jumping into these as
you can see we have a number of them
here listed and these are just like a
flash of some of the more common ones
like the Gan General adversarial Network
where you have two different models
competing against each other until they
find which one can beat the other one
can I think of a chess game where you
keep flipping who's in charge
and then we might look at the dbn
which is a deep belief Network
and they're used to recognize clusters
and generate image video sequences
generally
and we have the RBF Network rbfn which
is your radial basis function Network
which uses a different set of activation
functions
to figure out which is the right weights
Auto encoder the auto encoder is a
little different than the other ones in
that it looks for an error but the error
is based in finding data that groups
together so it's a way of sorting the
data out without knowing the answer
that's what an auto encoder does
RNN RNN has a couple different
definitions most of the time you now see
it as a recurrent neural network where
the output part of the output goes back
into the input so they call it this was
recurrent there's also another RNN which
deals with learning step by step as
opposed to over a set of data and
waiting it as you go
and there's the
lstm or long short-term memory recurrent
neural network it's also an RNN which
deals with how do you sort something
like a sentence out where the last word
depends on what the first word was and
so it slowly Waits things as it goes
through to figure out what's being said
again these are just a quick Flash and
even as I was describing them you should
have been like well why don't you hook
an auto encoder into again and then run
that with a dbn well there are tools to
go ahead and mix and match these things
so you will actually see uh when you
start doing layers you might have the
layers of one type of neural network
feed into the next neural network and
that's actually pretty common
and we talk about deep learning
libraries there's a lot of different
things out there but the big ones that
they usually talk about and most of
these are dealing with larger data is
like tensorflow Karas cross and
tensorflow played nice with each other
and cross kind of is is usually
integrated with tensorflow Cafe Fino
pytorch DL for J these are all different
deep learning libraries and there's many
more out there there's a side kit has a
deep learning library in it under python
Scala has one that they've been building
on there so these aren't the only ones
tensorflow is probably the most robust
one at this time but that's not to say
the other ones aren't catching up and
there's not all kinds of other stuff out
there who their learner simply learn
brings you Masters program in artificial
intelligence created in collaboration
with IBM to learn more about this course
you can find the course Link in the
description box below now going through
and talking about these things is all
fun but until you get to roll up your
sleeves and take a look at the code and
see what's going on it's really hard to
see what we're talking about so we're
going to do a deep learning demo on text
classification now to do this there's a
lot of different editors you can use for
python
currently my favorite my favorites
always one I'm currently using which
changes from month to month or year to
year depending what projects we're doing
we want to look at
Anaconda Navigator which is a nice job
building your different packages and
environments and go into Jupiter
notebook and so we'll go and go into
jupyter notebook and create our
uh are set up in there to run a neural
network
and so we open up our Jupiter we'll go
ahead and create a new Python 3
let's bring this out to the top
there we go uh so now we already have
our Jupiter 3 and I like to always give
it a setup on here so this is the text
classification we're doing today
we'll go ahead and rename that
and we were talking about if you
remember from the beginning of the thing
I said some of the the more the most one
of the most robust packages out there
right now is tensorflow along with Keras
which usually works nicely with
tensorflow so we're going to go ahead
and be working with the tensorflow and
the cross
we want to go ahead and do a number of
imports in here
uh this is just set up on this a lot of
this really isn't necessary for what
we're going to do there's different
things we can do so we're going to go
ahead and from future we're going to
import absolute import division print
function
these really don't do a lot other than
they help us kind of display things
later on you don't really need them you
can just do some basic print on this if
you want to so I'm not going to dig too
deeply into that setup
but we do want to take a look at
iteration tools
because we're always iterating over
things again you don't really need the
iteration tools because a lot of
tensorflow will do this for you
um but it's a lot of times when we're
building these packages we don't think
about it we just bring in all our tools
that we might need the big ones though
is our pandasdb and let me switch to a
draw thing it's always nice to kind of
let me give us arrows here uh
there we go
uh so we're looking at you can see here
here's our pandas pandas sits on top of
uh numpy our number array appendez is
our data frame just makes it really easy
to bring in the data view the data and
kind of work with the file that's what
these two are for and of course our
matplot library up here is for
displaying it and then we have the
sklearn these are for doing metrics and
pre-processing some of this you can
actually pull off of tensorflow I tend
to bounce back and forth between the
sklearn setup and by the way the SK
learning has its own neural network a
real simplified one compared to what
tensorflow does but we want to go ahead
and do this in tensorflow where we're
going to be using tensorflow cross and
cross is nice because it sits on top of
tensorflow
but it's easier to use it's a little bit
like you can think of tensorflow as
being the back end which you can program
in in Cross being a higher level
language which makes it easier to do
things in
and you can see here we went ahead and
printed out uh TF version we're using
tensorflow as a back end and you have
the tensorflow version 2.1.0 always
important to double check your versions
you never know what's not going to work
with what version of python and so forth
so it's important to check those this is
python36
I don't know if tensorflow is fully
integrated with three seven or three
eight yet I'm sure if it isn't it soon
will be
and so we're going to go ahead and bring
some data in this is
consumercomplaints.csv and if you want a
copy of this file just for your own to
play with you can put a note send a note
to Simply learn and they'll be happy to
send you a copy of that and then DF
stands for data frame that's very common
and we're using the pandas to go ahead
and read this
comma separated variable file in
and we'll go ahead and print what they
called the head if you've never worked
with data frames usually when you see
head it means the first five rows of
that data frame so you can see what's
going on in there
and give it just a moment to read that
in
there it goes and you'll see in here
that we have a date received product
mortgage credit reporting consumer loan
credit card sub product other mortgage
Nan Nan whatever that means whatever you
know it's just missing data right there
there's nothing in there vehicle load
and so forth uh then of course different
columns and this is a data Frame data
frame has rows and columns
and a lot of times when you're
processing data frames you can think of
mapping mapping reduce as a terminology
we're mapping we're either mapping each
of the rows like you might be running a
process on each row or you're running a
process on each column
and then one of the most common things
would be say to some parts of the row
together and have a total value of cost
or maybe some the total uh you may come
over here wherever I'm sure there's a
value in here
complaint ID well they don't actually
have a value but it'd be something you
would process by column you might want
to find out how many different companies
are listed here so that might be
something you'd run on the column
uh so that gives you an idea we have our
data frame DF head on there
and if you remember we're looking at
text classification so that makes sense
that we don't have any dollar values in
there and we're going to look at just a
couple of these columns certainly you
can do this
with a number of different objects on
here but we're going to take just the
columns Consumer complaint narrative
and the product column
and in this case when we handle null
values there's a lot of different ways
to handle null values but in this case
we're just going to go ahead and do just
the not null we only want those that are
not null on the Consumer complaint
narrative
and let's go ahead and print that do the
DF head
in Jupiter
the last line if you just have a
variable it automatically prints it out
so you could put print and put brackets
around DF head and do the same thing
and you can see here we have our
Consumer complaint narrative I have
outdated information on my credit report
and then you have your credit reporting
and so forth on each one of these an
account on my credit card was mistaken
the company refuses to provide me
verification the complaint regards to
square two something so we're going to
be looking at these complaint narratives
and trying to understand them
and can we write a code to
um optimize that
and then one of the things we always do
like up here you'll see where we already
removed the null values I would go ahead
and just double check
sum up the null values are there any
null values in our data frame and this
is just a simple way of looking at every
cell this includes Consumer complaint
the product and so forth
and you can see here we have
zero null value so that's good we're not
we're not going to play with the null
values today and then we can also go
ahead and take a look at our end
aspect we might be looking for which is
our product we have credit reporting
consumer loan credit reporting debt
collection debt collection let's go
ahead and just take a look at this
and see how many counts we have and you
can see here that there is under debt
collection forty seven thousand entries
under mortgage there's 36 000 entries
under credit card we have over 18 000
entries so this is a pretty big database
and you can see it going down all the
way down here with the different
entries that go underneath of product so
we have and one of the things we don't
spend a lot of time on in some of these
demos is what is really defining what
we're looking for
now if you're doing a data science
project usually start by exploring the
data and then you define what you're
really looking for and so we're kind of
skipping through that really that
particular process it does constitute a
small amount of time but as far as the
importance it's one of the most
important things you can do in data
science is ask the right questions so
even though you're spending 80 percent
of the time cleaning data building your
models and everything that 20 percent of
asking the right questions has a higher
impact and so you really should be
making sure that you understand what the
questions are that's domain knowledge so
we're talking in this case Banking and
so in this case it might be that as
Consumer complaint comes in
we can start looking at these consumer
complaints and what product they're
attached to and what that means
we're not going to dig too much into the
domain in asking the question what you
know what what exactly are we looking
for we really want to look into the
process
um as far as building a neural network
and so the first thing I'm going to do
is go ahead and split our data we need a
train set and a test set the train size
in this case we're going to do 80 of the
data is going to be for training our
neural network
and then we'll go ahead and use the
we'll switch that over for the test size
to be the 20 let me go ahead and run
that
and you can see here train size has 15
159 000 entries and then we'll test that
on we've held out roughly 40 000 of
those entries for our test size
for this exam
we're just going to split it
the first part of the data will be for
training our data and the second part
will be for testing our data so we're
not really doing a random setup in here
which is okay because this is an example
A lot of times you might split this one
of the things I do with neural networks
is I will split it into three sections
and I will test
two is training and the third as the
test this is obviously not big data you
don't want to do this on something that
might take days to process
and then I flip it and then so I'll
write it three times and those three
times will give me a nice Narrative of
how good my model is and then I'll
actually run the the final product on
all three sections to program it to
train it
so it gives me a good basis of what kind
of error I have versus you know on test
models while having a very robust model
to publish
in this case though we're just going to
go ahead and split it based on the first
part goes to the trade and the second
part goes to the test
so one of the next things whenever we
deal with text and this is such an
important line I want to go ahead and
just highlight the tokenize you'll see
the word tokenize
tokenizer setting it up
we're taking the words and putting them
into a format the computer can
understand
there is a lot of ways to tokenize words
in some cases you call them zero one two
three four
depending on what you're doing it might
just be a zero or one
um so when we talk about the encoder
we're usually talking about as far as
tokenize
we actually are looking at in this
particular case we'll be looking at each
word as its own feature uh so when we
looked up here remember right up here
let's see here we go money transfer let
me let me go back up just a note here
turn that off so I can go back up if you
remember up here we have this right here
Consumer complaint narrative so I would
be a feature have would be a feature
outdated would be a feature information
would be a feature on feature my feature
credit feature report and you think wow
that's a lot of features uh yes in
running through bills put out by the
United States that are being voted on in
this in the government
it comes out roughly 2.4 million
different words are used in those bills
that's a lot of features and so we're
going to go ahead and look at uh Max
words tokenize and tokenizing words
there's a lot that is happening here in
the tokenize setup so we'll go ahead and
let me just
put together some of this code here
so the tokenize is an actual object in
here text tokenizer and it has number of
words equals Max words so we're going to
limit it to a thousand words character
level equals false fit on text train
narrative only fit on the training data
and so this is kind of interesting
because it drops a lot of these words
why would it drop a word well on on is
probably used a bunch and really doesn't
have any value so that would be one of
the words they'll probably drop
and then also there's other things it
can do like it can also combine
combinations of word it might be that
this company this complaint these might
be always together so at some point it
might actually bundle some of these
words as a single feature there's a lot
of things that go into that we're not
going to go into too much detail because
you really have to go through the API on
these to understand all the different
encoding and setups you have
this is just a really fast way to do
this and it works
I like easy and I like things that work
I don't know about you but what I'm
running through and doing a lot of
different models
I might start tweaking these once I have
an answer
but until then
we want to go ahead and run the encoder
and do something simple like this and so
using SK learn utility to convert the
label strings to number index and so
here's our encoder label encoder encoder
fit and this is sklearn of course
everything's fit
and then we do y train equals the
encoder transform train product y test
equals encoder transform test product
now what's going on here now up here we
did the X where we have
let me view up here where we were
looking at
here we go I have outdated information
on my credit card report now we're
looking at this
credit reporting consumer loan credit
reporting and if you remember this is
our list of those debt collection here's
our list of them they're not a huge
number
and so we're going to encode these
differently this is just 0 1 2 3 4 and
so on not necessarily in that order
order by the way so be a little careful
on order so we're going to convert the
labels to one hot representation as our
next step and that's what this is doing
so we have our number classes in pmax y
train plus one our weight y train equals
utilities to categorical
so here is where we're taking our y
value in the training set and we do the
same with the test set
and we go ahead and run that so now
we've created a y train and a y test
where we've numbered our categorical
we've created a categorical data so it's
easy to read the answer and translate it
back and we've encoded
up here our X and we used a tokenizer
so a little different encoder puts in 0
1 2 3 for the different listings an
encoder or token
you can get Tongue Tied on these a
tokenizer takes and creates a huge in
this case we've limited to 1000 words
each word is its own token and to really
see what we're looking at let me go
ahead and we're going to inspect the
elements and see what we ended up with
let me run this and so we have our y
train shape there's our 1000 where does
that one thousand maximum words so we've
tokenized the top thousand words
as each as its own feature
and the same thing with the X test so
those are X train and X test we have our
y train shape and our y train test shape
and I thought the encoder depending on
how you set it
um either to 0 1 2 3 or in this case it
actually puts it out as 18. so it did
the same thing as a tokenizer as far as
putting it as a zero one so you have 18
choices there and if we count these I'm
guessing there's 18 there now this is
the part which is we look at the
encoders and the tokenizers
these are the tools you need to process
text
the computer doesn't see the hello on
you have to give it a zero or one in
each one of those and so this is all
about the text classification part
this is how we classify text is we have
to give it something that has a number
representation so once we've sorted out
our data and we've converted it into
something the computer can read for
doing text we want to go ahead and
create our model
and when we create our model one of the
things to be aware of is this is what we
call a black box model now they've come
a long ways in understanding how these
different processes are created and so
they're starting to understand how you
can put these together and go back and
say why why does it pick this why does
it pick that how does it balance that
but it's black boxing that it's really
hard to do it's really hard to go in
there and figure out why it picks one
over the other just by looking at the
neural network itself
there are other machine learning tools
like decision tree which make it much
easier to see those changes and how it
branches down but they don't perform as
well in many cases
and so we're going to go ahead and build
a model we'll go ahead and run this just
because it builds the model doesn't
actually start fitting it yet let's take
a look at these different pieces so we
have our model which is going to be
sequential that's a cross so we imported
it at the beginning from the cross setup
this tells us what's going on that we're
going to be running this from top to
bottom
and we're going to add a dense layer so
each time you see add let me do we look
at these ad each one of these
we're adding a row
so these are all rows
and then of course our rows you're like
what is a row so we talk about row these
are your layers we have you can see
right there input layer we're going to
add an activation to this layer and
we're going to use the ray Lou
activation then we're going to add a
dropout rate and what this does is they
found that when they process a neural
network instead of processing every
neuron each time you do the back
propagation to train it
you only process some of them so only
some of them are being trained by doing
that it's able to create the
differentiation between different
categories and it actually trains much
better instead of trying to train them
all at once
so that's what this is right here with
the Dropout 0.5 and then we added add
dense number classes
and the dense number classes is another
layer so up here we have our add an
activation Rayleigh layer so we have our
input our relu layer with a drop out of
0.5 then we have a dense layer
which uses a soft Max activation there
is a lot going on with Karas and these
models you can get lost in just the
activation here's our two activation
relu is one type of activation softmax
is another
they work differently and you can see
when we were talking about earlier we
talked about the different kinds of
neural networks
in Cross and tensorflow each of those
layers can be a different neural network
layer and can function differently and
you can stack them on top of each other
and feed them into each other
and then the final thing of course is
your compile we have what we're using
for loss
remember we want to minimize loss so
category this is a type of way of
minimizing that loss
atom is an optimizer
you'll find there's a number of
Optimizer atom is used for larger data
sets as you get to smaller data sets you
actually use a very different Optimizer
in here and we look at the accuracy
that's just when do we stop compiling
this data how far do we go until it
starts building what they call a bias it
starts it's overfitted we want to stop
at the right moment
and then finally we get to actually
training our model this is that black
box we're going to fit it to the data we
don't know what's actually going on in
there but we want to go ahead and run it
and I'm going to go ahead and start it
running because it takes a moment for it
to run through and you'll see the Epic
feed down here as it goes through epic
105 and so forth we'll freeze this just
for a second there we go so let's take a
look at this we have batch size batch
size Oops I meant to do that in an arrow
there we go uh batch size is how many
rows of data are we going to feed at a
time
so you can feed larger rows there's
there's different reasons to feed them
at different sizes
um
really a lot of times people just leave
this as a default depending and let it
choose for them
I'm not sure why they picked 32 in this
case there's probably when they were
messing with this 32 probably was a good
batch size for fitting it
back in math deals with differential
equations in some calculus which we
won't get into
uh so being aware of that that this
batch size affects how it does that that
back-end differential equation in the
reverse propagation
um tells us that this is actually a
pretty important number if you get it
too big it's going to not it's not going
to fit as well and if you get it too
small it takes way too long to process
and a lot of times uh there's what they
call a reinforced learning neural
network where it's batch size of one
what does that mean well that means
every action you take has a feedback you
program the neural network so you can
guess what your next action is that's
very common like in trying to beat video
games with a automated setup in a neural
network the Epic says we're going to go
through all the data in the training set
five times so that's what this remember
we talked about epics that's what the
Epic is
and we can see when I let it go out here
we're on Epic 2.5
so we'll go ahead and pause this I'm
going to go ahead and pause it for a
second let it finish running
or while we're waiting we can actually
take a look at some of this data here
and see what it's actually generating
for us
and so we look at this accuracy
loss so we want to minimize loss the
metrics is accuracy and you can see that
we're going to want the accuracy to go
up and we want the loss to go down and
the loss is going to be what we want to
minimize and this is just some of the
metrics and it talks about like value
loss
value accuracy and how they're changing
with each time we run it and so you get
to a point where it no longer gets
better or worse and at that point you
really want to stop running it
you can overfit it and overfit it means
it's going to miss some of the
generalities that you need when solving
some of these Solutions
and then I did talk too much about
what's actually going on here what are
we doing in the domain of this
particular one which it looks like it is
trying to figure out based on a Consumer
complaint narrative maybe they send in a
complaint
what product is it connected to so maybe
they get the complaints before the
product or something like that I I'm not
sure why you do this particular setup uh
that would be again a domain knowledge
in here so what we're doing is we're
using the Consumer complaint narrative
to predict what product they're talking
about if someone comes sends in an email
and says I have outdated information on
my credit report they're probably
talking about a credit card reporting if
you get a random email that says I
purchased a new car on blank the car
something something probably alone
that's what we're trying to do is if you
get a random input from the Consumer
complaint narrative you can point to the
product that they're referring to
without having to call them up and ask
them I guess
it might be useful not sure now that
we've gone through all five epics
let's go ahead and evaluate the accuracy
of our trained model and so we have
we're going to go ahead and do a score
for the model.evaluate a nice caress
setup where we can do the X test and the
Y test the batch size verbose and then
we're gonna it's gonna generate a score
and one of the things let's just go
ahead and print
so you can see why they broke out the
score let me just go ahead and print the
score in here let's go ahead and run
that
and so it's testing the score it's going
to take it a moment to go through all
the data and it gives us a nice score it
says the test accuracy these can mean a
lot of different things but we have a
0.5 to be honest without looking at the
data I would have to look and see
exactly which things it missed on and
how it how it scored on there but it's
it's getting about you know half of it
it's able to pull in half of it and say
hey if this is the complaint this is
what it's connected to
keep in mind when we're talking about
text that's really good can you imagine
some of the stuff I don't know if you've
ever worked in tech support or it or at
a counter in a in the mall or even in
taking order someplace as a waiter or
waitress or fast food or whatever
when you try to understand people it
gets pretty crazy so even an accuracy
score like this is probably pretty good
for understanding some of this text and
there might be steps you could do to
improve that and so let's go ahead and
go through here and look at actually
using it
and we'll punch this in it says how to
generate prediction on individual
examples we have our text labels and our
encoder class
uh and we'll just go through a
prediction equals model predict NP array
of X test of I
predict label text labels prediction
print test narrative so forth and this
is just let's go ahead and run it
because it's reading through print
statements can be very painful sometimes
unless you actually see what's going on
so when the president came out with the
HARP program dot dot dot the actual
label was mortgage the predicted label
was mortgage so when this person sent
this this complaint in maybe you don't
know what it's connected to yet you can
guess it's probably a mortgage I filed a
dispute with Capital One Bank on dot dot
dot dot and it says oh credit card well
it turns out it's actually debt
collection it was what it predicted
um so yeah missed a little bit uh if a
lot of times when you dispute something
it probably is a debt collection in this
case it was specific to a credit card
okay so we missed that one
I am disputing account number xxs with
Midland whatever actual test label debt
collection was debt collection I opened
a Barclay card on to help rebuild my
credit credit card credit card mortgage
mortgage so forth we can go through all
of these and you can see it does a
pretty good job it gets close or in fact
most of these it pulled in correctly
credit card reporting
this is a lot of what we talk about with
text
setup Cinnamon's a really big one
there's a whole sentimental libraries
out there uh whether someone is positive
or negative towards something if you're
pulling off Twitter feeds you might want
to look that up you might want to look
at connection towards postings in what
was it there was running stock I
actually do some some stock programming
code so I end up coming back to that a
lot
finding sentiment feeds on different
stock values and different stocks out
there on out of national Publications
really helps trying to predict what the
Stock's going to do what are people
going to do in buying and selling stock
same thing with this we can now predict
uh their complaints and try to figure
out what it is they're complaining about
hey there learner simple and brings you
Masters program in artificial
intelligence created in collaboration
with IBM to learn more about this course
you can find the course Link in the
description box below
that wraps up our deep learning demo on
text classification
and gives you a nice introduction
as opposed to shallow learning
bad joke on my part
but you can see how a deep learning
model can go in and do a lot of things
that you might not be able to do using
basic linear regression or many of the
other machine learning models
and text classification is one of those
where neural networks really shine
because they can pick up things that you
don't see you can't really measure in
other ways so what is a neural network
so hi guys I heard you want to know what
a neural network is here we have looks
like you just went shopping at a red tag
cell my robot's back so as a matter of
fact you have been using neural network
on a daily basis in today's world is
just amazing how much we use our new
technology we're not even aware of it
when you ask your Mobile Assistant to
perform a search for you you know like
saying you're Google or Siri or whoever
you use Amazon web self-driving cars so
that's the newest thing coming out
they're just now trying to make those
legal in different states in the US and
around the world even in the UK they now
have a self-driving cars going up and
down the street it's pretty amazing
these are all neural network driven
computer games use it a lot of computer
games are driven by neural networks in
the back end as part of the game system
and how it adjusts to the players and
it's also used in processing the map
images on your phone so every time you
do a navigation someplace and it opens
it up they now use neural networks to
help you find the quickest way to get
there neural network a neural network is
a system or Hardware that is designed to
operate like a human brain in today's
development this is so important to
understand because we don't have
anything else to compare it to I'm sure
someday in the future the computer will
redefine or the neural network or the AI
artificial intelligence will redefine
what these mean but as far as we can
today's world in today's commercial
development we have to compare it to
what humans do so it's we want to
compare and how it operates to a human
brain and how it solves problems like a
human does what can a neural network do
and really we're just going to dive in
deeper to what we just covered and look
at other examples so what can a neural
network do well let's list out the
things neural networks can do for you
translate text boy we got Google
translate and Microsoft has their own
translate they they have some really
cool they actually have an earpiece it's
supposed to start translating as you
talk what a cool technology what a cool
time to live identify faces can you
imagine all the uses for facial
identification in the case of our sample
or our code that we're going to look at
later we'll be identifying dogs and cats
so not quite as detailed as
understanding whose face belongs to who
I'm waiting for the Google Glasses to
come out so I can see who's who in the
identify faces as I'm walking around
have a little name tag over them not out
there yet but boy we are close we could
identify the faces and they have all
kinds of Technologies to bring that
information back to us recognize speech
goes along with the translate text so
now as you're talking into your
assistant it can use that to do commands
turn lights on all kinds of things you
can do with recognizing speech read had
written text they're starting to
translate all these old text documents
that they've had in storage instead of
doing it individually where somebody's
going through each text by themselves in
a room you can picture like an old
Raiders of the Lost Ark theme always in
the back you know archaeologists
studying the text now it's fed into a
computer they take a picture they even
use neural networks to take a scroll
that is so messed up that they can't
undo the scroll and they x-ray it and
then they use that x-ray to translate
the text off of it without ever opening
the scroll I mean just way cool stuff
they're starting to do with all this and
of course control robots what would be a
neural network without bringing in the
robots and we have our own favorite
robot in the middle who goes to our red
tag cell and go shopping for us so you
know these are just a few of the
wonderful things that neural networks
are being applied to it's such an infant
stage technology what a wonderful time
to jump in and there are a lot of other
things it goes into I mean we could
spend just forever talking about all the
different applications from business to
whatever you could even imagine they're
now applying neural networks to help us
understand so now we've talked a little
bit about all the cool things you can do
with the neural network let's dive in
and say how does a neural network work
so now we've come far enough to
understand and how neural network works
let's go ahead and walk through this in
a nice graphical representation they
usually describe a neural network as
having different layers and you'll see
that we've identified a Green Layer an
orange layer and a red layer the Green
Layer is the input so you have your data
coming in it picks up the input signals
and passes them to the next layer the
next layer does all kinds of
calculations and feature extraction it's
called The Hidden layer a lot of times
there's more than one hidden layer we're
only showing one in this picture but
we'll show you how it looks like in a
more detail a little bit and then
finally we have an output layer this
layer delivers the final result so the
only two things we see is the input
layer and the output layer now let's
make use of this neural network and see
how it works wonder how traffic cameras
identify Vehicles registration plate on
the road to detect speeding vehicles and
those breaking the law that got me going
through a red light the other day well
last month that's like the horrible
thing they send you this picture of you
and all your information because they
pulled it up off of your license plate
and your picture I should have gone
through the red light so here we are and
we have an image of a car and you can
see the license plates on there so let's
consider the image of this vehicle and
find out what's on the number plate the
picture itself is 28 by 28 pixels and
the image is fed as an input to identify
the registration plate each neuron has a
number called activation that represents
the grayscale value of the corresponding
pixel range and we range it from zero to
one one for a white pixel and zero for a
black pixel and you can see down here we
have an example where one of the pixels
is registered as like 0.82 meaning it's
probably pretty dark each neuron is lit
up when its activation is close to one
so as we get closer to black on white we
can really start seeing the details in
there and you can see again the pixel
shows this one up there it's like part
of the car and so it lights up so pixels
in the form of arrays are fed to the
input layer and so we see here the pixel
of a car image fed as an input and
you're going to see that the input layer
which is green is one dimension while
our image is two Dimension now when we
look at our setup that we're programming
in Python it has a cool feature that
automatically does the work for us if
you're working with an older neural
network pattern package you then convert
each one of those rows so it's all one
array so you'd have like Row one and
then just tack row two onto the end you
can almost feed the image directly into
some of these neural networks the key is
though is that if you're using a 28 by
28 and you get a picture of this 30 by
30 shrink the 30 by 30 down to fit the
28 by 28 so you can't increase the
number of input in this case Green Dots
it's very important to remember when you
work on neural networks and let's name
the inputs x y and X2 X3 respectively so
each one of those represents one of the
pixels coming in and the input layer
passes it to the hidden layer and you
can see here we now have two hidden
layers in this image in the orange and
each one of those pixels connects to
each one of those hidden layers and the
inter connections are assigned weights
at random so they get these random
weights that come through if x one
lights up then it's going to be X1 times
this weight going into the hidden layer
and we sum those weights the weights are
multiplied with the input signal and a
bias is added to all of them so as you
can see here we have X1 comes in and it
actually goes to all the different
hidden layer nodes or in this case
whatever you want to call them network
setup the orange dots and so you take
the value of X1 you multiply it by the
weight for the next hidden layer so X1
goes to Hidden layer one X1 goes to
Hidden Layer Two X1 goes hidden layer 1
node two hidden layer 1 node 3 and so on
and the bias a lot of times they just
put the bias in as like another Green
Dot or another orange Dot and they give
the bias a value one and then all the
weights go in from the bias into the
next node so the bias can change we
always just remember that you need to
have that bias in there there's things
that can be done with it generally most
of packages out there control that for
you so you don't have to worry about
figuring out what the bias is is but if
you ever dive deep into neural networks
you've got to remember there's a bias or
the answer won't come out correctly the
weighted sum of the input is fed as an
input to the activation function to
decide which nodes to Fire and for
feature extraction as a signal flows
within the hidden layers the weighted
sum of inputs is calculated and is fed
to the activation function in each layer
to decide which nodes to fire so here's
our feature extraction of the number
plate and you can see these are still
hidden nodes in the middle and this
becomes important we're going to take a
little detour here and look at the
activation function so we're going to
dive just a little bit into the math so
you can start to understand where some
of the games go on when you're playing
with neural networks in your programming
so let's look at the different
activation functions before we move
ahead here's our friendly red tag
shopping robot and so one is a sigmoid
function and the sigmoid function which
is 1 over 1 plus e to the minus X takes
the x value and you can see where it
generates almost a zero and almost a one
with a very small area in the Middle
where it crosses is over and we can use
that value to feed into another function
so if it's really uncertain it might
have a 0.1 or 0.2 or 0.3 but for the
most part it's going to be really close
to 1 and really close to this case 0 0
to 1. the threshold function so if you
don't want to worry about the
uncertainty in the middle you just say
oh if x is greater than or equal to 0 if
not then X is 0. so it's either 0 or 1.
really straightforward there's no in
between in the middle and then you have
the what they call the relu function and
you can see here where it puts out the
value but then it says well if it's over
1 it's going to be 1. and if it's less
than zero at zero so it kind of just
dead ends it on those two ends but
allows all the values in the middle and
again this like the sigmoid function
allows that information to go to the
next level so it might be important to
know if it's a 0.1 or a minus 0.1 the
next hidden layer might pick that up and
say oh this piece of information is
uncertain or this value has a very low
certainty to it and then the hyperbolic
tangent function and you can see here
it's a 1 minus E to the minus 2x over 1
plus e to the minus 2X and it's very
much along the same theme a little bit
different in here and that it goes
between minus one and one so you'll see
some of these that goes zero to one but
this one goes minus one to one and if
it's less than zero it's you know it
doesn't fire and if it's over zero it
fires and it also still puts out a value
so you still have a value that you can
get off of that just like you can with
the sigmoid function and the relu
function very similar in use and I
believe the originally used to be
everything was done in the sigmoid
function that was the most commonly used
and now they just kind of use more the
relu function the reason is one it
processes faster because you already
have the value and you don't have to add
another compute the one over one plus e
to the minus X for each hidden node and
the data coming off works pretty good as
far as putting it into the next level if
you want to know just how close it is to
zero how close is it not to functioning
you know is it minus point one minus
point two usually their float value you
so you get like minus Point minus
0.00138 or something so you know
important information but the relay is
most commonly used these days as far as
the setup we're using but you'll also
see the sigmoid function very commonly
used also now that you know what an
activation function is let's get back to
the neural network so finally the model
would predict the outcome of applying a
suitable activation function to the
output layer so we go in here we look at
this we have the optical character
recognition OCR is used on the images to
convert it into a text in order to
identify what's written on the plate and
as it comes out you'll see the red node
and the red node might actually
represent just the letter A so there's
usually a lot of outputs when you're
doing text identification we're not
going to show that on here but you might
have it even in the order it might be
what order the license plates in so you
might have a b c d e f g you know the
alphabet plus the numbers and you might
have the one two three four five six
seven eight nine ten places so it's a
very large array that comes out it's not
a small amount of about you know we show
three dots coming in eight hidden layer
nodes you know two sets of four we just
saw one red coming out a lot of times
this is uh you know 28 times 28 if you
did 30 times 30 that's you know 900
nodes so 28 is a little bit less than
that uh just on the input and so you can
imagine the hidden layer is just as big
each hidden layer is just as big if not
bigger then the output is going to be
there's so many digits yeah it's a lot
it's a huge amount of input and output
but we're only showing you just you know
it would be hard to show in one picture
and so it comes up and this is what it
finally gets out on the output as it
identifies a number on the plate and in
this case we have 0 8
d-03858 error in the output is back
propagated through the network and
weights are adjusted to minimize the
error rate this is calculated by a cost
function when we're training our data
this is what's used and we'll look at
that in the code when we do the data
training so we have stuff we know the
answer to and then we put the
information through and it says yes that
was correct or no because remember we
randomly set all the weights to begin
with and if it's wrong we take that
error how far off are you you know are
you off but is it if it was like minus
one you're just a little bit off if it's
like minus 300 was your output remember
when we're looking at those different
options you know hyperbolic or whatever
and we're looking at the Rel the Rel
could doesn't have a limit on top or
bottom it actually just generates a
number so if it's a way off you have to
adjust those weights a lot but if it's
pretty close you might have just relates
just a little bit and you keep adjusting
the weights until they fit all the
different training models you put in so
you might have 500 training models and
those weights will adjust using the back
propagation it sends the error backward
the output is compared with the original
result and multiple iterations are done
to get the maximum accuracy so not only
does it look at each one but it goes
through it and just keeps cycling
through these the data and making small
changes in the network until it gets the
right answers with every iteration the
weights at every interconnection and are
adjusted based on the error we're not
going to dive into that math because it
is a differential equation and it gets a
little complicated but I will talk a
little bit about some of the different
options they have when we look at the
code so we've explored a neural network
let's look at the different types of
artificial neural networks and this is
like the biggest area growing is how
these all come together let's see the
different types of neural network and
again we're comparing this to human
learning so here's a human brain I feel
sorry for that poor guy so we have a
feed for forward neural network simplest
form of a they call it an a neural
network data travels only in One
Direction input to Output this is what
we just looked at so as the data comes
in all the weights are added it goes to
the hidden layer all the weights are
added it goes to the next hidden layer
all the weights are added and it goes to
the output the only time you use the
reverse propagation is to train it so
when you actually use it it's very fast
when you're training it it takes a while
because it has to iterate through all
your training data and you start getting
into Big Data because you can train
these with a huge amount of data the
more data you put in the better train
they get the applications vision and
speech recognition actually they're
pretty much everything we talked about a
lot of almost all of them use this form
of neural network at some level radio
basis function neural network this model
classifies the data point based on its
distance from a Center Point what that
means is that you might not have
training data so you want to group
things together and you create Central
points and it looks for all the things
you know some of these things are just
like the other if you've ever watched
the Sesame Street as a kid that dates me
so it brings things together and this is
a great way if you don't have the right
training model you can start finding
things that are connected you might not
have noticed before applications power
restoration systems they try to figure
out what's connected and then based on
that they can fix the problem if you
have a huge power system cajonin self
organizing neural network vectors of
random dimensions are input to discrete
map comprised of neurons so they
basically find a way to draw the column
they say Dimensions or vectors or planes
because they actually chop the data in
one dimension two Dimension three
dimension four five six they keep adding
dimensions and finding ways to separate
the data and connect different data
pieces together applications used to
recognize patterns in data like in
medical analysis the hidden layer saves
its output to be used for future
prediction recurrent neural networks so
the hidden layers remember its output
from last time and that becomes part of
its new input you might use that
especially in robotics or flying a drone
you want to know what your last change
was and how fast it was going to help
predict what your next change you need
to make is to get which where the Drone
wants to go applications text-to-speech
conversation model so I talked about
drones but you know just identifying on
Lexis or Google assistant or or any of
these they're starting to add in I'd
like to play a song on my Pandora and
I'd like it to be at volume 90 percent
so you now can add different things in
there and it connects them together the
input features are taken in batches like
a filter this allows a network to
remember an image in Parts convolution
neural network today's world in photo
identification and taking apart photos
and trying to you know have you ever
seen that on Google where you have five
people together this is the kind of
thing separates all those people so then
it can do a face recognition on each
person applications used in signal and
image processing in this case I use
facial images or Google picture images
as one of the options modular neural
network it has a collection of different
neural networks working together to get
the output so wow we just went through
all these different types of neural
networks and the final one is to put
multiple neural networks together I
mentioned that a little bit when we
separated people in a larger photo in
individuals in the photo and then do the
facial recognition on each person so one
network is used to separate them and the
next network is then used to figure out
who they are and do the facial
recognition applications still
undergoing research this is a Cutting
Edge you hear the term Pipeline and
there's actual in Python code and in
almost all the different neural network
setups out there they now have a
pipeline feature usually and it just
means you take the data from one neural
network and maybe another neural network
or you put it into the next neural
network and then you take three or four
other neural networks and feed them into
another one so how we connect the neural
networks is really just Cutting Edge and
it's so experimental I mean it's almost
creative in its nature there's not
really a science to it because each
specific domain has different things
it's looking at so if you're in the
banking domain it's going to be
different than the medical domain then
the automatic car domain and suddenly
figuring out how those all fit together
is just a lot of fun and really cool so
we have our types of artificial neural
network we have our feet forward neural
network we have a radial basis function
neural network we have our kohenen
self-organizing neural network recurrent
neural network convolution neural
network and modular neural network where
it brings them all together and know the
colors on the brain do not match what
your brain actually does but they do
bring it out that most of these were
developed by understanding how humans
learn and as we understand more and more
of how humans learn we can build
something in the computer industry to
mimic that to reflect that and that's
how these were developed so exciting
part use case problem statement so this
is where you jump in this is my favorite
part let's use a system to identify
between a cat and a dog if you remember
correctly I said we're going to do some
python code and you can see over here my
hair is kind of sticking up over the
computer a cup of coffee on one side and
then a little bit of old school a pencil
and a pen on the other side you know
most people Now take notes I love the
stickies on the computer that's great
that's that is my computer I have sticky
notes on my computer in different colors
so not too far from today's programmer
so the problem is is we want to classify
photos of cats and dogs using a neural
network and you can see over here we
have quite a variety of dogs in the
pictures and cats and you know just
sorting out it is a cat it's pretty
amazing and why would anybody want to
even know the difference between a cat
and a dog okay you know why well I have
a cat door it'd be kind of fun that
instead of it identifying instead of
having like a little collar with a
magnet on it which is what my cat has
the door would be able to see oh that's
the cat that's our cat coming in oh
that's the dog we have a dog too that's
a dog I want to let in maybe I don't
want to let this other animal in because
it's a raccoon so you can see where you
could take this one step further and
actually apply this you could actually
start a little startup company idea
self-identifying door so this use case
will be implemented on python I am
actually in Python 3.6 it's always nice
to tell people the version of python
because it does affect sometimes which
modules you load and everything and
we're going to start by importing the
required packages I told you we're going
to do this in Cross so we're going to
import from Karas models sequential from
the cross layers conversion 2D or conv
2D Max pooling 2D flatten and dense and
we'll talk about what each one of these
do in just a second but before we do
that let's talk a little bit about the
environment we're going to work in and
you know in fact let me go ahead and
open a the website cross's website so we
can learn a little bit more about Karas
so here we are on the cross website and
it's a k-e-r-a-s DOT IO that's the
official website for Karas and the first
thing you'll notice is that cross runs
on top of either tensorflow
cntk and I think it's pronounced thano
or theano what's important on here is it
tensorflow and the same is true for all
these but tensorflow is probably one of
the most widely used currently packages
out there with the cross and of course
you know tomorrow this is all going to
change it's all going to disappear and
they'll have something new out there so
make sure when you're learning this code
that you understand what's going on and
also know the code I mean look when you
look at the code it's not as complicated
once you understand what's going on the
code itself is pretty straightforward
and the reason we like Karas and the
reason that people are jumping on it
right now is such a big deal is if we
come down here let me just scroll down a
little bit we talk about user
friendliness modularity easy
extensibility work with python Python's
a big one because a lot of people in
data science now use Python although you
can actually access cross other ways as
if we continue down here is layers and
this is where it gets really cool when
we're working with Karas you just add
layers on remember those hidden layers
we're talking about and we talked about
the r e l u activation you can see right
here let me just up that a little bit in
size there we go that's big I can add in
an relu layer and then I can add in a
soft Max layer in the next instance we
didn't talk about softmax so you can do
each layer separate now if I'm working
in some of the other kits I use I take
that and I have one setup and then I
feed the output into the next one this
one I can just add hidden layer after
hidden layer with a different
information in it which makes it very
powerful and very fast to spin up and
try different setups and see how they
work with the data you're working on and
we'll dig a little bit deeper in here
and a lot of this is very much the same
so when we get to that part I'll point
that out to you also now just a quick
side note I'm using Anaconda with python
in it and I went ahead and created my
own package and I called it the cross
python 36 because I'm in python36
Anaconda is cool that way you can create
different environments real easily if
you're doing a lot of different
experimenting with these different
packages probably want to create your
own environment in there and the first
thing is you can see right here there's
a lot of dependencies a lot of these you
should recognize by now if you've done
any of these videos if not kudos for you
for jumping in today pip install numpy
scipy the sci kit learn pillow and h5py
are both need needed for the tensorflow
and then putting the cross on there and
then you'll see here in PIP is just a
standard installer that you use with
python you'll see here that we did pip
install tensorflow sensor we're going to
do Karas on top of tensorflow and then
pip install and I went ahead and used
the GitHub so git plus git and you'll
see here github.com this is one of their
releases one of the most current release
on there that goes on top of tensorflow
you can look up these instructions
pretty much anywhere this is for doing
it on Anaconda certainly you'd want to
install these if you're doing it in
Ubuntu server setup you'd want to get I
don't think you need the H5 py in Ubuntu
but you do need the rest in there
because they are dependencies in there
and it's pretty straightforward and
that's actually in some of the
instructions they have on their website
so you don't have to initially go
through this just remember their website
on there and then when I'm under my
anaconda Navigator which I like you'll
see where I have environments and on the
bottom I created a new environment and I
called it cross python36 just to
separate everything you can say I have
Python 30.5 and python 3C six I used to
have a bunch of other ones but a kind of
cleaned house recently and of course
once I go in here I can launch my
Jupiter notebook making sure I'm using
the right environment that I just set up
this of course opens up my in this case
I'm using Google Chrome and in here I
could go and just create a new document
in here and this is all in your browser
window when you use the Anaconda do you
have to use anaconda and Jupiter
notebook no you can use any kind of
python editor whatever setup you're
comfortable with and whatever you're
doing in there so let's go ahead and go
in here and paste the code in and we're
importing a number of different settings
in here we have import sequential it's
under the models because that's the
model we're going to use as far as our
neural network and then we have layers
and we have conversion 2D Max pooling 2D
flatten dense and you can actually just
kind of guess at what these do we're
talking we're working in a 2d photograph
and if you remember correctly I talked
about how the actual input layer is a
single array it's not in two Dimensions
it's one dimension all these do is these
are tools to help flatten the image so
it takes a two-dimensional image and
then it creates its own proper setup you
don't have to worry about any of that
you don't have to do anything special
with the photograph you let the cross do
it we're going to run this and you'll
see right here they have some stuff that
is going to be depreciated and changed
because that's what it does everything's
being changed as we go we don't have to
worry about that too much if you have
warnings if you run it a second time the
warning will disappear and this is just
imported these packages for us to use
Jupiter is nice about this that you can
do each thing step by step and I'll go
ahead and also zoom in there a little
control plus that's one of the nice
things about being in a browser
environment so here we are back another
sip of coffee if you're familiar with my
other videos you notice I'm always
sipping coffee I always have a in my
case latte next to me on espresso so the
next step is to go ahead and initialize
we're going to call it the CNN or
classifier neural network and the reason
we call it a classifier is because it's
going to classify it between between two
things it's going to be cat or dog so
when you're doing classification you're
picking specific objects you're specific
it's a true or false yes no it is
something or it's not so first thing
we're going to create our classifier and
it's going to equal sequential so
there's sequential setup is the
classifier that's the actual model we're
using that's the neural network so we
call it a classifier and the next step
is to add in our convolution and let me
just do a Let Me shrink that down in
size you can see the whole line and
let's talk a little bit about what's
going on here I have my classifier and I
add something what am I adding well I'm
adding my first layer this first layer
we're adding in is probably the one that
takes the most work to make sure you
have it set correct and the reason I say
that this is your actual input and we're
going to jump here to the part that says
input shape equals 64 by 64 by 3. what
does that mean well that means that our
pictures coming in and there's these
pick pictures remember we had like the
picture of the car was 128 by 128 pixels
well this one is 64 by 64 pixels and
each pixel has three values that's where
these numbers come from and it is so
important that this matches I mentioned
a little bit that if you have like a
larger picture you have to reformat it
to fit this shape if it comes in as
something larger there's no input notes
there's no input neural network there
that will handle that extra space so you
have to reshape your data to fit in here
now the first layer is the most
important because after that Karas knows
what your shape is coming in here and it
knows what's coming out and so that
really sets the stage most important
thing is that input shape matches your
data coming in and you'll get a lot of
Errors if it doesn't you'll go through
there in picture number 55 doesn't match
it correctly and guess what it does it
usually gives you an error and then the
activation if you remember we talked
about the different activations on here
we're using the relu model like I said
that is the most common only used now
because one it's fast doesn't have the
added calculations in it it just says
here's the value coming out based on the
weights and the value going in and from
there you know it's uh if it's over one
then it's good or over zero it's good if
it's under zero then it's considered not
active and then we have this conversion
2D what the heck is conversion 2D I'm
not going to go into too much detail in
this because this has a couple of things
it's doing in here a little bit more in
depth than we're ready to cover in this
tutorial but this is used to convert
from the photo because we have 64 by 64
by 3 and we're just converting it to
two-dimensional kind of setup so it's
very aware that this is a photograph and
that different pieces are next to each
other and then we're going to add in a
second convolutional layer that's what
the conv stands for 2D so these are
hidden layers so we have our input layer
and our two hidden layers and they are
two dimensional because we're doing with
a two-dimensional photograph F and
you'll see down here that on the left
when we add a Max pooling 2D and we put
a pool size equals 2 2. and so what this
is is that as you get to the end of
these layers one of the things you
always want to think of is what they
call mapping and then reducing wonderful
terminology from the Big Data we're
mapping this data through all these
layers and now we want to reduce it to
only two sets in this case it's already
in two sets because it's a 2d photograph
but we had you know two Dimensions by we
actually have 64 by 64 by 3. so now
we're just getting it down to a two by
two just a two Dimension two-dimensional
instead of having the third dimension of
colors and we'll go ahead and run these
I'm not really seeing anything on our
run script because we're just setting up
this is all set up and this is where you
start playing because maybe you'll add a
different layer in here to do something
else to see how it works and see what
your output is that's what makes cross
so nice is I can with just a couple
flips of code put in a whole new layer
that does a whole new processing and see
whether that improves my run or makes it
worse and finally we're going to do the
final setup which is to flatten
classifier add a flatten setup and then
we're going to also add a layer a dense
layer and then we're going to add in
another dense layer and then we're going
to build it we're going to compile this
whole thing together so let's flip over
and see what that looks like and we've
even numbered them for you so we're
going to do the flattening and flatten
is exactly what it sounds like we've
been working in a two-dimensional array
of picture which actually is in three
dimensions because of the pixels the
pixels have a whole another dimension to
it of three different values and we've
kind of resized those down to two by two
but now we're just going to flatten it I
don't want to have multiple Dimensions
being worked on by tensor and by Karas I
want just a single array so it's
flattened out and then step four full
connection so we add in our final two
layers and you could actually do all
kinds of things with this you could
actually leave out this some of these
layers and play with them you do need to
flatten it that's very important then we
want to use the dense again we're taking
this and we're taking whatever came into
it so once we take all those different
the two Dimensions or three dimensions
as they are and we flatten it to one
dimension we want to take that and we're
going to pull it into units of 128. they
got that you say where do they get 128
from you could actually play with that
number and get all kinds of weird
results but in this case we took the 64
plus 64 is 128. you could probably even
do this with 64 or 32. usually you want
to keep it in the same multiple whatever
the data shape you're already using is
in and we're using the activation the
relu just like we did before and then we
finally filter all that into a single
output and it has how many units one why
because we want to know whether true or
false it's either a dog or a cat you
could say one is dog zero is cat or
maybe you're a cat lover and it's one is
cat and zero is dog and if you love both
dogs and cats you're going to have to
choose and then we use the sigmoid
activation if you remember from before
we had the relu and there's also the
sigmoid a sigmoid just makes it clear
it's yes or no we don't want to any kind
of in-between number coming out and
we'll go ahead and run this and you'll
see it's still all in setup and then
finally we want to go ahead and compile
and let's put the compiling our
classifier neural network and we're
going to use the optimizer atom and I
hinted at this just a little bit before
where does atom come in where does an
Optimizer come in well the optimizer is
the reverse propagation when we're
training it it goes all the way through
and says error and then how does it
readjust those weights there are a
number of them atom is the most commonly
used and it works best on large data
most people stick with the atom because
when they're testing on smaller data see
if their model is going to go through
and get all their errors out before they
run it on larger data sets they're going
to run it on atom anyway so they just
leave it on atom most commonly used but
there are some other ones out there you
should be aware of that that you might
try them if you're stuck in a bind or
you might blore that in the future but
usually Adam is just fine on there and
then you have two more settings you have
loss in metrics we're not going to dig
too much into loss or metrics these are
things you really have to explore Karas
because there are so many choices this
is how it computes the error there's so
many different ways to on your back
propagation and your training so we're
using the atom model but you can compute
the error by standard deviation standard
deviation squared they use binary cross
entropy I'd have to look that up to even
know what that is there's so many of
these a lot of times you just start with
the ones that look correct that are most
commonly used and then you have to go
read the cross site and actually see
what these different losses and metrics
and what different options they have so
we're not going to get too much into
them other than to reference you over to
the Cross website to explore them deeper
but we are going to go ahead and run
them and now we've set up our classifier
so we have an object classifier and if
you go back up here you'll see that
we've added in step one we added in our
layer for the input we added a layer
that comes in there and uses the relu
for Activation and then it pulls the
data so this is even though these are
two layers the actual neural network
layer is up here and then it uses this
to pull the data into a two by two so
into two dimensional array from a
three-dimensional array with the colors
then we flatten it so there's our Adder
flatten and then we add another dense
what they call dense layer this dense
layer goes in there and it downsizes it
to 128 it reduces it so you can look at
this as we're mapping all this data down
the two-dimensional setup and then we
flatten it so we map it to a flattened
map and then we take it and reduce it
down to 128 and we use the relu again
and then finally we reduce that down to
just a single output and we use the
sigmoid to do that to figure out whether
it's yes no true false in this case cat
or dog and then finally once we put all
these layers together we compile them
that's what we've done here and we've
compiled them as far as how it trains to
use these settings for the training back
propagation so if you remember we talked
about training our setup and when we go
into this you'll see that we have two
data sets we have one called the
training set and the testing set and
that's very standard in any data
processing is you need to have that's
pretty common in any data processing is
you need to have a certain amount of
data to train it and then you got to
know whether it works or not is it any
good and that's why you have a separate
set of data for testing it or you
already know the answer but you don't
want to use that as part of the training
set so in here we jump into part two
fitting the classifier neurom network to
the images and then from Karas let me
just zoom in there I always love that
about working with jupyter notebooks you
can really see we're going to come in
here we do the cross pre-processing and
image and we import image data generator
so nice of Karas it's such a high-end
product right now going out and since
images are so common they already have
all the stuff to help us process the
data which is great and so we come in
here we do train data to Gin and we're
going to create our object for helping
us trainer for reshaping the data so
that it's going to work with our setup
and we use an image data generator and
we're going to rescale it and you'll see
here we have one point which tells it
it's a float value on the rescale over
255. where does 255 come from well
that's the scale in the colors of the
pictures we're using their value from 0
to 255. so we want to divide it by 255
and it'll generate a number between 0
and 1. they have Shear range and zoom
range horizontal flip equals true and
this of course has to do with if the
photos are different shapes and sizes I
guess it's a wonderful package you
really need to dig in deep to see all
the different options you have for
setting up your images for right now
though we're going to stick with some
basic stuff here and let me go ahead and
run this code and again it doesn't
really do anything because we're still
setting up the pre-processing let's take
a look at this next set of code and this
one is just huge we're creating the
training set so the training set is
going to go in here and it's going to
use our train data gen we just created
dot flow from directory that's going to
access in this case the path data set
training set that's a folder so it's
going to pull all the images out of that
folder now I'm actually running this in
the folder that the data sets in so if
you're doing the same setup and you load
your data in there and you're doing this
make sure wherever your Jupiter notebook
is saving things to that you create this
path or you can do the complete path if
you need to you know C colon slash Etc
and the target size the batch size and
class mode is binary so the classes
we're switching everything to a binary
value back size what the heck is batch
size well that's how many pictures we're
going to batch through the training each
time and the target size 64 by 64 little
confusing but you can see right here
that this is just a general training and
you can go in there and look at all the
different settings for your training set
and of course with different data we're
doing pictures there's all kinds of of
different settings depending on what
you're working with let's go ahead and
run that and see what happens and you'll
see that it found 800 images belonging
to one classes so we have 800 images in
the training set and if we're going to
do this with the training set we also
have to format the pictures in the test
set now we're not actually doing any
predictions we're not actually
programming the model yet all we're
doing is preparing the data so we're
going to prepare a training set and the
test set so any changes we make to the
training set at this point also have to
be made to the test set so we've done
this thing we've done a train data
generator we've done our training set
and then we also have remember our test
set of data so I'm going to do the same
thing with that I'm going to create a
test data gen and we're going to do this
image data generator we're going to
rescale 1 over 255. we don't need the
other settings just the single setting
for the test data gen and we're going to
create our test set we're going to do
the same thing we did with the test set
except that we're pulling it from the
test set that folder and we'll run that
and you'll see in our test set we found
2 000 images that's about right we're
using twenty percent of the images as
test and eighty percent to train it and
then finally we've set up all our data
we've set up all our layers which is
where all the work is is cleaning up
that data and making sure it's going in
there correctly and we are actually
going to fit it we're going to train our
data set and let's see what that looks
like and here we go let's put the
information in here and let's just take
a quick look at what we're looking at
with our fit generator we have our
classifier DOT fit generator that's our
back propagation so the information goes
through forward with a picture and it
says oh you're the right or you're wrong
and then the error goes backward and
reprograms all those weights so we're
training our neural network and of
course we're using the training set
remember we created the training setup
here and then we're going steps per epic
so it's 8 000 steps epic means that
that's how many times we go through all
the pictures so we're going to rerun
each of the pictures and we're going to
go through the whole data set 25 times
but we're going to look at each picture
during each epic 8 000 times so we're
really programming the heck out of this
and going back over it and then they
have validation data equals test set so
we have our training set and then we're
going to have our test set to validate
it so we're going to do this all in one
shot and we're going to look at that
they're going to do 200 steps for each
validation and we'll see what that looks
like in just a minute let's go ahead and
run our training here and we're going to
fit our data and as it goes it says epic
one of 25. you start realizing that this
is going to take a while on my older
computer it takes about 45 minutes I
have a dual processor we're processing
uh 10 000 photos that's not a small
amount of photographs to process so if
you're on your laptop which I am it's
going to take a while so let's go ahead
and go get our cup of coffee in a sip
and come back and see what this looks
like so I'm back back he didn't know I
was gone I was actually a lengthy pause
there I made a couple changes and let's
discuss those changes real quick and why
I made them so the first thing I'm going
to do is I'm going to go up here and
insert a cell above and let's paste the
original code back in there and you'll
see that the original thing was steps
per epic 8 000 25 epics and validation
steps 2000 and I changed these to 4000
epics or four thousand steps per epic 10
epics and just 10 validation steps and
this will cause problems if you're doing
this as a commercial release but for
demo purposes this should work and if
you remember our steps per epic that's
how many photos we're going to process
in fact let me go ahead and get my
drawing pin out and let's just highlight
that right here we have 8 000 pictures
we're going through so for each epic I'm
going to change this to four thousand
I'm going to cut that in half so it's
going to randomly pick 4 000 pictures
each time it goes through an epic and
the Epic is how many processes so this
is 25 and I'm just going to cut that to
10. so instead of doing 25 runs through
8 000 photos each which you can do the
math of 25 times 8 000. I'm only going
to do ten through four thousand so I'm
going to this forty thousand times
through the processes and the next thing
I know that you'll you'll want to notice
is that I also change the validation
step and this would cause some major
problems in releasing because I dropped
it all the way down to 10. what the
validation step does is it says we have
2 000 photos in our trainings or in our
testing set and we're going to use that
for validation well I'm only going to
use a random 10 of those to validate so
not really the best settings but let me
show you why we did that let's scroll
down here just a little bit and let's
look at the output here and see what
that what's going on there so I've got
my drawing tool back on and you'll see
here it lists a run so each time it goes
through an epic it's going to do 4 000
steps and this is where the 4000 comes
in so that's where we have we have epic
one of ten four thousand steps is
randomly picking half the pictures in
the file and going through them and then
we're going to look at this number right
here that is for the whole epic and
that's
2411 seconds and if you remember
correctly you divide that by 60 you get
minutes if you divide that by 60 you get
hours or you can just divide the whole
thing by 60 times 60 which is 3600 if
3600 is an hour this is roughly 45
minutes right here and that's 45 minutes
to process half the pictures so if I was
doing all the pictures we're talking an
hour and a half per epic times 36 or no
25 they had 25 up above 25. so that's
roughly a couple days a couple days of
processing well for this demo we don't
want to do that I don't want to come
back the next day plus my computer did a
reboot in the middle of the night so we
look at this and we say okay let's we're
just testing this out my computer that
I'm running this on is a dual core
processor runs 0.9 gigahertz per second
for a laptop you know it was good about
four years ago but for running something
like this it's probably a little slow so
we cut the times down and the last one
was validation we're only validating it
on a random 10 photos and this comes
into effect because you're going to see
down here where we have accuracy value
loss value accuracy and loss those are
very important numbers to look at so the
10 means I'm only validating across 10
pictures that is where here we have
value this is acc's for accuracy value
loss we're not going to worry about that
too much and
act now accuracy is while it's running
it's putting these two numbers together
that's what accuracy is and value
accuracy is at the end of the Epic
what's our accuracy into the Epic what
is it looking at in this tutorial we're
not going to go so deep but these
numbers are really important when you
start talking about these two numbers
reflect bias that is really important
let me just put that up there and bias
is a little bit beyond this tutorial but
the short of it is is if this accuracy
which is being our validation per step
is going down and the value accuracy
continues to go up that means there's a
bias that means I'm memorizing the
photos I'm looking at I'm not actually
looking for what makes a dog a dog what
makes a cat a cat I'm just memorizing
them and so the more this discrepancy
grows the bigger the bias is and that is
really the beauty of the Cross neural
network it is a lot of built-in features
like this that make that really easy to
track so let's go ahead and take a look
at the next set of code so here we are
into part three we're going to make a
new prediction and so we're going to
bring in a couple tools for that and
then we have to process the image coming
in and find out whether it's an actual
dog or cat we can actually use this to
identify it and of course the final step
of part three is to print prediction
we'll go ahead and combine these and of
course you can see me there adding more
sticky notes to my computer screen
hidden behind the screen and you know
last one was don't forget to feed the
cat and the dog
so let's go and take a look at that and
see what that looks like in code and put
that in our Jupiter notebook all right
and let's paste that in here and we'll
start by importing numpy as NP not these
are very common package I pretty much
imported on any python project I'm
working on another one I use regularly
is pandas they're just ways of
organizing the data and then NP is
usually the standard in most machine
learning tools as a return for the data
array although you know you use the
standard data array from Python and we
have cross pre-processing import image
dishes that all look familiar because
we're going to take a test image and
we're going to set that equal to in this
case cat or dog one as you can see over
here and you know let me get my drawing
tool back on so let's take a look at
this we have our test image we're
loading and in here we have test image
one and this one has a data hasn't seen
this one at all so this is all new oh
let me shrink the screen down let me
start that over so here we have my test
image and we went ahead and the cross
processing has this nice m image setup
so we're going to load the image and
we're going to alter it to a 64 by 64
print so right off the bat we're going
to cross this nice that way it
automatically sets it up for us so we
don't have to redo all our images and
find a way to reset those and then we
use all those to set the image to an
array so again we're all in
pre-processing the data just like we
pre-processed before with our test
information and our training data and
then we use the numpy here's our numpy
that's uh from our right up here import
numpy as in p expand the dimensions test
image axes equals zero so it puts it
into a single array and then finally all
that work all that pre-processing and
all we do is we run the result we click
on here we go result equals classifier
predict test image and then we find out
well what is the test image and let's
just take a quick look and just see what
that is and you can see when I ran it a
comes up dog and if we look at those
images there it is cat or dog image
number one that looks like a nice floppy
eared lab friendly with this tongue
hanging out it's either that or a very
floppy eared cat I'm not sure which but
according to our software says it's a
dog and we have a second picture over
here let's just see what happens we run
the second picture we can go up here and
change this from dog image one to two
we'll run that and it comes down here
and says cat you can see me highlighting
it down there as cat so our process
works we are able to label a dog a dog
and a cat a cat just from the pictures
there we go cleared my drawing tool and
the last thing I want you to notice when
we come back up here to when I ran it
you'll see it has an accuracy of one and
the value accuracy of one well the value
accuracy is the important one because
the value accuracy is what it actually
runs on the test data remember I'm only
testing it on I'm only validating it on
random 10 photos and those 10 photos
just happen to come up one now when they
ran this on the server it actually came
up about 86 percent this is why cutting
these numbers down so far for a
commercial release is bad so you want to
make sure you're a little careful of
that when you're test your stuff that
you change these numbers back when you
run it on a more Enterprise computer
other than your old laptop that you're
just practicing on or messing with and
we come down here and again you know we
have the validation of cat and so we
have successfully built a neural network
that could distinguish between photos of
a cat and a dog imagine all the other
things you could distinguish imagine all
the different Industries you could dive
into with that just being able to
understand those two different sub
pictures what about mosquitoes could you
find the mosquitoes that bite versus the
mosquitoes that are friendly it turns
out the mosquitoes that bite us are only
four percent of the mosquito population
if even that maybe two percent there's
all kinds of industries that use this
and there's so many industries that are
just now realizing how powerful these
tools are just in the photos alone there
is a myriad of Industries sprouting up
and I said it before I'll say it again
what an exciting time to live in with
these tools and that we get to play with
brings you Masters program in artificial
intelligence created in collaboration
with IBM to learn more about this course
you can find the course Link in the
description box below what is deep
learning again this video is not about
deep learning but there are other videos
we have created in detail about what is
deep learning in this video we'll just
touch upon the basics so that that's
like a nice segue into tensorflow so
deep learning is
um in a way a subset of machine learning
and we use primarily neural networks in
deep learning and the underlying
technology behind artificial
intelligence is deep learning and here
we teach them how to recognize let's say
images or voice and so on and so forth
so it is a learning mechanism but here
unlike traditional machine learning the
data is far more complicated and far
more unstructured like it could be
primarily in the form of images or audio
files or text files and one of the core
components of deep learning is neural
network and the neural network somewhat
looks like this there is something known
as an input layer and then there is an
output layer and in between there are a
bunch of hidden layers so typically
there would be at least one hidden layer
and anything more than one hidden layer
is known as a deep neural network so any
neural network with more than three
layers all together right is known as a
deep neural network all right so what
are the functions of the various layers
let's take a quick look so the input
layer accepts the input so this could be
in the form of let's say if it is an
image it could be the pixel values of
the images so that's what the input
layer does and then it passes on to the
hidden layers and the hidden layers in
turn perform certain computations and
they have what is known as as a part of
the training they have these weights and
biases that they keep updating till the
training process is complete and each
neuron has multiple weights and there
will be one bias and these are like
variables and we will see when we go
into the tensorflow code what we
actually mean by that and so that's what
the hidden layer does it does a bunch of
computation and passes its values to the
output layer and then the output layer
in turn gives the output it could be in
the form of a class so for example if we
are doing classification it tells us
which class a particular image maybe
belongs to for example let's say if this
is a image classification application
then the input could be a bunch of
images of maybe cats and dogs and the
output will be like it will say okay if
this is activated this gives a 0 and
this gives a 1 that means it is a cat if
this gives a 1 and this gives a zero
that means it is a DOT so that is a kind
of a binary classification and that can
be extended with multiple neurons on the
output side to have many more classes
for example or it can also be used for
regression as well not necessarily only
classification again since this video is
not about deep learning or machine
learning or neural network we will
probably not go into a lot of details
but you can check other videos where we
have given a lot lot more details about
neural networks and deep learning and so
on so in order to develop a deep
learning application how do you go about
primarily there are two or three
components that are required in order to
develop deep learning application you
need obviously a programming language so
typically python is used and that's what
we are going to use in this particular
video but you can also use other
languages like Java or C plus plus and
so on and there are some libraries that
are readily available and for primarily
for doing machine learning and deep
learning programming so these are a list
of libraries these are by no means the
exhaustive list but some of the most
common ones like Keras thiano tensorflow
and so on and so forth tensorflow has
nowadays become very very popular this
is developed by Google and it is an open
source library and keros was there
before now Keras has actually now become
a part of tensorflow as well so it is
one player about tensorflow so in that
sense they're well integrated It's a
combination of Keras and the tensorflow
is pretty good then of course you have a
torch and dl4j and so on and so forth so
there are multiple libraries but this
video is about tensorflow and we will be
focusing on tensorflow what are the
benefits of tensorflow and what are its
components and our towards the end we
will show you a code in Python we've
written a code in Python by the way
tensorflow can be used with multiple
languages it supports multiple languages
though python is by far the most popular
language so let's take a look at what
exactly is tensorflow and why we are so
excited about tensorflow so tensorflow
offers apis now we can earlier without
when these libraries none of these
libraries were there even then we were
doing people were doing machine learning
and deep learning and so on but the
coding mechanism was much more
complicated what these Library select
tensorflow offer is they provide kind of
a high level API so that we don't have
to go really deep into writing all the
stuff that is required say to prepare a
neural network and to even configure or
even to program a neuron and so on right
so these are done by the library so all
you need to do is they offer a higher
level API you need to use that API and
call that API and maybe pass the data
and that would pretty much it's much
easier rather than actually going down
and writing everything by yourself so
tensorflow that way it offers apis for
to write your code in python or even C
plus plus and and so on other languages
Java as well it has an integration with
r as well apparently okay and it
supports CPUs as well as gpos now deep
learning applications are very compute
intensive especially the training
process needs a lot of computation it
takes very long as you can imagine
because the the data size is large and
there are so many iterative processors
there are so much of mathematical
calculations matrix multiplication and
so on and so forth so for that if you
perform these activities on a normal CPU
typically it would take much longer but
gpus are graphical processing units you
must have heard gpus in the context of
games and so on because where you need
the screen needs to be of high
resolution and the images need to be of
high resolution and so on so gpus there
as the name suggests graphical
Processing Unit were originally designed
for that but since they are very good at
handling this kind of iterative
calculations and so on now they are kind
of they are being used or leveraged
rather for doing or developing deep
learning applications and tensorflow
supports gpus as well as CPUs so I think
that's one of the major advantages of
tensorflow as well now again what is
exactly tensorflow it's a open source
Library developed by Google and open
source and primarily for deep learning
development but tensorflow also supports
traditional machine learning by the way
so if you want to do some traditional
machine learning we can do it however it
is probably a bit of an overhead to use
tensorflow for doing traditional machine
learning and this is really good for
performing deep learning activities and
again if you want to get into more
details about what's the difference
between machine learning and deep
learning there is another video about it
you can probably take a look at that
video what else it is developed
originally for large numerical
computations so original even tensorflow
was developed they never thought of it
as a keeping deep learning in mind but
ultimately it so happened that it's
really very good for deep learning
development and therefore Google has
open sourced it and tensorflow as the
name suggests the data is in the form of
what is known as tensors these are like
multi-dimensional arrays and they are
very handy in handling large amounts of
data and we will see that as well as we
move forward and then of course the
execution mechanism is in the form of
graphs so that makes it much easier to
execute this code in a distributed
manner across a cluster of computers and
and also using gpus and so on and so
forth right so that's a quick overview
about what is tensorflow we will see a
little bit more detail tell the two
major components that is basically the
tensors and the graphs let's take a look
at what they are so what are tensors
tensor is as I mentioned earlier it is
like a multi-dimensional array in which
the data is stored now when we are doing
deep learning especially the training
process you will have large amounts of
data and the data is in typically in a
very complicated format and it really
helps when you're able to put this use
this or store it in a compact way and so
tensors actually offer a very nice and
compact way of storing the data handling
the data during computation this is not
really for storing on your hard disk or
things like that but in memory when
you're doing the computation tenses are
really really very handy in terms of
keeping the data compact because they
are like multi-dimensional arrays so the
data is stored in tensors and then it is
fed into the neural network and then you
get the output all right so there are
some terms associated with tensors let's
get ourselves familiarized one is the
dimension and another is the rank so
what is dimension typically Dimension is
like the number of elements in a way so
for example this is a five by four
dimension tensor and then you can have
again the multi Dimensions right so you
can this can be like three by three by
three so that is uh the dimension and
then you have ranks so what are tensor
ranks ranks are basically traditionally
we would have thought of as Dimensions
that is actually in this case it is
called rank so it becomes easier when we
see examples so a tensors rank is
supposed to be zero when there is only
one element we also call this as scalar
so just one element and this is not
really a vector it's just an element
like 200 it's also known as a scale so
such a tensor is supposed to be having a
rank of zero then you have let's say one
dimensional array this is a vector with
a row of elements this has rank of one
now if you have traditionally what we
called as a two-dimensional like a
matrix for example then the rank is 2
and in this case the rank is three and
it can have more ranks as well as I
mentioned it is like a multi-dimensional
array so you can have rank five six and
so on okay so those are the
terminologies in tensorflow terms
dimensions and ranks so this is just to
make sure that we are picking the same
language so whenever we talk about a
rank of a tensor you understand what
exactly is meant by that now in addition
to tensors in which the data is actually
stored all right so the data is stored
in the tensors and then once you have
the data there is a computation that
needs to be done now the computation
happens in the form of graphs so what we
typically in a tensorflow program what
we do is it's not like traditional
programming where you just write a bunch
of lines and then everything gets
executed in sequence here we prepare
graphs various nodes and then these are
executed in the form of a session and
they use the data from these tensors now
I know this is a slightly New Concept
for a lot of you so it may be a little
difficult to probably understand in the
first cut but when we look at the code
when we go into the tutorial The Code
walkthrough I think that time it will
become much clearer as well but to start
with just uh we need to keep in mind
that we have to first prepare a graph
and when you're preparing the graph now
none of the code is actually getting
executed you write the code to prepare
the graph and then you execute that
graph so that's the way by creating a
session that's the way tensorflow
program works so and each of these
computation is represented as what is
known as a data flow graph and we will
also see that whenever you start a
tensorflow when you create an object
tensorflow object there will be what is
known as a default graph and then if
required but I know probably in the
beginning it may not be required but in
more advanced programming you can
actually have multiple graphs instead of
the default graph you can create your
own graph and have multiple graphs and
use it as well but there is always
whenever you create a tensorflow object
there will be a default graph and this
will be very nicely Illustrated in the
example code that we will take to
explain this so there it becomes much
clearer than in these slides so the
graph gets executed and it processes all
the data that we are feeding all the
external data will be fed in the form of
what is known as placeholders and then
you have variables and constants again
this will also become clear when we take
a look at the code and once you have the
graph then the execution can be enabled
either on regular CPUs or on gpus and
also in a distributed mode so that the
processing becomes much faster as I
mentioned the training of the models in
deep learning takes extremely long
because of the large amount of data and
therefore using tensorflow actually
makes it much easier to write the code
for gpus or CPUs and then execute it in
a distributed manner so this is how the
tensorflow program looks so there is you
need to build a computational graph
that's the first step and then you
execute that graph so the first step is
to write the code for repairing a graph
and then you create what is known as a
session and then in that session you ask
the session to execute this graph so
this will again become much clearer when
we look at the code as some of you may
be aware it's not that easy to set up
the tensorflow environment there are
several components there are several
possibilities for example you can set up
on Windows you can set upon Ubuntu now
Ubuntu has multiple versions which
version to use and then you have python
which release of python to use whether
to do a pip install already to do
install using anaconda and how do you
then link it up with jupyter notebook
these are multiple possibilities and it
takes up a lot of time to try all of
these so today what I'm going to do is
show you a tried and tested method of
setting up the tensorflow environment
and this will help primarily those who
are starting with tensorflow so that
they don't have to waste so much time on
setting up the environment in
experimenting with the insulation and
setting up of the environment now what
we are going to do is I will show you a
method by which you you know it is a
tied and tested method and of course
tensorflow home page has a install page
and it shows you some ways to install
but again the challenge is the same
there are multiple versions multiple
methods shown there so it's highly
confusing for somebody who is new as to
decide which one which path to take so
in today's session what we are going to
do is we will set up tensorflow on
Ubuntu and I'm going to show you in a
watchful box but then if you are using a
laptop with Ubuntu installed you can
straight away use the same method
however we need to keep one thing in
mind that the various releases and
versions of Ubuntu and
Python and then tensorflow not all of
them are compatible with each other so
these versions and releases need to be
very specific so I will tell you which
is the version and releases or what
combination is best suited for you to
get started and later on of course you
can then experiment with other
possibilities and other releases and so
on once you get familiar with tensorflow
to start with I would also like to
mention that it is a good idea to
install or start with Ubuntu environment
or a Linux any other Linux also but here
we will focus on Ubuntu rather than
Windows so for those who are already
let's say using a Windows system the
question may arise what do we do but
there is an easy option as you can see I
am actually using a virtual box so you
need to install virtualbox let me just
show you so this is the virtualbox
Oracle VM virtual box and there are tons
of videos on YouTube how to install a
virtualbox and how to create a Ubuntu
image I think we will not spend time on
that but if you're using Windows and my
preference would be to set up a virtual
box and set up your environment in
Ubuntu image so we will start by
assuming that you have an Ubuntu
environment especially release
14.04 LTS there are multiple Ubuntu
versions and again we will not try to
get the latest and the greatest versions
or latest and greatest releases but the
focus here is to take the releases and
versions which are working and where you
will not waste time so the setup process
will be smooth if you stick to these
releases you can of course experiment
later on with other versions and try out
but here we will be working with Ubuntu
1404 LTS and we will use Python 3.4 and
we will use tensorflow 1.5 this is a
tried and tested combination and I would
also recommend that you use the same if
you want a smooth start and so let's get
started with that let me log in to my
Ubuntu system okay so we have the Ubuntu
system running here now if you go to
tensorflow.org there is a page which
mentions how to install tensorflow and
as you can see there are multiple
possibilities you have Ubuntu you have
Windows and Mac OS and so on and so
forth and if you go to Ubuntu for
example further you will get multiple
options whether you want CPU or GPU and
whether you want to do pip install or
using a virtual native pip virtual EnV
anaconda and so on and so forth so all
these options are very complicated or
rather very confusing I would say not
complicated depending on whether you're
expert or of course I am talking about
beginners here but if you click on some
of these options they may look very easy
so for example if we go back and if you
select for example native pip it may
appear like oh this is just you know one
single or two steps and that's about it
everything gets installed you see here
it should be just one step install it
you say bit three install tensorflow and
everything gets installed unfortunately
it doesn't work that way so it's not as
easy as so don't get kind of fooled by
the Simplicity of the of the
documentation here again it's not their
fault because of the multiple
combinations of releases and so on and
so forth it is not that easy so what we
will do is we will take a slightly
roundabout method which is using
Anaconda which has a few more steps but
you're sure that this is going to work
so that is what we are going to do and
that's what I am going to show you so
what are the steps involved of course I
will not go exactly by what they have
mentioned here as I said I will show you
the steps which are again Sure Shot to
work whereas here again if you follow
just this document there will be some
variations which they have kind of not
documented so that's the reason I will
show you the steps separately all right
so these are the main four steps you
need to download and install anaconda
and then create a virtual environment
with python and release as I mentioned
is Python 3.4 and we will install
tensorflow version 1.5 and then we will
install and configure Jupiter which will
be our development environment all right
so let's get started this is our Ubuntu
and let's get a terminal started here
and
okay yes so
all right so we started the terminal and
then what we'll also do is we'll go to
the Anaconda website because we need to
install Anaconda we need to download and
install Anaconda so for that you need to
go to the Anaconda website so you can
just open a browser and
Google and you will find Anaconda
website
so this is the site
anaconda.org click on this link and it
will take you to the Anaconda website
you don't have to sign up or anything
like that just look for the download
Anaconda
so just click on that it will take you
to the download pages and it
automatically recognizes that you are on
Ubuntu so it will show you the links to
download right so as you can see it has
recognized that you are on a Linux
operating system so you can just click
on this download and
it will start the download there are two
versions of course python with python
3.6 and 2.7 I recommend you start with
python 3.6 now just want to clarify that
we will be actually using Python 3.4
this is just for the initial
installation but subsequently when we
set up the virtual environment you will
see there is one more step there where
we set up the virtual environment there
we will actually be using Python 3.4 so
just that you are not confused all right
so it has started the download we just
say no thanks for this and it is
downloading while it is downloading you
can click on this link saying how to
install Anaconda so there are a couple
of steps mentioned there that we will be
using so let me just in the meanwhile
click on this for some reason the
network is a little slow so it's taking
time all right so once the download is
done we will be using the options that
are mentioned here now the first step of
course is we we are already doing the
first step which is downloading the
installer for Linux the second step is
not mandatory so it's an optional step
most often you can actually skip that
and third step is what we are going to
do and since we are using at this point
we are using python 3.6 we should use
this command so basically the terminal
we have opened here is is to use or to
run this command that's the reason I
opened this terminal however make sure
that the download is complete before you
run this command so we'll just wait for
a couple of minutes and we'll come back
once the download is done all right so
as you can see the download is done this
is a fairly large file so if you are on
a slow internet or low internet
bandwidth then it might take quite a
while it's about
578 MB and so this is the file now what
you need to do is you need to go back to
this installation steps and let me
minimize this and
so this is the command that you need to
run so you can just directly copy and
paste this command from here
okay so I do copy right Mouse like
now it will ask you a bunch of questions
as documented here so most of them you
need to just say enter or yes and that's
about it except for the last step I will
just show you what I mean and here you
need to do press enter multiple times
just to make sure you agree to all these
agreement uh the license agreement and
so on and so forth so once you do
multiple
enters it will bring you to the next
step
all right so now here again they say do
you accept the terms you just say yes
and then press enter and here it will
ask you a couple of questions and pretty
much you need to just go for the default
version press enter to confirm you just
say press enter and this is primarily
when it will pretty much start the
installation process of anaconda this
might take a little while depending
again on your internet speed so you need
to have some patience we will also
probably come back once this
installation is done or if I if it asks
for any further questions
all right so here you'll get again one
more question do you use to wish the
installer to pre-pen so you just say yes
for this question and
keep going now the last question
I think we are pretty much at the end
and that is about Microsoft
VSS I guess yeah vs code so for this you
can just say no because we will not be
using this and that's it you're done so
this is a completion of installation of
anaconda so it's always a good idea to
exit and start a fresh terminal okay so
let's start again terminal
so we are done with the installation of
Anaconda the next step is to create a
virtual environment with Python 3.4 so
for that this is the command conda
create dash n and this is the name of
your virtual environment you can give
any name but for easy reference I have
given as tensorflow and we have to
specify the python version as I
mentioned earlier we will be using the
combination of Python 3.4 and tensorflow
1.5 now this is not the latest version
python has probably 3.6 at this point at
the time of creation of this video and
even tensorflow probably has the latest
version as 1.7 but then these
combinations sometimes may not work and
I found that after several trials and
errors 3.4 with 1.5 seems to be most
reliable and that's the reason I have
chosen this I would recommend you to try
with this first if you are a beginner
and later on maybe you can try out other
permutations and combinations all right
one step I just wanted to show you is
before we do the installation or
creation of the virtual environment if
you want to just clarify or confirm
whether Anaconda has been installed or
not you can run what is known as
Anaconda Navigator so just say Anaconda
Navigator
okay we need to edit this part hold on
one second I guess it is
all right so you see here when you start
Anaconda Navigator this
Anaconda Navigator will open up that is
the indication that Anaconda has been
installed properly so that's just a
quick check and you don't have to do
anything just you can go back and close
it once this comes up and then in the
meanwhile let me just open one more
terminal
for the creation of the virtual
environment and and
so this is your anaconda Navigator and
you can just say okay don't show me let
me just say okay and
um
then you can close this
you can say file you can do a file exit
or you can click on close button
whichever
all right so we will exit from here
we have a terminal open here we will use
this for our next steps all right so
what we have to do now is Type in this
piece off
code or this command
for some reason copy paste is not
working so it's a small command so it
shouldn't be that big of an issue so we
say conda
and we say
create
so this is the command for creating a
new environment slash
dash n and then we give the name of the
environment so we will we can give any
name for uh
convenience I'm just calling it
tensorflow but you can actually name it
anything and then we need to specify
with which version of python so we say
pip
Python and then that is equal
3.4 okay so it will create an
environment this will take a little
while it'll ask you a question just say
yes and then it will be done
so this is just a warning you can simply
ignore that
and for the question that it asks you
just say yes and I think you should be
good
all right so it says do you want to
proceed you say yes and then enter
this will again take a little bit a
little time so we will probably move
forward and then come back once this is
done
all right so the environment has been
created now it's always a good idea to
whenever some of these steps get over or
each step gets over you just exit the
terminal and start afresh with the new
terminal for some reason sometimes it
causes problem if you continue so I
found that it is a safe practice too
each time exit the terminal and start
afresh now that we have an environment
created we can by the name tensorflow
you can enter that environment by
calling the command Source activate
tensorflow or the name whatever name you
have given so you'll see here once you
do that you will get this the name of
the environment will be shown here so it
is like your own again a special
environment within within the system and
uh from here onwards you can do other
stuff like actually installing
tensorflow and so on and you will see
that it has installed Python 3 here so
3.4 right so it is installed that's what
we during creation of this environment
we wanted Python 3.4 so that is what it
is showing but we are not yet done so we
need to still install tensorflow so in
this environment you need to run a
command which will actually install the
crit combination of tensorflow now this
is as you can see it's a slightly
complicated command with install
dash dash ignore instance so what we can
do is from within here we can go and go
to tensorflow install page and there is
a sample code there you can pick up from
there so that you don't have to type in
so much so we say tensorflow
tensorflow let's sort of dark and I'll
also open up probably a notepad here so
that we can construct that command and
go here
is a Ubuntu if you come further down
there will be by the way we are doing
CPU installation I think
clear we are not there is a possibility
to do GPU as well we will probably
create a separate video for that and if
we go here yeah so again you probably if
you follow the entire document out here
it may still not work so I do not
recommend that for now at least for
beginners but just to copy this command
because this is a fairly complicated
command so this is much easier to copy
it from here that's the reason I'm on
this page
and we will then modify it according to
yes okay let's see
copy
and don't do it directly here because
that is probably not yet the right
command let us go and create a notepad
page
empty document I'll just say TF Dot txt
okay this is just a temporary file what
we need to do here is we need to adjust
this command to suit the versions that
we are installing here so let me just
explain what it is what you need to do
so first thing you can get rid of this
one yes okay now here these parts you
can simply ignore here you need to go
and change it to version 1.5
and I think we are good with that okay
so the cp34 indicates your Python 3.4 so
later on when you're trying to
experiment and if you want to do other
combinations of tensorflow and python
version then you can change this for
example you can change this to 3 6 both
these places you need to change this to
3 6 if you are using with python 3.6 and
similarly if you want the latest version
which is of tensorflow which is 1.7 you
need to change this to 1.7 and so on but
at this point to start with if you are a
beginner I would recommend that you
stick to this tensorflow 1.5 with Python
3.4 this is tried and test it and it
works okay just word of course question
that just by changing these versions the
whole thing will not work there may be
other places where you may have to some
of the steps may change so just a word
of caution that just by changing this if
you want to try out with a different
version some of the steps may also
change so you don't think that just by
changing this just one particular
command you will be uh fine okay so
that's been my experience so to start
with I would recommend use this
particular combination all right so it
looks good uh tensorflow 1.5 and let me
just reconform with my notes and I think
it looks good so this is install ignore
1.5 cp34 cp34 and so on okay
okay so let's hit enter
so it started the process again this
might take a little while so we will
pause the recording and then get back
when the installation is done okay so
looks like this installation is done and
as always let us
exit and then come back now you're
currently in the environment virtual
environment
um and it's a good practice before you
exit out of the terminal to come out or
out of the virtual environment so just
like you did Source activate you need to
do Source deactivate
and the name tensor
flow okay we will come back to the
original command prompt and then from
here you can do an exit now we need to
validate whether the tensorflow
installation has gone through properly
or not so the way to validate is to
first start Python and then import
tensorflow Library see if it Imports
without error so that's the best way to
validate so we'll open a terminal
and whenever you want to get into the
environment this is how you have to do
come to an environment and then say
source and obviously you can also use
existing so Source activate
tensorflow okay so we do this and you
need to run python from here remember
you should say Python 3 because you have
installed Python 3.4 now here when you
say import
tensorflow
SDF when you get back up prompt like
this without any errors that means
tensorflow installation has been
successful if tensorflow was not
installed properly you will get an error
saying tensorflow is not available our
module doesn't exist or something like
that so tensorflow installation is done
but now we will go through the process
of installing jupyter notebook so that
you can do your development so let me
just exit and in order to install your
jupyter notebook which is your which is
going to be your development environment
we will first install IPython using
conda so conda install IPython and then
pip install Jupiter so let's first do
the
I button Honda
install
Pi python again it will ask you a
question saying these are the components
do you want to install you just say yes
right you say yes
once again it will take a little while
so we might pause the recording and come
back once it is done
great so that did not take much
time anyways so we will as usual we will
exit which is Source deactivate
tensorflow
exit and then we will start a fresh
terminal in order to install Jupiter
now uh in some places uh they may say
um install Jupiter using Pick 3 but in
my experience it did not work with P3
and that's the reason we will use pip
however just keep in mind you need to
first go to the environment so Source
activate
tensorflow
and you need to use this command which
is PIP install Jupiter usually when
you're on Python 3 you use bit 3 but as
I said it did not work so
in my experience it works with the
installer with three so pip install
Jupiter
I hope you already noticed that it is
j-u-p-y-d-e-r
and not JPI
so again it's probably taking a little
while we will pause the recording and
come back once it is done okay so
Jupiter installation is also done once
again
you need to it's a good idea not you
need to but it's a good idea to come out
of this terminal session and then start
afresh
so we exit and we have a fresh terminal
already available here and we will once
again
go to tensorflow
and we say Jupiter
notebook
that's how you start your Jupiter
notebook
so this looks good now only thing is
since we are running uh for the first
time what you need to do is if you come
back to your command prompt it will show
you that if you're running it for the
first time it will say that you need to
copy paste this link so you you can just
for the first time only once you need to
do this copy this
and paste it in your browser and
remember this is only the first time
when you're running Jupiter you need to
do this from subsequently you don't have
to it will automatically
open up all right so now that we have
Jupiter installed and it starts up we
need to test whether tensorflow is
working fine or not and in order to do
that you can create a new
notebook
and you say import
ant tensorflow as TF shift enter if
everything is fine the supply is
installed properly you don't get any
errors and then it works fine this is an
indication that tensorflow got installed
successfully and that's about it so
that's all about installing tensorflow
with Python 3.4 so remember this is
installation of tensorflow 1.5 with
Python 3.4 on Ubuntu and once again in
case you have a Windows system in fact I
have also done it on a Windows system
you can use Virtual blocks to create a
virtual environment Ubuntu environment
and then follow these steps and there
are tons of videos to create virtualbox
and that's the reason we have not
included those steps in this and if
there are any comments there are better
ways to do this please mention it in the
comment section below or if you need any
further help just mention it and with
your email we will respond to you hey
there learner simply learn brings you my
master's program in artificial
intelligence created in collaboration
with IBM to learn more about this course
you can find the course Link in the
description box below now what are the
various elements of a tensorflow program
as I mentioned tensorflow program is
slightly different from regular
programming that we do so even if you're
familiar with python this may still be
new for you the way you write a
tensorflow program is different from the
regular Python Programming that you
would have done or even machine learning
program you would have written some
machine learning program using
scikit-learn or regular python libraries
this is different from even that so let
us see what are the various elements so
first of all the way we handle data
inside of a program itself is a little
different from how we normally do in a
normal programming language a variable
is a variable in your program right so
you have anything that can keep changing
you just create as a variable or even
constants in fact are actually created
as variables but in tensorflow the
storage in the program consists of three
types one is constants another is
variable and the third is a placeholder
so and they are there is a lot of
difference between these types and we
will see how they vary and how they are
used and so on so constants are like
variables which cannot be changed so for
example if you define a constant like
this this is how you define a constant
by the way the simplest format is like
this like for example B is equal to T F
dot constant and then you give a value
here a slightly more advanced version is
you also specified a type so you say TF
dot constant 2.0 TF float32 so the type
is of type float now in case of
constants you cannot during the
computation you cannot really change
these values so for example if you want
to change the value of B from 3 to 5 or
any other number it is not possible so
that is the meaning of constant all
right so then we have variables so
variables we are all familiar with what
are variables whenever we use
programming we use variables so this is
pretty much the same this is the way you
define when variables TF dot variable
now one thing you need to note is this
is the only type in which we have V
capital okay constant has C the small C
and placeholder as small P but variable
as capital V and TF dot variable and
then you give the a value and then you
can specify what is the type and then
you can use the variable change the
variable at any point in time with a
different value and so on you can update
the variable and so on so we will see
all of this in the code I will
illustrate how a variable is defined and
how it can be changed whereas a constant
cannot be changed and so on and so forth
and then we have placeholders
placeholders are really a special type
and this may be something completely new
for many of us who have been doing
programming but in tensorflow this is a
completely new concept so this is very
important to understand this
placeholders are like variables but only
thing is that they are used for feeding
the data from outside so typically when
you are performing some computations you
need to load data from a file or from an
image file or from a CSV file or
whatever so there is a provision with
the special kind of variables which can
be fed in a regular basis because the
reason one of the reasons for having
this kind of provision is that if you
get the entire input in one shot
typically it may be very difficult to
handle the memory and so on so I think
that was the reason they came up with
this mechanism where you can feed in
patches and there is a certain way of
populating the placeholder we call this
free dick feed underscore dick and this
is a parameter Name by the way and you
feed the placeholders right that is the
meaning here so there is a certain way
of reading the placeholders and we will
again see this in the example code as we
move forward okay so there are three
types one is the constant which cannot
be changed once you assign a value then
you have variables which are like normal
variables we are all familiar with and
then you have placeholders this is
primarily for feeding data from usually
from outside but of course you can also
for temporary testing purpose you can
feed within the product program as well
but primarily the purpose of a
placeholder is to get data from outside
so all right so those were the constants
variables and placeholders that is how
you handle data within a tensorflow
program and then you create a graph and
once you create a graph then you have
what is known as a session and then you
create a session object and you create a
session and then you run a particular
computation or a node or an operation
and so typically what you need to do is
every variable or a computation that you
perform is like an operation or a node
within a graph so initially the graph
will be what is known as the default
graph the moment you create a tensorflow
object or TF this TF here you see this
is the tensorflow object and again in
the code when we go into the code it
will become much easier to understand so
when you create a tensorflow object
there is like a default graph which
doesn't have any operations no nodes or
anything like that so it's like a clean
slate the moment you assign variables or
constants or placeholders each of them
is in tensorflow terms it is known as an
operation again you need to get familiar
with these terms and they are not very
intuitive this is not really an
operation you're just creating a
constant or a variable but and this C is
equal to a into B would traditionally or
would intuitively be an operation here
you're actually performing an operation
but in tensorflow terms each of these
everything is an operation so if you're
creating a constant that is an operation
another constant or variable that's an
operation so you can actually run each
of these and they are also known as
referred to as nodes and when you're in
your session you will actually run each
of these you can potentially run each of
these nodes okay so a typical example
would look like this so you have two
constants created a is equal to TF dot
constant its value is 5 b is equal to
this and then you say C is equal to a
into B and then you create a session now
remember all this you're just creating a
graph at this point no execution has
happened all right so only at this point
once you create a session and then you
say session or assess dot run C is when
actually this whole thing will get
executed all right so that is a
different way of programming compared to
our traditional way of writing program
so you need to get used to this new
format and when we look at the code as
we move forward and when we look in the
Jupiter notebook it will become much
easier probably to understand this
rather than in the slide so these are
the slides showing the code but what we
can do is go straight into the lab and
take a look at the various examples that
are there and starting from the very
basic one how you create variables and
so these are some of the slides that are
showing so this is about how to create
variables how to create constants and
the variables or constants can also be
strings so this is like our first hello
world program and we will talk about
placeholders how to define a placeholder
and how to execute and populate the
placeholder values into placeholder we
will see these examples in the lab
actually I will run the code and we will
also perform a small computation of
adding and multiplying these variables
and we will in the end we will take up
our use case implementation using
tensorflow so let's first go and see
those examples and then come back and
I'll explain this use case and then we
will execute the use case in the lab so
let's go and check our lab okay so I'm
in the jupyter notebook environment and
this is one of the development
environments we can use this is regular
python anytime you do Python Programming
we use Jupiter notebook or there are
other of course there are other ways of
using other tools like charm and so on
but for this particular tutorial I'm
comfortable using jupyter notebook so I
will show you in jupyter Notebook so
this is the very basic example to
demonstrate how to create variables and
constants and placeholders and what is
the difference between them how they
behave and so on and as I mentioned the
Assumption here is that you know at
least some basic Python Programming or
some programming language so at least
you understand the code here and first
two pieces of code you will not need
machine learning background but when we
do the use case of the case study there
it is expected that you know at least
some basic machine learning Concepts so
in case you need to brush up the machine
learning part you may have to do that
before you go to the third one but here
just at least some idea of programming
will be sufficient so what are we doing
here in this particular line or in this
particular cell we are importing
tensorflow it is uh as I mentioned it is
a library so we are importing tensorflow
and we are calling it TF so this is this
just a name you can give it any name but
it is very common and everywhere
wherever tensorflow programming is done
it is always named as TF but you can
name anything actually okay so this will
import the tensorflow into my session
now this is the way to create a variable
so let's start by creating a variable
and this is the name of my variable I am
starting by giving a name called 0 and I
say 0 is equal to TF dot variable and
then I'm giving the value of the
variable here so this is the very basic
way and the simplest way to create a
variable we will see a little later
there are other formats as well but the
bare minimum way of creating a variable
is this so I'm creating a variable by
the name zero and as I mentioned you
need to pay attention to the capital V
here in case of variable it is a capital
V uppercase constant and placeholders
are lowercase now I'm creating a
constant then the constant I'm naming it
as one and the way to create a constant
is TF dot constant and then you give the
value of the constant so the value of
the variable here is 0 and the value of
constant here is 1 and again constant
you can also have additional parameters
like a name and so on probably you must
have seen in other tutorials or in other
places where the code is written but the
basic format to construct to create a
constant is this this is sufficient to
create a constant so I created one
variable and one constant now what else
can we do again here there is no real
execution happening we are just building
a graph we have not yet executed any
tensorflow code we are building a graph
and I will show you how the graph looks
as well first let us understand the
constants and variables and so on and
then I will show you how the graph is
generated and so on so what I want to do
here is I want to add 0 and 1 and put it
into a new variable called new value so
what I do here tensorflow offers these
methods like add assign multiply or you
have matrix multiplication mat model and
so on right so I will use one of those
methods which is TF dot add and then
pass these two as parameters so we can
add a variable and a constant there are
no restrictions on that so this is a
variable and this is a constant so if I
do this it will be okay I did not
execute this code so it is giving an
error now we are good okay so what it
has done is new value will be equal to
TF dot add 0 1 so then we have one
variable and one constant now let's say
I want to change the value of the
variable because that is possible right
so we want to change the value of the
variable to something some new value so
we have in this case new value has one
because we added a 0 and a one so new
value has one now I want to assign this
to this originally what we called as
zero so I will call that as update is
equal to TF dot assign so assign is
basically changing the value so that's
what we are going to do here now it has
not complained we'll just say fine right
there are no issues now let us try this
something similar with this one as well
so 0 is a variable so we were able to
change the value and we will in a little
while we will see what exactly those
values are but before doing that now
let's say I will uncomment this part and
I want to do something similar for my
constant right one is a constant now I
want to let's say change the value of
the constant and make that also
something different okay so if I execute
this piece of code it will give an error
now again the error message may not be
very intuitive this doesn't say that you
cannot do this for a constant it will
just say tensor object has no attribute
assigned now that sometimes may be
confusing especially when you're
starting but the meaning here is that
one is a constant and you're trying
trying to modify the constant so it will
not allow that's the reason it is
complaining so let's put that back in
the comment and okay so that is done so
only variables you can modify now what
you have to do let's skip this piece of
code I'll come back to this in a bit but
let's say we start by creating you
remember I told you we need to create a
session so the way to create a session
there are a couple of ways of creating a
session but this is for beginners this
is the easiest way all you need to do is
assign a variable called SAS or you can
give any name and that is equal to TF
dot session that is the session method
here and you create a session object by
the name SAS now what I'll do is I'll
skip this as well I there is a purpose
behind that now let's go ahead and run
this piece of particular operation
remember I mentioned that everything you
need to run so as of now the code has
not really got executed tensorflow code
has not got executed you only created
the graph so only when you run through
the session is when the actually the
program gets executed so when you do
this now you see observe that it is
giving an error Okay the reason behind
that is remember we skip these two lines
of code for variables okay this and then
this now this is something very very
important to observe if you have
variables in your code what I mean by
that is let's say you are not using
variables but you have only constants
and placeholders then this will not
complain and you will not get this error
but in our case we also have variables
so whenever you have variables in that
case you need to do an initialization
and this is just a standard code there
is nothing that we need to add or modify
or anything like that this is a standard
piece of code you create this name of
course can be anything you can give us
any name but this Global variables
initializer is what you need to call TF
Dot Global variables initializer this
will kind of initialize all the
variables that you may be using and then
again remember this doesn't execute
anything right so all you're doing here
is you're creating an operation but in
order to run that operation you need to
also run this known as run init
underscore op so this operation you need
to run after creating the session all
right so we executed this now let us
execute this and now when I run this
piece of code it will run successfully
okay I hope you observed that so it is
whenever you use variables these two
lines of code one is the operation you
need to create an operation for saying
Global variables initializer and then
you need to run before running anything
else you need to do a session dot run
this operation okay again only if you
are using variables of course you
invariably in all your programs you will
use variables you cannot just write a
program with constants and placeholders
so you can pretty much assume that this
has to be there in pretty much all the
programs now again why this has not been
taken care of in the library that's a
different question but you need to keep
in mind and always remember if you don't
do this for whatever reason if you're
forgotten you will get an error and the
error won't be intuitive so you need to
remember this that this could be because
of the variables okay good so we have
seen how to create a constant and we
have seen how to create a variable and
we have seen that you cannot modify or
update a constant and we have also seen
if you have variables that you need to
execute or have these two lines of code
to initialize the variables and then we
have seen that after creating a graph
how to run the graph in a session and I
will show you a little bit more in
detail how exactly the graph gets
created and how it gets executed but
this was the first very quick code on
creating variables and constants now we
will keep going and we will also show
you or I will show you the placeholders
but before that one more small piece of
code so this is how you write a for Loop
okay so we are saying five times you run
this piece of code now like any Python
program this is nothing different so you
have a for Loop and this is indented
that's the reason and therefore you say
session dot Run update so this will run
five times and it will get printed
that's all so update will run for five
times each time it will come and do this
particular operation that's all we are
we are asking the system to do so let me
just run that and show you let me yeah
so it will okay so it has done five
times
one that's for it is basically adding if
you recall it starts from zero it adds
one to zero then one to one one to two
and so on so it's nothing but it is
generating five numbers one two three
four five that's it okay so then you can
also have your constants as strings for
example so you can also work in a
similar way you can work with string so
most of the computation happens mainly
on numerical values so we normally had
handle numbers but there will be
situations where you may have to handle
strings as well or text as well so this
is an example of text operation so you
can similar to numbers you just create
strings or store in constants instead of
a number you say hello is equal to TF
dot constant and then you assign the
string that you want it to be assigned
and then the next one is one more string
which is for world and then you say add
these two so it is nothing but
concatenation of these two words hello
and word
so we will do that and remember this is
only creation of the graph right so in
order to actually execute this graph you
need to run the session now you need to
keep in mind we don't have to create the
session once again because once you
create a session till you close the
session it remains valid okay so we have
created this session here now till you
say such Dot close it will remain for
you so that's why we did not create one
more so we are just reusing that session
SAS dot run hello world so in this hello
world what is the operation that we are
doing here the operation is add these
two concatenate these two so if I run
this it will print hello world okay now
that's string operation now we will take
a look at placeholders so this is
slightly more complicated and something
new even for people who have been
writing programs so but I'll just
explain it with uh with a quick example
first of all how do you declare or
create a placeholder do you just do
tf.play solver in the PS lowercase only
in case of variables the B is uppercase
but otherwise constant at least one as
it is lowercase and you just say because
as I mentioned this is a placeholder it
doesn't have any value so you unlike a
constant or your variable you can't
usually you will not specify any value
you just say what type of placeholder
you want and very often most of the
cases it is a floating point so we just
say placeholder of type float32 so this
TF dot floor 32 tells the system that it
is a floating Point okay and then so let
me just run this and then you can do
some or Define some computation like for
example B is equal to a into two now
remember here right now A has no value
but what we are saying here is at any
point later on when a gets some value
then B should be equal to twice the
value of a that's all we are saying here
okay and also as I mentioned earlier we
are not yet executing anything we are
just creating a graph here okay so now
you have that now how do you feed the
value or populate the placeholder there
is a certain syntax in which you can
populate the placeholder and you do that
using what is known as a dictionary
you're all probably familiar those with
python specially must be familiar with
the dictionaries so you need to create a
dictionary and then pass that in order
to feed the placeholder so in this
particular example if I want to run
something if if I want to calculate or
compute B obviously I need to feed the
value of a so the value of a I'm feeding
using this dictionary and I'm saying
that a is equal to 3. now there are
multiple ways of populating the value of
a because it typically this won't be
just a single a scalar value like in
this case right remember scalar so it's
just one value one number typically it
would be a vector or a tensor very often
it is a tensor so we will see step by
step so we can start with a very basic
example where a can be a scalar so in
this case a is equal to 3 that's what we
are saying feed underscore date in this
case feed underscore dick is the name of
a variable by the way so you can't give
anything else here so if you just say
feed is equal to this it will give you
an error all right so keep that in mind
either you specify feed underscore dick
is equal to and give the dictionary pass
the dictionary or use straight away like
in this case this is another format so
let's see first let me execute this it
has given 6 okay I think let me see if I
can clear out this particular cell so it
becomes easier to understand okay so it
is giving you six let me do that once
again clear out this and then okay and
let me change the format so this is one
way of doing it another way is you don't
even have to specify the name of this
parameter feed underscore deck you just
pass the dictionary and it will execute
okay but typically this makes the code
readable so that's the reason most of
the places whenever you find tensorflow
code you will see that they explicitly
mention feed underscore date so that the
code is readable otherwise it can get
confusing so I will also comment this
and retain the other format okay so I
hope it's clear so what we are doing
here we are running the operation B and
what exactly is BB is twice a a into two
so therefore what we're doing and a
therefore b gets a value of 6 and that's
what is being fed into result and that's
what is getting printed here okay now as
I mentioned a need not be just a scalar
value it can be a vector with a
multi-dimension array and so on and so
forth so let's start showing you
examples of that again let me just clear
out the outputs let me clear this and
let me also clear this for now okay you
can also do before running up this you
could we could also do clear all output
all output clear it will clear
everything but that will we'll have to
start all over again so I'm just doing
individually all right so in this
example we are taking a one dimension
array where rank is equal to one and I'm
feeding three four five now let us see
what happens if I run this right as you
can imagine it gets multiplied each
number gets multiplied so the result is
6 8 10 okay now as I said it can get
more complicated now let's say this is a
multi-dimensional vector so if you what
happens if you feed this okay so as you
can see this is pretty complicated now
but if you feed this again all you're
doing is there is okay there is a slight
change here again you can also Create
Your Dictionary outside and feed that
here you don't have to directly do it
here like in this case you see there are
variations of the exact syntax in the
way you write it right so there's a
slight variation so in this case you are
creating the dictionary straight away
here kind of in line but here you
created the dictionary separately
because it's more complicated and then
you are passing the dictionary here so I
said dictionary is equal to a and then
I'm saying this is what should be the
value of a this is and I'm feeding that
and then I am using directly that
dictionary here okay so this is
equivalent to putting this code here
right so if we execute this what happens
same so you get because a was uh let's
say this is a 3 by 4 by 2 right so that
is one yeah three by four by two right
so you see here this is a
multi-dimensional array so you feed that
and the values you see here it is one
two three it became two four six four
five six became eight ten twelve seven
eight nine became 14 16 18 and so on and
so forth okay I hope now you got an idea
about placeholders and in real life what
happens is you typically won't create
these dictionaries manually in since
this is a quick demo we did this you
will actually read the data into this
dictionary and then you feed that when
you're doing the computation okay so it
could be a CSV file or it could be a
image file so the input is basically
read and fed in usually it is also done
in patches so you don't read the entire
thing because there can be large amount
of data so that is the idea behind
having these placeholders and the way we
feed these placeholders there is a
provision there's a deliberately that
has been made a provision for this kind
of getting data into the program in
chunks okay now in this particular
example we will close the session here
and then I will show you what is the
other way of creating a session right so
now if I close the session here now if I
try to do anything with the session it
will give us an error so I will probably
not do that right now now there is
another way or another format and this
is a very common way of using the
session but I didn't want to start off
with this because this could get a
little confusing the a simpler way was
to create session saying s is equal to
TF dot session but typically you will do
it in what is known as a width block
this is a very common way of creating
session so once you have the graph you
would execute the graph in a session
using a width block and this is the
syntax so you say with TF dot session as
says all right and then you put all your
code here so what will happen is you
don't have to explicitly close the
session the moment this with block gets
completed the session gets closed okay
so let's just take a quick look at this
example now if I uh let me clear this
current output be clear okay so this is
the hello world example a little bit
earlier we did that so I'm doing with TF
dot session assassin result is equal to
SAS dot run hello Plus World and then I
say print result okay so within this
with block I'm doing all my computation
and this gets executed now let's say I
try to run remember I created a session
here now if I want to let's say do
something like run search dot run a it
is giving an error because the session
is no longer valid right so you cannot
either you have to run this if you want
to run this you have to run this in this
with block or you shouldn't have closed
the session here right you remember this
we said session Dot close so since we
have closed the session there is no uh
active session therefore it is giving an
error okay now there is a third way of
creating a session but for now I will
skip it because that is very very
specific to the notebooks and and so on
so we will just avoid it you start with
for beginners I think the best way is to
start with creating a separate session
and then writing your code and then
closing your session now of course
remember if you don't do this part says
Dot close it will not give any error or
anything like that only thing is that
it's a good practice and your resources
will get released otherwise if you do
multiple of these programs if they are
running then your resources will get
blocked that's the only thing and to
start with it is it is simpler to do it
this way so initially in you're doing
you start with this but as you move
forward as you become familiar with the
tensorflow programming I would recommend
all of you to get into this model most
of the programming tensorflow
programming is done like this with TF
dot session assets okay all right so
that was one book or one page of code
that we have done explaining about
variables constants and placeholders now
let me explain how the graph works so in
this example we are going to see how the
graph is created how the graph gets
executed and so on okay now that we got
a understanding of constants and
variables and placeholders okay so we
will start as usual by importing our
library and as I said whenever you
create a TF or tensorflow object there
will be what is known as a default graph
and you can get a handle of that by
saying get default graph so let's do
that and you can display the operations
remember every node in the graph is
considered as an operation right so as
of now as you can see we have not done
anything we have not created a constant
or variable or nothing right so there
was there is no operation so by the way
let's do one thing let's start with the
all output clear so that I don't have to
again do individually so I'll just run
these two lines of code okay now so yeah
so we have the default graph and as of
now there are no operations so there is
a method called get underscore
operations which which will kind of
display or show what are the various
operations that have been performed on
the graph so as I said to start with
there have no operations being performed
so when I try to display it's empty
there are no nothing is there no
operations okay now let's see what
happens when we slowly step by step
start writing or building the graph so
my first step may be to create a
constant okay remember in the previous
example I have shown you how to create a
constant in the most simple or the
simplest way which is like constant and
then the number right now a small
extension of that is you can actually
give a name to your constant now again
this need not be the same I can say a is
equal to name and x y z I can give
anything here okay this is just for your
reference but here for Simplicity
because there is a purpose why I am
using the same name as the name of the
variable and there are other reasons as
well why you would give a name again to
a constant or a variable this is again
useful when we are using what is known
as tensorboard otherwise it doesn't have
much value this additional name doesn't
have much value in this our case also we
will be seeing the graph and that's why
I'm using this but otherwise this having
additionally a name usually doesn't add
much value all right so let's create a
constant I created a const and now let
us see what's going on in the graph so I
will just say operations is equal to get
operations and if you see here now you
remember here it was blank now you see
the first operation is showing up and
this a that's the reason I put here so
it is showing as a and it says that okay
you have performed an operation and
there is a constant you have created a
constant okay all right so let's move on
let's say we want to create a second
constant and this is B so if I see what
does what are the operations list of
operations that have been performed you
see here there are two of them so you
have operation a which is a constant
then you have an operation B another
constant you recall we in the term
operation here is not very intuitive
right so we are just creating constants
or numbers or variables and it is saying
it's an operation but that's that's a
terminology in tensorflow everything is
an operation here okay all right so then
what else let's say I want to create a
third operation and that is basically
adding which is this is a real operation
in normal sense as well so let's say I'm
creating C which which is adding a and b
and the C value obviously will be a plus
b okay now if we want to see what is in
C and we say C to see here it is not
showing 30 it just says it is a tensor
and it is of type in 32 right because we
have not executed the graph we are still
only building the graph I hope this
makes it easy to understand okay so we
are just building the graph we have till
we do remember till we do a session and
run a particular operation you're not
actually executing anything okay you're
just building the graph all right so now
let us see what are the operations we
have done a b and now C so you will see
there are three of them A B and C okay
now let's do one more operation which is
D and here I am doing multiplication
remember we did a TF dot add now I'll do
TF dot multiply and I'm multiplying A
and B and I'm calling this operation as
D so a has uh I think 10 and B has 20 so
D will be at actually 200 when we
execute it right now it is just a tensor
and it is of shape in 32 okay and once
again we can see what are the operations
A B C D and let's do one last operation
here which is e which is once again
multiplication of c and d now that c has
30 which is a plus b and d has 200 which
is a into B so now C into D will be 30
into 200 will is which is equal to 6000
but as we have seen earlier that
multiplication will not happen right now
so now you have built a fairly simple
but operation graph which consists of a
b c d e one two three four five
operations okay so you built a small
it's not a very complicated one but a
simple graph now you need to execute
these operations so what you need to do
you need to create a session okay so
says dot TF is equal to session and then
you can run these session print says dot
run e now am I missing something do I
have to initialize the variables that's
a question for you think and tell me do
I have to initialize the variables then
remember I ran one piece of code I think
you must have got the answer I don't
have to because I am just using all
constants here I have not used any
variable here right so I don't have to
write execute that piece of code to
initialize the variables so I can just
create a session and run any of the
operations that I want to run so in this
case I am running session e operation e
right this operation e now I can
actually run individually instead of
doing directly E I could have run only a
or I could have run B and so on and so
forth but in this case I have run e
which in turn will do all the required
computations which is for example
assigning the value to A and B and C
doing a multiple application and so on
all those operations will be performed
which are required to do the computation
of e okay now there is a small piece of
code that you can use to find out what
are the various operations that are
there in this particular graph in this
case of course it was very easy very
straightforward I have created ABCDE but
in normal programming in real life this
can sometimes help to see how what is
there in your graph what kind of
operations are going on in your graph so
you can write this piece of code and it
may sometimes help debugging as well
okay all right so I finished my session
what do we have to do we need to close
the session right because I did not use
the width block so I need to close the
session as a good practice so I close
the session good so that's how the graph
Works uh I hope it was helpful in
understanding how variables and
constants and placeholders are created
in the previous example we saw and how
exactly you create a graph and then you
actually execute that graph that's the
way tensorflow program works that's the
structure of tensorflow program okay so
now that we understood the structure of
tensorflow programming let's take an
example and a classification example and
take some real data from from outside
external data CSV file and try to solve
a classification problem for that let's
first go back to the slides and
understand what is the problem statement
and then we will come back so okay so
what is the case here we have some
census data and which has all these
features or variables like the age of a
person what class work class what is
this education what is this marital
status gender and so on and so forth so
there are a bunch of a bunch of features
that are available and we have to
basically classify or not classify we
have to build a model for classifying
whether the income of this person is
above 50k or below 50k okay so there is
actually a label data available we will
then try to build a model and then see
what is the accuracy so that this is a
typical machine learning problem
actually but as I said tensorflow can be
used for doing machine learning as well
so we will as a simple example we will
take a machine learning X so this is how
the high level the code looks and in in
tensorflow programming we also take a
help of regular python libraries like
for example it could be numpy or it
could be scikit-learn or it could be
pandas in this case and because before
you actually develop or before you train
your model create your model you need to
prepare the data into a certain format
and sometimes if you're doing machine
learning activity you all are familiar
those familiar with machine learning
will know you need to split your data
into training and test data set so all
that can be done using regular
non-tensorflow libraries right so these
are like regular python libraries and
psychic learn and so on so we use that
and then prepare the data and then use
tensorflow for performing either the
machine learning or deep learning
activity now so what is the advantage of
using tensorflow right main advantage is
that tensorflow offers high level apis
right we talked about that so in this
case we will be using I'll explain what
is in this slide but before that I
wanted to just show you what is the API
that we will be using so we will be
using the estimator API we will be using
the estimator API to create a classifier
what is known as a linear classifier now
in order to do that you need to prepare
the data and you need to prepare the
data into a certain structure or format
and the API itself needs to be fed in a
certain way so that is exactly what we
are doing before we get into the regular
tensorflow code okay so these slides
will quickly show you what exactly is
happening but I will take you into
jupyter notebook and run that code and
and show you so here we are importing
the data the data is the name of the
file is sensors underscore data.csv it's
a CSV file and then there is a income
bracket is one of the columns which
basically is our Target that is what is
used that is the label rather but this
doesn't have numeric values so it has
the income so we need to convert that
into binary values either 0 or 1. so
that's what we are doing here and then
we are splitting using scikit-learn we
are doing splitting of the data into
test and training right so this is a
standard psychic learn people some of
you who are familiar with machine
traditional machine learning and you
have done machine learning in Python
using scikit-learn will immediately
recognize this this is a SQL Earnest
psychic learn and there is a readily
available method to split the data into
training and test that's what we are
doing here and then you need to create
what is known as the feature columns and
input functions before you can call the
API estimator API that's what we are
doing here and data and also there is
there are different ways of creating the
feature columns for numeric values and
for categorical values or rather for
continuous values and categorical values
okay so that's what has been two types
we are doing here we will see again in
the code as well and then you create
your model so this is basically model is
equal to TF dot estimator linear
classify and then you're feeding the
feature columns and also subsequently
you will call the training by passing
the input function and you specify how
many iterations to be done for the
training process and again this is all
typical machine learning process so
there is nothing specific for tensorflow
here and then you evaluate your model
because this is classification model so
you can again take help of the
scikit-learn to test the accuracy and
get the reports evaluation perform the
evaluation and get the report like this
one right so this is the classification
report and you will see what is the
Christian recall F1 score and so on
again this is a standard machine
learning process all right so that's
pretty much as far as the code here is
concerned once again let me take you
into our lab and we will get started
with this particular code now let me
clear out the outputs so that you can
step by step you can see the output so
this is our data and we will actually
this is just a display here the code
starts from here so before even we start
with our tensorflow part of it as I
mentioned we now remember in the
previous examples the data was kind of
cooked up internally we created some
data and we were using it right the
constants variables and so on here is a
real life example so you are actually
getting data from from outside so you
know before we get into tensorflow this
is actually regular python code so we
are using pandas for example to create
our data frame so we will import pandas
and then we will basically read our file
into a pandas data frame and this is how
the data looks right and if you do a
head it will give you about six readings
or observations five or six observations
and then you can take a look at how your
data is looking so for example you have
age work class education and so on and
so forth and you see here this last
column income underscore bracket it says
whether the income is greater than 50k
or less than 50k so this cannot be
understood or this cannot be fed into
our model or or you know so that's the
reason we need to convert this this will
not be understood right so this needs to
be converted saying okay maybe less than
50k 0 and 150k is 1 so that conversion
needs to be done before we feed it into
our model so that's what we are going to
do as we move forward so this is the
code for doing that okay so we take this
and then we Define a function and then
we run that function so now we have
wherever it is less than 50k it will
have 0 and greater than 50k it will have
1. so we have now updated the data then
the next part is to split our data into
training and test data set so how are we
going to do that we again will take the
standard python Library which is
scikit-learn SQL on library and we use
an existing function there which is
train test plate and what we are doing
here is basically splitting the data
into I'm sorry I need to run this and
then this okay sorry about that so once
I run this what is what are we doing
here again those are familiar with
scikit-learn will immediately recognize
I am splitting this into test and train
I'm saying test size is 30 so 30 of the
data should go into test and training
should have 70 percent okay so that's
all we are doing 0.3 indicates thirty
percent so you can again this can be
individual preferences some people do it
like 50 50 some people 2080 and in our
case we are doing uh 70 30. so training
is 70 and test is 30. okay all right so
we have so far we've been doing regular
non-tensorflow stuff so preparing the
data so that we can now use for
tensorflow and in tensorflow what we are
doing is we will be using the API called
estimator now estimator in order to call
estimator you need to prepare what is
known as feature columns so feature
columns have to be in a certain format
basically it is nothing but the columns
we have to put them in a certain format
and then when we are calling the the
training we need to pass what is known
as an input function again a certain way
in which you need to pass that function
so that is what we need to do before we
create the model and run the model for
training so the next few lines will be
doing that quick look at before we even
get in once again into the tensorflow
code so this is just showing us what are
the various columns names of The Columns
of this data so that is what is shown
here and this is more for when I was
writing the code I could copy paste from
here that's the reason I this line of
code is there instead of typing in these
values okay good so now is where the
actual tensorflow action starts so I
import tensorflow and now I will use
this feature column functionality to
create my feature columns and there is a
certain construct how you create the
feature columns again probably the
details of that is out of scope here but
just that you need to know that we need
to create feature columns and the way
you create feature columns for
continuous values and for categorical
values is slightly different and that's
the reason you have two different blocks
okay so for categorical values you need
to use what is known as TF dot feature
column dot categorical column with
vocabulary list and again within this
again there are two ways in which you
can create for or categorical values one
is a vocabulary list where you know how
many types are there so for example
gender column can have only male and
female so in such a case there are let's
say a finite number of values for a
given column then you use a column with
vocabulary list okay so and then you say
what is the name of the column and what
are the possible values so in this case
male and female now there will be
situations where the the values that
this column can have okay first of all
it is a categorical column that means it
can have names or some non-numerical
values but the number of values is
probably unknown or undefined so for
example occupation now occupation can
have any value it can be self-employed
it can be software Professional Bank a
teacher doctor so so many possibilities
are there and we don't know in the given
data how many occupations have been
listed right so that is where you use
what is known as categorical column with
hash bucket and you specify the hash
bucket size so what it what this says is
there can be a maximum of 1000 such
values so typically if you give a safe
number safely large number so that you
don't run out of these number of
occupations so I think thousand is a
safe number I don't think there will be
more than thousand occupations in a
given data so that's the idea behind it
but that's a judgment call we need to
take what should be the size of this
bucket size right hash bucket size so
these are two different types for
categorical values for creating the
feature column so we will execute this
for all the categorical columns
obviously you need to know the type of
the columns and that is where if you see
right at the beginning this information
was provided what is the data type and
you see here the column name and what
whether it is continuous or categorical
this information is provided in the
Census Data wherever we we pick the
Census Data from by the way this was
picked from online from a government
website and we stored locally so on that
website it is mentioned the details of
this data were mentioned and they have
mentioned whether each of these columns
is categorical or continuous so that
information we have to use and we have
to create feature columns using this
categorical method for the categorical
type of columns and then for numeric or
continuous values you just use what is
known as feature column dot numeric
column okay so these are all continuous
or numeric values like age education
number capital gain Capital loss hours
per work these are all numerical values
or continuous values right so you just
say TF dot feature underscore column dot
numeric underscore column and then give
the name of the column so once you do
that
I have executed the previous one yeah
this one now so your feature columns are
ready and uh you need to basically
create a vector of all these columns and
we call that as feet underscore calls
which is which is like a short form for
feature columns now as you have seen
I've mentioned earlier also there are
two things that are needed one is the
feature columns and another is the input
function so feature columns to First
create the model and then to train the
model you need to create an input
function so now we have the feature
columns ready next is to create an input
function and there is a certain way in
which you need to this construct is
pretty much very common construct so we
will use that so the input function
takes these X and Y values for training
and tests you remember these are the X
strain is the X values of the training
data set and Y train is the labels of
the training data set so that's what we
are using here x train y train in and
then we specify what is the batch size
batch size you remember I mentioned this
is what will say how many records need
to be read each time right so typically
in when we are doing the training
process we don't get the entire data in
one shot we do it in batches so what
size you specify here number of epochs
in this case we just say none but number
of epochses again uh probably the
definition is not required here but at a
very high level how many times the
entire data has to be passed through the
model right so if you have thousand
records and you pass all the Thousand
records three times for training purpose
then you call that as three epochs right
so there is a difference between the
batch size and epochs so when let's say
we have thousand records and you're
saying batch size is 100 that means
there will be 10 batches right so if you
take 10 batches then that is one Epoch
gets come completed okay then you the
training obviously doesn't get completed
in one round okay so you need to pass
this data again maybe a second time or a
third time till you get the right
accuracy that is known as epochs okay
again if you need more details about
this you may have to go through the
machine learning tutorial I think it
becomes much clearer okay so you have
your input function as well let's
execute that okay and now we are ready
to create the use the API right so what
will we do we will create the linear
classifier using
tf.estimator.linear classifier and this
guy needs feature columns right so
that's why we created we did all that so
much of manipulations to create this
feature columns so we have feature
columns so we pass that and we create
our model there may be a few warnings
don't worry about that so we will just
ignore that and now we use that model
and we actually what we did is we
created an instance of the model we have
not really run anything we just created
a instance of the model now we will
create the node for training so model
dot train and this needs the input
function remember we we created the
input function so we need to pass the
input function and then we say the steps
is now telling how many iterations this
training has to be run so we are saying
5000 and this may be it will probably
take a little long so we will okay but
that's fine I think we will leave it
5000. sometimes if you have probably a
less powerful machine you can cut it
down to maybe 1000 or something like
that but in this case I have a fairly
powerful system so I think it shouldn't
take much long let's uh go ahead and
give couple of minutes for it to get
over okay so let us go back yeah I think
this is that you see here the the r
Glass has disappeared that means this
computation is done yes okay this is
done right saving checkpoint for 5000
that means it is done so training is
done so now what we have to do we need
to do the evaluation so again this is a
standard machine learning method so a
methodology rather so training has done
has been done with the training data set
now you need to evaluate using your test
data set so that's what we are doing
here for example you see here x
underscore test is your the test data
set remember we used scikit-learn to
split the data into training and test
data set so X underscore test and a
quick question why are we not using Y
underscore test here like we did in case
of training obviously we are expecting
the model to predict the values y values
right so that's why we don't pass the Y
values here all right so let's execute
this for doing the evaluation all right
and then we put them in a proper format
because the output doesn't come out in a
proper format so we put it in a this
format and then we can just check yeah
so this is done and we can check what is
the first element in this what is the
probabilities and so on so it's just a
quick way to see if the values are there
or not you can take any value here right
any element here all right now that the
testing is done what we need to do again
we will probably put them in some kind
of this is more of a formatting thing we
will just uh run through quickly and
then we will use once again scikit-learn
for finding out the accuracy and so on
because scikit-learn offers what is
known as the classification report
functionality so we will use that and
find out how well our model has
performed so you see here again people
with machine learning background will be
immediately able to recognize this so it
gives us what is the Precision what is
the recall and F1 score this is pretty
much like you're getting like almost 85
percent accuracy which is pretty okay
and there are other ways to increase the
accuracy for example you can run the
training for more iterations that is one
way get more data and so on and so forth
or user this is a linear classifier we
could do have used a non-linear
classifier and so on so again multiple
ways of doing it to increase the
accuracy but the idea here was to
quickly show you a piece of uh
tensorflow code and that's what we have
done here hey there learner simply learn
brings you master's program in
artificial Intel is created in
collaboration with IBM to learn more
about this course you can find the
course Link in the description box below
we had the original tensorflow release
of 1.0 and then they came out with the
2.0 version and the 2.0 addressed so
many things out there that the 1.0
really needed so we start talking about
tensorflow 1.0 versus 2.0 I guess you
would need to know this for a legacy
programming job if you're pulling apart
somebody else's code the first thing is
that tensorflow 2.0 supports eager
execution by default it allows you to
build your models and run them instantly
and you can see here from tensorflow 1
to tensorflow 2. we have almost double
the code to do the same thing so if I
want to do with tf.session or tensorflow
session as a session the session run you
have your variables your session run you
have your tables initializer and then
you do your model fit X train y train
and then your validation data your X
value y value and your epics and your
batch size all that goes into the fit
and you can see here where that was all
just compressed to make it run easier
you can just create a model and do a fit
on it and you only have like that last
set of code on there so it's automatic
that's what they mean by the eager so if
you see the first part you're like what
the heck is all this session thing going
on that's tensorflow 1.0 and then when
you get into 2.0 it's just nice and
clean if you remember from the beginning
I said cross on our list up there
and across is the high level API in
tensorflow 2.0 cross is the official
high level API of tensorflow 2.0 it has
incorporated across as tf.cross cross
provides a number of model building apis
such as sequential functional and
subclassing so you can choose a right
level of abstraction for your project
and uh we'll hopefully touch base a
little bit more on this sequential being
the most common form that is your your
layers are going from one side to the
other so everything's going in a
sequential order
functional
is where you can split the layer so you
might have your input coming in one side
it splits into two completely mod
different models and then they come back
together and one of them might be doing
classification the other one might be
doing just linear regression kind of
stuff or neural basic reverse
propagation neural network and then
those all come together into another
layer which is your neural network
reverse propagation setup
subclassing is the most complicated as
you're building your own models and you
can subclass your own models into cross
so very powerful tools here this is all
the stuff that's been coming out
currently in the tensorflow cross setup
a third big change we're going to look
at is it in tensorflow 1.0
in order to use TF layers as variables
you would have to write TF variable
block so you'd have to pre-define that
in tensorflow 2 you just add your layers
in under the sequential and it
automatically defines them as long as
they're flat layers of course this
changes a little bit as a more
complicated tensor you have coming in
but all of it's very easy to do and
that's what 2.0 does a really good job
of and here we have a little bit more on
the scope of this and you can see how
tensorflow 1 asks you to do these
different layers and values if you look
at the scope and the default name you
start looking at all the different code
in there to create the variable scope
that's not even necessary until tensor
2.0 so you'd have to do one before you
do do what you see the code in 2.0 in
2.0 you just create your model it's a
sequential model and then you can add
all your layers in you don't have to
pre-create the um
variable scope so if you ever see the
variable scope you know that came from
an older version and then we have the
last two which is our API cleanup and
the autograph in the API cleanup
tensorflow one you could build models
using TF Gans TF app TF contrib TF Flags
Etc and tensorflow 2 a lot of apis have
been removed and this is just they just
clean them up because people weren't
using them and they've simplified them
and that's your TF app your TF Flags
your TF logging are all gone
so there's those are three Legacy
features that are not in 2.0 and then we
have our TF function and autograph
feature in the old version tensorflow
1-0 the python functions were limited
and could not be compiled or exported
re-imported so you were continually
having to redo your code you couldn't
very easily just put a pointer to it and
say hey let's reuse this
in tensorflow 2 you can write a python
function using the TF function to mark
it for the jit compilation for the
python jit so that tensorflow runs it as
a single graph autograph feature of TF
function helps to write graph code using
natural python syntax
now we just threw in a new word and you
graph graph is not a picture of a person
you'll hear graph x and some other
things graph is what are all those lines
that are connecting different objects so
if you remember from before where we had
the different layers going through
sequentially each one of those white
lined arrows would be a graph x that's
where that computation is taken care of
and that's what they're talking about
and so if you had your own special code
or python way that you're sending that
information forward you can now put your
own function in there instead of using
whatever function they're using in
neural networks this would be your
activation function although it could be
almost anything out there depending on
what you're doing next let's go for
hierarchy and architecture and then
we'll cover three basic Tools in
tensorflow before we roll up our sleeves
and dive into the example so let's just
take a quick look at tensorflow toolkits
in their hierarchy at the high level we
have our object oriented API so this is
what you're working with you have your
TF cross you have your estimators this
sits on top of your TF layers TF losses
TF metrics so you have your reusable
libraries for model building this is
really where tensorflow shines is
between the cross running your
estimators and then being able to swap
in different layers you can your losses
your metrics all of that is so built
into tensorflow makes it really easy to
use and then you can get down to your
low level TF API you have extensive
control over this you can put your own
formulas in there your own procedures or
models in there you could have it split
we talked about that earlier so with the
2.0 you can now have it split One
Direction where you do a linear
regression model and then go to the
other where it does a neural network and
maybe each neural network has a
different activation set on it and then
it comes together into another layer
which is another neural network so you
can build these really complicated
models and at the low level you can put
in your own apis you can move that stuff
around and most recently we have the TF
code can run on multiple platforms and
so you have your CPU which is basically
like on the computer I'm running on I
have eight cores and 16 dedicated
threads I hear they now have one out
there that has over a hundred cores uh
so you have your CPU running and then
you have your GPU which is your graphics
card
and most recently they also include the
TPU setup which is specifically for
tensorflow models uh neural network kind
of setup so now you can export the TF
code and it can run on all kinds of
different platforms for the most
diverse setup out there and moving on
from the hierarchy to the architecture
in the tensorflow 2.0 architecture we
have you can see on the left this is
usually where you start out with and
eighty percent of your time in data
science is spent pre-processing data
making sure it's loaded correctly and
everything looks right so the first
level in tensorflow is going to be your
read and pre-process data your TF data
feature columns
this is going to feed into your TF Cross
or your pre-made estimators and kind of
you have your tensorflow Hub that sits
on top of there so you can see what's
going on uh once you have all that set
up you have your distribution strategy
where are you going to run it are you
going to be running it on just your
regular CPU are you going to be running
it with the GPU edit in like I have a
pretty high-end graphics card so it
actually grabs that GPU processor and
uses it or do you have a specialized TPU
set up in there that you paid extra
money for it could be if you're in later
on when you're Distributing the package
you might need to run this on some
really high processors because you're
processing at a server level for let's
say net you might be processing this at
a distribute you're Distributing it not
the distribution strategy but you're
Distributing it into a server where that
server might be analyzing thousands and
thousands of purchases done every minute
and so you need that higher speed to
give them a to give them a
recommendation or a suggestion so they
can buy more stuff off your website or
maybe you're looking for a data fraud
analysis working with the banks you want
to be able to run this at a high speed
so that when you have hundreds of people
sending their transactions in it says
hey this doesn't look right someone's
scamming this person probably has a
credit card so when we're talking about
all those fun things we're talking about
saved model this is we were talking
about that earlier where it used to be
when you did one of these models it
wouldn't truncate the float numbers the
same and so a model going from one you
build the model on your machine in the
office and then you need to distribute
it and so we have our tensorflow serving
Cloud on premium that's what I was
talking about if you're like a banking
or something like that now they have
tensorflow Lite so you can actually run
it tensorflow on an Android or an iOS or
Raspberry Pi a little breakout board
there in fact they just came out with a
new one that has a built-in is this mini
TPU with the camera on it so it can
pre-process a video so you can load your
tensorflow model onto that talking about
an affordable way to beta test a new
product you have the tensorflow JS which
is for browser and node server so you
can get that out on the browser for some
simple computations that don't require a
lot of heavy lifting but you want to
distribute to a lot of end points and
now they also have other language
bindings so you can now create your
tensorflow back in save it and have it
accessed from C Java go c-sharp rust r
or from whatever package you're working
on so we kind of have an overview of the
architecture and what's going on behind
the scenes and in this case what's going
on as far as Distributing it let's go
ahead and take a look at three specific
pieces of tensorflow
and those are going to be constants
variables and sessions so very basic
things you need to know and understand
when you're working with the tensorflow
setup so constants in tensorflow in
tensorflow constants are created using
the function constant in other words
they're going to stay static the whole
time whatever you're working with the
Syntax for constant value d-type 9 shape
equals none name constant verify shape
equals false that's kind of the syntax
you're looking at and we'll explore this
with our hands on a little more in depth
and you can see here we do Z equals
tf.constant 5.2 name equals x d type is
a float that means that we're never
going to change that 5.2 it's going to
be a constant value and then we have our
variables in tensorflow variables in
tensorflow are in memory buffers that
store tensors
and so we can declare a two by three
tensor populated by ones you could also
do constants this way by the way as you
can create a an array of ones for your
constants I'm not sure why you do that
but you know you might need that for
some reason
and here we have V equals TF dot
variables
and then in tensorflow you have tf.1s
and you have the shape which is 2 3
which is then going to create a nice two
by three array that's filled with ones
and then of course you can go in there
in their variables so you can change
them it's a tensor so you have full
control over that and then you of course
have uh sessions in tensorflow a session
in tensorflow is used to run a
computational graph to evaluate to the
nodes and remember when we're talking a
graph or graph x we're talking about all
that information then goes through all
those arrows and whatever computations
they have they take it to the next node
and you can see down here where we have
import tensorflow as TF if we do x
equals a TF dot constant of 10 we do y
equals a TF constant of 2.0 or 20.0 and
then you can do Z equals TF dot variable
and it's a TF dot add X comma y
and then once you have that set up in
there you go ahead and knit your TF
Global variables initializer with TF
session as session you can do a session
run knit and then you print the session
run y
uh and so when you run this you're going
to end up with of course the 10 plus 20
is 30. and we'll be looking at this a
lot more closely as we actually roll up
our sleeves and put some code together
so let's go ahead and take a look at
that and for my coding today I'm going
to go ahead and go through anaconda and
then I'll use specifically the Jupiter
Notebook on there and of course this
code is going to work uh whatever
platform you choose whether you're in a
notebook the Jupiter lab which is just a
Jupiter notebook but with tabs for
larger projects we're going to stick
with Jupiter notebook pie charm uh
whatever it is you're going to use in
here you have your spider and your QT
console for different programming
environments the thing to note is kind
of hard to see but I have my main Pi 3
6. right now when I was writing this
tensorflow works in Python version three
six if you have python version three
seven or three eight you're probably
going to get some errors in there might
be that they've already updated it and I
don't know it now you have an older
version
but you want to make sure you're in a
python version 3 6 in your environment
and of course in Anaconda I can easily
set that environment up make sure you go
ahead and and pip in your tensorflow or
if you're in Anaconda you can do a conda
install tensorflow to make sure it's in
your package so let's just go ahead and
dive in and bring that up
this will open up a nice browser window
I just love the fact I can zoom in and
zoom out depending on what I'm working
on making it really easy to adjust a
demo for the right size go under new and
let's go ahead and create a new Python
and once we're in our new python window
this is going to leave it Untitled let's
go ahead and import import tensorflow as
TF at this point we'll go ahead and just
run it real quick no errors yay no
errors I
I do that whenever I do my imports
because I unbearably will have opened up
a new environment and forgotten to
install tensorflow into that environment
or something along those lines so it's
always good to double check
and if we're going to double check that
we also it's also good to know what
version we're working with and we can do
that simply by using the version command
and tensorflow
which you should know is is probably
intuitively the TF dot underscore
underscore version underscore underscore
and you know it always confuses me
because sometimes you do tf.version for
one thing you do TF dot underscore
version underscore for another thing uh
this is a double underscore in
tensorflow for pulling your version out
and it's good to know what you're
working with we're going to be working
in tensorflow version 2.1.0 and I did
tell you the the um we were going to dig
a little deeper into our constants and
you can do an array of constants and
we'll just create this nice array a
equals tf.constant and we're just going
to put the array right in there four
three six one we can run this and now
that is what a is equal to and if we
want to just double check that remember
we're in Jupiter notebook where you can
just put the letter a and it knows that
that's going to be print otherwise you'd
run you'd surround it in print and you
can see it's a TF tensor it has the
shape the type and the and the array on
here it's a two by two array and just
like we can create a constant we can go
ahead and create a variable and this is
also going to be a two by two array and
if we go ahead and print the V out we'll
run that and sure enough there's our TF
variable in here
then we can also let's just go back up
here and add this in here I could create
another tensor and we'll make it a
constant this time
and we're going to put that in over here
we'll have b t f constant and if we go
and print out V and B I'm going to run
that and this is an interesting thing
that always that happens in here you'll
see right here when I print them both
out what happens it only prints the last
one unless you use print commands so
important to remember that in jupyter
notebooks we can easily fix that by go
ahead and print and Surround V with
brackets and now we can see with the two
different variables we have we have the
three one five two which is a variable
and this is just a flat constant so it
comes up as a TF tensor shape two kind
of two and that's interesting to note
that this label is a tf.tensor and this
is a TF variable
so that's how it's looking in the back
end when you're talking about the
difference between a variable and a
constant
the other thing I want you to notice is
that in variable we capitalize the V and
with the constant we have a lowercase C
little things like that can lose you
when you're programming and you're
trying to find out hey why doesn't this
work uh so those are a couple little
things to note in here and just like any
other array in math we can do like a
concatenate or concatenate the different
values here and you can see we can take
a b concatenated you just do a tf.concat
values and there's our a b axes on one
hopefully you're familiar with axes and
how that works when you're dealing with
matrixes and if we go ahead and print
this out you'll see right here we end up
with a tensor so let's put it in as a
constant nut as a variable and you have
your array 4 3 7 8 and 6145 is
concatenated the two together and
a couple things on this our axes equals
one this means we're doing the columns
so if you had a longer array like right
now we have an array that is like you
know has a shape one whatever it is two
comma two
axes zero
is going to be your first one and axis
one is going to be your second one and
it translates as columns and rows if we
had a shape let me just put the word
shape here
um
so you know what I'm talking about it's
very clear and this is I'll tell you
what I spend a lot of time
looking at these shapes and trying to
figure out which direction I'm going in
and whether to flip it or whatever
so you can get lost in which way your
Matrix is going which is column which is
rows are you dealing with the third axes
or the second axis axis one you know
zero one two that's going to be our
columns and if you can do columns then
we also can do rows and that is simply
just changing the concatenate we'll just
grab this one here and copy it we'll do
the whole thing over
control copy
Ctrl V and changes from axis one
to axis zero and if we run that you'll
see that now we concatenate by row as
opposed to column
and you have four three six one seven
eight four seven so it just brings it
right down and turns it into rows versus
columns you can see the difference there
your output in this really you want to
look at the output sometimes just to
make sure your eyes are looking at it
correctly and it's in the format I find
visually looking at it is almost more
important than understanding what's
going on because conceptually your mind
just just too many dimensions sometimes
the second thing I want you to notice is
this says a numpy array so tensorflow is
utilizing numpy as part of their format
as far as Python's concerned and so you
can treat you can treat this output like
a numpy array because it is just that
it's going to be a numpy array another
thing that comes up uh more than you
would think is filling uh one of these
with zeros or ones and so you can see
here we just create a tensor tf.zeros
and we give it a shape we tell it what
kind of data type it is in this case
we're doing an integer and then if we
print out our tensor again we're in
Jupiter so I can just type out tensor
and I run this you can see I have a nice
array of the shape three comma four of
zeros one of the things I want to
highlight here is integer 32. if I go to
the tensorflow data types I want you to
notice how we have float 16 float 32
float 64 complex if we scroll down
you'll see the integer down here 32. the
reason for this is that we want to
control how many bits are used in the
Precision
this is for exporting it to another
platform so what would happen is I might
run it on this computer where python
goes does a float to indefinite however
long it wants to and then we can take it
but we want to actually say hey we don't
want that high Precision we want to be
able to run this on any computer and so
we need to control whether it's a TF
float 16. in this case we did an integer
32.
we could also do this as a float so if I
run this as a float32 that means this
has a 32-bit Precision you'll see Zero
Point whatever and then to go with zeros
we have ones if we're going from the
opposite side and so we can easily just
create a tensorflow with ones
and you might ask yourself why would I
want zeros and ones and your first
thought might be to initiate a new
tensor usually we initiate a lot of this
stuff with random numbers because it
does a better job solving it if you
start with a uniform set of ones or
zeros you're dealing with a lot of bias
so be very careful about starting a
neural network for one of your rows or
something like that with ones and zeros
on the other hand uh I use this for
masking you can do a lot of work with
masking you can also have it might be
that one tenths a row is masked you know
zero is is false one is true or whatever
you want to do it
um and so in that case you do want to
use the zeros and ones and there are
cases where you do want to initialize it
with all zeros or all ones and then swap
in different numbers as a as the tensor
learns so it's another form of control
but in general you see zeros and ones
you usually are talking about a mask
over another array and just like in
numpy you can also do reshapes so if we
take our remember this is shape three
comma four maybe we want to swap that to
four comma three
and if we print this out
you will see let me just go and do that
control V let me run that and you'll see
that the the order of these is now
switched instead of four across now we
have three across and four down
and just for fun let's go back up here
where we did the ones
and I'm going to change the ones to TF
dot random uniform uh and we'll go ahead
and just take off well we'll go and
leave that we're going to run this
and you'll see now we have 0.0441 and
this way you can actually see how the
reshape looks a lot different uh
0.041.15.71 and then instead of having
this one it rolls down here to the 0.14
and this is what I was talking about
sometimes you feel a lot of times you
fill these with random numbers and so
this is the random.uniform is one of the
ways to do that now I just talked a
little bit about this float 32 and all
these data types one of the things that
comes up of course is recasting your
data
so if we have a d-type float32 we might
want to convert these two integers
because of the project we're working on
I know one of the projects I've worked
on ended up wanting to do a lot of round
off so that it would take a dollar
amount or a float value and then have to
run it off to a dollar amount so we only
wanted two decimal points
um and in which case you have a lot of
different options you can multiply by
100 and then round it off or whatever
you want to do there's a lot of or then
convert it to an integer with one way to
round it off kind of
a cheap and dirty trick
so we can take this and we can take the
same tensor and we'll go ahead and
create a as an integer and so we're
going to take this tensor we're going to
tf.cast it and if we print
tensor and then we're going to go ahead
and print
our
tensor
let me just do a quick copy and paste
and when I'm actually programming I
usually type out a lot of my stuff just
to double check it in doing a demo copy
and paste works fine but sometimes be
aware that copy and paste can copy the
wrong code over
personal choice depends on what I'm
working on
and you can see here we took a float32
4.6 4.2 and so on and it just converts
it right down to a integer value sorry
integer 32 setup and remember we talked
about a little bit about reshape
as far as flipping it and I just did
four comma three on the reshape up here
and we talked about axis 0 axis one one
of the things that is important to be
able to do is to take one of these
variables we'll just take this last one
tensor as integer
and I want to go ahead and transpose it
and so I can do we'll do a
equals TF dot transpose
and we'll do our tensor integer in there
and then if I print the A out and we run
this
you'll see it's the same array but we've
flipped it so that our columns and rows
are flipped this is the same as
reshaping so when you transpose you're
just doing a reshape what's nice about
this is that if you look at the numbers
The Columns when we went up here and we
did the reshape they kind of rolled down
to the next row so you're not
maintaining the structure of your Matrix
so when we do a reshape up here they're
similar but they're not quite the same
and you can actually go in here and
there's settings in the reshape that
would allow you to turn it into a
transform
uh so when we come down here it's all
done for you and so there are so many
times you have to transpose your digits
that this is important to know that you
can just do that you can flip your rows
and columns rather quickly here and just
like numpy you can also do multiple your
different math functions we'll look at
multiplication and so we're going to
take matrix multiplication of tensors
we'll go ahead and create a as a
constant 5839 and we'll put in a vector
v 4 comma two and we could have done
this where they matched where this was a
two by two array but instead we're going
to do just a two by one array and the
code for that is your TF dot mat mole so
Matrix multiplier and we have a times V
and if we go ahead and run this up let's
make sure we print out our av on there
and if we go ahead and run this
you'll see that we end up with 36 by 30.
and if it's been a while since you've
seen The Matrix math this is five times
four plus eight times two
three times four plus nine times two
and that's where we get the 36 and 30.
now I know we're covering a lot really
quickly as far as the basic
functionality
so the Matrix or your Matrix multiplier
is a very commonly used back-end tool as
far as Computing
um
different models or linear regression
stuff like that one of the things
is to note is that just like in numpy
you have all of your different math so
we have our TF math
and if we go in here we have functions
we have our cosines absolute angle all
of that's in here so all of these are
available for you to use in the
tensorflow model
and if we go back to our example and
let's go ahead and pull oh let's do some
multiplication that's always good we'll
stick with our av our
constant a and our vector v
and we'll go ahead and do some bitwise
multiplication and we'll create an AV
which is a times B let's go and print
that out
and you can see coming across here we
have the 4 2 and the 5839 and it
produces 20 32 618
and that's pretty straightforward if you
look at it you have four times five is
twenty four times eight is uh 32 that's
where those numbers come from
we can also quickly create an identity
Matrix
which is basically your main values on
the diagonal being ones and zeros across
the other side let's go ahead and take a
look and see what that uh
looks like we can do let's do this uh
so we're going to get the shape this is
a simple way very similar to your numpy
you can do a DOT shape and it's going to
return a tuple in this case our rows and
columns and so we can do a quick print
we'll do rows oops
and we'll do columns
and if we run this you can see we have
three rows two columns
and then if we go ahead and create an
identity Matrix
the scripts
the script for that hit a wrong button
there the script for that looks like
this
where we have the number of rows equals
rows the number of columns equals
columns and D type is a 32 and then if
we go ahead and just print out our
identity
you can see we have a nice identity
column with our ones going across here
now clearly we're not going to go
through every math module available but
we do want to start looking at this as a
prediction model and seeing how it
functions so we're going to move on to a
more of a
direct setup where you can actually see
the full tensorflow in use for that
let's go back and create a new setup
and we'll go in here new Python 3 module
there we go
bring this out so it takes up the whole
window because I like to do that
hopefully it made it through that first
part and you have a basic understanding
of tensorflow as far as being a series
of numpy arrays you got your math
equations and different things that go
into them we're going to start building
a full
set up as far as the numpy so you can
see how Cara sits on top of it and the
different aspects of how it works the
first thing you want to do is we're
going to go ahead and do a lot of
imports date times warning SCI Pi SCI Pi
is your
maths or the back end scientific math
warnings because whenever we do a lot of
this you have older versions newer
versions and so sometimes when you get
warnings you want to go ahead and just
suppress them we'll talk about that if
it comes up on this particular setup and
of course date time
pandas again is your data frame think
rows and columns we import it as PD
numpy is your numbers array which of
course tensorflow is integrated heavily
with Seaborn for our graphics and the
Seaborn as SNS is going to be set on top
of our matplot library which we import
as MPL and then of course we're going to
import our matplot library pipelot as
PLT and right off the bat we're going to
set some graphic colors patch Force Edge
color equals true the style we're going
to use the 538 style you can look this
all up there's when you get into matplot
Library into Seabourn there are so many
options in here it's just kind of nice
to make it look pretty when we start the
um we start up that way we don't have to
think about it later on
uh and then we're going to take and we
have our mplrc we're going to put a
patch Ed color dim Gray Line with again
this is all part of our Graphics here in
our setup we'll go ahead and do an
interactive shell node interactivity
equals last expression uh here we are PD
for pandas options display Max columns
so we don't want to display more than
50. and then our matplot library is
going to be inline this is a Jupiter
notebook thing the matplot library
inline and then warnings we're going to
filter our warnings and we're just going
to ignore warnings that way when they
come up we don't have to worry about
them not really what you want to do when
you're working on a major project you
want to make sure you know those
warnings and then
filter them out and ignore them later on
and if we run this it's just going to be
loading all that into the background
so that's a little back end kind of
stuff then we want to go ahead and do is
we want to go ahead and import our
specific packages that we're going to be
working with which is under Karas now
remember cross kind of sits on
tensorflow so when we're importing cross
and the sequential model we are in
effect importing
tensorflow underneath of it we just
brought in the math probably should have
put that up above and then we have our
cross models we're going to import
sequential now if you remember from our
slide there was three different options
let me just flip back over there so we
can have a quick recall on that and so
in Cross we have sequential functional
and subclassing so remember those three
different setups in here we talked about
earlier and if you remember from here we
have a sequential where it's going one
tensorflow layer at a time you go kind
of look at think of it as going from
left to right or top to bottom or
whatever Direction it's going in but it
goes in One Direction all the time where
functional can have a very complicated
graph of directions you can have the
data split into two separate tensors and
then it comes back together into another
tensor all those kinds of things and
then subclassing is really the really
complicated one where now you're adding
your own subclasses into the tensor to
do external computations right in the
middle of like a huge flow of data but
we're going to stick with sequential
it's not a big jump to go from
sequential to functional but we're
running a sequential tensorflow and
that's what this first import is here we
want to bring in our sequential and then
we have our layers and let's talk a
little bit about these layers this is
where cross and tensorflow really are
happening this is what makes them so
nice to work with is all these layers
are pre-built so from Cross we have
layers import dents from Cross layers
import LST M when we talk about these
layers
cross has so many built-in layers you
can do your own layers the dense layer
is your standard neural network by
default it uses relu for its activation
and then the lstm is a long short term
memory layer since we're going to be
looking probably at sequential data
we want to go ahead and do the lstm and
if we go into
Karas and we look at their layers this
is a cross website you can see as we
scroll down for the cross layers that
are built in we can get down here and we
can look at let's see here we have our
layer activation our base layers
activation layer weight layer weights
there's a lot of stuff in here we have
the relu which is the basic activation
that was listed up here for layer
activations you can change those and
here we have our core layers and our
dense layer so you have an input layer a
dense layer and then we've added a more
customized one with the long-term
short-term memory layer and of course
you can even do your own custom layers
in Cross there's a whole functionality
in there if you're doing your own thing
what's really nice about this is it's
all built in even the convolutional
layers this is for processing Graphics
there's a lot of cool things in here you
can do this is why cross is so popular
it's open source and you have all these
tools right at your fingertips so from
Cross we're just going to import a
couple layers the dense layer
and the long short-term memory layer and
then of course from sklearn our site kit
we want to go ahead and do our min max
scalar standard scaler for pre-editing
our data and then metrics just so we can
take a look at the errors and compute
those let me go ahead and run this and
that just loads it up we're not
expecting anything from the output and
our file coming in is going to be air
quality.csv let's go ahead and take a
quick look at that this is in Open
Office it's just a standard you know we
can do excel whatever you're using for
your spreadsheet and you can see here we
have a number of columns a number of
rows it actually goes down to like 8 000
the first thing we want to notice is
that the first row is kind of just a
random number put in going down probably
not something we're going to work with
the second row is Bandung I'm guessing
that's a reference for the profile if we
scroll to the bottom which I'm not going
to do because it takes forever to get
back up they're all the same the same
thing with the status the status is the
same we have a date so we have a
sequential order here
here is the jam which I'm going to guess
is the time stamp on there so we have a
date and time
we have our O3 CEO NO2 reading SO2
noco2 VOC and then some other numbers
here pm1 PM 2.5 pm4 pm10
without actually looking through the
data I mean some of this I can guess is
like temperature humidity I'm not sure
what the PMS are but we have a whole
slew of data here so we're looking at
air quality as far as an area in a
region and what's going on with our date
time stamps on there and so code wise
we're going to read this into a pandas
data frame so our data frame DF is a
nice abbreviation commonly used for data
frames equals pd.read CSV and then our
the path to it just happens to be on my
D drive separated by spaces and so if we
go ahead and run this
we'll print out the head of our data and
again this looks very similar to what we
were just looking at
being in Jupiter I can take this and go
the other way make it real small so you
can see all the columns going across and
we get a full view of it
or we can bring it back up in size
that's pretty small on there overshot
but you can see it's the same data we
were just looking at we're looking at
the number we're looking at the profile
which is the band done the date we have
a time stamp our O3 count Co and so
forth on here
and this is just your basic pandas
printing out the top five rows we could
easily have done three rows
five rows ten whatever you want to put
in there by default that's five for
pandas now I talked about this all the
time so I know I've already set it at
least once or twice during this video
most of our work is in pre-formatting
data what are we looking at how do we
bring it together so we want to go ahead
and start with our date time let's come
in in two columns we have our date here
and we have our time
and we want to go ahead and combine that
and then we have this is just a simple
script in there that says combine date
time that's our formula we're building
or we're going to submit our pandas data
frame
and the tab name when we go ahead and do
this that's all of our information that
we want to go ahead and create and then
goes for Iron Range DF shape zero so
we're going to go through the whole
setup and we're going to list tab a pin
DF location I and here is our date going
in there and then return the numpy array
list tab d-types date time 64. that's
all we're doing we're just switching
this to a date time stamp and if we go
ahead and do DF date time equals combine
date time and then I always like to
print we'll do DF head
just so we can see what that looks like
and so when we come out of this we now
have our setup button here and of course
it's edited on to the far right here's
our date time you can see the formats
changed so there's our we've added in
the date time column and we've brought
the date over and we've taken this
format here and it's an actual variable
with a zero zero zero one here well that
doesn't look good so we need to also
include the time part of this we want to
convert it into hourly data so let's go
ahead and do that to do that to finish
combining our date time let's go ahead
and create a a little script here to
combine the time in there same thing we
just did we're just creating a numpy
array returning a numpy array and
forcing this into a date time format and
we can actually spend hours just going
through these conversions how do you
pull it from the pandas data frame how
do you set it up so I'm kind of skipping
through it a little fast because I want
to stay focused on tensorflow and cross
keep in mind this is like 80 percent of
your coding when you're doing a lot of
this stuff is going to be reformatting
these things resetting them back up so
that it looks right on here and you know
it just takes time to to get through all
that but that is usually what the
companies are paying you for that's what
the big bucks are for
and we want to go ahead and a couple
things going on here is we're going to
go ahead and do our date time we're
going to reorganize some of our setup in
here convert into hourly data we just
put a pause in there
um now remember we can select from DF
are different columns we're going to be
working with and you're going to see
that we actually dropped a couple of the
columns those ones I showed you earlier
they're just repetitive data so there's
nothing in there that exciting and then
we want to go ahead and we'll create a
second
data frame here let me just get rid of
the DF head
and df2 is we're going to group by date
time and we're looking at the mean value
and then we'll print that out so you can
see we're talking about
we have now reorganized this so we put
in date time 03 Co so now this is in the
same order as it was before and you'll
see the date time now has our zero zero
same date 1 2 3 and so on so let's group
the data together so there's a lot more
manageable and in the format we want and
in the right sequential order
and if we go back to there we go our air
quality you can see right here we're
looking at
um
these columns going across we really
don't need since we're going to create
our own date time column we can get rid
of those these are the different Columns
of information we want and that should
reflect right here in the columns we
picked coming across so this is all the
same columns on there that's all we've
done is reformatted our data
grouped it together by date and then you
can see the different data coming out
set up on there and then as a data
scientist first thing I want to do is
get a description what am I looking at
and so we can go ahead and do the df2
describe and this gives us our you know
describe gives us our basic data
analytics information we might be
looking for like what is the mean
standard deviation
minimum amount maximum amount we have
our first quarter second quarter and
third quarter
numbers also in there so you can get a
quick look at a glance describing the
data or descriptive analysis and even
though we have our quantile information
in here we're going to dig a little
deeper into that
we're going to calculate the quantile
for each variable we're going to look at
a number of things for each variable
we'll see right here q1 we can simply do
the quantile 0.25 percent which should
match our 25 percent up here and we're
gonna be looking at the min Max and
we're just going to do this is basically
we're breaking this down for each
different
variable in there one of the things
that's kind of fun to do we're going to
look at that in just a second let me get
put the next piece of code in here we've
got to clean out some of our we're going
to drop a couple thing our last rows and
first row because those have usually
have a lot of null values and the first
row is just our titles so that's
important it's important to drop those
rows in here and so this right here as
we look at our different quantiles again
it's it's the same and we're still
looking at the 25
quantile here we're going to do a little
bit more with this
so now that we've cleared off our first
and last rows we're going to go ahead
and go through all of our columns and
this way we can look at each column
individually and so we'll just create a
q1 Q3 min max Min IQR Max IQR and
calculate the quantile of I of df2
we're basically doing the math that they
did up here but we're splitting it apart
that's all this is
and this happens a lot because you might
want to look at individual if this was
my own project I would probably spend
days and days going through what these
different values mean
one of the biggest data science
things we can look at that's important
is uh use your use your common sense you
know if you're looking at this data and
it doesn't make sense and you go back in
there and you're like wait a minute what
the heck did I just do at that point you
probably should go back and double check
what you have going on
now we're looking at this and you can
see right here here's our attribute for
our O3 so we've broken it down we have
our q1 5.88 Q3 10.37 if we go back up
here here's our 5 8 we've rounded it off
10.37s in there
so we've basically done the same math
just split it up we have our minimum and
our Max IQR and that's computed let's
see where is it here we go uh q1 minus
1.5 times IQR and the IQR is your Q3
minus q1 so that's the difference
between our two different quarters and
this is all data science as far as the
hard math I'm really not we're actually
trying to focus on cross and tensorflow
you still got to go through all this
stuff I told you 80 of your programming
is going through and understanding what
the heck happened here
what's going on what does this data mean
and so when we're looking in that we're
going to go ahead and say hey
um
we've computed these numbers and the
reason we've computed these numbers is
if you take the minimum value and it's
less than your minimum IQR
that means something's going wrong there
and it usually in this case is going to
show us an outlier so we want to go
ahead and find the minimum value if it's
less than the minimum minimum IQR it's
an outlier and if the max value is
greater than the max IQR we have an
outlier and that's all this is doing low
outliers found minimum value High
outlier is found really important
actually outliers are almost everything
in data sometimes sometimes you do this
project just to find the outliers
because you want to know crime detection
what are we looking for we're looking
for the outliers what doesn't fit a
normal business deal and then we'll go
ahead and throw in just threw in a lot
of code oh my goodness so we have if
your max is greater than IQR print
outlier is found what we want to do is
we want to start cleaning up these
outliers and so we want to convert we'll
do create a convert Nan x max IQR equals
Max underscore IQR mini QR equals
midnight QR this is this is just saying
this is the data we're going to send
that's all that is in Python and if x is
greater than the max IQR and X is less
than the Min IQR x equals null we're
going to set it to null why because we
want to clear these outliers out of the
data now again if you're doing fraud
detection you would do the opposite you
would be cleaning everything else this
not in that Series so that you can look
at just the outlier and then we're going
to convert the Nan hum again we have X
Maxi QR is a hundred percent Min iqrs
Min IQR if x is greater than Max IQR and
X is less than Min IQR again we're going
to return null value otherwise it's
going to remain the same value x x
equals X
and you can see as we go through the
code if I equals our hum then we go
ahead and do that's the that's a column
specific to humidity that's your he Lim
column then we're going to go ahead and
convert do the run a map on there and
convert the non h2m you can see here
it's this is just clean up we run we've
found out that humidity probably has
some weird values in it we have our
outliers
that's all this is and so when we go
ahead and finish this and we take a look
at our outliers and we run this code
here
we have a low outlier 2.04 we have a
high outlier 99.06 outliers have been
interpolated
that means we've given them a new value
chances are these days when you're
looking at something like these sensors
coming in they probably have a failed
sensor in there something went wrong
that's the kind of thing that you really
don't want to do your data analysis on
so that's what we're doing is we're
pulling that out and then converting it
over and setting it up method linear
so we interpolate method linear is it's
going to fill that data in based on a
linear regression model of similar data
same thing with this up here with the
df2i interpolate that's what we're doing
again this is all data prep we're not
actually talking about tensorflow we're
just trying to get all our data
set up correctly so that when we run it
it's not going to cause problems or have
a huge bias
so we've dealt with outliers
specifically in humidity
and again this is one of these things
where when we start running
um
we ran through this you can see down
here that we have our outliers found
high low outliers
migrated them in we also know there's
other issues going on with this data how
do we know that some of it's just
looking at the data playing with it
until you start understanding what's
going on let's take the temp value and
we're going to go ahead and and use a
logarithmic function on the temp value
and it's interesting because it's like
how do you how do you heck do you even
know to use logarithmic on the temp
value that's domain specific we're
talking about being an expert in air
care I'm not an expert in Air Care
um you know it's not what I go look at I
don't look at Air Care data in fact this
is probably the first Air Care data
setup I've looked at but the experts
come in there and they come to even say
hey in data science this is a
exponentially very variable on here so
we need to go ahead and do
transform it
and use a logarithmic scale on that
so at that point that would be coming
from your
data here we go data science programmer
overview does a lot of stuff connecting
the database and connecting in with the
experts data analytics a lot of times
you're talking about somebody who is a
data analysis might be all the way
usually a PhD level data science
programming level interfaces database
manager that's going to be the person
who's your admin working on it
so when we're looking at this we're
looking at something they've sent to me
and they said hey domain Air Care this
needs to be this is a skew because the
data just goes up exponentially and
affects everything else and we'll go
ahead and take that data let me just go
ahead and run this
just for another quick look at it we
have our uh we'll do a distribution DF
we'll create another data frame from the
temp values and then from a data set
from the log temp so we put them side by
side and we'll just go ahead and do a
quick histogram this is kind of nice
plot a figure figure size here's our PLT
from matplot library and then we'll just
do a distribution underscore DF there's
our data frames this is nice because it
just integrates the histogram right into
pandas love pandas and this is a chart
you would send back to your data
analysis and say hey is this what you
wanted this is how the data is
converting on here as a data science
scientist the first thing I note is
we've gone from a 10 20 30 scale to 2.5
3.0 3.5 scale
and the data itself has kind of been
adjusted a little bit based on some kind
of a skew on there so let's jump into
we're getting a little closer to
actually doing our
cross on here we'll go ahead and split
our data up and this of course is any
good data scientists you want to have a
training set and a test set and we'll go
ahead and do the train size
we're going to use 0.75 percent of the
data make sure it's an integer we don't
want to take a slice as a float value
give you a nice error and we'll have our
train size is 75 percent and the test
size is going to be of course the train
size minus the length of the data set
and then we can simply do train comma
test here's our data set
which is going to be the train size the
test size and then if we go and print
this let me just go and run this we can
see how these values split it's a nice
split of 1298 and then 433 points of
value that are going to be for our setup
on here and if you remember we're
specifically looking at the data set
where did we create that data set from
that was from up here that's what we
called the logarithmic value of the temp
that's where the data set came from so
we're looking at just that column with
this train size and the test with the
train and test data set here and let's
go ahead and do uh converted an array of
values into a data set Matrix we're
going to create a little
um
set up in here we create our data set
our data set is going to come in we're
going to do a look back of one so we're
going to look back one piece of data
going backward
and we have our data X and our data y
for Iron Range length of data set look
back minus one
this is creating let me just go ahead
and run this actually the best way to do
this
is to go ahead and create this data and
take a look at the shape of it let me go
ahead and just put that code in here
so we're going to do a look back one
here's our trainex our train Y and it's
going to be adding the data on there and
then when we come up here
and we take a look at the shape
there we go and we run this piece of
code here
we look at the shape on this and we have
a new slightly different change on here
but we have a shape of X 1296 comma 1
shape of Y train y Test X text y
and so what we're looking at is that
the X comes in
and we're only having a single value out
uh we want to predict what the next one
is that's what this little piece of code
is here for what are we looking for well
we want to look back one that's the um
then what we're going to train the data
with is yesterday's data yesterday says
Hey the humidity was at 97 what should
today's humidity be at if it's 97
yesterday is it going to go up or is it
going to go down today if 97 does it go
up to 100 what's going on there and so
our we're looking forward to the next
piece of data which says Hey tomorrow's
is going to you know today's humidity is
this this is what tomorrow's humidity is
going to be that's all that is all that
is is stacking our data so that our Y is
basically X Plus 1 or X could be y minus
1.
and then a couple things to note is our
X data we're only dealing with the one
column but you need to have it in a
shape that has it by The Columns so you
have the two different numbers and since
we're doing just a single point of data
we have and you'll see with the train y
we don't need to have the extra shape on
here now this is going to run into a
problem and the reason is is that we
have what they call a Time step
and the time step is that long-term
short-term memory layer so we're going
to add another reshape on here let me
just go down here and put it into the
next cell and so we want to reshape the
input array in the form of sample time
step features
we're only looking at one feature
and I mean this is one of those things
when you're playing with this you're
like why am I getting an error in the
numpy array why is this giving me
something weird going on so we're going
to do is we're going to add one more
level on here instead of being 12.99 one
we want to go one more
and when they put the code together in
the back you can see we kept the same
shape the 12.99 we added the one
dimension and then we have our train X
shape one and this could have depends
again on how far back in the long
short-term memory you want to go that is
what that piece of code is for and that
reshape is and you can see the new shape
is now one uh 12.99 one one versus the
1299 one and then the other part of the
shape 432 one one again this is our TR
our X in and of course our test X and
then our Y is just a single column
because we're just doing one output that
we're looking for so now we've done our
80 percent
um you know that's all the the breading
all the code reformatting our data
bringing it in now we want to go ahead
and do the fun part which is we're going
to go ahead and create and fit the lstm
neural network uh and if we're going to
do that the first thing we need is we're
going to need to go ahead and create a
model and we'll do this sequential model
and if you remember sequential means it
just goes in order that means we have if
we have two layers the layers go from
layer one to Layer Two or layer 0 to
layer one this is different than
functional functional allows you to
split the data and run two completely
separate models and then bring them back
together we're doing just sequential on
here and then we decided to do the long
short term memory and we have our input
shape which it comes in again this is
what all this switching was we could
have easily made this one two three or
four going back as far as the end number
on there we just stuck to going back one
and it's always a good idea when you get
to this point where the heck is this
model coming from what kind of models do
we have available
and there's let me go and put the next
model in there uh because we're going to
do two models and the next model is
going to go ahead and we're going to do
dents so we have model equal sequential
and then we're going to add the lstm
model and then we're going to add a
dense model and if you remember from the
very top of our code
when we did the Imports oops there we go
our cross this is it right here here's
our importing a dense model and here's
our importing an lstm now just about
every tensorflow model uses dense your
dense model is your basic
forward propagation reverse propagation
error where it does reverse propagation
to program the model so any of your
neural networks you've already looked at
that Lex and says here's the error and
sends the error backwards that's what
this is
the long short term memory is a little
different the real question that we want
to look at right now is where do you
find these models what kind of models do
you have available and so for that let's
go to the Cross website which is
across.io if you go under API slash
layers and I always have to do a search
just search for cross API layers it'll
open up and you can see we have
your base layers right here class
trainable weights all kinds of stuff
like there your Activation so a lot of
your layers you can switch how it
activates relu which is like your
smaller arrays or if you're doing
convolutional neural networks the
convolution usually uses a relu your
sigmoid all the way up to softmax soft
plus all these different choices as far
as how those are set up and what we want
to do is we want to go ahead and if you
scroll down here you'll see your core
layers and here is your dense layer so
you have an input object your dense
layer your activation layer embedding
layer this is your your kind of your one
setup on there that's most common uh
convolutional neural networks or
convolutional layers these are like for
doing uh image categorizing uh so trying
to find objects in a picture that kind
of thing uh we have pooling layers so as
you have the layers come together
um usually bring them down into a single
layer although you can still do like
Global Max pulling 3D and there's just I
mean this list just goes on and on
there's all kinds of different things
hidden in here as far as what you can do
and it changes you know you go in here
and you just have to do a search for
what you're looking for and figure out
what's going to work best for you
as far as which project you're working
on long short term memory is a big one
because this is when we start talking
about text
what if someone says the what comes
after the uh The Cat in the Hat little
kids book there
starts programming it and so you really
want to know not only what's going on
but it's going to be something that has
a history the history behind it tells
you what the next one coming up is now
once we've built all our different you
know we built our model we've added our
different layers we went in there play
with it remember if you're in functional
you can actually link these layers
together and they Branch out and come
back together if you do a uh the sub
set up then you can create your own
different model you can embed a model in
there that might be coming linear
regression you can embed a linear
regression model as part of your
functional split and then have that come
back together with other things so we're
going to go ahead and compile your model
this brings everything together we're
going to put in what the loss is which
we'll use the mean squared error and
we'll go ahead and use the atom
Optimizer clearly there's a lot of
choices on here depending on what you're
doing and just like any of these
different prediction models if you've
been doing any side kit from python
you'll recognize that we have to then
fit the model so what are we doing in
here we're going to send in our train X
our train y we're going to decide how
many epics we're going to run it through
500 is probably a lot for this I'm
guessing it'd probably be about two or
three hundred probably do just fine our
batch size
so how many different when you process
it this is the math behind it if you're
in data analytics you might try to know
what this number is as a data scientist
where I haven't had the PHD level math
that says this is why you want to use
this particular batch size you kind of
play with this number a little bit you
can dig deeper into the math
see how it affects the results depending
what you're doing and there's a number
of other settings on here we did verbose
two I'd have to actually look that up to
tell you what verbose means I think
that's actually the default on there if
I remember correctly there's a lot of
different settings when you go to fit it
the big ones are your epic and your
batch size those are what we're looking
for
and so we're going to go ahead and run
this
and this is going to take a few minutes
to run because it's going through
500 times through all the data so if you
have a huge data set this is the point
where you're kind of wondering oh my
gosh is this going to finish tomorrow
if I'm running this on a single machine
and I have a terabyte terabyte of data
going into it
if this is my personal computer and I'm
running a terabyte of data into this you
know this is running rather quickly
through all 500 iterations but you got a
terabyte of data we're talking something
closer to days week
you know even with a
3.5 gigahertz machine in in eight cores
it's still going to take a long time to
go through a full terabyte of data
and then we want to start looking at
putting it into some other framework
like spark or something that will the
process on there more across multiple
processors and multiple computers
and if we scroll all the way down to the
bottom you're going to see here's our
square mean error 0.0088
if we scroll way up you'll see it kind
of oscillates between 0.088 and 08089
it's right around 2 to 250 where you
start seeing that oscillation where it's
really not going anywhere so we really
didn't need to go through a full 500
epics
uh you know if you're retraining this
stuff over and over again it's kind of
good to note where that error zone is so
you don't have to do all the extra
processing of course if you're going to
build a model
we want to go ahead and run a prediction
on it
so let's go ahead and make our
prediction remember we have our training
test set and our test set or the we have
the
trainx and the train y for training it
or train predict and then we have our
test X and our test y going in there so
we can test to see how good it did
and we come in here we have you'll see
right here we go ahead and do our train
predict equals model predict train X
and test predict model predict Test X
why would we want to run the prediction
on trainx well it's not a hundred
percent on its prediction we know it has
a certain amount of error and we want to
compare the error we have on what we
programmed it with with the error we get
when we run it on new data this never C
the model's never seen before and one of
the things we can do we go ahead and
invert the predictions this helps us
level it off a little bit more
get rid of some of our bias we have
train predict equals and NP
exponential M1 the train predict and
then train y equals the exponential M1
for train Y and then we do the given
that with train test predict and test y
um again reformatting the data so that
we can it all matches and then we want
to go ahead and calculate the root mean
square error so we have our train score
which is your math square root times the
mean square root error train Y and train
predict and again we're just um this is
just feeding the data through so we can
compare it and the same thing with the
test
and let's take a look at that because
really the code makes sense if you're
going through it line by line you can
see we're doing but the answer really
helps to zoom in so we have a train
score which is
2.40 of our root mean square error
and we have a test score of 3.16 of the
root mean square error
if these were reversed if our test score
is better than our training score then
we've over trained something's really
wrong at that point you got to go back
and figure out what you did wrong
because you should never have a better
result on your test data than you do on
your training data and that's how we put
them both through that's why we look at
the error for both the training and the
testing when you're going out and
quoting your publishing this you're
saying hey how good is my model it's the
test score that you're showing people
this is what it did on my test data that
the model had never seen before this is
how good my model is and a lot of times
you actually want to put together like a
little formal code
where we actually want to print that out
and if we print that out you can see
down here
test prediction and standard deviation
of data set 3.16 is less than
4.40 I'd have to go back and we're up
here if you remember we did the square
means error this is standard deviation
that's why these numbers are different
it's saying the same thing that we just
talked about
3.16 is less than 4.40 model is good
enough we're saying hey this is this
model is valid we have a valid model
here so we can go ahead and go with that
and along with putting a formal print
out of there we want to go ahead and
plot what's going on
uh and this we just want a pretty graft
here so that people can see what's going
on when I walk into a meeting and I'm
dealing with a number of people they
really don't want these numbers they
don't want to say hey what's I mean
standard deviation unless you know what
statistics are you might be dealing with
a number of different departments head
of sales might not work with standard
deviation or have any idea what that
really means number wise and so at this
point we really want to put it in a
graph so we have something to display
and with displaying you gotta remember
that we're looking at the data today
going into it and what's going to happen
tomorrow
so let's take a quick look at this we're
going to go ahead and shift the train
predictions for plotting we have our
train predict plot
NP MP like data set train predict plot
set it up with null values
you know it's just kind of it's kind of
a weird thing where we're creating the
um the data groups as we like them and
then putting the data in there is what's
going on here so we have our train
predict plot which is going to be our
look back our length plus look back
we're just it's going to equal train uh
train predict so we're creating this
basically we're taking this and we're
dumping the train predict into it so now
we have our nice train predict plot
and then we have the shift test
predictions for the plotting we're going
to continue more of that oops looks like
I put it in here double no it's just uh
yeah they put it in here double
um didn't mean to do that
we really only need to do it once oh
here we go
um this is where the problem was is
because this is the test predict
so we have our training prediction we're
doing the shift on here and then the
test predict we're going to look at that
same thing we're just creating those two
data sets uh test predict plot length
prediction set up on there
and then we're going to go through the
plotting the original data set and the
predictions so we have a Time axes
always nice to have your time set on
there set that to the time array time
axes lap all this is setting up the time
variable for the bottom and then we have
a lot of stuff going on here as far as
setting up our figure
let's go ahead and run that and then
we'll break it down
we have on here our main plot we have
two different plots going on here the
ispu going up in the data and the ispu
here with all these different settings
on it
and so we look at this we have our ax1
that's the main plot I mean our ax
that's the main plot and we have our ax1
which is the secondary plot over here so
we're doing a figure PLT or plt.figure
and we're going to dump those two graphs
on there and so we take and if you go
through the code piece by piece which
we're not going to do we're going to do
the the
data set here
exponential reverse exponential so it
looks correctly we're going to label it
the original data set we're going to
plot the train predict plot that's what
we just created we're going to make that
orange and we'll label it train
prediction test predict plot we're going
to make that red and label it test
prediction and so forth set our ticks up
let's actually just put ticks time axes
gets its ticks the little little marks
that are going along the axes that kind
of thing and let's take a look and see
what these graphs look like
and these are just kind of fun you know
when you show up into a meeting and this
is the final output and you say hey this
is what we're looking at here's our
original data in blue
here's our training prediction you can
see that it trains pretty close to what
the data is up there
I would also probably put a like a
little little time stamp and do just
right before and right after where we go
from a train to test prediction and you
can see with the test prediction the
data comes in in red
and then you can also see what the
original data set looked like behind it
and how it differs and then we can just
isolate that here on the right that's
all this is is just the test prediction
on the right uh and it's you know
there's you'll see what the original
data set there's a lot of Peaks we're
missing and a lot of lows were missing
but as far as the actual test prediction
it's pretty it does pretty good it's
pretty right on you can get a good idea
what to expect for your ispu and so from
this we would probably publish it and
say hey this is what you expect and this
is our area of this is a range of error
that's the kind of thing I put out
on a daily basis maybe we predict the
sales are going to be this or maybe
weekly so you kind of get a nice you
kind of flatten the um
data coming out and you say hey this is
what we're looking at the big takeaway
from this is that we're working with
let me go back up here oops oh too far
there we go
um is this model here this is what this
is all about we work through all of
those pieces all the tensorflows and
that is to build this sequential model
and we're only putting in the two layers
this can get pretty complicated if you
get too complicated it never it never
verges into a usable model so if you
have like 30 different layers in here
there's a good chance you might crash it
kind of thing so don't go too haywire on
that and that you kind of learn as you
go again it's domain knowledge and also
starting to understand all these
different layers and what they mean
the data
analytics behind those layers is
something that your data analysis
professional would come in and say this
is what we want to try
but I tell you as a data scientist a lot
of these basic setups are common and I
don't know how many times
working with somebody and they're like
oh my gosh if I only did a tangent H
instead of a relu activation I worked
for two weeks to figure that out well as
a data science I can run it through the
model in you know five minutes instead
of spending two weeks doing the the math
behind it so that's one of the
advantages of data scientists is we do
it from programming side in a data
analytics is going to look for it how
does it work in math and this is really
the core right here of tensorflow and
cross is being able to build your data
model quickly and efficiently and of
course with any data science putting out
a pretty graph so that your shareholders
again we want to take and reduce the
information down to something people can
look at and say oh that's what's going
on they can see stuff what's going on as
far as the dates and the change in the
ispu now let's talk a little bit about a
recurrent neural networks so neural
networks are of different types we have
CNN we have RNN and so on now ottoman is
one type of neural network RNN stands
for recurrent neural network the
networks like CNN and an are feed
forward Network which means that the
information pretty much only goes from
left to right or from front to back
whichever way you call it whereas in
case of recurrent neural network there
will be some information traveling
backwards as well so that is why it is
known as recurrent neural network and
each of these types have a specific
application so for example convolutional
neural networks or CNN are very good for
doing image processing and object
detection using for video and images
whereas recurrent neural networks are
pretty good for doing NLP or speech
recognition and so on okay so for the
next few minutes we will kind of focus
on recurrent neural networks and we will
see an example of how we can use RNN to
do a Time series analysis so in a
typical neural network this is how it
looks right so where you have a neuron
and then the inputs are coming to the
neuron and then you have an output which
goes to other neurons in the other
layers in case of recurrent neural
network what happens is you have the
inputs let's say at a given point in
time but then a part of the previous
output also gets fed in along with the
inputs for the given time now this can
be a little confusing so let's see if we
can take a little expanded view of this
so this is another view of one single
neuron which is what is known as in an
unfolded manner okay so if we are
getting inputs or data over a period of
time then this is how the neuron will
look remember these are not three
neurons this is one neuron and it is
what is known as it is shown in a
unfolded way okay we are on we have
unfolded this single neuron over a
period of time so at a given time let's
say T minus 1 an input of x t minus 1 is
fed to the neuron and it gets a output
of Y T minus 1 then the next instant
which is x t at a time T right there is
an input of x t and then there is an
output of y t so this is a single neuron
but is displayed in an unfolded way so
this is like expanding it over a period
of time so let's start with this part
here this particular neuron gets an
input at an instant T minus 1 let's say
time is equal to T minus one it gets an
input of x t minus 1 and it gives an
output of Y T minus then when we go to
the instant T here it gets an input of x
t and it also additionally gets an input
from this previous time frame T minus 1.
so this y t minus 1 gets fed here and
that results in YT all right then when
we go to the time t plus 1 there is an
input of x t minus 1 at that given time
plus the input from the previous time
frame which is a time frame of T that
also gets fed in here and then we get an
output of y t plus 1 okay so let me
explain once again this is a single
neuron so these are not three different
neurons a single neuron seen over a
period of time from T minus 1 to t plus
1 and unlike a regular feed forward
neuron which only gets X in this case
the input is X and all also another
input which is coming from the previous
time frame and that is what this Loop is
in this on the left hand side diagram
this Loop is indicating that okay so on
the right hand side it is represented in
an unfolded way okay so the input if we
take a time frame T for example is not
only x t which is the input which is the
normal input at the time frame T but it
is also getting an input from the
previous time frame which is this y t
minus 1 will also being fed as an input
that is a key differentiator here okay
similarly at the instant t plus 1 it is
getting a normal input of x t plus 1
plus this y t is actually being fed here
and then the output comes as y t plus
one okay so this is the construct of a
recurrent neural network now recurrent
neural networks again can be of
different subtypes it can be one to one
one to many many to one and many to many
depending on what kind of application we
want to use one of the examples is like
the stock stock price so you're feeding
so there is only one input only thing is
it is spread over a period of time so
you're feeding the stock price input
that is what comes here as an input and
you get an output which is again the
stock price which is probably predicted
over the next uh two days three days or
10 days or whatever so the number of
outputs and inputs is the same there is
only one input there is only one output
only thing is it is spread over time so
variables are not many then you have one
too many so there is one input but you
are expecting multiple outputs what is
an example of this let's say you want to
caption an image so how do you want to
do that what is the input that will go
here is just an image which is one input
but what is output you are looking for
you are looking for a phrase maybe right
not just a word but a phrase so it's
like cat is sitting on a mat right so
the image is there is only one image in
which consists of the cat sitting on a
mat but the output or like maybe three
or four words which is saying the cat is
sitting on a mat so this is one to many
right then you can have many to one
examples of this are like you're feeding
some text and you want to know the
output whether the text is what kind of
sentiment is expressed by the text it
could be positive it could be negative
so that's the output you're looking for
so you feed a large number of words
maybe the text May consist of words or
lines or whatever so that is what is the
multiple inputs but the output all it
says is the sentiment is positive it is
just one output or the sentiment is
negative just one output right so this
is minute over then we have many too
many so what is an example of this let's
say you want to do some translation so
how do you do a translation you feed in
a sentence maybe in a particular
language English and then you want
another a sentence actually in another
language so that is like there are
multiple inputs one sentence can consist
of multiple words and the output also is
a sentence consisting of multiple words
all right so how do we Implement RNN
this is an example of implementing an
RNN for a particular use case so in our
particular example we have the data
about milk production over a few months
and using RNN we want to predict because
this is time series analysis so RNN is
good to do time series analysis so using
RNN we want to predict what will be the
milk production in the future so let us
see how we can do that I will first take
you through quickly through the slides
and then I will actually run this in in
a Jupiter notebook the code I will run
it in the jupyter notebook so this is
how it looks the code looks so while the
sustensorflow you still use the standard
python libraries like numpy and pandas
and even matplotlib to do some initial
processing getting the data processing
it cleaning it whatever all that can be
still done in within the same program so
that's what we're doing here we're
importing some libraries like numpy and
pandas and then we read the data file
and if we plot the data we see here it
is the data for for the years 1962 to 75
and we can see that there is a certain
Trend right so this is how a typical
time series data would look again some
of you if you are not familiar with time
series and time series analysis and so
on I would recommend you to go through
some videos around that which will make
it easy to understand this so this is
our typical time series data would look
in this case it is nothing but there is
only one variable which is milk
production and it is spread across
several years so this is going from 1962
to 75 and that value has been plotted so
if you see there is a certain kind of a
trend here which is basically going
upwards and there will be some
seasonality so time series data has
these three components right so it will
have a trend it will have a seasonality
it will have seasonality and then it
will have some Randomness so that's what
this graph is showing and now if we want
to perform analysis on this time series
analysis on this first thing we need to
do is split the data into train and test
and in this case we we will just use a
straightforward method which is taking
the data for the first 13 years so the
idea here is we need to train our model
right this is time series data so what
and we want to predict for the future so
the way we need to use this is we have
to take the data for a certain period of
time and train our model and then we use
a part of the known data so that we can
then ask our model to predict for that
duration and compare it with the known
information so what do we mean by that
so here if you see this data I will show
you in the notebook as well Jupiter
notebook as well this has 13 years of
data so what we are doing is we are
taking the first 12 years and using that
as our training set right so 12 years we
are doing and this has about I think 14
years of data so we are taking 13 years
of data for training purpose and then we
are using the last one year remaining
one year of data for testing purpose
because here what we can do is this one
year data we know the value right
because if we want to compare the
accuracy or anything like that we need
to know the value so in this case we
know the values of this one particular
year so we will use that but at the same
time we train the model for 13 years of
data and for the 14th Year we will ask
our model to predict and then compare it
with this known value so that we know
how accurate our model is I hope that
makes sense okay so that's what we are
doing here 156 is nothing but 12 into 13
on a monthly basis we get the data the
next step is to scale the data this is
all regular data manipulation data
munching activity and then you split it
you are basically assigning the train
and test data to the to the scaled
variables and then you need to read the
data in batches so it is very important
as I mentioned earlier also that instead
of reading the entire data in one shot
you feed the data in batches so in order
to do that we are writing a function for
that that's all we are doing here so
that is still here what we have done is
regular Python Programming there is no
tensorflow as of now here okay so so far
what we have done is preparation data
preparation data munching now what we
are doing is actually training our model
so this is where tensorflow now comes
into picture so we are importing first
step is to import the tensorflow library
so this is how you do it import
tensorflow SDF and then you can Define
some variables or constants whichever
way now here of course there are a
couple of ways of doing it you can
create them as tensorflow nodes but that
is a little bit of an overhead we will
just use the regular python variables
are constant so here I am creating
regular python variables and I am saying
number of inputs is one so instead of
hard coding I'm just storing them as
variables so this is number of inputs is
one number of time steps is 12 number of
neurons is unread and so on right and
then learning rate is 0.03 and number of
iterations we are saying is four
thousand then we create placeholders now
we will be storing the independent
variables in X and the dependent
variables in y and this we will read
from an external file remember I told
you placeholders are used for getting
data from outside and then feeding it to
our model so that's the reason we have
two placeholders one for reading the X
values and another for reading the Y
values in this step we are only just
creating the nodes right so this is just
creating the graph and similarly we are
mentioning what is the loss function and
what is Optimizer and how to run the
optimizer and once that is done you are
initializing the variables and then
you're creating a saver variable or a
saver instance so saver is basically
nothing but in machine learning you can
train your model and you can save it for
later use so that's where this saving
comes into play we will see how to use
that as well and then the last step is
to create your session and run this
graph right so we are initializing the
variables remember this we run init so
which is nothing but this one so we are
initializing the variables whatever are
there instead of hard coding remember in
earlier case we were hard coding how
many iterations so here we are saying
for the given number of iterations which
are stored in this maybe how many
iterations We Said is What is the value
of iterations this is training
iterations is four thousand so we
specify that based on the number of
training iterations next batch will
basically fetch the data and then we run
the session we are basically saying
train is the node which we will run in
the session and this will basically
train our model and this is more for
printing every 100 times you print it
that's all this there's nothing more to
it so for example the output would look
somewhat like this this is for zero this
for 100 next for 200 and so on and then
you save the model remember saver we
created so you save the model for later
use so this is our test data remember we
are doing this on training data right so
once the training data training is done
we then try to create the inference on
the test data so that we can compare how
accurate this is right so this is how
the test set would look and then we will
basically restore this model okay
because in the previous slide we created
the model and we stored it so now we
have to restore the model and then run
our test data against it and see what
are the values that are predicted what
are the Y values and then compare with X
to see the accuracy so train seed is
what we are seeing here so that is what
will have the the predicted values and
these predicted values what we want to
do is we want to create an additional
column because remember we need to
compare because we want to find out the
accuracy of this model so we need to
compare the predicted values with the
actual value so what we are doing here
is adding another column called
generator and assign a value to that all
right so this is the result of the
prediction and then if we need to
reshape because we have to show this in
the form of a monthly results so that's
what we're doing here and once we
generate the results and then display it
we can actually see it month wise here
and the actual and the generated values
Okay so we create a data frame which is
a combination of the actual values and
the predicted values from the tests are
set and then we can plot to see how the
trend is as you can see pretty much the
actual value is in blue color and
generated or predicted value the curve
is in yellow color the trend is
maintained so it will probably it's not
a hundred percent accurate but the trend
is maintained all right so let's do one
thing let's go into the jupyter notebook
and take a look at how this works in
tensorflow environment actually the code
I will walk you through the code this is
my Jupiter notebook and the data is
taken from this particular Link in case
you're interested you can download it
from there and this is the data for the
years 1962 to 75. the first thing we do
is import these libraries because before
we start with the actual tensorflow
activity we need to prepare the data and
so on so for that you can use your
regular python libraries which are like
numpy pandas and so on so that's what we
are doing here and then we read the data
using pandas into a data frame so this
is a data frame milk is a data frame and
we will do some quick exploratory
analysis just to see how our data is
looking initials five records that we'll
get one two three four five that is the
head so it goes from 1960 to January to
1962 May once we re-change or basically
we need to split this into month
specifically separately so that's what
we are doing here day to time so we kind
of do a little bit of a re-indexing and
then if we plot this this is how the
data look as you can see there is a
clear upward Trend and there is some
seasonality and so on but anyway we will
not break that up into those components
we will just use RNN to predict and test
okay so the next thing is to check some
if we can do run like info it will tell
us what is the information about this
data set so let us just run that okay so
it is just telling us how many total
columns and what is the size of the file
and so on and so forth again in case you
are interested in doing some exploratory
analysis so what we'll do next is we
will take the 13 years of data for our
training data set so what are we going
to do now let me step back so what have
we seen here we have seen that there are
168 records so which is nothing but 14
years of data we now have to split this
into our training and test data set so
how do we do that I think in case of
Time series we cannot do it like 80 20
like we do in normal letting in normal
machine learning process here it is time
series data so what we are going to do
is we will take out of this 14 years we
will take the first 13 years of data and
we will use that for training and then
we will test it with the last which is
the 14th year data we will use it for
testing okay so that's what we are going
to do here so let's split that so
training set will consist of my 156
records which is nothing but 12 13 into
12 right my 13 years of data I am taking
for training and then my test set will
consist of the bottom 12 observations
which is last one year of data which is
the 14th year okay so that's now done
training and test splitting is done the
next step is to do some normalizing
which is basically we'll use our scaling
rather we will use min max Scala and we
will just scale the data and now we do
it for both tests as well as train now
we are ready to create our RNN model but
before that one quick thing in order to
fetch the data we have to create a
function so that's what we are doing
here we create a function called Next
batch and how you want to fetch the
steps they are all defined as our
constants if you remember and that's
what this function is all about so we
Define that function we will be calling
that in our Training Method All right so
from here onwards up to there we are
done with the preparation of the data
and whatever functions or whatever are
required from here onwards is where the
tensorflow parts starts so the first
thing we do is import the tensorflow
library typically this is a very
standard way of importing the library we
say import tensorflow as TF now TF is
nothing but a name so you can change it
to tf1 or ABC or XYZ whatever so this
part you can change but by and large
everybody uses this so we I would rather
recommend you also use the same syntax
so you say import tensorflow SDF so it's
very easy for others to understand as
well and then you declare or Define a
bunch of constants that's what we are
doing here like for example the number
of time steps in this case it is 12
number of neurons there are 100 number
of outputs it is only one and so on
right and then the learning rate and all
that we are declaring or defining those
variables in this particular block next
is to create placeholders you remember
the placeholders are used for feeding
the data so we have X and Y X is for the
input which is the independent variable
and Y is the output which is the
dependent variable and in our case these
are not different characteristics or
features but it is the same one feature
or one variable but it is over a period
of time so that's the only difference so
that's what we are doing here and now we
need to create our Network right the
neural network so in our case we said we
will create a RNN layer now there are
different ways or different formats of
RNN which is probably not in scope of
this module so for now we will just
assume we are creating one RNN layer and
one type of RNN is Gru cell so we will
use Gru cell and we use this apis for
creating our layer so each cell will be
with an output projection wrapper there
is a need for doing that and again the
details of this we will probably do in
another video where we talk in detail
about RNN so for now we are creating a
GRU cell with the wrapper and the Syntax
for creating the gru cell is like this
the number of units which we see said
there should be 100 neurons what will be
the activation function in this case it
is relu and then number of outputs in
our case it is only one okay so we
create the cell here okay then we got
the gru cell and then we say what is our
output which is nothing but we get the
outputs and the states and their states
and which is uh the way we get that is
DF Dot N N dot Dynamic underscore RNN so
if we call this and we pass the cell and
the data which is basically in
placeholder again remember all we are
doing here is we are creating just the
nodes for the graph so nothing is
actually getting executed from a
tensorflow perspective okay all right so
once that is done now we need to pass
this calculate the outputs and the
states using the dynamic underscore RNN
method and we pass the cell as a
parameter and then the X values as a
parameter and if we run that we will
have the outputs and the states and this
is where we actually run the Training
Method so or create not really run the
Training Method but we create the nodes
for the training and the optimizer then
we have the initialization of the
variables and we save this model we
create a saver object just to save the
model because we will then restore it
and run it to do our predictions so this
is what we will do here so that is
another node and this is where we
actually create a session and run the
training okay so let's go ahead and do
that that will take a little while
because we said 4000 iterations so we
will allow it to finish we will probably
come back once the training is completed
all right so the training is done now
let's go and run this on the test data
so let's just quickly take a look at the
test data set so this is our one year
data for the year of 1975 so if we see
this this from January February and so
on and we will pass this to our model so
what are we doing here we are restoring
the model here for example right this is
the path that we have given when we were
saving the model let's go back once and
show you let me show you where we did
the saving of the model yes so we save
the model here so that again we will
restore that and we will use that to run
it on our test data and see how well it
predicts so let's go ahead and run this
and this is just 12 records so this will
not take time remember I told you
training is what takes a lot of time the
regular inference this is called
inference doesn't take much time right
so it just depends on how much data
there were only 12 records here so it
was very quick but in training what we
do we pass this multiple times there are
iterations four thousand iterations
needed for example so that takes longer
and in general in machine learning deep
learning training is what takes the
maximum amount of time all right so
let's go ahead and see the results we we
will in order to plot it we will have to
kind of adjust the format of the results
otherwise we will not be able to see it
in a proper way and then we will so this
is our so what we have done is we
created a data frame which consists of
the predicted value and the and the
actual value so this is the data frame
so this is the actual value this is 834
782 and so on this is what our model has
predicted so it may not be so easy to
see in a tabular format so let's go
ahead and plot it so that we can compare
these two so when we plot it you will
see that the trend is more or less
maintained right so we go from the blue
color is the actual values and the
yellow color or the orange color
whatever is it is the one which is
generated by our model so it pretty much
is following the actual Trend so not bad
for such a quick iteration and training
this tutorial is about object detection
we will walk you through through a
tensorflow code using which we will do
object detection in images we will tell
you what are the libraries that are
required a little bit about the Coco
data set and then we will show you the
implementation code itself a demo of the
code all right so let's get started so
what is the tensorflow object detection
API this is an open source framework
which is actually provided by the
tensorflow team and there are trained
models available and the sample code is
also available which we can use in order
to easily detect objects in images and
videos this is pretty robust and can
detect objects fairly quickly and this
is very easy for people to use as well
people with very less or no knowledge of
machine learning or deep learning can
also with a little bit of Python
Programming knowledge can actually use
this API this library to build object
detection applications this is a list of
libraries that are required and they
have been shown in the code as well the
exact purpose of each of this Library
why it is required is out of the scope
of this tutorial but we will see in the
code as we walk through some of these
libraries how and why they are used the
Coco data set Coco stats for common
objects in context so this data set
consists of 300 000 images of 90 most
commonly found objects like chairs and
tables and so on and so forth so this
model has been trained or in fact a set
of models have been trained on this data
set and this is pretty good to detect
the most common objects in the images
and videos so with that let's get into
the code all right so the first part is
to import all these libraries and this
we have shown you in the slides as well
again a large part of this will be for
doing some helper functions and maybe
for visualizing the images and so on and
so forth so that's the reason they are
required as I said the exact details of
each and every Library probably is out
of scope but these are needed so as a
first step maybe you just go ahead and
include these libraries and run the code
and maybe at later point we can discuss
what each of these libraries does now
this will work with tensorflow version
higher than 1.4 so if you are having
tensorflow version below 1.4 you may
have to upgrade to a higher version so
let me go ahead and execute the cell and
we also need this line of code to make
sure that once we run this object
detection the labeled images are
displayed within this notebook and many
of you by now must be familiar with this
and we will import a few utility
libraries and you will see that we will
be using some of these for visualization
purpose so once the objects are detected
we need to display the information what
that object is and then what percentage
of confidence the model has so all these
details that's the utility functions
that stored here and then the next part
is to prepare the model as I mentioned
we will be using an existing trained
model the tensorflow team has actually
provided these models the one that we
will be using is SSD with the mobile net
but you can actually use any one of the
ones that are listed in this URL let me
just quickly take you through this URL
these are a bunch of models trained
models that are readily available for
anybody to use it is open source and let
me scroll down the only thing is that if
there are some of them with where the
accuracy is much higher but they take
longer and there are some where the
accuracy is not so high and they are
much faster so they are faster but
accuracy may not be that very high so
you can play around with some of these
and we in this particular exercise or in
this particular tutorial we are using
this SSD model which is SSD mobile at
version one so that's the model that
we'll be using so in this cell we are
primarily creating a bunch of variables
with the various for example the name of
the model the path and so on and so
forth so that we will be using these
names in The Next Step which is to
download this model and install it
locally these are also referred to as
Frozen model so once they are trained
and then you kind of extract or you you
freeze the model so that's the reason
they are called Frozen models so that
others can just use this without any
further training so this is where we
download and extract our model locally
so this will take a little while let me
see if I can wait or maybe pause the
video and come back once it is done
might take a little while let's see if
it completes I have a pretty high speed
network but even then it takes some time
so that's good but this part is over now
let us see this part and yes both are
done so once that is done we need to
load some label mapping basically what
this will do is your model as you may be
aware by now if you do some
classification the model will actually
not give any output the text it will
give some numbers so if there are five
classes it will say okay this belongs to
model class one or two or three or five
and so on the numbers now each of these
will obviously the numbers will not not
make any sense to the outside world so
we need to do some small mapping so in
this case let's say one may be a chair
two maybe a table three maybe a balloon
and so on so that kind of number to text
mapping we need to do and that is what
is being done in this particular cell
and then we have a helper code which
will load the image and convert it into
a numpy array so that the number array
is what gets processed and used by the
model to do the detection part of it so
that is what this method is all about so
we will be there later on we will be
calling that function and next is
preparation for detection so here we are
basically telling where the images are
stored and how many images or what is
the naming convention or format of the
images now if you want you can modify
this code for example currently I have
test underscore images as my folder so
let me go and show this to you so this
is under my object detection folder I
have another subfolder where I am
storing my images which is text
underscore images now you can rename
this folder and give some other name and
then in your code you can probably give
that particular name for the subfold
similarly the format of these files what
is the name and format of this files
here it is in a very simple format which
is the names of the files are like beach
one beach two Beach three and so on so I
have taken Beach as the theme so I have
images which are related to beaches so
this is beach one beach two and then
these three are few others but we will
use these three for our demo and so
that's what I'm saying here the name of
the images will be which something dot
jpeg which is jpeg format and in this
curly braces basically we will will be
filled with either one two or three
based on in this part particular for
Loop okay so that is what this is doing
all right so the next step is to run
inference on these images in a loop and
what we are basically doing here is
getting these images one by one and then
running through the model to find out
what are the objects that can be
detected and then against each of the
object a box will be drawn and it will
be labeled with the name and the
percentage of accuracy or confidence
that the model detects these objects
okay so that is the function here and so
let me just run that piece of code and
here is basically where we are calling
this function so we are loading this
images and then we are calling this
function for each image and then we are
displaying this using the matplotlab
library so let's run this it will take
one image at a time and then detect the
images now the beauty is that the same
form of the code can be used for doing
object detection in a video so we have
another video for doing object detection
in a video so most of the code out there
will be reused from here and the only
thing is that instead of reading the
images from the local storage we read
the frames from the video and there is a
neat little video reader that is
available and it will be shown in the
other video and frame by frame we read
the video and then pass on to this
function and it will act as if each of
these frames is an image and then it
will do the same object detection on the
entire video so that's in a separate
video just look out for that and
actually the information about that is
provided in the cards the I symbol so
that's the the video object detection in
video that's the separate tutorial all
right so now that we have all the pieces
together this the large cell is where
the whole action takes place so let's
run this and see how it looks so it will
take probably a little while and there
are about three images let's see what it
detects there we go so good so the first
one it has detected a person and that
too with 97 accuracy which is uh I think
pretty good okay and then the next image
it detects
umbrella and chair there are a few other
options but it's not able to detect it
has detected umbrella with 63 percent
accuracy or confidence rather and the
chair with the 58 percent again not bad
then let's see the next image so here
these are actually balloons hot air
balloons but the model thinks it is kite
which is uh probably not that bad it
sees there's something in the sky and
therefore probably it thinks it is a
kite and it detects that with 65 percent
confidence okay so that was pretty much
all I wanted to show you in this
particular tutorial about object
detection in images and in this video I
will walk you through the tensorflow
code to perform object detection in a
video so let's get started this part is
basically you're importing all the
libraries we need a lot of these
libraries for example numpy we need
image i o date time and Hill and so on
and so forth and of course matplotlib so
we import all these libraries and then
there are a bunch of variables which
have some parts for the files and
folders so this is regular stuff let's
keep moving then we import the
matplotlib and make it in line and a few
more Imports all right and then these
are some warnings we can just ignore
them so if I run this code once again it
will go away all right and then here or
not C2 the model preparation what we're
going to do is we're going to use an
existing neural network model so we are
not going to chain a new one because
that really will take a long time and it
needs a lot of computation resources and
so on and it is really not required
there are already models that have been
trained and in this case it is the SSD
with mobile net that's the model that we
are going to use and this model is
trained to detect objects and it is
readily available as open source so we
can actually use this and if you want to
use other models there are a few more
models available
you can click on this link here and let
me just take you there there are a few
more models but we have chosen this
particular one because this is faster it
may not be very accurate but that is one
of the faster models but on this link
you will see a lot of other models that
are readily available these are trained
models some of them would take a little
longer but they may be more accurate and
so on so you can probably play around
with these other models okay so we will
be using that model so this piece of
code this line is basically importing
that model and this is also known as a
frozen model the term we use is frozen
model so we import download and import
that and then we will actually use that
model in our code right so these two
cells we have downloaded and import the
model and then once it is available
locally we will then load this into our
program right so we are loading this
into memory and then you need to perform
a couple of additional steps which is
basically we need to to map the numbers
to text as you may be aware when we
actually build the model and when we run
predictions the model will not give a
text the output of the model is usually
a number so we need to map that to a
text so for example if the network
predicts that the output is 5 we know
that 5 means it is an airplane things
like that so this mapping is done in
this next cell all right so let's keep
moving and then we have a helper code
which will basically load the data or
load the images and transform into lump
IRAs this is also used in doing object
detection in images so we are actually
going to reuse because video is nothing
but it consists of frames which in turn
are images so we are going to pretty
much use reuse the same code which we
use for doing object detection in images
so this is where the actual detection
starts so here this is the path for
where the images are stored so this is
once again we are reusing the code which
we wrote for detecting objects in an
image so this is the path where the
images were stored and this is the
extension and this was done for about
two or three images so we will continue
to use this and we go down I'll skip
this section so this is the cell where
we are actually loading the video and
converting it into frames and then using
frame by frame we are detecting the
objects in the image so in this code
what we are doing basically is there are
a few lines of code what they do is
basically once they find an object a box
will be drawn around those each of those
objects and the input file the name of
the input video file is traffic it is
the extension is MP4 and we have this
video reader it's a excellent object
which is basically part of this class
called image i o so we can read and
write videos using that and the video
that we are going to use is traffic dot
MP4 you can use any mp4 file but in our
case I picked up video which has like
cars so let me just show you so this is
in this object detection folder I have
this mp4 file I'll just quickly create
this video it's a little slow yeah okay
so here we go this is the video it's a
short one relatively small video so that
for this particular demo and what it
will do is once we run our code it will
detect each of these cars and it will
annotate them as cars so in this
particular video we only have cars we
can later on see with another video I
think I have cat here so we can also try
with that but let's first check with
this traffic video so let me go back so
we will be reading this frame by frame
and you know actually we will be reading
a video file but then we will be
analyzing it frame by frame and we will
be reading them at 10 frames per second
that is the rate we are mentioning here
and analyzing it and then annotating and
then writing it back so you will see
that we will have a video file named
something like this traffic underscore
annotated and we will see the annotated
video so let's go back and run through
this piece of code and then we will come
back and see the annotated video this
might take a little while so I will
pause the video after running this
particular cell and then come back to
show you the results all right so let's
go ahead and run it so it is running now
and it is also important that at the end
you close the video writer so that it is
similar to a file pointer when you open
a file you should also make sure you
close it so that it doesn't hog the
resources so it's very similar at the
end of it the last piece or last line of
code should be video underscore writer
Dot close all right so I'll pause and
then I'll come back okay so I will see
you in a little bit all right so now as
you can see here the processing is done
The Hourglass has disappeared that means
the video has been processed so let's go
back and check the annotated video we'll
go back to my file manager so this was
the original
traffic.mp4 and now you have here
traffic underscore annotated MP4 so
let's go and run this and see how it
looks
you see here it has got each of these
cars getting detected let me pause and
show you so we pause here it says car 70
let us allow it to go a little further
it detects something on top what is that
truck okay so I think because of the
board on top it somehow thinks there is
a truck let's play some more and see if
it detects anything else so this is
again a car looks like so let us here so
this is a car and it has confidence
level of 69 percent okay this is again a
car all right so basically till the end
it goes and detects each and every car
that is passing by now we can quickly
repeat this process for another video
let me just show you the other video
which is a cat again there is uh this
cat is not really moving or anything but
it is just standing there staring and
moving a little slowly and our
application will our network will detect
that this is a cat and even when the cat
moves a little bit in the other
direction it'll continue to detect and
show that it is a cat Okay so yeah so
this is our original video is let's go
ahead and change our code to analyze
this one and see if it detects how
Network detects this cat close this here
we go and go back to my code all we need
to do is change this traffic to cat the
extension it will automatically pick up
because it is given here and then it
will run through so very quickly once
again what it is doing is this video
reader video underscore reader has a
neat little feature or interface whereby
you can say for frame in video
underscore reader so it will basically
provide frame by frame so you are in a
loop frame by frame and then you take
that each frame that is given to you you
take it and analyze it as if it is an
image individual image so that's the way
it works out is very easy to handle this
all right so now let's once again run
just this cell rest of the stuff Remains
the Same so I will run this cell again
it will take a little while so the r
classes come back I will pause and then
come back in a little while all right so
the processing is done let's go and
check the annotated video go here so we
have get annotated dot MP4 let's play
this all right so you can see here it is
detecting the cat and in the beginning
you also saw it detected something else
here there looks like a detected one
more object so let's just go back and
see what it has detected here let's see
yes so what is it trying to show here
it's too small not able to see but it is
trying to detect something I think it is
saying it is a car I don't know all
right okay so in this video there's only
pretty much only one object which is the
cat and uh let's wait for some time and
see if it continues to detect it when
the cat turns around and moves as well
just in a little bit that's going to
happen and we will see there we go and
in spite of turning the other way I
think our network is able to detect that
it is a cat so let me freeze and then
show here it is actually still continues
to detect it as a cat all right so
that's pretty much it I think that's the
only object that it detects in this
particular video okay so close this so
that's pretty much it thank you very
much for watching this video hey there
learner simply learn brings you Masters
program in artificial intelligence
created in collaboration with IBM to
learn more about this course you can
find the course Link in the description
box below in this tutorial we will take
the use case of recognizing handwritten
digits this is like a hello world of
deep learning and this is a nice little
MNS database is a nice little database
that has images of handwritten digits
nicely formatted because very often in
deep learning and neural networks we end
up spending a lot of time in preparing
the data for training and with MNS
database we can avoid that you already
have the data in the right format which
can be directly used for training and
mnist also offers a bunch of built-in
utility functions that we can straight
away use and call those functions
without worrying about writing our own
functions and that's one of the reasons
why MNS database is very popular for
training purposes initially when people
want to learn about deep learning and
tensorflow this is the database that is
used and it has a collection of 70
000 handwritten digits and a large part
of them are for training then you have
tests just like in any machine learning
process and then you have validation and
all of them are labeled so you have the
images and their label and these images
they look somewhat like this so they are
handwritten images collected from a lot
of individuals people have these are
samples written by human beings they
have handwritten these numbers these
numbers going from zero to nine so
people have written these numbers and
then the images of those have been taken
and formatted in such a way that it is
very easy to handle so that is MNS
database and the way we are going to
implement this in our tensorflow is we
will feed this data especially the
training data along with the label
information and the data is basically
these images are stored in the form of
the pixel information as we have seen in
one of the previous slides all the
images are nothing but these are pixels
so an image is nothing but an
arrangement of pixels and the value of
the pixel either it is lit up or it is
not or in somewhere in between that's
how the images are stored and that is
how they are fed into the neural network
and for training once the network is
trained when you provide a new image it
will be able to identify within a
certain error of course and for this we
will use one of the simpler neural
network configurations called softmax
and for Simplicity what we will do is we
will flatten these pixels so instead of
taking them in a two-dimensional
arrangement we just flatten them out so
for example it starts from here it's a
28 by 28 so there are 7 for 84 pixels so
pixel number one starts here it goes all
the way up to 28 then 29 starts here and
goes up to 56 and so on and the pixel
number 784 is here so we take all these
pixels flatten them out and feed them
like one single line into our neural
network and this is a what is known as a
soft Max layer what it does is once it
is trained it will be able to identify
what digit this is so there are in this
output layer there are 10 neurons each
signifying a digit and at any given
point of time when you feed an image
only one of these 10 neurons gets
activated
so for example if this is strained
properly and if you feed a number nine
like this then this particular neuron
gets activated so you get an output from
this neuron let me just use a pen or a
laser to show you here okay so you're
feeding the number nine let's say this
has been trained and now if you're
feeding a number nine this will get
activated now let's say you feed one to
the trained Network then this neuron
will get activated if you feed two this
neuron will get activated and so on I
hope you get the idea so this is one
type of a neural network or an
activation function known as soft Max
layer so that's what we will be using
here this is one of the simpler ones for
quick and easy understanding so this is
how the code would look we will go into
our lab environment in the cloud and we
will show you there directly but very
quickly this is how the code looks and
let me run you through briefly here and
then we will go into the Jupiter
notebook where the actual code is and we
will run that as well so as a first step
first of all we are using python here
and that's why the syntax of the
language is Python and the first step is
to import the tensorflow library so and
we do this by using this line of code
saying import tensorflow as TF TF is
just for convenience so you can name
give any name and once you do this TF is
tensorflow is available as an object in
the name of TF and then you can run its
methods and accesses its attributes and
so on and so forth and mnis database is
actually an integral part of tensorflow
and that's again another reason why we
as a first step we always use this
example mnist database example so you
just simply import mnist database as
well using this line of code and and you
slightly modify this so that the labels
are in this format what is known as one
hot true which means that the label
information is stored like an array and
let me just use n to show what exactly
it is so when you do this one hot true
what happens is each label is stored in
the form of an array of 10 digits and
let's say the number is 8 okay so in
this case all the remaining values there
will be a bunch of zeros so this is like
array at position zero this is at
position one position two and so on and
so forth let's say this is position
seven then this is position 8 that will
be 1 because our input is 8 and again
position 9 will be zero okay so one hot
encoding this one hot encoding true will
kind of load the data and such a way
that the labels are in such a way that
only one of the digits has a value of
one and that indicates So based on which
digit is one we know what is the label
so in this case the eighth position is
one therefore we know this sample data
the value is eight similarly if you have
a 2 here let's say then the labeled
information will be somewhat like this
so you have your labels so you have this
as 0 the 0th position the first position
is also zero the second position is one
because this indicates number two and
then you have third as zero and so on
okay so that is the significance of this
one hot true all right and then we can
check how the data is uh looking by
displaying the the data and as I
mentioned earlier this is pretty much in
the form of digital form like numbers so
all these are like pixel values so you
will not release see an image in this
format but there is a way to visualize
that image I will show you in a bit and
this tells you how many images are there
in each set so they're training there
are 55 000 images in training and in the
test set there are 10 000 and then
validation there are 5000. so all
together there are 70 000 images all
right so let's move on and we can view
the actual image by using the matplot
flip library and this is how you can
view this is the code for viewing the
images and you can view them in color or
you can view them in grayscale so the
cmap is what tells in what way we want
to view it and what are the maximum
values and the minimum values of the
pixel value so these are the Max and
minimum value so of the pixel values so
maximum is one because this is a scaled
value so one main means it is white and
zero means it is black and in between is
it can be anywhere in between black and
white and the way to train the model
that is a certain way in which you write
your tensorflow code and the first step
is to create some placeholders and then
you create a model in this case we will
use the softmax model one of the
simplest ones and placeholders are
primarily to get the data from outside
into the neural network so this is a
very common mechanism that is used and
then of course you will have variables
which are your remember these are your
weights and biases so for in our case
there are 10 neurons and each neuron
actually has
784 because each neuron takes all the
inputs if we go back to our slide here
actually every neuron takes all the 784
inputs right this is the first neuron it
has it receives all the 784 this is the
second neuron this also receives all the
700 so each of these inputs needs to be
multiplied with the weight and that's
what we are talking about here so these
are this is a matrix of
784 values for each of the neurons and
so it is like a 10 by 784 Matrix because
there are 10 neurons and similarly there
are biases now remember I mentioned
biases only one per neuron so it is not
one per input unlike the weights so
therefore there are only 10 biases
because there are only 10 neurons in
this case so that is what we are
creating a variable for biases so this
is something little new in tensorflow
you will see unlike our regular
programming languages where everything
is a variable here the variables can be
of three different types you have
placeholders which are primarily used
for feeding data you have variables
which can change during the course of
computation and then a third type which
is not shown here are constant so these
are like fixed numbers all right so in a
regular programming language you may
have everything has variables are at the
most variables and constants but in
tensorflow you have three different
types placeholders variables and
constants and then you create what is
known as a graph so tensorflow
programming consists of graphs and
tensors as I mentioned earlier so this
can be considered ultimately as a tensor
and then the graph tells how to execute
the whole implementation so that the
execution is stored in the form of for
graph and in this case what we are doing
is we are doing a multiplication TF you
remember this TF was created as a
tensorflow object here one more level
one more so TF is available here now
tensorflow has what is known as a matrix
multiplication or maximal function so
that is what is being used here in this
case so we are using the matrix
multiplication of tensorflow so that you
multiply your input values x with W
right this is what we were doing x w
plus b you're just adding B and this is
in very similar to one of the earlier
slides where we saw Sigma x i w i so
that's what we are doing here matrix
multiplication is multiplying all the
input values with the corresponding
weights and then adding the bias so that
is the graph we created and then we need
to Define what is our loss function and
what is our Optimizer so in this case is
we again use the tensorflows apis so TF
dot NN softmax cross entropy with logits
is the API that we will use and reduce
mean is what is like the mechanism
whereby which says that you reduce the
error and Optimizer for doing detection
of the error what Optimizer are we using
so we are using gradient descent
Optimizer we discussed about this in
couple of slides earlier and for that
you need to specify the learning rate
you remember we saw that there was a
slide somewhat like this and then you
define what should be the learning rate
how fast you need to come down that is
the learning rate and this again needs
to be tested and tried and to find out
the optimum level of this learning rate
it shouldn't be very high in which case
it will not converge or shouldn't be
very low because it will in that case it
will take very long so you define the
optimizer and then you call the method
minimize for that Optimizer and that
will kick-start the training process and
so far we've been creating the graph and
in order to actually execute that graph
we create what is known as a session and
then we run that session and once the
training is completed we specify how
many times how many iterations we wanted
to run so for example in this case we
are saying Thousand Steps so that is a
exit strategy in a way so you specify
the exit condition so training will run
for thousand iterations and once that is
done we can then evaluate the model
using some of the techniques shown here
so let us get into the code quickly and
see how it works so this is our Cloud
environment now you can install
tensorflow on your local machine as well
I'm showing this demo on our existing
Cloud but you can also install
tensorflow on your local machine and
there is a separate video on how to set
up your tensorflow environment you can
watch that if you want to install your
local environment or you can go for
other any cloud service like for example
Google Cloud Amazon or Cloud Labs any of
these you can use and run and try the
code okay so it has got started
with a login
all right so this is our deep learning
tutorial code
and this is our tensorflow environment
and so let's get started
the first we have seen a little bit of a
Code walkthrough uh in the slides as
well now you will see the actual code in
action so the first thing we need to do
is import tensorflow and then we will
import the data and we need to adjust
the data in such a way that the
one hot is encoding is set to True one
hot encoding right as I explained
earlier so in this case the label values
will be shown appropriately and if we
just check what is the type of the data
so you can see that this is a data sets
python data sets and if we
check the number of images the way it
looks so this is how it looks it is an
array of type float32
similarly
the number if you want to see what is
the number of
cleaning images there are 55 000 and
there are test images 10 000 and then
validation images
5000. now let's take a quick look at the
data itself visualization so we will use
um matplotlib for this
and if we take a look at the shape now
shape gives us like the dimension of the
tensors or or the arrays if you will so
in this case the training data set if we
see the size of the training data set
using the method shape it says there are
55 000 and 50 000 by 784 so remember the
784 is nothing but the 28 by 28 28 into
28 so that is equal to 784 so that's
what it is showing now we can take just
one image and just see what is the the
first image and see what is the shape so
again size obviously it is only 784.
similarly you can look at the image
itself the data of the first image
itself so this is how it it shows a
large part of it will probably be zeros
because as you can imagine
in the image only certain areas are
written rest is blank so that's why you
will mostly see Zero say that it is
black or white but then there are these
values are so the values are actually
they are scaled so their values are
between 0 and 1. okay so this is what
you're seeing so certain locations there
are some values and then other locations
there are zeros
so that is how the data is stored and
loaded
if we want to actually see what is the
value of the handwritten image if you
want to view it this is how you view it
so you create like do this reshape and
matplotlib has this
feature to show you these images so we
will actually use the function called IM
show and then if you pass this
parameters appropriately you will be
able to see the different images now I
can change the values in this position
so which image we are looking at right
so we can say
if I want to see what is there in maybe
5000
right so
5000 has three similarly you can just
say five what is in five
five as eight
what is in 50.
again H so basically by the way if
you're wondering uh how I'm executing
this code shift enter in case you are
not familiar with jupyter notebooks
shift enter is how you execute each cell
individual cell and if you want to
execute the entire program you can go
here and say run all
so that is how
this score gets executed and here again
we can check what is the maximum value
and what is the minimum value of this
pixel values as I mentioned this is it
is scaled so therefore it is between the
value slide between 1 and 0. now this is
where we create our model
the first thing is to create the
required placeholders and variables and
that's what we are doing here as we have
seen in the slides so we create one
placeholder and we create two variables
which is for the weights and biases
these two variables are actually
matrices so each variable has 784 by 10
chill values okay so one for this 10 is
for each neuron there are 10 neurons and
784 is for the pixel values inputs that
are given which is 28 into 28 and the
biases as I mentioned one for each
neuron so there will be 10 biases they
are stored in a variable by the name b
and this is the graph which is basically
the multiplication of these matrix
multiplication of X into W and then the
bias is added for each of the neurons
and the whole idea is to minimize the
error so let me just execute I think
this code is executed then we Define
what is our the Y value is basically the
label value so this is another
placeholder we had X as one placeholder
and white underscore true as a second
placeholder and this will have values in
the form of uh 10 digit 10 digit arrays
and since we set one hot encoded the
position which has a one value indicates
what is the label for that particular
number all right then we have cross
entropy which is nothing but the loss
loss function and we have the optimizer
we have chosen gradient descent as our
Optimizer that the training process
itself so the training process is
nothing but to minimize the cross
entropy which is again nothing but the
loss function
so we Define all of this in the form of
a graph so the up to here remember what
we have done is we have not exactly
executed any tensorflow code till now we
are just preparing the graph the
execution plan that's how the tensorflow
code works so the whole structure and
format of this code will be completely
different from how we normally do
programming so even with people with
programming experience may find this a
little difficult to understand it and it
needs quite a bit of practice so you may
want to view this video also maybe a
couple of times to understand this flow
because the way tensorflow programming
is done is slightly different from the
normal programming some of you who let's
say have done maybe spark programming to
some extent will be able to easily
understand this but even in spark the
the programming the code it's self is
pretty straightforward behind the scenes
the execution happens slightly
differently but in tensorflow even the
code has to be written in a completely
different way so the code doesn't get
executed in the same way as you have
written so that that's something you
need to understand and a little bit of
practices needed for this so so far what
we have done up to here is creating the
variables and feeding the variables and
or rather not feeding but setting up the
variables and the graph that's all
defining maybe the what kind of a
network you want to use for example we
want to use a soft Max and so on so you
have created the variables how to load
the data loaded the data viewed the data
and prepared everything but you have not
yet executed anything in tensorflow now
the next step is the execution in
tensorflow so the first step for doing
any execution in 10 the flow is to
initialize the variables so anytime you
have any variables defined in your code
you have to run this piece of code
always so you need to basically create
what is known as a node for initializing
so this is a node you still are not yet
executing anything here you just created
a node for the initialization
so let us go ahead and create that and
here onwards is where you will actually
execute your code in tensorflow and in
order to execute the code what you will
need is a session tensorflow session so
TF dot session will give you a session
and there are a couple of different ways
in which you can do this but one of the
most common methods of doing this is
with what is known as a with Loop so you
have a width TF dot session as says and
with the colon here and this is like a
block starting of the block and these
indentations tell how far this block
goes and this session is valid till this
block gets executed so that is the
purpose of creating this with block this
is known as a width block so with TF dot
session as says you say SAS dot run in
it now says Dot one will execute a node
that is specified here so for example
here we are saying says dot run SAS is
basically an instance of the session
right so here we are saying TF dot
session so an instance of the session
gets created and we are calling that SAS
and then we run
a node within that one of the nodes in
the graph so one of the nodes here is
init so we say run that particular node
and that is when the initialization of
the variables happens now what this does
is if you have any variables in your
code in our case we have W is a variable
and B is a variable so any variables
that we created you have to run this
code you have to run the initialization
of these variables otherwise you will
get an error
that is the that's what this is doing
then we within this with block we
specify a for Loop and we are saying we
want the system to iterate for thousand
steps and perform the training
that's what this for Loop does run
training for thousand iterations
and what it is doing basically is it is
fetching the data or these images
remember there are about 50 000 images
but it cannot get all the images in one
shot because it will take up a lot of
memory and performance issues will be
there so this is a very common way of
Performing deep learning training you
always do in batches so we have maybe 50
000 images but you always do it in
batches of 100 or maybe 500 depending on
the size of your system and so on and so
forth so in this case we are saying okay
get me 100 images at a time and get me
only the training images remember we use
only the training data for training
purpose and then we use test data for
test purpose you must be familiar with
machine learning so you must be aware of
this but in case you are not in machine
learning also not this is not specific
to deep learning but in machine learning
in general you have what is known as as
training data set and test data set your
available data typically you will be
splitting into two parts and using the
training data set for training purpose
and then to see how well the model has
been trained you use the test data set
to check or test the validity or the
accuracy of the model so that's what we
are doing here and You observe here that
we are actually calling an mnist
function here so we are saying mnist
train dot next batch
right so this is the advantage of using
MNS database because they have provided
some very nice helper functions which
are readily available otherwise this
activity itself we would have had to
write a piece of code to fetch this data
in batches that itself is a lengthy
exercise so we can avoid all that if we
are using MNS database and that's why we
use this for the initial learning phase
okay so when we say fetch what it will
do is it will fetch the images into X
and the labels into Y and then you use
this batch of 100 images and you run the
training so SAS dot run basically what
we are doing here is we are running the
training mechanism which is nothing but
it passes this through the neural
network passes the images through the
neural network finds out what is the
output and if the output obviously
initially it will be wrong so all that
feedback is given back to the new
Network and thereby all the W's and B's
get updated till it reaches thousand
iterations in this case the exit
criteria is thousand but you can also
specify probably accuracy rate or
something like that for the as an exit
criteria so here it is it just says that
okay this particular image was wrongly
predicted so you need to update your
weights and biases that's the feedback
given to each neuron and that is run for
1000 iterations and typically by the end
of this thousand iterations the model
would have learned to recognize these
handwritten images obviously it will not
be 100 accurate okay so once that is
done
after so this happens for 1000
iterations once that is done you then
test the accuracy of these models by
using the test data set right so this is
what we are trying to do here the code
may appear a little complicated because
if you're seeing this for the first time
you need to understand uh the various
methods of tensorflow and so on but it
is basically comparing the output with
what has been what is actually there
that's all it is doing so you have your
test data and you are trying to find out
what is the actual value and what is the
predicted value and seeing whether they
are equal or not t f not equal right and
how many of them are correct and so on
and so forth and based on that the
accuracy is calculated as well so this
is the accuracy and that is what we are
trying to see how accurate the model is
in predicting these numbers or these
digits okay so let us run this this
entire thing is in one cell so we will
have to just run it in one shot it may
take a little while let us see and uh
not bad so it has finished the thousand
iterations and what we see here as an
output is the accuracy so we see that
the accuracy of this model is around 91
percent okay now which is pretty good
for such a short exercise within such a
short time we got 90 percent accuracy
however in real life this is probably
not sufficient so there are other ways
in to increase the accuracy we will see
probably in some of the later tutorials
how to improve this accuracy how to
change maybe the hyper parameters like
number of neurons or number of layers
and so on and so forth and so that this
accuracy can be increased Beyond 90
percent
a learner simply learn and bring your
Master's
in artificial
in this course you can find the course
Link in the description box below we're
gonna Dive Right into what is Karas
we'll also go all the way through this
into a couple of tutorials because
that's where you really learn a lot is
when you roll up your sleeves so when so
what is Karas cross is a high level deep
learning API written in Python for easy
Implement implementation of neural
networks it uses deep learning
Frameworks such as tensorflow Pi torch
Etc is back in to make computation
faster
and this is really nice because as a
programmer there is so much stuff out
there and it's evolving so fast it can
get confusing and having some kind of
high level order in there we can
actually view it and easily program
these different neural networks is
really powerful it's really powerful to
to have something out really quick and
also be able to start testing your
models and seeing where you're going
so cross works by using complex deep
learning Frameworks such as tensorflow
Pi torch ml played
Etc as a back-end for fast computation
while providing a user-friendly and easy
to learn front-end and you can see here
we have the cross API specifications and
under that you'd have like TF Karas for
tensorflow thano cross and so on and
then you have your tensorflow workflow
that this is all sitting on top of
and this is like I said it organizes
everything the heavy lifting is still
done by tensorflow or whatever you know
underlying package you put in there and
this is really nice because you don't
have to
um dig as deeply into the heavy end
stuff while still having a very robust
package you can get up and running
rather quickly and it doesn't distract
from the processing time because all the
heavy lifting is done by packages like
tensorflow this is the organization on
top of it so the working principle of
Karas
uh the working principle of cross is
cross uses computational graphs to
express and evaluate mathematical
expressions
you can see here we put them in blue
they have the expression expressing
complex problems as a combination of
simple mathematical operators where we
have like the percentage or in this case
in Python that's usually your left your
remainder or multiplication you might
have the operator of x to the power of
0.3 and it uses useful for calculating
derivatives by using back propagation so
if we're doing with neural networks we
send the error back up to figure out how
to change it this makes it really easy
to do that without really having not
banging your head and having to hand
write everything it's easier to
implement distributed computation and
for solving complex problems specify
input and outputs and make sure all
nodes are connected
and so this is really nice as you come
in through is that as your layers are
going in there you can get some very
complicated different setups nowadays
which we'll look at in just a second
and this just makes it really easy to
start spinning this stuff up and trying
out the different models
so we look at Cross models uh cross
model we have a sequential model
sequential model is a linear stack of
layers where the previous layer leads
into the next layer
and this if you've done anything else
even like the SK learn with their neural
networks and propagation in any of these
setups this should look familiar you
should have your input layer it goes
into your layer one layer two and then
to the output layer
and it's useful for simple classifier
decoder models
and you can see down here we have the
model equals across sequential and this
is the actual code you can see how easy
it is we have a layer that's dense your
layer one as an activation they're using
the relu in this particular example and
then you have your name layer one layer
dense relu name Layer Two and so forth
and they just feed right into each other
so it's really easy just to stack them
as you can see here and it automatically
takes care of everything else for you
and then there's a functional model and
this is really where things are at this
is new make sure you update your cross
or you'll find yourself running this
doing the functional model you'll run
into an error code because this is a
fairly new release
and he uses multi-input and multi-output
model the complex model which Forks into
two or more branches
and you can see here we have our image
inputs equals your cross input shape
equals 32 by 32 by 3.
you have your dense layers dense 64
activation relu and this should look
similar to what you already saw before
but if you look at the graph on the
right it's going to be a lot easier to
see what's going on you have two
different inputs and one way you could
think of this is maybe one of those is a
small image and one of those is a full
sized image and that feedback goes into
you might feed both of them into one
node because it's looking for one thing
and then only into one node for the
other one and so you can start to get
kind of an idea that there's a lot of
use for this kind of split and this kind
of setup we have multiple information
coming in but the information is very
different even though it overlaps and
you don't want it to send it through the
same neural network and they're finding
that this trains faster and is also has
a better result depending on how you
split the data up and how you Fork the
models coming down
and so in here we do have the two
complex models coming in we have our
image inputs which is a 32 by 32 by
three or three channels or four if
you're having an alpha channel uh you
have your dense your layers dense is 64
activation using the ray Lou very common
uh x equals dense inputs X layers dense
x64 activation equals relu x outputs
equals layers dense 10 X model equals
cross model inputs equals inputs outputs
equals outputs name equals minced model
uh so we add a little name on there and
again this is this kind of split here
this is setting us up to have the input
go into different areas
if you're already looking across you
probably already have this answer what
are neural networks but it's always good
to get on the same page and for those
people who don't fully understand neural
networks to dive into them a little bit
or do a quick overview
oh networks are deep learning algorithms
modeled after the human brain they use
multiple neurons which are mathematical
operations to break down and solve
complex mathical problems
and so just like the neuron one neuron
fires in and it fires out to all these
other neurons or nodes as we call them
and eventually they all come down to
your output layer
and you can see here we have the really
standard graph input layer a hidden
layer and an output layer one of the
biggest parts of any data processing is
your data pre-processing
so we always have to touch base on that
with a neural network like many of these
models they're kind of uh when you first
start using them they're like a black
box you put your data in you train it
and you test it and see how good it was
and you have to pre-process that data
because bad data in is bad outputs so in
data pre-processing we will create our
own data examples set with Karas the
data consists of a clinical trial
conducted on 2100 patients ranging from
ages 13 to 100 with a the patients under
65 and the other half over 65 years of
age
we want to find the possibility of a
patient experiencing side effects due to
their age and you can think of this in
today's world with covid what's going to
happen on there and we're going to go
ahead and do an example of that in our
live Hands-On like I said most of this
you really need to have Hands-On to
understand so let's go ahead and bring
up our anaconda and open that up and
open up a Jupiter notebook for doing the
python code in now if you're not
familiar with those you can use pretty
much any of your setups I just like
those for doing demos and showing people
especially shareholders it really helps
because it's a nice visual so let me go
and flip over to our anaconda and the
Anaconda has a lot of cool two tools
they just added data lore and IBM Watson
Studio clad into the Anaconda framework
but we'll be in the Jupiter lab or
Jupiter notebook I'm going to do a
jupyter notebook for this because I use
the lab for like large projects with
multiple pieces it has multiple tabs
where the notebook will work fine for
what we're doing
and this opens up in our browser window
because that's how Jupiter dope so
Jupiter notebook is set to run
it will go under new create a new Python
3 and it creates an Untitled python
we'll go ahead and give this a title and
we'll just call this a cross
tutorial
and let's change that to Capital there
we go I'm going to just rename that and
the first thing we want to go ahead and
do is uh
get some pre-processing tools involved
and so we need to go ahead and import
some stuff for that like our numpy do
some random number generation
I mentioned SK learner your site kit if
you're installing sklearn the sklearn
stuff it's a site kit you want to look
up
that should be a tool of anybody who is
doing data science if you're not if
you're not familiar with the sklearn
toolkit it's huge but there's so many
things in there that we always go back
to and we want to go ahead and create
some train labels and train samples for
training our data
and then just a note of what we're we're
actually doing in here let me go ahead
and change this this is kind of a fun
thing you can do we can change the code
to markdown
and then markdown code is nice for doing
examples once you've already built this
our example data we're going to do
experimental
there we go experimental drug was tested
on 2100 individuals between 13 to 100
years of age half the participants are
under 65 and 95 percent of participants
are under 65. experience no side effects
well 95 percent of participants over 65
experience side effects
so that's kind of where we're starting
at and this is just a real quick example
because we're going to do another one
that's a little bit more complicated
information
uh and so we want to go ahead and
generate
our setup so we want to do for I and
range and we want to go ahead and create
if you look here we have random integers
train the labels of pen so we're just
creating some random data
and let me go ahead and just run that
and so once we've created our random
data and if you if I mean you can
certainly ask for a copy of the code
from Simply learning they'll send you a
copy of this or you can zoom in on the
video and see how we went ahead and did
our train samples of pin
um
and we're just using this I do this kind
of stuff all the time I was running a
thing on uh that had to do with errors
following a bell-shaped curve on a
standard distribution error and so what
do I do I generate the data on a
standard distribution area to see what
it looks like and how my code processes
it since that was the Baseline I was
looking for in this we're just doing uh
generating random data for our setup
button here
and we could actually go in print some
of the data up let's just do this print
we'll do train
samples and we'll just do the first five
pieces of data in there to see what that
looks like and you can see the first
five pieces of data in our train samples
is 49 85 41 68 19 just random numbers
generated in there that's all that is
and we generated significantly more than
that
let's see 50 up here 1000 yeah so
there's 1 000 here 1000 numbers we
generated and we could also if we wanted
to find that out we could do a quick
print the length of it
so or you could do a shape kind of thing
and if you're using numpy
although the link for this is just fine
and there we go it's actually 2100 like
we said in the demo setup in there
and then we want to go ahead and take
our labels oh that was our train labels
we also did samples didn't we
so we could also print
do the same thing
labels
and let's change this to
labels
and labels
and run that just to double check and
sure enough we have 2100 and they're
labeled one zero one zero one zero I
guess that's if they have symptoms or
not one symptoms zero none and so we
want to go ahead and take our trade
labels and we'll convert it into a numpy
array and the same thing with our
samples
and let's go ahead and run that
and we also Shuffle this is just a neat
feature you can do in numpy right here
put my drawing thing on which I didn't
have on earlier I can take the data and
I can Shuffle it uh so we have our so
it's it just randomizes it that's all
that's doing
um we've already randomized it so it's
kind of an Overkill it's not really
necessary
but if you're doing a larger package
where the data is coming in and a lot of
times it's organized somehow and you
want to randomize it just to make sure
that that you know the input doesn't
follow a certain pattern
that might create a bias in your model
and we go ahead and create a scalar the
scalar range minimum Max scalar feature
range zero to one
and then we go ahead and scale the
scaled train samples we're going to go
ahead and fit and transform the data so
it's nice and scaled and that is the age
so you can see up here we have 49 85 41.
we're just moving that so it's going to
be between 0 and 1. and so this is true
with any of your neural networks
you really want to convert the data to
zero and one otherwise you create a bias
so if you have like a hundred crates of
bias versus
the math behind it gets really
complicated
if you actually start multiplying stuff
because a lot of multiplication addition
going on in there that higher end value
will eventually multiply down and it
will have a huge bias as to how the
model fits it and then it will not fit
as well and then one of the fun things
we can do in jupyter Notebook is that if
you have a variable and you're not doing
anything with it it's the last one on
the line it will automatically print
and we're just going to look at the
first five samples on here it's just
going to print the first five samples
and you can see here we go
0.9195 0.791 so everything's between 0
and 1.
and that just shows us that we scaled it
properly and it looks good it really
helps a lot to do these kind of print
UPS halfway through you never know
what's going to go on there
I don't know how many times I've gotten
down and found out that the data sent to
me that I thought was scaled was not
and then I have to go back and track it
down and figure it out on there
so let's go ahead and create our
artificial neural network
and for doing that this is where we
start diving into tensorflow and cross
tensorflow
if you don't know the history of
tensorflow
it helps to jump into we'll just use
Wikipedia
careful don't quote Wikipedia on these
things because you get in trouble uh
but it's a good place to start back in
2011 Google brain built disbelief as a
proprietary machine learning setup
tensorflow became the open source for it
so tensorflow is a Google product and
then it became
open sourced and now it's just become
probably one of the de factos when it
comes for neural networks as far as
where we're at so when you see the
tensorflow setup
it's got like a huge following there are
some other setups like the side kit
under the SK learn has their own little
neural network but the tensorflow is the
most robust one out there right now and
cross sitting on top of it makes it a
very powerful tool so we can leverage
both the cross easiness in which we can
build a sequential setup on top of
tensorflow
and so in here we're going to go ahead
and do our input of tensorflow
and then we have the rest of this is all
Karas here from number two down uh we're
going to import from tensorflow the
cross connection and then you have your
tensorflow across models import
sequential it's a specific kind of model
we'll look at that in just a second if
you remember
from the files that means it goes from
one layer to the next layer to the next
layer there's no funky splits or
anything like that
uh and then we have from tensorflow
across layers we're going to import our
activation and our dense layer
and we have our Optimizer atom this is a
big thing to be aware of how you
optimize your data when you first do it
atoms as good as any atom is usually
there's a number of Optimizer out there
there's about there's a couple of main
ones but atom is usually assigned to
bigger data
uh it works fine usually the lower data
does it just fine but atom is probably
the mostly used but there are some more
out there and depending on what you're
doing with your layers your different
layers might have different activations
on them and then finally down here
you'll see our setup where we want to go
ahead and use the metrics
and we're going to use the tensorflow
cross metrics for categorical cross
entropy so we can see how everything
performs when we're done that's all that
is a lot of times you'll see us go back
and forth between tensorflow and then
site kit has a lot of really good
metrics also for measuring these things
again it's the end of the day you know
at the end of the story how good is your
model do and we'll go ahead and load all
that and then comes the fun part I
actually like to spend hours messing
with these things
and four lines of code you're like oh
you're gonna spend hours on four lines
of code
no we don't spend hours on four lines of
code that's not what we're talking about
when I say spend hours on four lines of
code uh what we have here and explain
that in just a second we have a model
and it's a sequential model if you
remember correctly we mentioned the
sequential up here where it goes from
one layer to the next
and our first layer is going to be your
input
it's going to be what they call dense
which is uh usually it's just dense and
then you have your input and your
activation
how many units are coming in we have 16
what's the shape What's the activation
and this is where it gets interesting
because we have in here uh relu
on two of these and softmax activation
on one of these
there are so many different options for
what these mean and how they function
how does the relu how does the softmax
function
and they do a lot of different things
we're not going to go into the
activations in here that is what really
you spend hours doing is looking at
these different activations
and just some of it is just almost like
you're playing with it
like an artist you start getting a fill
for like a inverse tangent activation or
the 10h activation
takes up a huge processing amount so you
don't see it a lot yet it comes up with
a better solution especially when you're
doing uh when you're analyzing Word
documents and you're tokenizing the
words and so you'll see this shift from
one to the other because you're both
trying to build a better model and if
you're working on a huge data set it'll
crash the system it'll just take too
long to process
and then you see things like soft Max
softmax
generates an interesting
setup
where a lot of these when you talk about
really oops let me do this uh regular
there we go relu has a setup where if
it's less than zero it's zero and then
it goes up
um and then you might have what they
call Lazy setup where it has a slight
negative to it so that the errors can
translate better same thing with softmax
it has a slight laziness to it so that
errors translate better all these little
details
make a huge difference on your model so
one of the really cool things about data
science that I like is you build your
what they call you build to fail and
it's an interesting design setup oops I
forgot the end of my code here
the concept to build a fail is you want
the model as a whole to work so you can
test your model out
so that you can do uh
you can get to the end and you can do
your let's see where was it overshot
down here you can test your test out how
the quality of your setup on there
and
see where did I do my tensorflow oh here
we go I did it was right above me there
we go we started doing your cross
entropy and stuff like that is you need
a full functional set of code so that
when you run it
you can then test your model out and say
hey it's either this model works better
than this model and this is why and then
you can start swapping in these models
and so when I say I spend a huge amount
of time on pre-processing data is
probably eighty percent of your
programming time
well between those two it's like 80 20.
you'll spend a lot of time on the models
once you get the model down once you get
the whole code and the flow down set
depending on your data your models get
more and more robust as you start
experimenting with different inputs
different data streams and all kinds of
things and we can do a simple model
summary here
's our sequential here's our layer our
output our parameter
this is one of the nice things about
cross is you just you can see right here
here's our sequential one model boom
boom boom boom everything's set and
clear and easy to read so once we have
our model built uh the next thing we're
going to want to do is we're going to go
ahead and train that model
and so the next step is of course model
training and when we come in here this a
lot of times it's just paired with the
model because it's so straightforward
it's nice to print out the model setup
so you can have a tracking
but here's our model
the keyword in Cross is compile
Optimizer atom learning rate another
term right there that we're just
skipping right over that really becomes
the meat of
the setup is your learning rate so
whoops I forgot that I had an arrow but
I'll just underline it
a lot of times the learning rate set to
0.0 uh set to 0.01 depending on what
you're doing this learning rate can
overfit and underfit so you'd want to
look up I know we have a number of
tutorials out on overfitting and
underfitting that are really worth
reading once you get to that point and
understanding and we have our loss
sparse categorical cross entropy so this
is going to tell Karas how far to go
until it stops
and then we're looking for metrics of
accuracy so we'll go ahead and run that
and now that we've compiled our model
we want to go ahead and run it fit it so
here's our model fit we have our scaled
to train samples
our train labels our validation split in
this case we're going to use 10 percent
of the data for validation
batch size another number you kind of
play with not a huge difference as far
as how it works
but it does affect how long it takes to
run and it can also affect the bias a
little bit most of the time though a
batch size is between 10 to 100
depending on just how much data you're
processing in there we want to go ahead
and Shuffle it we're going to go through
30 epics
and put a verbose of two and let me just
go ahead and run this and you can see
right here here's our epic here's our
training
here's our loss now if you remember
correctly up here we set the loss see
where was it
compiled our data
there we go loss so it's looking at the
sparse categorical cross entropy
this tells us that as it goes how how
much how much does the error go down is
the best way to look at that and you can
see here the lower the number the better
it just keeps going down and vice versa
accuracy we want let's see where's my
accuracy
value accuracy at the end
and you can see 0.619 0.69 0.74 it's
going up we want the accuracy would be
ideal if I made it all the way to one
but we also the loss is more important
because it's a balance you can have 100
accuracy and your model doesn't work
because it's overfitted
again you will look up overfitting and
underfitting models
and we went ahead and went through 30
epics it's always fun to kind of watch
your code going
to be honest I usually uh the first time
I run it I'm like oh that's cool I get
to see what it does and after the second
time of running it I'm like I like to
just not see that and you can request
those of course in your code repress the
warnings in the printing
and so the next step is going to be
building a test set and predicting it
now so here we go we want to go ahead
and build our test set and we have just
like we did our training set
a lot of times you just split your your
initial setup but we'll go ahead and do
a separate set on here
and this is just what we did above
there's no difference as far as
the randomness that we're using to build
this set on here the only difference is
that
we are already did our scalar up here
well it doesn't matter because the the
data is going to be across the same
thing but this should just be just
transformed down here instead of fit
transform because you don't want to
refit your data on your testing data
there we go now we're just transforming
it because you never want to transform
the test data easy mistake to make
especially on an example like this where
we're not doing
you know we're randomizing the data
anyway so it doesn't matter too much
because we're not expecting something
weird
and then we went ahead and do our
predictions the whole reason we built
the model is we take our model we
predict and we're going to do here's our
X scale data batch size 10 verbose and
now we have our predictions in here and
we could go ahead and do a
oh we'll print
predictions
and then I guess I could just put down
predictions in five so we can look at
the first five of the predictions
and what we have here is we have our age
and the prediction on this age versus
what we think it's going to be but what
we think is going to they're going to
have symptoms or not
and the first thing we notice is it's
hard to read because we really want a
yes no answer so we'll go ahead and just
round off the predictions
using the ARG Max the numpy ARG Max for
prediction so it just goes to a zero one
and if you remember this is a Jupiter
notebook so I don't have to put the
print I can just put in rounded
predictions and we'll just do the first
five and you can see here zero one zero
zero zero so that's what the predictions
are that we have coming out of this is
no symptoms symptoms no symptoms
symptoms no symptoms
as we were talking about at the
beginning we want to go ahead and take a
look at this there we go confusion
matrixes for accuracy check
most important part
when you get down to the end of the
story how accurate is your model before
you go and play with the model and see
if you can get a better accuracy out of
it and for this we'll go ahead and use
the site kit the SK learn metrics site
kit being where that comes from
import confusion Matrix some iteration
tools and of course a nice matplot
Library
that makes a big difference so it's
always nice to um
I have a nice graph to look at pictures
worth a thousand words
and then we'll go ahead and call it CM
for confusion Matrix y true equals test
labels y predict rounded predictions
and we'll go ahead and load in our cm
and
I spend too much time on the plotting
going over the different plotting code
you can spend like cool we have whole
tutorials on how to do your different
plotting on there but we do have here is
we're going to do a plot confusion
Matrix there's our CM our classes
normalize false title confusion Matrix
cmap is going to be in blues
and you can see here we have uh to the
nearest cmap titles all the different
pieces whether you put tick marks or not
the marks the classes the color bar
so a lot of different information on
here as far as how we're doing the
printing of the of the confusion Matrix
you can also just dump the confusion
Matrix into a Seaborn and real quick get
an output it's worth knowing how to do
all this uh when you're doing a
presentation to the shareholders you
don't want to do this on the Fly you
want to take the time to make it look
really nice like our guys in the back
did and let's go ahead and do this we
forgot to put together our CM plot
labels
we'll go ahead and run that
and then we'll go ahead and call the
little the definition
for our mapping
and you can see here plot confusion
Matrix that's our the little script we
just wrote and we're going to dump our
data into it so our confusion Matrix our
classes title confusion Matrix and let's
just go ahead and run that
and you can see here we have our basic
setup no side effects 195 had side
effects 200 no side effects that had
side effects so we predicted the 10 of
them who actually had side effects and
that's pretty good I mean I I don't know
about you but you know that's five
percent error on this and this is
because there's 200 here that's where I
get five percent is uh divide these both
by by two and you get five out of a
hundred uh you can do the same kind of
math up here not as quick on the Fly
because it's 15 or 195. not an easily
rounded number but you can see here
where they have 15 people
who predicted to have no uh with the no
side effects but had side effects
kind of setup on there and these
confusion Matrix are so important at the
end of the day this is really where
where you show whatever you're working
on comes up and you can actually show
them hey this is how good we are or not
how messed up it is
so this was a uh I spent a lot of time
on some of the parts but you can see
here is really simple uh we did the
random generation of data but when we
actually built the model coming up here
here's our model summary
and we just have the layers on here that
we built with our model on this and then
we went ahead and trained it and ran the
prediction
now we can get a lot more complicated uh
let me flip back on over here because
we're going to do another demo
so that was our basic introduction to it
we talked about the uh oops there we go
okay so implementing a neural network
with Karas after creating our samples
and labels we need to create our cross
neural network model we will be working
with a sequential model which has three
layers and this is what we did we had
our input layer our hidden layers and
our output layers and you can see the
input layer coming in was the age Factor
we had our hidden layer and then we had
the output are you going to have
symptoms or not so we're going to go
ahead and go with something a little bit
more complicated
training our model is a two-step process
we first compile our model and then we
train it in our training data set so we
have compiling compiling converts the
code into a form of understandable by
machine
we use the atom in the last example a
gradient descent algorithm to optimize a
model and then we trained our model
which means it let it learn on training
data
and I actually had a little backwards
there but this is what we just did is we
if you remember from our code we just
had oops we go back here
um
here's our model that we created
summarized uh we come down here and we
compile it
so it tells it hey we're ready to build
this model and use it and then we train
it and this is the part where we go
ahead and fit our model and and put that
information in here and it goes through
the training on there and of course we
scaled the data which was really
important to do and then you saw we did
the creating a confusion Matrix with
Karas as we are performing
classifications on our data we need a
confusion Matrix to check the results a
confusion Matrix breaks down the various
misclassifications as well as correct
classifications to get the accuracy
and so you can see here this is what we
did with the true positive false
positive true negative false negative
and that is what we went over let me
just scroll down here
on the end we printed it out and you see
we have a nice printout of our confusion
Matrix with the true positive false
positive false negative true negative
and so the blue ones we want those to be
the biggest numbers because those are
the better side and then we have our
false predictions on here as far as this
one so I had no side effects but we
predicted
let's see no side effects predicting
side effects and vice versa if getting
your learning started it's half the
battle what if you could do that for
free visit skillup by simply learn click
on the link in the description to know
more
now uh saving and loading models with
Karas we're going to dive into a more
complicated demo
uh and you can say oh that was a lot of
complication before well if you broke it
down we randomized some data we created
the cross setup we compiled it we
trained it we predicted and we ran our
Matrix
uh so we're going to dive into something
a lot a little bit more fun is we're
going to do a face mask detection with
Karas so we're going to build a cross
model to check if a person is wearing a
mask or not in real time and this might
be important if you're at the front of a
store this is something today which is
um
might be very useful as far as some of
our you know making sure people are safe
and so we're going to look at mask and
no mask and let's start with a little
bit on the data
and so in my data I have with a mask and
you can see they just have a number of
images showing the people in masks and
again if you want some of this
information contact simply learn and
they can send you some of the
information as far as people with and
without masks so you can try it on your
own and this is just such a wonderful
example of this setup button here
so before I dive into the mass detection
talk about being in the current with
covid and seeing if people are wearing
masks this particular example I had to
go ahead and update to a python 3.8
version it might run in a three seven
I'm not sure I haven't I kind of skipped
three seven and installed three eight
so I'll be running in a three Python 3 8
and then you also want to make sure your
tensorflow is up to date because the the
call functional
uh layers with that's where they split
if you remember correctly from back uh
oh let's take a look at this remember
from here the functional model in a
functional layer allows us to feed in
the different layers into different you
know different nodes into different
layers and split them a very powerful
tool very popular right now in the edge
of where things are with neural networks
and creating a better model so I've
upgraded to python 3.8 and let's go
ahead and open that up and go through
our next example which includes multiple
layers
programming it to recognize whether
someone wears a mask or not and then
saving that model so we can use it in
real time so we're actually almost a
full
end-to-end development of a product here
of course this is a very simplified
version and it'd be a lot more to it
you'd also have to do like recognizing
whether it's someone's face or not all
kinds of other things go into this
so let's go ahead and jump into that
code and we'll open up a new Python 3
oops python3
it's working on it there we go
um and then we want to go ahead and
train our mask we'll just call this
train mask
and we want to go ahead and train mask
and save it
so it's it's uh it's save mask train
mask detection not to be confused with
masking data a little bit different
we're actually talking about our
physical mask on your face
and then from the cross standpoint we
got a lot of imports to do here
and I'm not going to dig too deep on the
Imports we're just going to go ahead and
notice a few of them so we have in here
alt D there we go
have something to draw with a little bit
here we have our image processing
and the image processing right here let
me underline that
deals with how do we bring images in
because most images are like a a square
grid and then each value in there has
three values for the three different
colors cross and tensorflow do a really
good job of working with that so you
don't have to do all the heavy listening
and figuring out what's going to go on
uh and we have the mobile net average
pooling 2D this again is
how do we deal with the images and
pulling them dropout's a cool thing
worth looking up if you haven't when as
you get more and more into Karas and
tensorflow it'll Auto drop out certain
nodes that way you'll get a better
um the notes just kind of die and they
find that they actually create more of a
bias and a help and they also add
processing time so they remove them and
then we have our flattened that's where
you take that huge array with the three
different colors and you find a way to
flatten it so it's just a
one-dimensional array instead of a two
by two by three
dense input we did that in the other one
so that should look a little familiar
oops there we go our input our model
again these are things we had on the
last one here's our Optimizer with our
atom
um we have some pre-processing on the
input that goes along with bringing in
the data in more pre-processing with
image to array loading the image this
stuff is so nice it looks like a lot of
works you have to import all these
different modules in here but the truth
is is it does everything for you you're
not doing a lot of pre-processing you're
letting the software do the
pre-processing
um and we're gonna be working with the
setting something to categorical
again that's just a conversion from a
number to a category 0 1 doesn't really
mean anything it's like true false
um label binarizer the same thing uh
we're changing our labels around and
then there's our train test split
classification report
our IM utilities let me just go ahead
and scroll down here a notch for these
this is something a little different
going on down here this is not part of
the tensorflow or the SK learn this is
the site kit setup and tensorflow above
the path this is part of opencv and
we'll actually have another tutorial
going out with the opencv so if you want
to know more about opencv you'll get a
glance on it in this software especially
the net the second piece when we reload
up the data and hook it up to a video
camera we're going to do that on this
round
but this is part of the opencv thing and
you'll see CV2 is usually how that's
referenced but the IM utilities has to
do with how do you rotate pictures
around and stuff like that
and resize them and then the matplot
library for plotting because it's nice
to have a graph tells us how good we're
doing and then of course our numpy
numbers array and just a straight OS
access wow so that was a lot of imports
uh like I said I'm not going to spend I
spend a little time going through them
but we didn't want to go too much into
them
and then I'm going to create some
variables that we need to go ahead and
initialize we have the learning rate
number of epics to train for and the
batch size and if you remember correctly
we talked about the learning rate to the
negative 4.0001
a lot of times it's 0.001 or 0.001
usually it's in that variation depending
on what you're doing and how many epics
and they kind of play with the epics the
epics is how many times we're going to
go through all the data
now I have it as two the actual setup is
for 20 and 20 works great the reason I
have it for two is it takes a long time
to process one of the downsides of
Jupiter
is a Jupiter isolates it to a single
kernel so even though I'm on an eight
core processor with 16 dedicated threads
only one thread is running on this no
matter what so it doesn't matter so it
takes a lot longer to run even though
tensorflow really scales up nicely and
the batch size is how many pictures do
we load at once in process again those
are numbers you have to learn to play
with depending on your data and what's
coming in and the last thing we want to
go ahead and do is there's a directory
with the data set we're going to run
uh and this just has images of mass and
not masks
if we go in here you'll see data set
and you have pictures with mass they're
just images of people with mass on their
face
and then we have the opposite let me go
back up here without masks so it's
pretty straightforward they look kind of
Askew because they tried to format them
into very similar setup on there so
they're they're mostly squares you'll
see some that are slightly different on
here
and that's kind of important thing to do
on a lot of these data sets
get them as close as you can to each
other and we'll we actually will run in
the in this processing of images up here
and the cross layers and importing and
dealing with images it does such a
wonderful job of converting these at a
lot of it we don't have to do a whole
lot with
so you have a couple things going on
there
and so we're now going to be this is now
loading the images and let me see
and we'll go ahead and create data and
labels here's our here's the features
going in which is going to be our
pictures and our labels going out and
then for categories in our list
directory directory and if you remember
I just flashed that at you it had a face
mask or or no face mask those are the
two options
and we're just going to load into that
we're going to append the image itself
and the labels so we're just going to
create a huge array and you can see
right now this could be an issue if you
had more data at some point thankfully I
have a 32 gig hard drive or Ram
even that doesn't you could do with a
lot less of that probably under 16 or
even eight gigs would easily load all
this stuff
and there's a conversion going on in
here I told you about how we are going
to convert the size of the image so it
resizes all the images that way our data
is all identical the way it comes in
and you can see here with our labels we
have without mask without mask without
mask
the other one would be with mask those
are the two that we have going in there
and then we need to change it to the why
not hot encoding
and this is going to take our
up here we had was it labels and data we
want the labels to be categorical so
we're going to take labels and change it
to categorical and our labels then equal
a categorical list
we'll run that and again if we do
labels and we just do the last or the
first 10. let's do the last 10 just
because
minus 10 to the end there we go just so
we can see where the other side looks
like we now have one that means they
have a mask one zero one zero so on
one being they have a mask and zero no
mask and if we did this in reverse
I just realized that this might not make
sense if you've never done this before
let me run this
0 1. so 0 is do they have a mask on zero
do they not have a mask on one so this
is the same as what we saw up here
without mask one equals the second value
is without mass so with mass without
mask
and that's just a with any of your data
processing
we can't really zero if you have a zero
one output
it causes issues as far as training and
setting it up so we always want to use a
one hot encoder if the values are not
actual linear value or regression values
they're not actual numbers
if they represent a thing
and so now we need to go ahead and do
our trainax test X train y test y train
split test data
and we'll go ahead and make sure it's
going to be random and we'll take 20
percent of it for testing and the rest
for setting it up as far as training
their model
this is something that's become so cool
when they're training these set they
realize we can augment the data what
does augment mean well if I rotate the
data around and I zoom in I zoom out I
rotate it share it a little bit flip it
horizontally
fill mode as they do all these different
things to the data it is able to it's
kind of like increasing the number of
samples I have so if I have all these
perfect samples what happens we only
have part of the face or the face is
tilted sideways or all those little
shifts cause a problem if you're doing
just a standard set of data so we're
going to create an augment in our image
data generator which is going to rotate
zoom and do all kinds of cool things and
this is worth looking up this image data
generator and all the different features
it has
um a lot of times I'll the first time
through my models I'll leave that out
because I want to make sure there's a
thing we call build to fail which is
just cool to know you build the whole
process and then you start adding these
different things in uh so that you can
better train your model and so we go and
run this and then we're going to load
um and then we need to go ahead and you
probably would have gotten an error if
you hadn't put this piece in right here
I haven't run it myself because the guys
in the back did this we take our base
model and one of the things we want to
do is we want to do a mobilenet V2 and
this will be this is a big thing right
here include the top equals false
a lot of data comes in with a label on
the top row so we want to make sure that
that is not the case
and then the construction of the head of
the model that will be placed on the top
of the base model
we want to go ahead and set that up
and you'll see a warning here I'm kind
of ignoring the warning because it has
to do with the size of the pictures and
the weights for input shape
um so they'll switch things to defaults
to saying hey we're going to Auto shape
some of this stuff for you you should be
aware of that with this kind of imagery
we're already augmenting it by moving it
around and flipping it and doing all
kinds of things to it so that's not a
bad thing in this but another data it
might be if you're working in a
different domain
and so we're going to go back here and
we're going to have we have our base
model we're going to do our head model
equals our base model output
and what we've got here is we have an
average pooling 2D pool size 77 head
model
head model flatten so we're flattening
the data so this is all processing and
flattening the images and the pooling
has to do with some of the ways it can
process some of the data we'll look at
that a little bit when we get down to
the lower level in this processing it
and then we have our dense we've already
talked a little bit about a dense just
what you think about and then the head
model has a drop out of 0.5
what we can do with a drop out
the Dropout says that we're going to
drop out a certain amount of nodes while
training so when you actually use the
model
it will use all the nodes but this drops
certain ones out and it helps stop
biases from forming so it's really a
cool feature in here they discovered
this a while back we have another dense
mode and this time we're using soft Max
activation lots of different activation
options here softmax is a real popular
one for a lot of things so is the relu
and you know there's we could do a whole
talk on activation formulas uh and why
what their different uses are and how
they work
when you first start out you'll you'll
use mostly the relu and the soft Max for
a lot of them uh just because they're
they're some of the basic setups it's a
good place to start
and then we have our model equals model
inputs equals base model dot input
outputs equals head model so again we're
still building our model here we'll go
ahead and run that
and then we're going to Loop over all
the layers in the base model and freeze
them so they will not be updated during
the first training process
so for layer and base model layers
layers.trainable equals false
a lot of times when you go through your
data you want to kind of jump in part
way through I I'm not sure why in the
back they did this for this particular
example but I do this a lot when I'm
working with series and and specifically
in stock data I wanted to iterate
through the first set of 30 Data before
it does anything
um I would have to look deeper to see
why they froze it on this particular one
and then we're going to compile our
model so compiling the model atom
init layer Decay
initial learning rate over epics and we
go ahead and compile our loss is going
to be the binary cross entropy which
will have that print out Optimizer for
opt metrics is accuracy
same thing we had before not a huge jump
as far as the previous code
and then we go ahead and we've gone
through all this and now we need to go
ahead and fit our model so train the
head of the network print info training
head run
now I skipped a little time because it
you'll see the run time here is at 80
seconds per epic takes a couple minutes
for it to get through on a single kernel
one of the things I want you to notice
on here while we're while it's finishing
the processing
is that we have up here our augment
going on so anytime the trainex and
trading y go in there's some Randomness
going on there it is jiggling it around
what's going into our setup uh of course
we're batch sizing it so it's going
through whatever we set for the batch
values how many we process at a time
and then we have the steps per epic uh
the train X the batch size validation
data here's our test X and Test Y where
we're sending that in
uh and this again it's validation one of
the important things to know about
validation is our when both our training
data and our test data have about the
same accuracy that's when you want to
stop that means that our model isn't
biased if you have a higher accuracy on
your testing you've trained it and your
accuracy is higher on your actual test
data then something in there is probably
uh has a bias and it's overfitted
so that's what this is really about
right here with the validation data and
validation steps
so it looks like it's let me go ahead
and see if it's done processing looks
like we've gone ahead and gone through
two epics again you could run this
through about 20 with this amount of
data and it would give you a nice
refined model at the end we're going to
stop at two because I really don't want
to sit around all afternoon and I'm
running this on a single thread so now
that we've done this we're going to need
to evaluate our model and see how good
it is and to do that we need to go ahead
and make our predictions
these are predictions on our test X to
see what it thinks are going to be so
now it's going to be evaluating the
network and then we'll go ahead and go
down here
and we will need to uh turn the index in
because remember it's it's either 0 or 1
it's a 0 1 0 1 so you have two outputs
uh not wearing uh wearing a mask not
wearing a mask and so we need to go
ahead and take that argument at the end
and change those predictions to a zero
or one coming out and then to finish
that off we want to go ahead and let me
just put this right in here and do it
all in one shot we want to show a nicely
formatted classification report so we
can see what that looks like on here
and there we have it we have our
Precision uh it's 97 with the mask
there's our F1 score support without a
mask 97 percent
um so that's pretty high high setup on
there you know you three people are
going to sneak into the store who are
without a mask and the things they have
a mask and there's going to be three
people with a mask that's going to flag
the person at the front to go oh hey
look at this person you might not have a
mask that's if I guess it's a setup in
front of a store
um so there you have it and of course
one of the other cool things about this
is if someone's walking into the store
and you take multiple pictures of them
um you know this is just an it would be
a way of flagging and then you can take
that average of those pictures and make
sure they match or don't match if you're
on the back end and
because we're gonna this is just cool I
love doing this stuff uh so we're going
to go ahead and take our model and we're
going to save it so model save
massdetector.model we're going to give
it a name we're going to save the format
in this case we're going to use the H5
format
and so this model we just programmed has
just been saved so now I can load it up
into say another program what's cool
about this is let's say I want to have
somebody work on the other part of the
program well I just saved the model they
upload the model now they can use it for
whatever and then if I get more
information and we start working with
that at some point
I might want to update this model and
make a better model and this is true of
so many things where I take this model
and maybe I'm running a prediction on
making money for a company and as my
model gets better
I want to keep updating it and then it's
real easy just to push that out to the
actual end user and here we have a nice
graph you can see the training loss and
accuracy as we go through the epics we
only did the you know only shows just
the one Epic coming in here but you can
see right here as the
value loss train accuracy and value
accuracy
starts switching and they start
converging and you'll hear converging
this is a convergence they're talking
about when they say you're you're I know
when I work in the site kit with sklearn
neural networks this is what they're
talking about a convergence is our loss
and our accuracy come together and also
up here and this is why I'd run it more
than just two epics as you can see they
still haven't converged all the way so
that would be a cue for me to keep going
but what we want to do is we want to go
ahead and create a new Python 3
program
and we just did our train mask so now
we're going to go ahead and import that
and use it and show you in a live action
get a view of both myself in the
afternoon along with my background of an
office which is in the middle still of
reconstruction for another month
and we'll call this a mask
detector
and then we're going to grab a bunch of
a few items coming in
we have our
mobilenet V2 import pre-processing input
so we're still going to need that we
still have our tensorflow image to array
we have our load model that's where most
of stuff's going on
this is our CV2 or opencv again I'm not
going to dig too deep into that we're
going to flash a little opencv code at
you and we actually have a tutorial on
that coming out
our numpy array our IM utilities which
is part of the opencv or CV2 setup
and then we have of course time and just
our operating system so those are the
things we're going to go ahead and set
up on here and then we're going to
create
this takes just a moment
our module here which is going to do all
the heavy lifting
so we're going to detect and predict the
mask we have frame face net Mass net
these are going to be generated by our
opencv we have our frame coming in and
then we want to go ahead and create a
mask around the face it's going to try
to detect the face and then set that up
so we know what we're going to be
processing through our model
and then there's a frame shape here this
is just our height versus width that's
all HW stands for they've called it blob
which is a CV2 DNN blob form image frame
so this is reformatting this Frame
that's going to be coming in literally
from my camera and we'll show you that
in a minute that little piece of code
that shoots that in here
uh and we're going to pass the blob
through the network and obtain the face
detections so facenet dot set in Port
blob
detections face net forward
print detections shape
so this is this is what's going on here
this is that model we just created we're
going to send that in there and I'll
show you in a second where that is but
it's going to be under face net
and then we go ahead and initialize our
list of faces their corresponding
locations and the list of predictions
from our face mask Network
we're going to Loop over the detections
and this is a little bit more work than
you think as far as looking for
different faces what happens if you have
a crowd of faces
so We're looping through the detections
and the shapes going through here
and probability associated with the
detection here's our confidence of
detections
we're going to filter out weak detection
by ensuring the confidence is greater
than the minimum confidence
uh so we've said it remember zero to one
so 0.5 would be our minimal confidence
probably is pretty good
um
put in compute bounding boxes for the
object if I'm zipping through this it's
because we're going to do an opencv and
I really want to stick to just the cross
part
and so I'm just kind of jumping through
all this code you can get a copy of this
code from Simply learn and take it apart
or look for the opencv coming out
and we'll create a box uh the box sets
it around the image
ensure the bounding boxes fall within
the dimensions of the frame
so we create a box around which we hope
it's going to be the face extract the
face Roi convert it from BGR to RGB
Channel
again this is an opencv issue not really
an issue but it has to do with the order
I don't know how many times I've
forgotten to check the order colors
we're working with opencv because
there's all kinds of fun things when red
becomes blue
and blue becomes red and we're going to
go ahead and resize it process it frame
it face frame setup again the face the
CBT color we're going to convert it
we're going to resize it image to array
pre-process the input pin the face
locate face start x dot y and x boy that
was just a huge amount and I skipped
over a ton of it but the bottom line is
we're building a box around the face and
that box because the opencv does a
decent job of finding the face and that
box is going to go in there and see hey
does this person have a mask on it
uh and so that's what that's what all
this is doing on here and then finally
we get down to this where it says
predictions equals mass net dot predict
faces batch size
32. uh so these different images where
we're guessing where the face is are
then going to go through and generate an
array of faces if you will and we're
going to look through and say does this
face have a mask on it and that's what's
going right here is our prediction
that's the big thing that we're working
for
and then we return the locations and the
predictions the location just tells
where on the picture it is and then the
prediction tells us what it is is it a
mask or is it not a mask
all right so we've loaded that all up
so we're going to load our serialized
face detector model from disk and we
have our the path that it was saved in
obviously going to put in a different
path depending on where you have it or
however you want to do it and how you
saved it on the last one we trained it
uh then we have our weights path
and so finally our face net here it is
equals
cb2.dnn.readnet
prototext path awaits path and we're
going to load that up on here so let me
go ahead and run that
and then we also need to I'll just put
it right down here I always hate
separating these things in there and
then we're going to load the actual mass
detector model from disk this is the the
model that we saved so let's go ahead
and run that on there also so this is
pulling in all the different pieces we
need for our model and then the next
part is we're going to create open up
our video
and this is just kind of fun because
it's all part of the opencv
the video setup
let me just put this all in as one
there we go so we're going to go ahead
and open up our video we're going to
start it and we're going to run it until
we're done
and this is where we get some real like
kind of live action stuff which is fun
this is what I like to work about with
images and videos is that when you start
working with images and videos it's all
like right there in front of you it's
Visual and you can see what's going on
uh so we're going to start our video
streaming this is grabbing our video
stream Source zero start
that means it's grabbing my main camera
I have hooked up
and then you know starting video you're
going to print it out here's our video
Source equals zero start Loop over the
frames from the video stream
oops a little redundancy there
um let me close
I'll just leave it that's how they had
it in the code so uh so while true we're
going to grab the frame from the
threaded video stream and resize it to
have the maximum width of 400 pixels so
here's our frame we're going to read it
from our visual stream
we're going to resize it
and then we have a returning remember we
returned from the our procedure the
location and the prediction so detect
and predict mask we're sending it the
frame we're sending it the face net and
the mass net so we're sending all the
different pieces that say this is what's
going through on here
and then it returns our location and
predictions and then for our box and
predictions in the location and
predictions
and the boxes is again this is an opencv
set that says hey this is a box coming
in from the location
because you have the two different
points on there
and then we're going to unpack the box
and predictions and we're going to go
ahead and do mask without a mask equals
prediction
we're going to create our label no mask
and create color if the label equals
mask l0225 and you know it's going to
make a lot more sense when I hit the Run
button here but we have the probability
of the label
we're going to display the label
bounding box rectangle on the output
frame
and then we're going to go ahead and
show the output from the frame CV2 IM
show frame frame and then the key equals
CV2 weight key one we're just going to
wait till the next one comes through
from our feed
and we're going to do this until we hit
the stop button pretty much
so are you ready for this to see if it
works we've distributed our our model
we've loaded it up into our distributed
code here we've got it hooked into our
camera and we're going to go ahead and
run it
and there it goes it's going to be
running and we can see the data coming
down here and we're waiting for the
pop-up
and there I am in my office with my
funky headset on
uh and you can see in the background my
unfinished wall and it says up here no
mask oh no I don't have a mask on I
wonder if I cover my mouth
what would happen
uh you can see my no mask
goes down a little bit I wish I'd
brought a mask into my office it's up at
the house but you can see here that this
says you know there's a 95 98 chance
that I don't have a mask on and it's
true I don't have a mask on right now
and this could be distributed this is
actually an excellent little piece of
script that you can start you know you
install somewhere on a video feed on a
on a security camera or something and
then you'd have this really neat setup
saying hey do you have a mask on when
you enter a store or a public
transportation or whatever it is where
they're required to wear a mask
let me go ahead and stop that
now if you want a copy of this code
definitely give us a holler we will be
going into opencv and another one so I
skipped a lot of the opencv code in here
as far as going into detail
really focusing on the cross saving the
model uploading the model and then
processing a streaming video through it
so you can see it that the model works
we actually have this working model that
hooks into the video camera
which is just pretty cool and a lot of
fun
so I told you we're going to dive in and
really Roll Up Our Sleeve and do a lot
of coding today we did the basic uh demo
up above for just pulling it across and
then we went into a cross model where we
pulled in data to see whether someone
was wearing a mask or not so very useful
in today's world as far as a fully
running application
in artificial intelligence created in
collaboration with IBM to learn more
about this course you can find the
course Link in the description box below
what is in it for you today we're going
to cover what is Karas computational
graphs what are neural networks
sequential models and then we'll do a
Hands-On demo so you can see what's
going on with sequential models in Karas
so what is Karas
cross is a high level python deep
learning API
which is used for easy implementation of
neural networks it has multiple
low-level back-ends like tensorflow
thano pie torch Etc which are used for
fast computation
so you could think of this as cross
being almost its own little programming
language and then it sits on neural
networks in this case uh the ones listed
were tensorflow thano and pytorch which
can all integrate with the cross model
this makes a very diverse and also makes
it very easy to use and switch around
with different things cross is very user
friendly as far as neural network
software goes as a high level API
computational graphs so computational
graphs are really the heart and soul of
neural networks we talk about a
computational graph there are a visual
representation of expressing and
evaluating mathematical equations the
nodes and data flow in a graph
correspond to mathematical operations
and variables
you'll hear a lot some of the terms you
might hear are node and Edge The Edge
being the data flow in this case it
could also represent an actual value
they have oh I think in spark they have
a graph x which works just on Computing
edges there's all kinds of stuff that
has evolved from computational graphs
we're focusing just on Karas and on
neural networks so we're not going to go
into great detail on everything a
computational graph does it is a core
component of a neural network is what's
important to know on this
so cross offers a python user-friendly
front-end while maintaining a strong
computation Power by using a low level
API like tensorflow Pi torch Etc which
use computational graphs as a back end
so one this allows for abstraction of
complex problems while specifying
control flow
if you've ever looked at some of the
back end or the original versions of
tensorflow it's really a nightmare you
have all these different settings you
have to put in there and create it's a
lot of a lot of back-end programming
this is like the old computers when you
had to tell it how to dispose of a
variable and how to properly re-allocate
the memory for use
all that is covered nowadays in our
higher level programming well this is
the same thing with Karas is it covers a
lot of this stuff and does things for
you that you would could spend hours on
just trying to figure out
it's useful for calculating derivatives
by using back propagation we're
definitely not going to teach a class on
derivatives in this little video but
understanding a derivative is the rate
of change so if you have a particular
function you're using in your neural
network a lot of them is just simple y
equals MX plus b your euclidean geometry
where you just have a simple slope times
the intercept and they get very
complicated they have the inverse
tangent function for Activation as
opposed to just a linear euclidean model
and you can think about this as you have
your data coming in and you have to
alter it somehow
well you alter it going down to get an
answer you end up with an error and that
error goes back up and you have to have
that back propagation with the
derivative you want to know how it
changed so that you can figure out how
to adjust it for the error
a lot of that's hidden so you don't even
have to worry about it with Karas and in
today's cross it'll even if you create
your own formula for computing an answer
it will automatically compute the back
prop the the derivative for you in a lot
of cases
it's easier to implement distributed
computation so cross is a really nice
way to package it and get it off on
different computers and share it and it
allows parallelism which means that two
operations can run simultaneously
so as we start developing these back
ends it can do all kinds of cool things
and utilize multiple cores gpus on a
computer to get that parallel processing
up
what are neural networks
well like I said there are already we
talked about in computational edges you
have a node and you have a connection or
your Edge so neural networks are
algorithms fashioned after the human
brain which contain multiple layers each
layer contains a node called a neuron
which performs a mathematical operation
they break down complex problems into
simple operations
so one an input layer takes in our data
and pre-processes it when we talk about
pre-processing when you're dealing with
neural networks you usually have to
pre-process your data so that it's
between
-1 and 1 or 0 and 1 into some kind of
value that's usable that occurs before
it gets to the neural network in fact 80
percent of data science is usually in
prepping that data and getting it ready
for your different models
two you have hidden layer performs a
non-linear transformation of input now
it can do a hidden a linear
transformation it can use just a basic
euclidean geometry and you could think
of a node adding all the different
connections coming in so each connection
would have a weight and then it would
add to that weight plus an Intercept in
the node itself so you can actually use
euclidean geometry but a lot of these
get really complicated they have all
these different formulas and they're
really cool to look at but when you
start looking at them look at how they
work you really don't need to know the
high math behind it to figure them out
and figure out what they're doing
which is really cool that means a lot of
people can use this without having to go
get a PhD in mathematics
number three the output layer takes the
results from hidden layer transform them
and gives a final output
so sequential models so what makes this
a sequential model sequential models are
linear stacks of layers where one layer
leads to the next it is simple and easy
to implement and you just have to make
sure that the previous layer is the
input to the next layer
so you have used for plain stack of
layers where each layer has one input
and one output tensor and this is what
tensorflow is named after is each one of
these layers is like a tensor each node
is a tensor and then the layer is also
considered a tensor of values
and it's used for simple classifier
declassifier models you can it's also
used for regression models too so it's
not just about this is something this is
a teapot this is a cat this is a dog
it's also used for generating
regret the actual values you know this
is worth ten dollars that's worth thirty
dollars the weather is going to be 90
degrees out or whatever it is so you can
use it for both classifier and
declassifier models
and one more note when we talk about
sequential models the term sequential is
used a lot and it's used in different
areas in different notations when you're
in data science so when we talk about
time series we'll talk about sequential
that is something very different uh
sequential in this case means it goes
from the input to layer 1 to layer 2 to
the output so it's very directional it's
important to note this because if you
have a sequential model can you have a
non-sequential model and the answer is
yes if you master the basics of a
sequential model you can just as easily
have another model that shares layers
you can have another model where you
have an input coming in and it splits
and then you have one set that's doing
one set of nodes maybe they're doing a
yes no kind of node where it's either
putting out a zero or a one a classifier
and the other one might be regression
it's just processing numbers and then
you recombine them for the output that's
what they call across the cross API
so there's a lot of different
availabilities in here and all kinds of
cool things you can do as far as
encoding and decoding and all kinds of
things and you can share layers and
things like that
we're just focusing on the basic cross
model with the sequential model
so let's dive into the meat of the
matter let's do a do a demo on here
today's demo in this demo we'll be
performing flower classification using
sequential model and cross and we'll use
our model to classify between five
different types of flowers
now for this demo and you can do this
demo whatever platform you want or
whatever
user interface for developing python I'm
actually using anaconda and then I'm
using Jupiter notebooks to develop in
and if you're not familiar with this you
can go under environment once you've
created an environment you can come in
here to open a terminal window and if
you don't have the different modules in
here you can do your conda install or
whatever module it is
just happened that this particular setup
didn't have a Seabourn in it which I
already installed
so here's our anaconda and then I'm
going to go back
and start up my Jupiter notebook
where I already created a new python
project python3 come in Python 3.8 on
this particular one
sequential model for flowers
so lots of fun there so we're going to
jump right into this the first thing is
to make sure you have all your modules
installed
so if you don't have numpy pandas
matplot library and Seabourn in the
cross
and SK learn or site kit it's not
actually sklearn you'll need to go ahead
and install all of those
now having done this for years and
having switched environments and doing
different things
I get all my imports done and then we
just run it and if we get an error we
know we have to go back and install
something
um right off the bat though we have
numpy pandas matplot Library Seaborn
these are built on top of each other
panda is the data frame and built on top
of numpy the
data array
and then we bring in our SK learn or
scikit this is the site kit setup SCI
kit even though you use sklearn to bring
it in it's a sidekit and then our cross
we have our pre-processing the images
image data generator
um our model this is our basic model our
sequential model
uh and then we bring in from Cross
layers import dents optimizers
these optimizers a lot of them already
come in these are your different
optimizers and it's almost a lot of this
is so automatic now
um Adam
is the a lot of times the default
because you're dealing with a large data
and then we get our SGD which is a
smaller data does better on smaller
pieces of data and I'm not going to go
into all of these different optimizers
we didn't even use these in the actual
demo you just have to be aware that they
are different optimizers and the Digger
the more you dig into these models
you'll hit a point where you do need to
play with these a little bit but for the
most part leave it at the default when
you're first starting out
and we're doing just the sequential
you'll see here layers dense
and then if we come down a little bit
more when they put this together and
they're running the dense layers you
also see they have drop out they have
flattened they have activation they have
the convolutional layer 2D Max pooling
2D batch normalization
what are all these layers and we get to
the model we're going to talk about them
a lot of times when you're just starting
you can just import cross dot layers and
then you have your drop out your flatten
your convolutional neural network 2D
and we'll we'll cover what these do in
the actual example when we get down
there what I want you to take from here
though is you need to run your Imports
and load your different aspects of this
and of course your tensorflow TF because
this is all built on tensorflow
and then finally import random is RN
just for random generation
and then we get down here we have our
CV2
that is your open image or your opencv
they call it for processing images
that's what the cvd 2 is
uh we have our tqdm
the TQ DM is for is a progress bar just
a fancy way of adding when you're
running a process you can view the bar
going across in the Jupiter setup not
really necessary but it's kind of fun to
have
um we want to be able to shuffle some
files again these are all different
things pills and other
um
image processor it goes with the CV2 a
lot of times you'll see both of those
and so we run those we've got to bring
them all in
and the next thing is to set up our
directories
and so we come into the directories
there's an important thing to note on
here other than we're looking at a lot
of flowers which is fun
uh as we get down here we have our
directory archive flowers that just
happens to be where the different files
for different flowers are put in
we're denoting an X and a z and the x is
the date of the image and the Z is the
tag for it what kind of flower is this
and the image size is really important
because we have to resize everything if
you have a neural network and if you
remember from our neural networks let me
flip back to that slide
we look at this slide we have two input
nodes here with an image you have an
input node depending on how you set it
up for each pixel and that pixel has
three different color schemes usually in
it sometimes four so if you have a
picture that's 150 by 150 you multiply
150 times 150 times three that's how
many nodes input layers coming in that
means this is a massive input a lot of
times you think oh yeah it's just a
small amount of data or something like
that no it's a full image coming in and
then you have your hidden layers A lot
of times they match what the image size
is coming in so each one of those is
also just as big and then we get down to
just a single output
so that's kind of a thing to note in
here what's going on behind the scenes
and of course each one of these layers
has a lot of processes and stuff going
on
and then we have our different
directories on here let me go and run
that so I'm just setting the directories
that's all this is archive flowers Daisy
sunflower tulip dandelion Rose just our
different directories that we're going
to be looking at
and then we want to go ahead and we need
to assign labels remember we defined x
and z
so we're just going to create a
definition here
and the first thing is a return flower
type okay
just returns it what kind of flower it
is I guess assign the label to it but
we're going to go ahead and make our
train data
and when you look at this there's a
couple things to take away from here the
first one is we're just appending right
onto our numpy array the image we're
gonna let numpy handle all that
different aspects as far as 150 by 150
by three we just dump it right into the
numpy which makes it really easy we
don't have to do anything funky on the
processing and we want to leave it like
that and I'm going to talk about that in
a minute and then of course we have to
have the string a pin the label on there
and I want you to notice right here
we're going to read the image in
and then we're going to size it and this
is important because we're just changing
this to 150 by 150. we're resizing the
image so it's uniform every image comes
in identical to the other ones this is
something that's so important is when
you're resizing or reformatting your
data you really have to be aware of
what's going on
with images it's not a big deal because
with an image you just resize it so it
looks squishy or spread out or stretched
the neural network picks up on that and
it doesn't really change how it
processes it
so let's go ahead and run that
and now we've got our definition set up
on there
and then we want to go ahead and make
our
training data so make the trained data
daisy flower daisy directory print
length of X so here we go let's go and
run that and we're just loading up the
flower daisy so this is going all in
there and it's setting it's adding it in
to the our setup on there that to our x
and z setup and we see we have 769.
and then of course you can see this nice
bar here this is the bar going across is
that little added code in there that
just makes it really cool for doing
demos not necessarily when you're
building your own model or something
like that but if you're going to display
this to other people adding that little
what was it called
um
tqdm I can never remember that but the
tqdm module in there is really nice and
we'll go ahead and do sunflowers and of
course you could have just created an
array of these
but this has an interesting problem
that's going to come up and I want to
show you something
it doesn't matter how good the people in
the back are or how good you are
programming
errors are going to come up and you got
to figure out how to handle them and so
when we get all the way down to
the um where is it dandelion here's our
dandelion directory we're going to build
Jupiter has some cool things it does
which makes this really easy to deal
with
but at the same time you want to go back
in there depending on how many times you
rerun this how many times you pull this
so when you're finding errors
I'm going in here there's a couple
things you can do and we're just going
to um oh it wasn't there it is there's
our error I knew there was an error
this processed
1062 out of 1065.
now I can do a couple things one I could
go back into our definition
and I can just put in here try and so if
it has a bad conversion because this is
where the air is coming from uh just
skip it that's one way to do it
when you're doing a lot of work in data
science and you look at something like
this where you're losing three points of
data at the end you just say okay I lost
three points who cares or you can go in
there and try to delete it it really
doesn't matter for this particular demo
and so we're just going to leave that
error right alone and skip over because
it's already added all the other files
in there and this is a wonderful thing
about Jupiter notebook is that I can
just continue on there in the x and z
which we're creating is still running
and we'll just go right into the next
flower row so all these flowers are in
there
um that's just a cool thing about
Jupiter notebook
and then we can go ahead and just take a
quick look and see
what we're dealing with and this is of
course really when you're dealing with
the other people and showing them stuff
this is just kind of fun where we can
display it on the plot Library here
and we're just going to go through and
let's see what we got here uh looks like
we're going to do like five of each of
them I think
is that how they set this up
plot Library five by two okay oh I see
how they did it okay so two each so we
have five by two set up on our axes and
we're just going to go in and look at a
couple of these flowers
it's always a good thing to look at some
of your data no matter what you're doing
we've reformatted this to 150 by 150 you
can see how it really blurs this one up
here on the Tulip that is that resize to
150 by 150. and these are what's
actually going in these are all 150 by
150 images you can check the dimensions
on the side
and you can see just a quick sampling of
the flowers we're actually going to
process on here and again like I said at
the beginning most of your work in data
science is reprocessing
this different information so we need to
go ahead and take our labels
and run a label encoder on there and
then we're just going to Ellie is a
label encoder one of the things we
imported
and then we always use the fit
to categorical y comma 5 x here's our
array
X so if you look at this here's our fit
we're going to transform Z
that's our Z array we created
and then we have Y which equals that and
then we go ahead and do to categorical
we want five different categories
and then we create our X in P array of x
x equals x over 255.
so what's going on here there's two
different Transformations one we've
turned our categories into zero one two
three four five as the output and we
have taken our X array
and remember the X array is three values
of your different colors
this is so important to understand when
we do this across a numpy array this
takes every one of those three colors so
we have 150 by 150 pixels
out of those 150 by 150 pixels they each
have three
um color arrays and those color arrays
range from 0 to 250. so when we take the
x equals x over 255
I'm sorry range between 0 to 255. this
converts all those pixels to a number
between 0 and 1. and you really want to
do that when you're working with neural
networks now if you do a linear
regression model it doesn't affect it as
much and so you don't have to do that
conversion if you're doing straight
numbers but when you're running neural
networks if you don't do this you're
going to create a huge bias and that
means they'll do really good on
predicting one or two things and they'll
just totally die on a lot of other
predictions
so now we have our
X and Y values X being the data n y
being our known output
and with any good setup we want to
divide this data into our training so we
have X train we have our X test this is
the data we're not going to program the
model with and of course your y train
corresponds to your X train and your y
test corresponds to your X test the
outputs
and this is when we do the train test
split this was from the site kit sklearn
we imported train test split and we're
just going to go ahead and do the test
size at about a quarter of the data 0.25
and of course random is always good
this is such a good tool I mean
certainly you can do your own division
um
you know you could just take the first
you know 0.25 of the data or whatever to
the length of the data not real hard to
do but this is randomized so if you're
running this test a few times you can
kind of get an idea whether it's going
to work or not
sometimes what I will do
is I'll just split the data into three
parts
and then I'll test it on two with one
being the or I train it on two of those
parts with one being the test and I
rotate it so I come up with three
different answers which is a good way of
finding out just how good your model is
but for setting up let's stick with the
X train X test and the SK learn package
and then we're going to go ahead and do
a random seed
now a lot of times the cross actually
does this automatically but we're going
to go ahead and set it up on here and
you can see we did an NP random seed
from 42 and we get a nice RN number and
then we do TF random we set the seed so
you can set your Randomness at the
beginning of your tensorflow and that's
what the tf.random dot set is
so that's a lot of prep
um all this prep and then we finally get
to the exciting part
um this is where you probably spend once
you have the data prepped and you have
your pipeline going and you have
everything set up on there this is the
part that's exciting is building these
models
and so we look at this model one we're
going to designate a sequential they
have the API which is across the cross
tensorflow API versus sequential
sequential means we're going one layer
to the next so we're not going to split
the layer and bring it back together
it looks almost the same with the
exception of bringing it back together
so it's not a huge step to go from this
to an API
and the first thing we're going to look
at is our convolutional neural network
in 2D
so what's going on here there's a lot of
stuff that's going on here the default
for well let's start with the beginning
what is a convolutional 2d Network
well a convolutional 2d Network creates
a number of small windows and those
small Windows float over the picture and
each one of them is their own neural
network and it's basically becomes like
a uh a categorization and then it looks
at that and it says oh if we add these
numbers up a certain way uh we can find
out whether this is the right flower
based on this this little window
floating around which looks at different
things
and we have filters 32 so this is
actually creating 32 Windows is what
that's doing
and the kernel size is five by five so
we're looking at a five by five square
remember it's 150 by 150 so this narrows
it down to a five by five it's a 2d so
it has your X Y coordinates and when we
look at this five by five remember each
one of these is it's actually looking at
five by five by three
so we're actually looking at 15 by 15
different pixels
and padding is just
usually I just ignore that
activation by default is relu we went
ahead and put the relu in there
there's a lot of different activations
Ray Lu is for your smaller uh when you
remember I mentioned atom when you have
a lot of data data use an atom kind of
activation or use an atom processing
we're using the rayleu here uh
it kind of gives you a yes or no but it
it doesn't give you a full yes or no it
has a a zero and then it kind of shoots
off at an angle
very common it's the most common wand
and then of course here's our input
shape 150 by 150 by 3 pixels
and then we have to pool it so whenever
you have a two convolutional 2D
layer we have to bring this back
together and pull this into a neural
network and then we're going to go ahead
and repeat this
so we're going to add another Network
here one of the cool things if you look
at this is that it as it comes in it
just kind of automatically assumes
you're going down to the next layer
and so we have another convolutional
neural network 2D here's our Max pooling
again we're going to do that again Max
pooling and we're just going to filter
on down
now one of the things they did on this
one is they changed the kernel size they
changed the number of filters and so
each one of these steps kind of looks at
the data a little bit differently
and that's kind of cool because then you
get a little added filtering on there
this is where you start playing with the
model you might be looking at a
convolutional neural network which is
great for image classifications
um
we get down to here one of the things we
see is flattened so we add we just
flatten it remember this is 150 by 150
by three well and actually the pool size
changes so it's actually smaller than
that flattened just puts that into a 1D
array so instead of being you know a
tensor of this really complexity with
the the pixels and everything it's just
flat and then the dense
is just another activation on there by
default it is probably rayleu as far as
this activation
and then oh yeah here we go in
sequential they actually added the
activation as relu so this just because
this is sequential this activation is
attached to the dense
and there's a lot of different
activations but Rayleigh is the most
common one and then we also see a soft
Max softmax is similar but it has its
own kind of variation and one of the
cool things you know what let me bring
this up because if we if you don't know
about these activations this doesn't
make sense
and I just did a quick Google search on
images of tensorflow activations
um I should probably look at which
website this is
but this is the output of the values so
as your X as it adds in all those
weighted X values going into the node
it's going to activate it a certain way
and that's a sigmoid activation and you
can see it goes between 0 and 1 and has
a nice curve there this also shows the
derivatives and if we come down the
seven popular activation functions
non-linear activations there's a lot of
different options on this let me see if
I can find the
oops we can find the specific to relu
so this is a leaky Ray Lou and you can
see instead of it just being 0 and then
a value between going up it has a little
leaky there otherwise your Rayleigh
loses some notes they just become
inactive
um but you can see there's a lot of
different options here here's a good one
right here with the rayleau you can see
the Rayleigh function on the upper on
the upper left here and then the Leaky
Rayleigh over here on the right which is
very commonly used also
one of the things I use with processing
um language is the Sig is the
exponential one or the tangent H the
hyperbolic tangent because they have
that nice funky curve that comes in that
has a whole different meaning and
captures word use better
again these are very specific to domain
and you can spend a lot of time playing
with different models for our basic
model we'll stick to the relu and the
soft Max on here and we'll go ahead and
run and build this model
so now that we've had fun playing with
all these different models that we can
add in there we need to go ahead and
have a batch size on here
128 epics 10
. this means that we're going to send
128 uh rows of data or flowers at a time
to be processed
and the epics 10 that's how many times
we're going to Loop through all the data
deuce the values and verbose verbose
equals one means that we're going to
show what's going on
value monitor what we're monitoring
we'll see that as we actually train the
model this is what's what's going to
come out of there if you set the verbose
equal to zero you don't have to watch it
train the model although it is kind of
nice to actually know what's going on
sometimes
and since we're still working on
bringing the data in here's our batch
side here's our epics we need to go
ahead and create a data generator this
is our image data generator
and it has all the different settings in
here almost all of these are defaults so
if you're looking at this going oh my
gosh this is confusing most of the time
you can actually just ignore most of
this vertical flip so you can randomly
flip pictures you can randomly
horizontally flick them you can shift
the picture around this kind of helps
gives you multiple data off of them
zooming rotation there's all kinds of
different things you can do with images
most of these we're just going to leave
as false we don't really need to do all
that um
setup because we already have a huge
amount of data if your short data you
can start flipping like a horizontal
picture and it will generate it's like
doubling your data almost
so the upside is you double your data
the downside is that if you already have
a bias in your data you already have
um
5000 sunflowers and only two roses
that's a huge bias it's also going to
double that bias that is the downside of
that
and so we have our model model compile
and this you're going to see in all the
cross we're going to take this model
here we're going to take all this
information as far as how we want it to
go and we're going to compile it
this actually builds the model and so
we're going to run that and I want you
to notice uh learning rate
very important this is the default zero
zero one there's there you really don't
this is how slowly it adjusts to find
the right answer
and the more data you have you might
actually make this a smaller number with
larger with you have a very small sample
of data you might go even larger in that
and then we're going to look at the loss
categorically categorical cross entropy
most commonly used
and this is uh how how much it improves
the model is improving is what this
number means or yeah that's that's
important on there and then the accuracy
we want to know just how good our model
is on the accuracy
and then
one of the cool things to do is if
you're in a group of people who are
studying the model if you're in
shareholders you don't want to do this
is you can run the model summary
I do this by default and you can see the
different layers that you built into
this model just a quick summary on there
so we went ahead and we're going to go
ahead and create a
we'll call it history but we want to do
a model fit generator
and so what this history is doing is
this is tracking what's going on as well
it fits the model
now there's a lot of new setups in here
where they just use fit and then you put
the generator in here
we're going to leave it like this even
though the new default
is a little different on that it doesn't
really matter it does the same thing and
we'll go ahead and just run that
and you can see while it's running right
here we're going through the epics we
have one of ten now we're going through
6 to 25 here's our loss we're printing
that out so you can see how it's
improving and our accuracy the accuracy
gets better and better and this is 6 out
of 25. this is going to take a couple
minutes to process because we are
training 150 by 150 by 3 pixels across
six layers or eight layers whatever it
was
that is a huge amount of processing so
this will take a few minutes to process
this is when we talk about the hardware
and the problems that come up in data
science and why it's only now just
exploding being able to do neural
networks this is why this process takes
a long time
now you should have seen a jump on the
screen here because I did pause the
recorder to let this go ahead and run
all the way through it's epics
let's go ahead and take a look and see
what these epics are and if you set the
verbose to zero instead of one it won't
show what's going on in the behind the
scene to just training it so we look at
this epic 10 epic so we went through all
the data 10 times if I remember
correctly there's roughly a gig of data
there so that's a lot of data
the first thing you're going to notice
is the 270 seconds that's how much each
of those epics took to run and so if you
divide 60 in there you roughly get about
five minutes worth of each epic so if I
have 10 epics that's 50 minutes almost
an hour of run time
that's a big deal we talk about
processing uh in on this particular
computer I actually have what is it uh
eight cores with 16 dedicated threads so
it runs like a 16 core computer it
alternates the threads going in and it
still takes it five minutes for each one
of these epics so you start to see that
if you have a lot of data this is going
to be a problem if you have a number of
models you want to find out how good the
models are doing what model to use
and so each of those models could take
all night to run in fact I have a model
I'm running now that takes over uh takes
about a day and a half to test each
model it takes four days to do with the
whole data so what I do is I actually
take a small piece of the data
test it out to find out get an idea of
how the different setups are going to do
and then I increase that size of the
data and then increase it again and I
can just take that that curve and kind
of say okay if the data is doing this
then I need to add in more dense layers
or whatever so you can do a small chunks
of data then figure out what it costs to
do a large set of data and what kind of
model you want
the loss as we see here continues to go
down this is the error this is how much
error is in there it really isn't a
user-friendly number other than the more
it Trends down the better and so if you
continue to see the loss going down
eventually get to the point where it
stops going down and it goes up and down
and kind of wavers a little bit at that
point you know you've run too many epics
you're starting to get a bias in there
and it's not going to give you a good
model fit
the accuracy just turns us into
something that we can use and so the
accuracy is what percentage of guesses
in this case is categorical so this is
the percentage of guesses are correct
value loss is similar you know it's a
minus a value loss
and then you have the value accuracy and
you'll see the value accuracy is pretty
similar to the accuracy just rounds it
off basically and so a lot of times you
come down here and you go okay we're
doing 0.5.6
0.7 and that is 70 accuracy or in this
case
68.59 accuracy that's a very usable
number and it's very important to have
if you're identifying uh flowers that's
probably good enough if you can get
within a close distance and knowing what
flower you're identifying if you're
trying to figure out whether someone's
going to die from a heart attack or not
you might want to rethink it a little
bit or re-key how you're building your
model so if I'm working with a uh a
group of clients shareholders in a
company or something like that you don't
really want to show them this you don't
want to show them hey you know this is
what's going on with the accuracy these
are just numbers and so we want to go
and put the finishing touches just like
when you are building a house and you
put in the frame and the trim on the
house it's nice to have something a nice
view of what's going on and so we'll go
ahead and do a pi plot and we'll just
plot the history of the loss the history
of the value loss
over here epics train and test and so
we're just going to compute these this
is really important and what I want you
to notice right here is when we get to
about oh five epics a little more than
five six epics you see a crossover here
and it starts Crossing as far as the
value loss and what's going on here is
you have the loss in your actual model
and your actual data and you have the
value loss where it's testing it against
the the test data the data wasn't used
to program your model wasn't used to
train your model on
and so when we see this crossing over
this is where the bias is coming in this
is becoming overfitted and so when you
put these two together uh right around
five and six you start to see how it
does this this switch over here and
that's really where you need to stop
right around five yeah six
um it's always hard to guess because at
this point the model is kind of a black
box
uh see but you know that right around
here if you're saving your model after
each run you want to use the one that's
right around five epics because that's
the one that's going to have the least
amount of bias so this is really
important as far as guessing what's
going on with your model and its
accuracy and when to stop uh it also is
you know I don't show people this mess
up here
um I show somebody this kind of model
and I say this is where the training and
the testing comes in on this model
it just makes it easier to see and
people can understand what's going on
so that completes our demo and you can
see we did what we were set out to do we
took our flowers and we were able to
classify them within about yeah 68 70
accuracy whether it's going to be a
dahlia sunflower cherry blossom Rose a
lot of other things you can do with your
output as far as a uh different tables
to see where the errors are coming from
and what problems are coming up
and we're going to take a look at image
classification using cross and the basic
setup and we'll actually look at two
different demos on here
what's in it for you today what is image
classification
Intel image classification data creating
neural networks with Karas and the vgg16
model
what is image classification
the process of image classification
refers to assigning classes to an entire
image images can be classified based on
different categories like weather it is
a nighttime or daytime shot what the
image represents Etc you can see here we
have mountains looking for mountains
we'll actually be doing some
pictures of scenery and stuff like that
in deep learning we perform image
classification by using neural networks
to extract features from images and
classify them based on these features
you can see here where it has like what
computer sees and it says oh yeah we see
mostly 4S maybe a little bit of
mountains because the way the image is
and this is really where one of the
areas that neural networks really shines
if you try to run this stuff through
more like a linear regression model
you'll still get results but the results
kind of miss a lot of things as the as
the neural networks get better and
better at what they do with different
tools we have out there
uh so Intel image classification data
the data being used is the Intel image
classification data set which consists
of images of six types of land areas and
say we have Forest building glaciers and
mountains sea and Street
and you can see here is a couple of the
images out of there as a setup in the in
the
Intel image classification data that
they use
and then we're going to go into creating
a neural networks with Karas
the convolutional neural network that we
are creating from scratch looks as shown
below
you'll see here we have our input layer
um
they have it listed Max pooling so you
have
as they're coming in with the input
layer and this the input layer is
actually
um before this but the first layer that
it's going to go into is going to be a
convolutional neural network then you
have a Max pooling that pulls those the
the convolutional neural networks
returns in this case they have two of
those that is very standard with
convolutional neural networks one of the
ones that I was looking at earlier that
was standard being used by I want to one
of the larger companies I can't remember
which one for doing a large amount of
identification had two convolutional
neural networks each with their Max
pooling and then about 17 dense layers
after it we're not going to do that
heavy duty of a of a code but we'll get
you ahead in the right direction and
that gives you an idea of what you're
actually going to be looking at when you
look at the flattened part and then the
dense we're talking like 17 dense layers
afterwards
I find that a lot of the stuff I've been
working on I end up maxing it out right
around nine dense layers it really
depends on what you have going in and
what you're working with
and the vgg16 model
vgg16 is a pre-trained CNN model which
is used for image classification it is
trained on a large varied data set and
fine-tuned to fit image classification
data sets with ease
and you can see down here we have the
input coming in uh the convolutional
neural network one to one one to two and
then pooling and then we do two to one
two to two convolutional Network then
pooling three to two and you can see
there's just this huge layering of
convolutional neural networks and in
this case they have five such layers
going in and then in three dents going
out or uh more now when they took this
setup this actually won an award back in
2019 for this particular setup
and it does it does really good except
that again we only show the three dense
layers here and as you find out
depending on your data going in what you
have set up that really isn't enough on
one of these setups and I'm going to
show you why we restricted it because it
does take up a lot of processing power
in some of these things
so let's go ahead and rope our sleeves
and we're going to look at both the
setups we're going to start with the the
first classification and then we'll go
into the vgg 16 and show you how that's
set up now I'm going to be using
anaconda and let me flip over to my
anaconda so you can see what that looks
like
now I'm running in the Anaconda here
you'll see that I've set up a main
Python 3 8 I always put that in there
because this is where I'm doing like
most of my kind of playing around uh
this is done in Python version 3.8 we're
not going to dig too much into versions
uh at this point you should already have
cross installed on there usually cross
takes a number of extra steps
and then our usual
um uh setup is the numpy the pandas uh
your SK your site kit which is going to
be the SK learn your Seaborn and I'll
show you those in just a minute and then
I'm just going to be in the Jupiter lab
where I've created a new
notebook in here
and let's flip on over there to my blank
notebook
now there's a couple cool things to note
in here is that one I use the the
Anaconda Jupiter notebook setup because
it keeps everything separate except for
cross cross is actually running
separately in the back I believe it's a
c program
uh what's nice about that is that it
utilizes the multi-processors on the
computer and I'll mention that just in a
little bit when we actually get down to
running the code
it's and when we look in here the couple
things to note is here's our
oops I thought I grabbed the other
drawing thing but here's our numpy and
our pandas right here and our operating
system this is our PSI kit you always
import it as SK learn for the
classification report we're going to be
using well usually in Port like Seabourn
brings in all of your pipelot library
also
kind of nice to throw that in there I
can't remember if we're actually using
Seabourn if they just the people in the
back just threw that together and then
we have the sklearn shuffle for
shuffling data here's our matplot
library that the Seaborn is pretty much
built on
CV2 if you're not familiar with that
that is our image module for importing
the image and then of course we have our
tensorflow down here which is what we're
really working with
and then the last thing is just for
visual effect while we're running this
if you're doing a demo and you're
working with the partners or the
shareholders this TQ DM is really kind
of cool it's an extensible progress bar
for Python and I'll show you that too
remember data science is not I mean you
know most is code when I'm looking
through this code I'm not going to show
half of this stuff to the shareholders
or anybody I'm working with they don't
really care about pandas and all that we
do because we want to understand how it
works
uh so we need to go ahead and import
those different
um
setup on there and then the next thing
is we're going to go ahead and set up
our classes uh now we remember if we had
Mountain Street Glacier Building C and
Forest those were the different images
that we have coming in
and we're going to go ahead and just do
class name labels and we're going to
kind of match that class name of ifri
class name
equals the class names so our labels are
going to match the names up here
and then we have the number of classes
in the print the class names and the
labels and we'll go ahead and set the
image size this is important that we
resize everything because if you
remember with neural networks
they take one size data coming in and so
when you're working with images you
really want to make sure they're all
resized to the same setup it might
squish them it might stretch them that
generally does not cause a problem in
these and some of the other tricks you
can do with if you if you need more data
and this is one that's used regularly
we're not going to do it in here is you
can also take these images and not only
resize them but you can tilt them one
way or the other crop parts of them so
they process slightly differently and
they'll actually increase your accuracy
of some of these predictions and so you
can see here we have Mountain equals
zero that's what this class name label
is Street equals one Glacier equals two
buildings equals three C4 Forest equals
five
now we did this as an enumerator so each
one is zero through five a lot of times
we do this instead as
um
zero one zero one zero one so you have
five outputs and each one's a zero or a
one coming out so the next thing we
really want to do is we want to go ahead
and load the data up and just put a
label in there loading data just just so
you know what we're doing I'm going to
put in the loading data down here make
sure it's well labeled and we'll create
a definition for this
and this is all part of your
pre-processing
at this point you could replace this
with all kinds of different things
depending on what you're working on and
if you once you download you can go
download this data set uh send a note to
the simply learned team here in YouTube
and they'll be happy to direct you in
the right direction and make sure you
get this path here so you have the right
whatever wherever you saved it a lot of
times I'll just abbreviate the path or
put it as a sub thing and just get rid
of the directory but again double check
your paths we're going to separate this
into a segment for training and a
segment for testing and that's actually
how it is in the folder let me just show
you what that looks like
so when I have my lengthy path here
where I keep all my programming simply
learned this particular setup we're
working on image classification and
image classification clearly you
probably wouldn't have that lengthy of a
list
and when we go in here you'll see
sequence train sequence test they've
already split this up this is what we're
going to train the data in and again you
can see buildings Force Glacier Mountain
C Street and if we double click let's go
under Forest you can see all these
different Forest images and there's a
lot of variety here I mean we have
winter time we have summertime so it's
kind of interesting you know here's like
a Fallen Tree versus
a road going down the middle that's
really hard to train and if you look at
the buildings
a lot of these buildings you're looking
up a skyscraper we're looking down the
setup
here's some trees with one I want to
highlight this one it has trees in it
let me just open that up so you can see
it a little closer
the reason I want to highlight this is I
want you to think about this we have
trees growing is this the city or a
forest
so this kind of imagery makes it really
hard for a classifier and if you start
looking at these you'll see a lot of
these images do have trees and other
things in the foreground weird angles
really a hard thing for a computer to
sort out and figure out whether it's
going to be a forest or a city
and so in our loading of data uh one we
have to have the path the directory
we're going to come in here we have our
images and our labels
so we're going to load the images in one
section the label is in another
um
and if you look through here it just
goes through the different folders in
fact let me do this let me
there we go as we look at this we're
just going to Loop through the three the
six different folders that have the
different Landscapes and then we're
going to go through and pull each file
out
and each label so we set the label we
set the folder for file and list here's
our image path join the paths this is
all kind of not General stuff so I'm
kind of skipping through it really quick
and here's our image setup if you
remember we're talking about the images
we have our CV2 reader so it reads the
the image in it's going to go ahead and
take the image and convert it to from
blue green red to red green blue this is
a CV2 thing almost all the time it
Imports it and instead of importing it
as a standard that's used just about
everywhere it Imports it with the BGR
versus RGB RGB is pretty much a standard
in here you have to remember that with
CV2 and then we're going to go ahead and
resize it this is the important part
right here we've set it we've decided
what the size is and we want to make
sure all the images have the same size
on them
and then we just take our images we're
just going to impend the image pin the
label and then the images it's going to
turn into a numpy array this just makes
it easier to process and manipulate and
then the labels is also a numpy array
and then we just return the output
append images and labels and we return
the output down here
so we've loaded these all into memory we
haven't talked much there'd be a
different setup in there because there
is ways to feed the files directly into
your cross model but we want to go ahead
and just load them all because it's
really
he for today's processing and what our
computers can handle that's not a big
deal
and then we go ahead and set the train
images train labels test images test
labels and that's going to be returned
in our output append and you can see
here we did um images and labels set up
in there and it just loads them in there
so we'll have these four different
categories let me just go ahead and run
that
uh
so now we've gone ahead and loaded
everything on there
and then if you remember from before uh
we imported let me just go back up there
Shuffle here we go here's our sklearn
utilities import Shuffle and so we want
to take these labels and shuffle them
around a little bit just mix them up so
it's not having the same if you run the
same process over and over uh then you
might run into some problems on there
and just real quick let's go ahead and
do
a plot so we can just you know we've
looked at them as far as from outside of
our code we pulled up the files and I
showed you what that was going on we can
go ahead and just display them here too
and I tell you when you're working with
different people
this should be highlighted right here
this thing is like when I'm working on
code and I'm looking at this data and
I'm trying to figure out what I'm doing
I skipped this process
the second I get into a meeting and I'm
showing what's going on to other people
I skip everything we just did so and go
right to here where we want to go ahead
and display some images and take a look
at it
and in this display I've taken them and
I've resized the images to 20 by 20.
[Music]
pretty small so we're going to lose just
a massive amount of detail and you can
see here these nice pixelated images I
might even just stick with the folder
showing them what images we're
processing
again this is yeah be a little careful
this
maybe resizing it was a bad idea in fact
let me try it without resizing it see
what happens oops so I took out the
image size and then we put this straight
in here
one of the things again this is um
but the D there we go one of the things
again that we want to note
whenever we're working on these things
uh is the CV2
there are so many different uh image
classification setups it's really a
powerful package when you're doing
images
but you do need to switch it around so
that it works with the pi plot and so
make sure you take your numpy array and
change it to a uinteger 8 format because
it comes in as a float otherwise you'll
get some weird images down there and so
this is just basically we've split up
our we've created a plot
we went ahead and did the plot 20 by 20
or plot figure size is 20 by 20 and then
we're doing 25 so 5x5 subplot nothing
really going on here too exciting but
you can see here where we get the images
and really when you're showing people
what's going on this is what they want
to see so you skip over all the code and
you have your meeting you say okay
here's our images of the building
um don't get caught up in how much work
you do get caught up in what they want
to see so if you want to work in data
science that's really important to know
and this is where we're going to start
having fun uh here's our model this is
where it gets exciting when you're
digging into these models
and you have here let me get
there we go
when you have here if you look here
here's our convolutional neural network
2D
and 2D is an image you have two
different dimensions x y and even though
there's three colors it's still
considered 2D if you're running a video
you'd be convolutional neural network 3D
if you're doing a series going across a
Time series it might be one D
and on these you need to go ahead and
have your convolutional neural network
if you look here there's a lot of really
cool settings going on to dig into we
have our input shape so everything's
been set to 150 by 150 and it has of
course three different color schemes in
it that's important to notice
activation
default is relu this is small amounts of
data being processed on a bunch of
little
neural networks
and right here is the 32 that's how many
of these convolutional neural networks
are being strung up on here
and then the three by three
uh when it's doing its steps it's
actually looking at a little three by
three Square on each image and so that's
what's going on here and with
convolutional neural networks the window
floats across and adds up all these
numbers going across on this data and
then eventually it comes up with a 30 in
this case 32 different feature options
that it's looking for and of course you
can change that 32 you can change the
three by three so you might have a
larger setup as you know if you're going
across 150 by 150 that's a lot of steps
so we might run this as 15 by 15.
there's all kinds of different things
you can do here
we're just putting this together again
that would be something you would play
with to find out which ones are going to
work better on this setup and there's a
lot of play involved
it's really where it becomes an art form
is guessing at what that's going to be
the second part I mentioned earlier and
I can only begin to highlight this when
you get to these dense layers one is the
activation is a raylu they use a relu
and a soft Max here
it's a whole a whole setup just
explaining why these are different
um
and how they're different because
there's also an exponential there's a
tangent in fact uh there's just a ton of
these and you can build your own custom
activations depending on what you're
doing
a lot of different things go into these
activations there are two or three major
thoughts on these activations and relu
and softmax are well rayleu
you're really looking at just the number
you're adding all the numbers together
and you're looking at euclidean geometry
ax plus b x 2 plus cx3 plus a bias
with softmax this belongs to the party
of
um it's activated or it's not except
it's they call it soft Max because when
you get the the to zero instead of it
just being zero it's actually slightly
a little bit less than zero so that when
it trains it doesn't get lost
there's a whole series of these
activations another activation is the
tangent
where it just drops off and you have
like a very narrow area where you have
from minus one to one or exponential
which is zero to one so there's a lot of
different ways to do the activation
again we can do that would be a whole
separate lesson on here we're looking at
the convolutional neural network and
we're doing the two pools this is so
common you'll see two two convolutional
neural networks decked on top of each
other each with its own Max pull
underneath and let's go ahead and run
that so we built our model there and
then we need to go ahead and
compile the model so let's go ahead and
do that
uh we are going to use the atom
Optimizer the bigger the data the atom
fits better on there just some other
Optimizer but I think atom is a default
I don't really play with the optimizer
too much that's like the you once you
get a model that works really good you
might try some different optimizers but
atoms usually the most and then we're
looking at loss
pretty standard we want to minimize our
lot we want to
maximize the loss of error and then
we're going to look at accuracy
everybody likes theocracy I'm going to
tell you right now
I started talking to people and like
okay what's what's the loss on this and
that and as a data science yeah I want
to know how the lot what's going on with
that we'll show you why in a minute
but everybody wants to see accuracy we
want to know how accurate this is
and then we're going to run the fit and
I wanted to do this just so I can show
you
even though we're in Python setup in
here where jupyter notebook is using
only a single processor I'm going to
bring over my little CPU tool this is
eight cores on 16 dedicated threads so
it shows up as 16 processors
and actually I got to run this and then
move it over so we're going to run this
and hopefully it doesn't destroy my mic
and as it comes in you can see it's
starting to do go through the epics and
we said I said it for five epics
and then this is really nice because
cross uses all the different threads
available
so it does a really good job of doing
that this is going to take a while if
you look at here it's uh
ETA two minutes and 25 seconds 24
seconds so this is roughly two and a
half minutes per epic and we're doing
five epics so this is going to be done
in roughly 15 minutes
I don't know about you but I don't think
you want to sit here for 15 minutes
watching The Green bars go across so
we'll go ahead and let that run
and there we go uh there was our 15
minutes it's actually less than that uh
because I did when I went in here
realized that uh where was it
here we go here's our model compile
here's our model flip fit and here's our
epics so I did four epics so a little
bit better a little more like 10 to 11
minutes instead of uh doing the full
15. and when we look at this here's our
model we did we talked about the
compiler here's our history we're going
to history equals the model fit we'll go
into that in just a minute
and we're looking at is we have our
epics here's our validation split so as
we train it uh we're weighing the
accuracy versus you kind of pull some
data off to the side while you're
training it and the reason we do that is
that you don't want to overfit and we'll
look at that chart in just a minute
uh here's back size
this is just how many images you're
sending through at a time the larger the
batch it actually increases the
processing speed and there's reasons to
go up or down on the batch size because
of the uh the the smaller the batch is a
certain point where you get two large of
a batch and it's trying to fit
everything at once uh so my 128 is kind
of big depends on the computer you're on
what it can handle
and then of course we have our train
images and our train labels going in
telling it what we're going to train on
and then we look at our four epics here
here's our accuracy we want the accuracy
to go up and we get all the way up to
0.83 or 83 percent now this is actual
percentage based pretty much and we can
see over here our loss we want our loss
to go down really fluctuates 55
1.2.77.48
uh so we have a lot of things going on
there let's go ahead and graph those
turn that up
and our team in the back did a wonderful
job of putting together
um this basic plot set up
um here's our subplot coming in we're
going to be looking at um
uh from the history we're going to see
that the accuracy and the value accuracy
labels and set up on there and we're
going to also look at loss and value
loss so you can see what this looks like
what's really interesting about this
setup and let me let me just go ahead
and show you because without actually
seeing the plots it doesn't make a whole
lot of sense uh
it's just basic plotting of of the data
using the pi plot Library
and I want you to look at this this is
really interesting
um
when I ran this the first time I had
very different results
um and they vary greatly and you can see
here our accuracy continues to climb
there's a crossover here
put it in here right here's our
crossover
and I point that out because as we get
to the right of that crossover where our
accuracy and we're like oh yeah I got
0.8 percent we're starting to get an
overfit here that's what this this
switchover means
um as our value as a training set versus
a value accuracy stays the same and so
that this is the one we're actually
really want to be aware of and where it
crosses
that's kind of where you want to stop at
and we can see that also with the train
loss versus the value loss right here we
did one Epic and look how it just
flat lines right there with our loss
so really one Epic is probably enough
and you're going to say wow okay point
eight percent
um certainly if I was working with the
shareholders
um telling them that it has an 80
accuracy isn't quite true and we'll look
at that a little deeper it really comes
out here that the accuracy of our actual
values is closer to point four one
percent right here even after running at
this number of times and so you really
want to stop right here at that
crossover one Epic would have been
enough so the data is a little
overfitted on this when we do four epics
foreign
there we are okay
my drawing won't go away
um let's see if I can get there we go
for some reason I've killed my drawing
ability and my recorder
all right took a couple extra clicks uh
so let's go ahead and take a look at our
actual test loss so you see where a
cross is over that's where I'm looking
at that's where we start overfitting the
model
and this is where if we were going to go
back and continually upgrade the model
we would start taking a look at the
images and start rotating them we might
start playing with the convolutional
neural network instead of doing the
three by three window we might expand
that or you know find different things
that might make a big difference as far
as the way it processes these things so
let's go ahead and take a look at our
our test loss and remember we had our
training data now we're going to look at
our test images and our test labels
for our test loss here this is just
model evaluate just like we did fit up
here
where was it
um one more model fit with our training
data going in now we're going to
evaluate it on the and and this data has
not been touched yet so this model has
never seen this data this is on uh
completely new information as far as the
model is concerned of course we already
know what it is from the labels we have
and this is what I was talking about
here's the actual accuracy right here
0.48
or 4847 so this 49 of the Time guesses
what the image is
uh and I mean really that's the bottom
dollar does this work for what you're
needing does 49 work do we need to
upgrade the model more
um in some cases this might be uh oh
what was I doing I was working on stock
evaluations and
we were looking at what stops were the
top performers well if I get that 50
correct on top performers uh I'm good
with that that's actually pretty good
for stock evaluation in fact the number
I had for stock was more like
30 something percent as far as being a
top performer stock much harder to
predict but at that point you're like
well you'll make money off of that so
again this number right here depends a
lot on the domain you're working on
foreign
we want to go ahead and bring this home
a little bit more as far as looking at
the different setup in here and one of
the from sklearn if you remember
actually let's go back to the top we had
the classification report and this came
in from our sklearn or Psy kit setup and
that's right here you can see it right
here on the
see there we go
classification report right here
sklearnmetrics import classification
report this we're going to look at next
a lot of this stuff depends on who
you're working with
so when we start looking at precision
you know this is like for each value I
can't remember what 111 was probably
mountains so if 44 percent is not good
enough if you're doing like
um you're in the medical department and
you're doing cancer is it is this
cancerous or not and I'm only 44
accurate not a good deal you know I
would not go with that
um so it depends on what you're working
with on the different labels and what
they're used for Facebook
you know 44 I'm guessing the right
person I would hope it does a little bit
better than that
um but here's our main accuracy this is
what almost everybody looks at they say
oh 48 that's what's important
um again it depends on what domain
you're in and what you're working with
and now we're going to do the same model
oops somehow I got my there it goes I
thought I was going to get stuck on
there again uh this time we're going to
be using the vgg
16. and remember this one is uh all
those layers
going into it so it's basically a bunch
of convolutional neural networks getting
smaller and smaller on here and so we
need to go ahead and um import all our
different stuff from Cross we're
importing the main one is the V g16
setup on there and just aim that there
we go
um
there's kind of a pre-processing images
applications pre-process input this is
all part of the VG g16 setup on there
and once we have all those we need to go
ahead
and create our model
and we're just going to create a vgg16
model in here inputs model inputs
outputs model inputs I'm not going to
spend as much time as they did on the
other one uh we're going to go through
it really quickly one of the first
things I would do is if you remember in
Cross you can trade treat a model like
you would a layer
and so at this point I would probably
add a lot of dense layers on after the
vgg 16 model and create a new model with
all those things in there and we'll go
ahead and run this because here's our
model coming in and our setup
and it'll take it just a moment to
compile that what's funny about this is
I'm waiting for it to download the
package since I pre-ran this it takes it
a couple minutes to download the vgg 16
model into here and so we want to go
ahead and train features for the model
we're going to predict that we're going
to predict the train images and we're
going to test features on the predict
test images on here
and then I told you I was going to
create another model too and the people
in the back did not disappoint me they
went ahead and did just that
and this is really an important part
um this is worth stopping for I told you
I was going to go through this really
quick
so here's our uh
we at we have our model two
um coming in and we've created a model
up here with the vgg16 model equals
model inputs model inputs and so we have
our vgg16 this has already been
pre-programmed
uh and then we come down here and I want
you to notice on this right here layered
model 2 layers minus four to One X layer
X
um
we're basically taking this model and
we're adding stuff onto it and so uh
we've taken we've just basically
duplicated this model we could have done
the same thing by using model up here as
a layer we could have the input go to
this model
and then have that go down here so we've
added on this whole setup this whole
block of code from 13 to 17 has been
added on to our vgg16 model and we have
a new model with the layer my input and
X down here
let's go ahead and run that and compile
it and that was a lot to go through
right there when you're building these
models this is the part that gets so
complicated
did you get stuck playing in and yet
it's so fun uh it's like a puzzle how
can I Loop these models together
and in this case you can see right here
that the layers we're just copying
layers over and adding each layer in
this is one way to build a new model
and we'll go ahead and run that
like I said the other ways you can
actually use the model as a layer I've
had a little trouble playing with it
sometimes when you're using the Straight
model over
you run into issues
um
and it seems like it's going to work and
then you mess up on the input and the
output layers there's all kinds of
things that come up
let's go ahead and do the new model
we're going to compile it again here's
our metrics accuracy sparse categorical
loss pretty straightforward just like we
did before you got to compile the model
and just like before we're going to take
our create a history on the history is
going to be a new model fit train 128
and just like before if you remember
when we started running this stuff we're
going to have to go ahead and it's going
to light up our setup on here and this
is going to take a little bit to get us
all set up it's not going to just happen
in a couple minutes so let me go ahead
and pause the video and run it and then
we'll talk about what happened
okay now when I ran that these actually
took about six minutes each
um so it's a good thing I put it on hold
we did four epics actually this topic
says at 10 and switch it to four because
I didn't want to wait an hour
and you can see here our accuracy
and our loss numbers going down and just
at a glance
it actually performed if you look at the
accuracy
0.2658 so our accuracy is going down or
you know 26 percent
um 34 35 and you can see here at some
point it just kind of kicks the bucket
again this is overfitting
that's always an issue when you're
running on programming these different
neural networks
and then we're going to go ahead and
plot the accuracy history we built that
nice little subroutine up above so we
might as well use it
and you can see it right here
there's that crossover again
and if you look at this look how the how
the uh the red shifts up how the uh our
loss functions and everything crosses
over we're over fitting after one Epic
um we're clearly
not helping the problem or doing better
um we're just gonna it's just gonna
Baseline this one actually shows what
the training versus the loss
um value loss maybe second epic so on
here we're now talking more between the
first and the second epic and that also
shows kind of here so somewhere in here
it starts over fitting
and right about now you should be saying
oh uh something went wrong there I
thought that
um when we went up here and ran this
look at this we have the accuracy up
here it's hitting that 48 percent
and we're down here
um
you look at the score down here that
looks closer to 20 percent not nearly
anywhere in the ballpark of what we're
looking for
and we'll go ahead and run it through
the the actual test features here and
and there it is
um we actually run this on the Unseen
data and everything
0.18 or 18 percent
um I don't know about two but I wouldn't
want you know at 18 this did a lot worse
than the other one
I thought this was supposed to be the
supermodel the model that beats all
models the vgg16 that won the awards and
everything well the reason is is that
um one we're not pre-processing the data
so it needs to be more there needs to be
more
um as far as like rotating the data at
you know 45 degree angle taking partials
of it so you can create a lot more data
to go through here
um and that would actually greatly
change the outcome on here and then we
went up here we only added a couple
dense layers uh and we added a couple
convolutional neural networks
this huge pre-trained setup is looking
for a lot more information coming in as
far as how it's going to train and so
this is one of those things where I
thought it would have done better and I
had to go back and research it and look
at it and say why didn't this work why
am I getting only uh 18 here instead of
44 or better
and that would be wise it doesn't have
enough training data coming in and again
you can make your own training data so
it's not that we have a shortage of data
it's that that some of that has to be
switched around and moved around a
little bit
and this is interesting right here too
if you look at the Precision
we're getting it on number two and yet
we had zero on everything else so for
some reason it's not seeing the
different variables in here so it'd be
something else to look in and try to
find track down
um
and that probably has to do with the
input but you can see right here we have
a really good solid 0.48 up here and
that's where I'd really go with is
starting with this model and then we
look at this model and find out why are
these numbers not coming up better is it
the data coming in where is the setup on
there
and that is the art of data science
right there is finding out which models
work better and why hey hey simply
IBM to learn more about this course you
can find the course Link in the
description box below deep learning
interview
we're going to go from the very basics
oral networks and learning into some of
the more commonly used models so you can
have an understanding of what kind of
questions are going to come up and what
you need to know in interview questions
we'll start with a very general concept
of what is deep learning this is where
we take large volumes of data in this
case on cats and dogs or whatever A lot
of times you use a training setup to
train your model remember it's kind of
like a magic Black Box going on there
then we use that to extract features or
extract information and in this case
classify the image of a cat and a dog so
the primary takeaway we're talking about
deep learning is it learns from large
volumes of structures and even
unstructured data and uses complex
algorithms to train neural network it
also performs complex operations to
extract hidden patterns and features and
if we're going to discuss deep learning
in this very simplified overview and we
also have to go over what is a neural
network this is a common image you'll
see of a drawing of a forward
propagation neural network and it's it's
human brain inspired so system which
replicate the way humans learn so this
has inspired how our own neurons and our
brain fire but at a much simplified
level obviously it's not ready to take
over the human population and be our
leader yet not for many years it's very
much in its infant stage but it's
inspired by how our brains work and they
use a lot of other Inspirations you can
study brains of moths and other animals
that they've used to figure out how to
improve these neural networks the most
common one consists of three layers of
network and this is generally how you
view these networks is you have an input
you have a hidden layer and an output
and the neural network is broken up into
many pieces but we focus just on the
neural network it's always on the hidden
layers that we're making all the
adjustments and figuring out how to best
set up those hidden layers for their
functions to both train faster and to
function better when we look at this of
course we have our input hidden in
output each layer contains neurons
called as nodes that perform various
operations and you can see here we have
the list of the nodes we have their
input nodes and our output nodes and
then our hidden layer nodes and it's
used in deep learning algorithm like CNN
RNN Gan Etc we'll address some of these
models a little closer at least the most
common models as we go down the list and
we study the Deep learning and the
neural network framework let's start
with what is a multi-layer precipitron
or MLP a lot of time is it referred to
and you'll see these abbreviations I'll
be honest I have to write them down on a
piece of paper and go through them
because I never remember what they all
mean even though I play with them all
the time what is a multi-layer
precipitron well if you look at the
image on the right it's very similar to
what we just looked at you have your
input layer your hidden layer and your
output layer and that's exactly what
this is it has the same structure of a
single layer precipitron with one or
more hidden layers except the input
layer each node in the other layers uses
a non-linear activation function what
that means is your input layer is your
data coming in and then your activation
function is based upon all those nodes
and weights being added together and
then it has the output MLP uses
supervised learning method called back
propagation for training the model very
key word there is back propagation
single layer precipitron can classify
only linear reciprobole classes with
binary output 0 1 but the MLP can
classify by non-linear classes so let's
break this down just a little bit the
multi-layer precipitron with an input
layer and a hidden layer and an output
layer as you see that it comes in there
it has adds up all the numbers and
weights depending on how your setup is
it then goes to the next layer that then
goes to the next hidden layer if you
have multiple hidden layers and finally
to the output layer the back propagation
takes the error that it sees so whatever
the output is it says hey this has an
error to it it's wrong and it sends that
error backwards from where it came from
and there's a lot of different functions
used to train this based on that error
and how that error goes backwards in the
notes so forward is you get your answers
back where it is for training you see
this every day even my Google pixel
phone has this it they train the neural
network which takes a lot more data to
train than it does to use and then they
load up that neural network into in this
case I have a pixel 2 which actually has
a built-in neural network for processing
pictures and so it's just a forward
propagation I I use when it processes my
photos but when they were trading it you
use a back propagation to train it with
the errors they had we'll be coming back
to different models that are used for
right now though multi-layer precipitron
MLP put that down is your vocabulary
word and of course back propagation what
is data normalization and why do we need
it this is so important we spend so much
time in normalizing our data and getting
our data clean and setting it up so we
talk about data there's a pre-processing
step to standardize the data so whatever
we have coming in we don't want it to be
a you know one gigabyte file here a two
gigabyte picture here and a three
kilobyte text there even as a human I
can't process those all in the same
group I have to reformat them in some
way that Loops them together to the
standardized format we use this data
normalization in pre-processing to
reduce it and eliminate data redundancy
a lot of times the data comes in and you
end up with two of the same images or uh
the same information in different
formats then we want to rescale values
to fit into a particular range for
achieving better convergence what this
means is with most neural networks they
form a bias we've seen this in recently
in attacks on neural networks where they
light up one pixel or one piece of the
view and it skews the whole answer so
suddenly because one pixel is really
bright it doesn't know what to do well
when we start rescaling it we put all
the values between say minus one and one
and we change them and refit them to
those values it helps get rid of that
bias helps fix for some of those
problems and then finally we restructure
the data and improve the Integrity we
want to make sure that we're not missing
values or we don't have partial data
coming in one way to look at this is bad
data in bad data out so we want clean
data in and you want good answers coming
out one of the most basic models used is
a boltzmann machine so let's address
what is a boltzmann machine and if you
know we just did the ml P multi-layer
precipitron so now we're going to come
into almost a simplified version of that
and in this we have our visible input
layer and we have our hidden layer the
boltzmann machines are almost always
shallow they're usually just two layer
neural Nets that make stochastic
decisions whether a neuron should be on
or off true false yes no first layer is
a visible layer and second layer is the
hidden layer nodes are connected to each
other across layers but no two nodes are
the same layer connected hence it is
also known as restricted boltzmann
machine now that we've covered a basic
MLP or multi-layer precipitron and we've
gone over the boltzmann machine also
known as the restricted boltzmann
machine let's talk a little bit about
activation formulas and this is a huge
topic that can get really complicated
but it also is automated so it's very
simple so you have both a complicated
and a simple at the same time so what is
the role of activation functions in a
neural network activation function
decides whether a neuron should be fired
or
that's the most basic one and that
actually changes a little bit because
it's either weather fired or not in this
case activation function or what value
should come out when it's fired but in
these models we're looking at just the
boltzmann restricted layers so this is
what causes them to fire either they
don't or they do it's a yes or no true
false All or Nothing it accepts a
weighted sum of the inputs the bias as
input to any activation function so
whatever our activation function is it
needs to have the sum of the weights
times the input so each input if you
remember on that model and let's just go
back to that model real quick and then
you always have to add a bias and you
can look at the bias if you remember
from your euclidean geometry you draw a
straight line formula for that line has
a y coordinate at the end it's always c
x plus M or something like that where m
is where it crosses the Y coordinates if
you're doing a straight line with these
weights it's very similar but a lot of
times we just add it in as its own
weight we take it as a node of a 1 value
coming in and then we compute its new
weight and that's how we compute that
bias just like we compute all the other
weights coming in the node which gets
fires depends on the Y value and then we
have a step function and the step
function this is where I remember I said
is going to get complicated and simple
all at the same time we have a lot of
different step functions we have the
sigmoid function we have just a standard
step function we have the relu it's
pronounced like Ray the ray from the Sun
and Lou like a name so relu function and
we have the tangent H function and if
you look at these they all have
something similar they all either force
it to be one value or the other they
force it to be in the case of the first
three is zero or one and in the last one
it's either a minus one or a one and you
can easily convert that to a zero one
yes no true false and on this one of the
most common ones is the step function
itself because there is no middle value
there is no discrepancy that says well
I'm not quite sure but as you get into
different models probably the most
commonly used used to be the sigmoid was
most commonly used but I see the relu
used more often really depending on what
you're doing you just have to play with
these and find out which one works best
depending on the data and your output
the reason to have a non-zero one answer
or something kind of in the middle is
when you're looking at this and it's
coming out you can actually process that
middle ground as part of the answer into
another neural network so it might be
that the relu function says hey this is
only a 0.6 not a one and even though the
one is what's going into the next neural
network or the next hidden layer as an
input the 0.6 value might also be going
in there to let you know hey this is not
a straight up one or straight up zero
someplace in the middle this is a little
uncertain what's coming out here so it's
a very powerful tool open the basic
neural network you usually just use the
step function it's yes or no let's take
a a big step back and take a kind of an
overview the next function is what is a
cost function that we're going to cover
this is so important because this is
your end result that you're going to do
over and over again and use to decide
whether the model is working or not
whether you need to try a different step
function whether you need to try a
different activation whether you need to
try a fully different model used so what
is the cost function cost function is a
measure to evaluate how good your
model's performance is it is also
referred as loss or error used to
compute the error of the output layer
during back propagation there's our back
propagation where we're training our
model that's one of our keywords mean
squared error is an example of a popular
cost function and so here we have the
cost function C equals half of Y minus y
predicted and then you square that so
the first thing is you know real quick
if you haven't done statistics this is
not a percentage it's not a percentage
of how accurate it is it's just a
measurement of the error and we take
that error if we're training it and we
push that error backwards through the
neural network and we use that at
through the different training functions
depending on what model you're using to
train the neural network so when you
deploy the network you're usually done
training it because it takes a lot of
computational force to train it this is
a very simple model and so you deploy
the trained one but we want to know how
your error is and so how do we do that
well you split your data part of your
data is for training and part of your
data is for testing and then we can also
test the error on there so it's very
important and then we're going to go one
more step on this we've got to look at
both the local and the global setup it
might work great to test your data on
what you have on your computer but
that's different than in the field so
when we're talking about all these
different tests in the error test as far
as your loss you don't you want to make
sure that you're in a closed environment
when you do initial testing but you also
want to open that up and make sure you
follow up with the testing on the larger
scale of data because it will change it
might not fit the larger scale there
might be something in there in the way
you brought the data in specifically or
the data group you used or any of those
could cause an error so very important
to remember that we're looking at both
the local and the global context of our
error and just one other side note on a
lot of the newer models of neural
networks by comparing the error we get
on the data our training data with a
portion of the test data we can actually
figure out how good the model is whether
it's overfitted or not we'll go into
that a little bit more as we go into
some of the different models so we have
our output we're able to figure out the
error on it based on the Square means
usually although there's other functions
used so we want to talk about what is
gradient descent another vocabulary word
gradient descent is an optimization
algorithm to minimize the cost function
or to minimize the error aim is to find
the local or Global Minima of a function
determine the direction the model should
take to reduce the error so as we're
looking at this we have our squared
error that we just figured out the cop
based on the cost function it says how
bad is my model fitting the data I just
put through it and then we want to
reduce that error here so how do you
figure out what direction to do that in
well it could be that you're looking at
just that line of that line of data
coming in so that would be a local
Minima we want to know the error of that
particular setup coming in and then you
have your Global your Global Minima we
want to minimize it based on the overall
data we're putting through it and with
this we can figure out the global
minimum cost we want to take all those
local minimum costs of each piece of
data coming in and figure out the global
one how are we going to adjust this
model to fit all the data we don't want
it to be biased just on three or four
lines of data coming in we want it to
kind of extrapolate a general answer for
all the data coming in but this of
course we mentioned it briefly about
back propagation this is where it really
comes in handy is training our model
neural network technique to minimize the
cost function helps to improve the
performance of the network back
propagates the error and updates the
weights to reduce the error so as you
can see here is a very nice depiction of
a back propagation we have our are
predicted y coming out and then we have
since it's a training set we already
know the answer and the answer comes
back and based on case the square means
was one of the functions we looked at
one of the activation functions based on
cost function that cost function then
depending on what you choose for your
back propagation method and there's a
number of them we'll change the weights
it will change the weight going to each
of the one of those nodes in the hidden
layer and then based upon the error
that's still being carried back it'll
change the weights going to the next
hidden layer and then it computes an
error level on that and sends that back
up and you're going to say well if it
computes the error into the first hidden
layer and fixes it why would it stop
there well remember we don't want to
create a biased neural network so we
only make small adjustments on these
weights we don't make a big adjustment
that changes everything right off the
bat so no matter how far back you go
you're always going to have a small
amount of error and that's still going
to continue to go all the way back up
the hidden layers for right now focus on
the back propagation is taking that
error and moving it backwards on the
neural network to change the weights and
help program it so that it'll have the
correct answers so far we've been
talking about forward propagation neural
networks everything goes forwards goes
left to right uh but let's let's take a
little detour and let's see what is the
difference between a feed forward neural
network and a recurrent neural network
now this is in the function not when
we're training it using the back
propagation so you've got new
information coming in and you want to
get the answer and there's a couple
different networks out there and we want
to know we have a feed forward neural
network and we have a new vocabulary
term recurrent neural network a feed
forward neural network signals travel in
one direction from input to Output no
feedback loops considers only the
current input cannot memorize previous
inputs one example of one of these feed
forward neural networks and we've
covered a number of them but one of the
ones that has a big highlight nowadays
is the CNN a convolutional neural
network tensorflow the one put out by
Google is probably most known for their
CNN where the information goes forward
it first takes a picture splits it apart
goes through the individual pixels on
the picture so it picks up a different
reading then calculates based on that
goes into a regular feed forward neural
network and then gives your
categorization on there now we're not
covering the CNN today but we do have a
video out that you can look up on
YouTube put out by simply learn the
convolutional neural network wonderful
tutorial check that out and learn a lot
more about the convolutional neural
network but you do need to know that the
CNN is a forward propagation neural
network only so it's only moving in One
Direction so we want to look at a
recurrent neural network signals travel
in both directions making it a looped
Network considers the current input
along with the previous received inputs
for generating the output of a layer has
the ability to memorize past data due to
its internal memory and you can see they
have a nice image here we have our input
if for some reason they always do the
recurrent neural Network in Reverse from
bottom up in the images kind of a
standard although I'm not sure why your
X goes into your hidden layer and your
hidden layer the answer for part of the
answer from that it generates feeds back
into the hidden layer so now you have an
input of both X and part of the Hidden
layer and then that feeds into your
output now if we go back to the forward
let me just go back a slide and we're
looking at our forward propagation
Network one of the tricks you can do to
use just a forward propagation network
is if you're in a what they call a Time
sequence that's a good term to remember
or a Time series meaning that it's
sequential data each term comes after
the other you can trick this by creating
your input nodes as with the history so
if you know that you have values 1 5 and
7 going in and you know what the output
is from one what those outputs are you
can expand the input to include the
history input that's one of the ways to
trick a forward propagation Network into
looking at that but when you do with the
recurrent neural network you let the
hidden layer do that for you it sends
that data and reprocesses it back into
itself what are some of the applications
of recurrent neural network the RNN can
be used for sentiment analysis and text
mining getting up early in the morning
is good for health and it's a positive
sentiment one of the catches you really
want to look at this when you're looking
at the language is that I could switch
this around and totally negate the
meaning of what I'm doing so no longer
be positive so when you're looking at a
sentence knowing the order of the words
is as important as the meaning of the
words you can't just count how many good
words there are versus bad words to get
a positive sentiment you now have to
know what they're addressing and there's
lots of other different uses kids are
playing football or soccer as we call it
in the U.S RN can help you capture an
image So based on previous information
coming in it refeeds that back in and
you have a image Setter and then time
series problems like predicting the
prices of stocks in a month or quarter
or sell of products can be solved using
an RNN and this is a really good example
you have whatever your stocks were doing
earlier this month will have a huge
effect of what they're doing today if
you're investing so having an RNN Model
A recurrent neural network feeding into
itself was happening previously allows
it to take that model and program in
that whole series without having to put
in the whole a month at a time of data
you only put in one day at a time but if
you keep them in order it will look back
and say oh this because what happened
yesterday I need some information from
that and I'm going to use that to help
predict today's and so on and so on
we're going to go back to our activation
functions remember I told you Rayleigh
was one of the most common functions
used so let's talk a little bit more
about relu and also softmax softmax is
an activation function that generates
the output between 0 and 1. it divides
each output such that the total sum of
the outputs is equal to one it is often
used in the output layers softmax L of
the N equals e to L on the N over the
absolute value of e to the L so what
does this function mean I mean what is
actually going on here so we have our
output notes and our output nodes are
giving us let's say they gave us
1.2.9 and 0.4 as a human being I look at
that and I say well the greatest value
is 1.2 so whatever category that is if
you have three different categories
maybe you're not just doing if it's a
cat or it's a dog or oh let's say it's a
cow maybe we had cats and dogs earlier
why the cats and dogs are hanging out
with a cow I don't know but we have a
value and it might say 1.2 is a cat 0.9
is the dog and point four is a cow for
some reason it thinks that there's a
chance of it being any one of these
three items and that's how it comes out
of the output layer well as a human I
can look at 1.2 and say this is
definitely what it is it's definitely a
cat or whatever it is I mean
different kinds of cars might be a
bitter whether it's a car truck or a
motorcycle maybe that'd be a better
example well from a computer standpoint
that might be a little confusing because
they're just numbers waving at us and so
with the soft Max we want all those
numbers to always add up to one so when
I add three numbers together I want the
final output to be one on there and so
it goes through this formula changes
each of these numbers in this case it
changes them to 0.46 0.34 and 0.20 they
all add up to one and that's a lot
easier to register because it's very set
it's a set output it's never going to be
more than one it's never going to be
less than zero and so you can see here
that there's probably a pretty high
chance that it's the first one so you're
as a human being we have no problem
knowing that but this output can then
also go into say another input so it
might be an automated car that's picking
up images and it says that image in
front of us is probably a big truck we
should deal with it like it's a big
truck it's probably not a motorcycle or
whatever those categories are that's the
soft Max part of it but now we have the
relu well what where's the relu coming
from well the relu is what's generating
the 1.2 and the 0.9 and the 0.4 and so
if you remember our relu stands for
rectified linear unit and is the most
widely used activation function we
looked at a number of different
activation functions including tangent
age the step function remember I said
the step function is really used if
that's what your actual output is
because then you know it's a zero or one
but the ray Loop if you have that as
your output you now have a discrepancy
in there and if that's going into
another neural network or another
process having that discrepancy is
really important and it gives an output
of x if x is positive and 0 otherwise so
it says my x value is going to be
somewhere between 0 or 1 and then the
usually unless it's really uncertain the
output is usually a one or zero and then
you have that little piece of
uncertainty there that you can send
forward to another Network or you can
look at to know that there's uncertainty
involved and is often used in the hidden
layers this is what's coming out of the
Hidden layers into the output layer
usually or as we reference the
convolutional neural network the CNN so
you'd have to go to another video to
review the relu is the most common used
for convolutional part of that Network
it has a bunch of little pieces that are
very simplified looking at all the
different images or different sections
of the map and the Rayleigh works really
good for that like I said there's other
formulas used but that this is the most
common one and you'll see that in the
hidden layers going maybe between one
layer and the next layer so just a quick
recap we have our soft Max which means
that if you have numerous categories
only one of them is going to be picked
but you also want to have some value
attached to it how well it picked it and
you put that between 0 and 1 so it's
very standardized so we have our soft
Max we looked at that let's go back one
we looked at that here where it
transforms the numbers and then we have
our relu function which takes the
information and the summation and puts
it between a zero and a one where it's
either clearly a zero or pending on how
confident our model is it'll go between
the zero and one value what are hyper
parameters well this is a great
interview question hyper parameters when
you are doing neural networks this is
what you're playing with most the time
once you've gotten the data formatted
correctly a hyper parameter is a
parameter whose value is set before the
learning process begins determines how a
network is trained and the structure of
the network this includes things like
the number of hidden units how many
hidden layers are you going to have and
how many nodes in each layer learning
rate learning rate is usually multiplied
once you've figured out the error and
how much you want to change the weights
we talked about or I mentioned it
earlier just briefly you don't want to
just make a huge change otherwise you're
going to have a biased model so you only
take some little incremental changes and
that's what the learning rate is is a
small incremental changes epics how many
times are you going to go through all
the data in your training set so one
Epic is one trip through all the data
and there's a lot of other things
depending on which model you're working
with and which programming scripture
working with like the python sklearn
package will have a slightly different
than say Google's tensorflow package
which will be a little bit different
than the spark machine learning package
so these are just some examples of the
hyper parameters and so you see in here
we have a nice image of our data coming
in and we train our model then we do a
comparison to see how good our model is
and then we go back and we say hey this
this model is pretty good but it's
biased so then we send it back and we
change our hyper parameters to see if we
can get an unbiased model or we can have
a better prediction on it that matches
our data closer what will happen if
learning rate is set too low or too high
we have a nice couple graphs here we
have one over here is a learning rate
set two load you can see that it slowly
Works its way down the curve and on the
right you can see a learning rate set
too high it's just bouncing back and
forth when your learning rate is too low
so we studied two slides over this what
the learning rate was training of the
model will progress very slowly as we
are making very tiny updates to the
weights we'll take many any updates
before reaching the minimum point so I
just mentioned epic going through all
the data might have to go through all
the data a thousand times instead of 500
times for it to train learning rate too
high causes undesirable Divergent
Behavior to the loss function due to
drastic updates and weights at times it
may fail to converge or even diverge so
if you have your learning rate set too
high and it's training too quickly maybe
you'll get lucky and it trains after one
epic run but a lot of times it might
never be able to train because the
weights are changing too fast they flip
back and forth too easy and you see down
here we've introduced two new terms
converge and diverge a converge means
that our model has reached a point where
it's able to give a fairly good answer
for all the data we put in all those
weights have adjusted and has minimized
the error diverge means that the data is
so chaotic that it can never manage to
to train to that data the data is just
too chaotic for it to train so we have
two new words there converge and diverge
are important to know also what is drop
out and batch normalization Dropout is a
technique of dropping out hidden
invisible units of a network randomly to
prevent overfitting of data it doubles
the number of iterations needed to
converge the network so here we have our
standard neural network and then after
applying Dropout now it doesn't mean we
actually delete the node the node is
still there and we're still going to use
that note what it means is that we're
only going to work with a few of the
nodes a lot of times I think the most
common one right now used is 20 percent
so you'll drop out 20 percent of the
nodes when you do your training you
reverse propagate your data and then
you'll randomly pick another 20 nodes
the next time you go through an epic
data training so each time you go
through one Epic you will randomly pick
20 of those nodes not to not to mess
with and this allows for less
overfitting of the data so by randomly
doing this you create some I guess it
just kind of pulls some nodes off to the
sides it says we're going to handle the
data later on so we don't over fit batch
normalization is a technique to improve
the performance and stability of neural
network the idea is to normalize the
inputs in every layer so that they have
mean output and activation of zero and
standard deviation of one this question
covers a lot of different things which
is great it's a great interview question
because it pulls in that you have to
understand what the mean value is so a
mean output activation of zero that
means our average activation is zero so
when you normalize it remember usually
we're going between
-1 and 1 on a lot of these it's a very
standard setup so you have to be very
aware that this is your mean output
activation of zero and then we have our
standard deviation of one so we want to
keep our error down to just a one value
the benefits of this doing a batch
normalization is it provides
regularization it trains faster Higher
Learning rates and weights are easier to
initialize what is the difference
between batch gradient descent and
stochastic gradient descent batch
gradient descent batch gradient computes
the gradient using the the entire data
set it takes time to converge because
the volume of data is huge and weights
update slowly so you can look at the
batches a lot of times if you're using
big data batch the data in but you still
go through a full epic you still go
through all the data on there so batch
gradient descent means you're going to
use it to fit all the data and look for
convergence there stochastic gradient
descent stochastic gradient computes the
gradient using a single sample it
converges much faster than batch
gradient because it updates weight more
frequently explain overfitting and
underfitting and how to combat them
overfitting happens when a model learns
the details and noise and the training
data to the degree that it adversely
impacts the execution of the model on
the new information it is more likely to
occur with non-linear models that have
more flexibility when learning a Target
function an example of this would be if
you're looking at say cars and trucks
and motorcycles it might only recognize
trucks that have a certain box like
shape it might not be able to notice a
flatbed truck unless it's only a
specific kind of flatbed truck or only
Ford trucks because that's what it saw
on the training set this means that your
model performs great on your train data
and great on maybe a small test amount
of data but when you go to we use it in
the rural World it leaves out a lot and
start and is not very functional outside
of your small area your small laboratory
data coming in underfitting doing the
opposite when you underfit your data
underfitting alludes to a model that is
neither well trained on training data
nor can generalize to new information
usually happens when there is less and
improper data to train a model as a bird
performance and accuracy so if you're
using underfitted data and you generate
a model and you distribute that in a
commercial Zone you'll have a lot of
people unhappy with you because it's not
going to give them very good answers so
we've explained overfitting and
underfitting so now we want to ask how
to combat them combating overfitting and
underfitting resampling the data to
estimate the model accuracy k-fold cross
validation having a validation data set
to evaluate the model so we do the
resampling we're randomly going to be
picking out data we'll run it a few
times to see how that works depending on
our random data and how we sample the
data to generate our model and then we
want to go ahead and validate the data
set by having our training data and then
keeping some data on the side testing
data to validate it how are weights
initialized in a network initializing
all weights to zero all the weights are
set to zero this makes your model
similar to a linear model so if you have
linear data coming in doing a basic
setup like that might work all the
neurons in every layer perform the same
operation given the same output and
making the Deep net useless right there
is a key word it's going to be useless
if you initialize everything to zero at
that point be looking into some other
machine learning tools initializing all
weights randomly here the weights are
assigned randomly by initializing them
very close to zero it gives better
accuracy to the model since every neuron
performs different computations and here
we have the weights are set randomly we
have our input layer the hidden layers
and the output layer and W equals in P
random random in layer size L layer size
L minus one this is the most commonly
used is to randomly generate your
weights what are the different layers in
CNN convolutional neural network first
is the convolutional layer that performs
a convolutional operation we have our
other video out if you want to explore
that more so go into detail exactly how
the C the convolutional layer works in
the CNN as far as creating a number of
smaller picture windows that go over the
data the second step is as a Rayleigh
layer relu brings non-linearity to the
network and converts all the negative
pixels to zero output is rectified
feature map so it goes into a mapping
feature there pooling layer pooling is a
down sampling operation that reduces the
dimensionality of the feature map so we
have all our relu layer which is pulling
all these little Maps out of our
convolutional layer it's taking that
picture and a little creating a little
tiny neural networks to look at
different parts of the picture uh then
we need to pull it together and then
finally the fully connected layer so
we've flattened our pulling layer out
and we have a fully connected layer
recognizes and classifies the objects in
the image and that's actually your
forward propagation reverse propagation
training model usually I mean there's a
number of different models out there of
course what is pooling in CNN and how
does it work pooling used to reduce the
spatial dimensions of a CNN performs
down sampling operation to reduce the
dimensionality creates a pulled feature
map by sliding a filter Matrix over the
input Matrix I mentioned that briefly on
the previous slide it's important to
know that you have if you see here they
have a rectified feature map and so each
one of those colors like the yellow
color that might be one of the a smaller
little neural network using the relu
you'll look at it'll just kind of go
over the main picture and look at all
the different areas on the main picture
so you might step one two three four
spaces and then you have another one
that's also looking at features and it
has a 2785 five each one of those is a
map so it might be the first one might
be a map looking for cat ears and the
second one looking for human eyes when
it does this you then have this
rectified feature map looking at these
different features and the max pooling
with the two by two filters and a
Strider two stride means instead of
skipping every pixel you're going to go
every two pixels you take the maximum
values and you can see over here we look
at a pulled feature map one of the
features says hey I had a max value of
eight so somewhere in here we saw a
human eye labeled as eight pretty high
label and maybe seven was a human hand
and maybe four was cat whiskers or
something that we thought might be cat
whiskers four is kind of a low number in
this particular case compared to the
other ones so you have your full pool
feature map you can see the process here
is we have our stepping we look for the
max value and then we create a pulled
feature map of the maxed values how does
a lstm network work that's long short
term memory so the first thing to know
is that an lstms are a special kind of
recurrent neural network capable of
learning long-term dependencies
remembering information for long periods
of time is their default Behavior we did
look at the RNN briefly talked about how
the hidden layer feeds back into itself
with the lstm as a much more complicated
feedback and you can see here we have
the hidden layer of T minus one and the
hidden Larry that's what the H stands
for hidden layer of T and the formula is
going in as we can see here we have the
hidden layers we have T minus 1 and then
h of T where T stands for time so this
is a series remember working with series
and we want to remember the past and you
can see you have your X your input of T
and that might be a frame in a video as
the frame comes in they usually use in
this one the tangent H activation
formula but you also see that it goes
through a couple other formulas the
Omega formula and so when it combines
these that then goes into the next layer
your next hidden layer that then goes
into the data that's submitted to the
next input so you have your X of t t
plus one so when you have that coming in
then you have your H value that's coming
forward from the last process and
depending on how many of these Omega
structures you put in there depends on
how long term the memory gets so it's
important to remember this is more for
your long-term recurrent neural networks
the three steps in an lstm step one
decides what to forget and what to
remember step two selectively update
cell State values So based on what we
want to remember and forget we want to
update those cell values and then decide
what part of the current state make it
to the output so now we have to also
have an output on there what are
Vanishing and exploding gradients this
is a great question to fix all our
neural networks while training an RNN
your slope can become either too small
or too large and this makes the training
difficult when the slope is too small
the problem is known as Vanishing
gradient so our slope we have our change
in X and our change in y when the slope
decreases gradually to a very small
value sometimes negative and makes
training difficult when the slope tends
to grow exponentially instead of
decaying this problem is called
exploding gradient the slope grows
exponentially you see a nice graph of
that here issues in gradient problem
Long training time poor performance and
low accuracy what is the difference
between Epic bat and iteration and deep
learning epic an epic represents one
iteration over the entire data set so
that's everything you're going to go
ahead and put into that training model
batch we cannot pass the entire data set
into the neural network at once so we
divide the data set into a number of
batches and then iteration if we have 10
000 images as data and a batch size of
200 then the Epic should run 10 000
times over 200 so that means we have our
total number over the 200 equals 50
iteration so in each epic we're running
over all the data set we're going to
have 50 iterations and each of those
iterations includes a batch of 200
images in this case why tensorflow is
the most preferred library in deep
learning what first tensorflow provides
both C plus plus and python apis that
makes it easier to work on has a faster
compilation time than other deep
learning libraries like Karas and torch
tensorflow supports both CPUs and gpus
Computing devices so right now
tensorflow is at the top of the market
because it's so easy to use for both
programmer side and for Hardware side
and for the speed of getting something
up and running what do you mean by
tensor and tensorflow tensor is a
mathematical object represented as a
rays of higher Dimension these arrays of
data with different dimensions and ranks
that are fed as input to the neural
network are called tensors you can see
here we have a tensor of Dimensions five
comma four so it's a two-dimensional
tensor coming in you can look at an
image like this that each one of those
pixels is a different value if it's a
black and white so it might be zero and
ones and then each one represents a
black and white image in a color photo
you might either find a different value
system or you might have a tensor value
that has the X Y coordinates as we see
here Plus colors so you might have three
more different dimensions for the three
different images the red the blue and
the yellow coming in and even as you go
from one layer or one tensor to the next
these layers might change we might
flatten them might bring in numerous in
the case of the convergence neural
network we have all those smaller
different mappings of features that come
in so each one of those layers coming
through is a tensor if it has multiple
Dimensions coming in and weights
attached to it what are the programming
elements in tensorflow well we have our
constants constants are parameters whose
value does not change to define a
constant we use tf.constant command
example a equals tf.constant 2.0 TF
float32 so it's a tensorflow value of
32. b equals TF constant 3.0 print a b
if we did a print of a b we'd have
tf.constant and then of course B is that
instance of it variables variables allow
us to add new trainable parameters to
graph to Define a variable we use
tf.variable command and initial lies
them before running the graph in session
example W equals TF variable 0.3 D type
TF float32 or b equals the TF variable
minus 3 comma D type float 32
placeholders placeholders allow us to
feed data to a tensorflow model from
outside a model it permits a value to be
assigned later to define a placeholder
we use TF placeholder command example a
equals TF placeholder b equals a times
two with the TF session as sess result
equals session run B comma feed
dictionary equals a 3.0 print result so
we have a nice example of the replace
holder session a session is run to
evaluate the nodes this is called as the
tensorflow run time so for example you
have a equals TF constant 2.0 b equals
TF constant 4.0 C equals a plus b and at
this point you'd go ahead and create a
session equals TF session and then you
could evaluate the tensor C print
session run C that would input c as an
input into your session what do you
understand by a computational wrath
everything in tensorflow is based on
creating a computational graph it has a
network of nodes where each node
performs an operation nodes represent
mathematical operation and edges
represent tensors since data flows in a
form of a graph it is also called a data
flow graph and we have a nice visual of
this graph or graphic image of a
computational graph and you can see here
we have our input nodes our add multiply
nodes and our multiply node at the end
and then we have the edges where the
data flows so we have from a going to C
A going to D you can see we have a two
flowing four flowing explain generative
adversarial Network along with an
example suppose there's a wine shop that
purchases wine from dealers which they
will resell later dealer point of the
wine our shop owner that then sells it
for a profit but there are some
malfactor dealers who sell fake wine in
this case the shop owner should be able
to distinguish between fake and
authentic wine the forger will try to
different techniques to sell fake wine
and make sure are certain techniques go
past the shop owner's check so here's
our forager fake wine shop owner the
shop owner would probably get some
feedback from the wine experts that some
of the wine is not original the owner
would have to improve how he determines
whether a wine is fake or authentic goal
of forger to create wines that are
indistinguishable from the authentic
ones goal of shop owner to accurately
tell if the wine is real or not there
are two main components of generative
adversarial Network and we'll refer it
to as a noise Vector coming in we have
our forger who's going to generate fake
wine and then we have our real authentic
wine and of course our shop owner who
has to figure out whether it's real or
fake the generator is a CNN that keeps
producing images that are closer in
appearance to the real images while the
discriminator tries to determine the
difference between real and fake images
the ultimate aim is to make the
discriminator to learn to identify real
and fake images what is an auto encoder
the network is trained to reconstruct
its inputs it is a neural network that
has three layers here here the input
neurons are equal to the output neuron
the Network's Target outside is same as
the input it uses dimensionality
reduction to restructure the input input
image comes in we have our Latin space
representation and then it goes back out
reconstructing the image it works by
compressing the input to a Latin space
representation and then reconstructing
the output from this representation what
is bagging and boosting bagging and
boosting are Ensemble techniques where
the idea is to train multiple models
using the same learning algorithm and
then take a call so we have in here
where we're bagging we take a data set
and we split it we have our training
data and our test data very standard
thing to do then we're going to randomly
select data into the bags and train your
model separately so we might have bag
one model one bag two model two bag
three model three and so on in boosting
the emphasis is to select the data
points which give wrong output in order
to improve the accuracy so in boosting
we have our data set again we split it
to test data and train data and we'll
take tag one and we'll train the model
data points with wrong predictions then
go into Bag two and we then train that
model and repeat so with this we have
come to the end of this deep learning
boot camp if you have found this session
informative and interesting please
consider subscribing to our YouTube
channel until next time thank you stay
safe and keep learning
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here