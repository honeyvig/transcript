convolution neural network tutorial my
name is richard kirschner with the
simply learn team that's wwe
get certified get ahead today we're
going to be covering the convolutional
neural network tutorial
do you know how deep learning recognizes
the objects in an image and really this
particular neural network is how image
recognition works it's very central one
of the biggest building blocks for image
recognition it does it using convolution
neural network and we over here we have
the basic picture of a hummingbird
pixels of an image fed as input you have
your input layer coming in so it takes
that graphic and puts it into the input
layer you have all your hidden layers
and then you have your output layer and
your output layer one of those is going
to light up and say oh it's a bird we're
going to go into depth we're going to
actually go back and forth on this a
number of times today so if you're not
catching all the image
don't worry we're going to get into the
details so we have our input layer
accepts the pixels of the image as input
in the form of arrays and you can see up
here where they've actually labeled each
block of the bird in different arrays so
we'll dive into deep as to how that
looks like and how those matrixes are
set up here hidden layer carry out
feature extraction by performing certain
calculations and manipulation so this is
the part that kind of reorganizes that
picture multiple ways until we get some
data that's easy to read for the neural
network this layer uses a matrix filter
and performs convolution operation to
detect patterns in the image and if you
remember that convolution means to coil
or to twist so we're going to twist the
data around and alter it and use that
operation to detect a new pattern there
are multiple hidden layers like
convolution layer rel u is how that is
pronounced when that's the rectified
linear unit that has to do with the
activation function that's used
pooling layer also uses multiple filters
to detect edges corners eyes feathers
beak etc and just like the term says
pooling is pulling information together
and we'll look into that a lot closer
here so if you're if it's a little
confusing now we'll dig in deep and try
to get you uh squared away with that and
then finally there is a fully connected
layer that identifies the object in the
image so we have these different layers
coming through in the hidden layers and
they come into the final area and that's
where we have a one node or one neural
network entity that lights up that says
it's a bird
what's in it for you we're going to
cover an introduction to the cnn what is
convolution neural network how cnn
recognizes images we're going to dig
deeper into that and really look at the
individual layers in the convolutional
neural network and finally we do a use
case implementation using the cnn we'll
begin our introduction to the cnn by
introducing the pioneer of convolutional
neural network jan lecun he was the
director of facebook ai research group
built the first convolutional neural
network called lynette in 1988 so these
have been around for a while and have
had a chance to mature over the years it
was used for character recognition tasks
like reading zip code digits imagine
processing mail and automating that
process
cnn is a feed forward neural network
that is generally used to analyze visual
images by producing data with a
grid-like topology a cnn is also known
as a convent and very key to this is we
are looking at images that was what this
was designed for and you'll see the
different layers as we dig in near some
of the other some of them are actually
now used since we're using uh tensorflow
and keras in our code later on you'll
see that some of those layers appear in
a lot of your other neural network
frameworks but in this case this is very
central to processing images and doing
so in a variety that captures multiple
images and really drills down into their
different features in this example here
you see flowers are two varieties orchid
and a rose i think the orchid is much
more dainty and beautiful and the rose
smells quite beautiful of a couple rose
bushes in my yard they go into the input
layer that data is in sent to all the
different nodes in the next layer one of
the hidden layers based on its different
weights and its setup it then comes out
and gives those a new value those values
then are multiplied by their weights and
go to the next hidden layer and so on
and then you have the output layer and
one of those nodes comes out and says
it's an orchid and the other one comes
out and says it's a rose
it was well it was trained what
separates the cnn or the convolutional
neural network from other neural
networks is a convolutional operation
forms the basis of any convolutional
neural network in a cnn every image is
represented in the form of arrays of
pixel values so here we have a real
image of the digit 8
that then gets put on to
its pixel values represented in the form
of an array in this case you have a two
dimensional array and then you can see
in the final in form we transform the
digit eight into its representational
form of pixels of zeros and ones where
the ones represent in this case the
black part of the eight and the zeros
represent the white background to
understand the convolution neural
network or how that convolutional
operation works we're going to take a
side step and look at matrixes in this
case we're going to simplify it and
we're going to take two matrices a and b
of one dimension now kind of separate
this from your thinking as we learned
that you want to focus just on the
matrix aspect of this and then we'll
bring that back together and see what
that looks like when we put the pieces
for the convolutional operation here
we've set up two arrays we have in this
case our single dimension matrix and we
have a equals 537597
and we have b equals one two three so in
the convolution as it comes in there's
gonna look at these two and we're gonna
start by doing multiplying them a times
b and so we multiply the arrays element
wise and we get five six six
where five is the five times one six is
three times two and then the other six
is two times three and since the two
arrays aren't the same size they're not
the same setup we're going to just
truncate the first one and we're going
to look at the second array multiplied
just by the first three elements of the
first array now that's going to be a
little confusing remember a computer
gets to repeat these processes hundreds
of times so we're not going to just
forget those other numbers later on
we'll see we'll bring those back in and
then we have the sum of the product in
this case 5 plus 6 plus 6 equals 17. so
in our a times b our very first digit in
that matrix of a times b is 17. and if
you remember i said we're not going to
forget the other digits so we now have
three two five we move one set over and
we take three two five and we multiply
that times b and you'll see that 3 times
1 is 3 2 times 2 is 4 and so on and so
on we sum it up so now we have the
second digit of our a times b product in
the matrix and we continue on with that
same thing so on and so on so then we
would go from 375 to 759 to 597. this
short matrix that we have for a we've
now covered all the different entities
in a that match three different levels
of b now in a little bit we're going to
cover where we use this math at this
multiplying of matrixes and how that
works
but it's important to understand that
we're going through the matrix of
multiplying the different parts to it to
match the smaller matrix with the larger
matrix i know a lot of people get lost
at is you know what's going on here with
these matrixes uh oh scary math not
really that scary when you break it down
we're looking at a section of a and
we're comparing it to b so when you
break that down your mind like that you
realize okay so i'm just taking these
two matrixes and comparing them and i'm
bringing the value down into one matrix
a times b we're reducing that
information in a way that will help the
computer see different aspects let's go
ahead and flip over again back to our
images
here we are back to our images talking
about going to the most basic
two-dimensional image you can get to
consider the following two images the
image for the symbol backslash when you
press the backslash the above image is
processed and you can see there for the
image for the forward slash is the
opposite so we click the forward slash
button that flips uh very basic we have
four pixels going in can't get any more
basic than that here we have a little
bit more complicated picture we take a
real image of a smiley face
then we represent that in the form of
black and white pixels so if this was an
image in the computer it's black and
white and like we saw before we convert
this into the zeros and one so where the
other one would have just been a matrix
of just four dots now we have a
significantly larger image coming in so
don't worry we're going to bring this
all together here in just a little bit
layers in convolutional neural network
we're looking at this we have our
convolution layer and that really is the
central aspect of processing images in
the convolutional neural network that's
why we have it and then that's going to
be feeding in and you have your relu
layer which is you know as we talked
about the rectified linear unit we'll
talk about that a little bit later the
relu isn't how it act is how that layer
is activated is the math behind it what
makes the neurons fire you'll see that a
lot of other neural networks when you're
using it just by itself is for
processing smaller amounts of data where
you use the atom activation feature for
large data coming in now because we're
processing small amounts of data in each
image the relu layer works great you
have your pooling layer that's where
you're pulling the data together pooling
is a neural network term it's very
commonly used i like to use the term
reduce so if you're coming from the map
and reduce side you'll see that we're
mapping all this data through all these
networks and then we're going to reduce
it we're going to pull it together and
then finally we have the fully connected
layer that's where our output is going
to come out so we have started to look
at matrixes we've started to look at the
convolutional layer where it fits in and
everything we've taken a look at images
so we're going to focus more on the
convolution layer since this is a
convolutional neural network a
convolution layer has a number of
filters and perform convolution
operation every image is considered as a
matrix of pixel values consider the
following five by five image whose pixel
values are only zero and one now
obviously when we're dealing with color
there's all kinds of things that come in
on color processing but we want to keep
it simple and just keep it black and
white and so we have our image pixels
so we're sliding the filter matrix over
the image and computing the dot product
to detect the patterns and right here
you're going to ask where does this
filter come from this is a bit confusing
because the filter is going to be
derived later on we build the filters
when we program or train our model so
you don't need to worry what the filter
actually is but you do need to
understand how a convolution layer works
is what is the filter doing filter and
you'll have mini filters you don't have
just one filter you'll have lots of
filters that are going to look for
different aspects and so the filter
might be looking for just edges it might
be looking for different parts we'll
cover that a little bit more detail in a
minute right now we're just focusing on
how the filter works as a matrix
remember earlier we talked about
multiplying matrixes together and here
we have our two dimensional matrix and
you can see we take the filter and we
multiply it in the upper left image and
you can see right here 1 times 1 1 times
0 1 times 1 we multiply those all
together then sum them and we end up
with the convolved feature of 4. we're
going to take that and sliding the
filter matrix over the image and
computing the dot product to detect
patterns so we're just going to slide
this over we're going to predict the
first one and slide it over one notch
predict the second one and so on and so
on all the way through until we have a
new matrix and this matrix which is the
same size as the filter has reduced the
image and whatever filter whatever
that's filtering out it's going to be
looking at just those features reduced
down to a smaller matrix so once the
feature maps are extracted the next step
is to move them to the relu layer so the
relu layer the next step first is going
to perform an element-wise operation so
each of those maps coming in if there's
negative pixels so it says all the
negative pixels to zero and you can see
this nice graph where it just zeros out
the negatives and then you have a value
that goes from zero up to whatever value
is coming out of the matrix this
introduces non-linearity to the network
so up until now we have a we say
linearity we're talking about the fact
that the feature has a value so it's a
linear feature this feature
came up and has let's say the feature is
the edge of the beak you know it's like
or the backslash that we saw um you'll
look at that and say okay this feature
has a value from negative 10 to 10 in
this case if it was one it'd say yeah
this might be a beak it might not might
be an edge right there a minus five
means no we're not even going to look at
it to zero and so we end up with an
output and the output takes all these
features all these filtered features
remember we're not just running one
filter on this we're running a number of
filters on this image and so we end up
with a rectified feature map that is
looking at just the features coming
through and how they weigh in from our
filters so here we have an input of a
looks like a toucan bird
very exotic looking real image is
scanned in multiple convolution and the
relu layers for locating features and
you can see up here is turn it into a
black and white image and in this case
we're looking in the upper right hand
corner for a feature and that box scans
over a lot of times it doesn't scan one
pixel at a time a lot of times it will
skip by two or three or four pixels to
speed up the process that's one of the
ways you can compensate if you don't
have enough resources on your
computation for large images and it's
not just one filter slowly goes across
the image you have multiple filters have
been programmed in there so you're
looking at a lot of different filters
going over the different aspects of the
image and just sliding across there and
forming a new matrix
one more aspect to note about the relu
layer is we're not just having one value
coming in
so not only do we have multiple features
going through but we're generating
multiple relu layers for locating the
features that's very important to note
you know so we have a quite a bundle we
have multiple filters multiple rail u uh
which brings us to the next step forward
propagation now we're going to look at
the pooling layer the rectified feature
map now goes through a pooling layer
pooling is a down sampling operation
that reduces the dimensionality of the
feature map that's all we're trying to
do we're trying to take a huge amount of
information and reduce it down to a
single answer this is a specific kind of
bird this is an iris this is a rose so
you have a rectified feature map and you
see here we have a rectified feature map
coming in
we set the max pooling with a two by two
filters and a stride of two and if you
remember correctly i talked about not
going one pixel at a time uh well that's
where the stride comes in we end up with
a two by two pooled feature map but
instead of moving one over each time and
looking at every possible combination we
skip a step we skip a few there we go by
two we skip every other pixel and we
just do every other one and this reduces
our rectified feature map which as you
can see over here 16 by 16 to a four by
four so we're continually trying to
filter and reduce our data so that we
can get to something we can manage and
over here you see that we have the max
three four one and two and in the max
pooling we're looking for the max value
a little bit different than what we were
looking at before so coming from the
rectified feature we're now finding the
max value and then we're pulling those
features together so instead of think of
this as image of the map think of this
as how valuable is a feature in that
area how much of a feature value do we
have we just want to find the best or
the maximum feature for that area they
might have that one piece of the filter
of the beaks at all i see a one in this
beak in this image and then it skips
over and says i see a three in this
image and says oh this one is rated as a
four we don't want to sum it together
because then you know you might have
like five ones and it'll say ah five but
you might have uh four zeros and one ten
and that 10 says well this is definitely
a beak where the ones will say probably
not a beak a little strange analogy
since we're looking at a bird but you
can see how that pulled feature map
comes down and we're just looking for
the max value in each one of those
matrixes pooling layer uses different
filters to identify different parts of
the image like edges corners body
feathers eyes beak etc i know i focus
mainly on the beak but obviously each
feature could be each a different part
of the bird coming in so let's take a
look at what that looks like structure
of a convolution neural network so far
this is where we're at right now we have
our input image coming in and then we
use our filters and there's multiple
filters on there that are being
developed to kind of twist and change
that data and so we multiply the
matrixes we take that little filter
maybe it's a two by two we multiply it
by each piece of the image and if we
step two then it's every other piece of
the image that generates multiple
convolution layers so we have a number
of convolution layers we have
set up in there just looking at that
data we then take those convolution
layers we run them through the relu
setup and then once we've done through
the relu setup and we have multiple
values going on multiple layers that are
relative then we're going to take those
multiple layers and we're going to be
pooling them so now we have the pooling
layers or multiple poolings going on up
until this point we're dealing with
sometimes there's multiple dimensions
you can have three dimensions some
strange data setups that aren't doing
images but looking at other things they
can have four five six seven dimensions
uh so right now we're looking at 2d
image dimensions coming in into the
pooling layer so the next step is we
want to reduce those dimensions or
flatten them so flattening flattening is
a process of converting all of the
resultant two-dimensional arrays from
pooled feature map into a single long
continuous linear vector so over here
you see where we have a pooled feature
map maybe that's the bird wing and it
has values 6847 and we want to just
flatten this out and turn it into 6847
or a single linear vector and we find
out that not only do we do each of the
pooled feature maps we do all of them
into one long linear vector so now we've
gone through our convolutional neural
network part and we have the input layer
into the next setup all we've done is
taken all those different pooling layers
and we've flattened them out and
combined them into a single linear
vector going in so after we've done the
flattening we have just a quick recap
because we've covered so much so it's
important to go back and take a look at
each of the steps we've gone through the
structure of the network so far is we
have our convolution where we twist it
and we filter it and multiply the
matrixes we end up with our
convolutional layer which uses the relu
to figure out the values going out into
the pooling as you have numerous
convolution layers that then create
numerous pooling layers pulling that
data together which is the max value
which one we want to send forward we
want to send the best value and then
we're going to take all of that from
each of the pooling layers and we're
going to flatten it and we're going to
combine them into a single input going
into the final layer once you get to
that step you might be looking at that
going boy that looks like the normal
into it to most neural network and
you're correct it is
so once we have the flattened matrix
from the pooling layer that becomes our
input so the pooling layer is fed as an
input to the fully connected layer to
classify the image and so you can see as
our flattened matrix comes in in this
case we have the pixels from the
flattened matrix fed as an input back to
our toucan or whatever that kind of bird
that is i need one of these to identify
what kind of bird that is it comes into
our ford propagation network uh and that
will then have the different weights
coming down across and then finally it
selects that that's a bird and that is
not a dog or a cat in this case
even though it's not labeled the final
layer there in red is our output layer
our final output layer that says bird
cat or dog
so quick recap of everything we've
covered so far we have our input image
which is twisted and multiplied the
filters are multiplied times the matrix
and the two matrixes multiplied all the
filters to create our convolution layer
our convolution layers there's multiple
layers in there because it's all
building multiple layers off the
different filters then goes through the
relu as this activation and that creates
our pooling and so once we get into the
pooling layer we then and the pooling
look for who's the best what's the max
value coming in from our convolution and
we take that layer and we flatten it and
then it goes into a fully connected
layer our fully connected neural network
and then to the output and here we can
see the entire process how the cnn
recognizes a bird this is kind of nice
because it's showing the little pixels
and where they're going you can see the
filter is generating this convolution
network and that filter shows up in the
bottom part of the convolution network
and then based on that it uses the relu
for the pooling the pooling then find
out which one's the best and so on all
the way to the fully connected layer at
the end or the classification in the
output layer so that'd be a
classification neural network at the end
so we covered a lot of theory up till
now and you can imagine each one of
these steps has to be broken down in
code so putting that together can be a
little complicated not that each step of
the process is overly complicated but
because we have so many steps uh we have
one two three four five different steps
going on here with sub steps in there
we're going to break that down and walk
through that in code so in our use case
implementation using the cnn we'll be
using the cfar10 dataset from canadian
institute for advanced research for
classifying images across 10 categories
unfortunately they don't let me know
whether it's going to be a toucan or
some other kind of bird but we do get to
find out whether it can categorize
between a ship a frog deer bird airplane
automobile cat dog horse truck so that's
a lot of fun and if you're looking
anything in the news at all of our
automated cars and everything else you
can see where this kind of processing is
so important in today's world and very
cutting edge as far as what's coming out
in the commercial deployment i mean this
is really cool stuff we're starting to
see this just about everywhere in
industry so great time to be playing
with this and figuring it all out let's
go ahead and dive into the code and see
what that looks like when we're actually
writing our script
before we go on let's do uh one more
quick look at what we have here let's
just take a look at data batch one keys
and remember in jupiter notebook i can
get by with not doing the print
statement if i put a variable down there
it'll just display the variable and you
can see under data batch one for the
keys since this is a dictionary we have
the batch one label data and file names
so you can actually see how it's broken
up in our data set so for the next step
or step four as we're calling it uh we
want to display the images using matte
plot library there's many ways to
display the images you can even well
there's other ways to drill into it but
matplot library is really good for this
and we'll also look at our first reshape
setup or shaping the data so you can
have a little glimpse into what that
means uh so we're going to start by
importing our map plot and of course
since i am doing jupyter notebook i need
to do the matplot inline command so it
shows up on my page so here we go we're
going to import matplot library.pipelot
as plt and if you remember matplot
library the pie plot is like a canvas
that we paint stuff onto and there's my
percentage sign matplot library inline
so it's going to show up in my notebook
and then of course we're going to import
numpy as np for our numbers python array
setup and let's go ahead and set
x equals to data batch 1. so this will
pull in all the data going into the x
value and then because this is just a
long stream of binary data we need to go
a little bit of reshaping so in here we
have to go ahead and reshape the data we
have 10 000 images okay that looks
correct and this is kind of an
interesting thing it took me a little
bit to i had to go research this myself
to figure out what's going on with this
data and what it is is it's a 32 by 32
picture and let me do this let me go
ahead and do a drawing pad on here so we
have 32 bits by 32 bits and it's in
color so there's three bits of color now
i don't know why the data is
particularly like this it probably has
to do with how they originally encoded
it but most pictures put the three
afterward so what we're doing here is
we're going to take uh the shape we're
going to take the data which is just a
long stream of information and we're
going to break it up into 10 000 pieces
and those 10 000 pieces then are broken
into three pieces each and those three
pieces then are 32 by 32. you can look
at this like an old-fashioned projector
where they have the red screen or the
red projector the blue projector and the
green projector and they add them all
together and each one of those is a 32
by 32 bit so that's probably how this
was originally formatted was in that
kind of ideal things have changed so
we're going to transpose it and we're
going to take the three which was here
and we're going to put it at the end so
the first part is reshaping the data
from a single line of bit data or
whatever format it is into 10 000 by 3
by 32 by 32 and then we're going to
transpose the color factor to the last
place so it's the image then the 32 by
32 in the middle that's this part right
here and then finally we're going to
take this which is three bits of data
and put it at the end so it's more like
we do we process images now and then as
type this is really important that we're
going to use an integer eight you can
come in here and you'll see a lot of
these they'll try to do this with a
float or a float 64. what you got to
remember though is a float uses a lot of
memory so once you switch this into
something that's not integer eight which
goes up to 128 you are just gonna the
the amount of ram let me just put that
in here is going to go way up the amount
of ram that it loads
so you want to go ahead and use this you
can try the other ones and see what
happens if you have a lot of ram on your
computer but for this exercise this will
work just fine and let's go ahead and
take that and run this so now our x
variable is all loaded and it has all
the images in it from the batch one data
batch one and just to show we were
talking about with the as type on there
if we go ahead and take x0 and just look
for its max value let me go ahead and
run that uh you'll see it doesn't oops i
said 128 it's 255. uh you'll see it
doesn't go over 255 because it's
basically an ascii character is what
we're keeping that down to we're keeping
those values down so they're only 255 0
to 255 versus float value which would
bring this up
exponentially in size and since we're
using the matplot library we can do
oops that's not what i wanted since
we're using the matplot library we can
take our canvas and just do a plt dot i
am for image show and uh let's just take
a look at what x0 looks like and it
comes in i'm not sure what that is but
you can see it's a very low grade image
broken down to the minimal pixels on
there and if we did the same thing oh
let's do uh let's see what one looks
like hopefully it's a little easier to
see run on there not enter let's hit the
run on that
and we can see this is probably a semi
truck that's a good guess on there and i
can just go back up here instead of
typing the same line in over and over
and we'll look at three uh that looks
like a dump truck unloading uh and so on
you can do any of the 10 000 images and
just jump to 55
looks like some kind of animal looking
at us there probably a dog and just for
fun let's do just one more uh run on
there and we can see a nice car for our
image number four uh so you can see we
paste through all the different images
it's very easy to look at them and
they've been reshaped to fit our view
and what the
matplot library uses for its format so
the next step is we're going to start
creating some helper functions we'll
start by a one hot encoder to help us or
processing the data remember that your
labels they can't just be words they
have to switch it and we use the one hot
encoder to do that and then we'll also
create a class uh cfar helper so it's
gonna have an init and a setup for the
images and then finally we'll go ahead
and run that code so you can see what
that looks like and then we get into the
fun part where we're actually going to
start creating our model our actual
neural network model so let's start by
creating our one hot encoder we're going
to create our own here uh and it's going
to return an out and we'll have our
vector coming in and our values equal
10. what this means is that we have the
10 values the 10 possible labels and
remember we don't look at the labels as
a number because a car isn't one more
than a horse that'd be just kind of
bizarre to have horse equals zero car
equals one plane equals two cat equals
three so a cat plus a car equals what uh
so instead we create a numpy array of
zeros and there's gonna be ten values so
we have a ten different values in there
so you have zero or one one means it's a
cat zero means it's not a cat
in the next line it might be that uh one
means it's a car zero means it's not a
car so instead of having one output with
a value of zero to ten you have 10
outputs with the values of 0 to 1.
that's what the one hot encoder is doing
here and we're going to utilize this in
code in just a minute so let's go ahead
and take a look at the next helpers we
have a few of these helper functions
we're going to build and when you're
working with a very complicated python
project dividing it up into separate
definitions and classes is very
important otherwise it just becomes
really ungainly to work with so let's go
ahead and put in our next helper which
is a class and this is a lot in this
class so we'll break it down here and
let's just start uh some put a space
right in there there we go that was a
little bit more readable at a second
space so we're going to create our class
the cipher helper and we'll start by
initializing it now there's a lot going
on in here so let's start with the init
part uh self dot i equals zero i'll come
in a little bit we'll come back to that
in the lower part we want to initialize
our training batches so when we went
through this there was like a meta batch
we don't need the meta batch but we do
need the data batch one two three four
five and we do not want the testing
batch in here this is just the self all
train batches so we're going to come
make an array of all those different
images and then of course we left the
test batch out so we have our
self.testbatch
we're going to initialize the training
images and the training labels and also
the test images and the test labels so
these are just this is just to
initialize these variables in here then
we create another definition down here
and this is going to set up the images
let's just take a look and see what's
going on in there now we could have all
just put this as part of the
init part
since this is all just helper stuff but
breaking it up again makes it easier to
read it also makes it easier when we
start executing the different pieces to
see what's going on so that way we have
a nice print statement to say hey we're
now running this and this is what's
going on in here we're going to set up
these self training images at this point
and that's going to go to a numpy array
v stack and in there we're going to load
up
in this case the data for d itself all
trained batches again that points right
up to here so we're going to go through
each one of these uh files or each one
of these data sets because they're not a
file anymore we've brought them in data
batch one points to the actual data and
so our self training images is going to
stack them all into our into a numpy
array and then it's always nice to get
the training length and that's just the
total number of uh self training images
in there and then we're going to take
the self training images and let me
switch marker colors because i am
getting a little too too much on the
markers up here oops there we go bring
down our marker change
so we can see it a little better and at
this point this should look familiar
where did we see this well when we
wanted to uh
look at this above and we want to look
at the images in the matplot library we
had to reshape it so we're doing the
same thing here we're taking our self
training images and based on the
training length total number of images
because we stacked them all together so
now it's just one large file of images
we're going to take and look at it as
our our three video cameras that are
each displaying a 32 by 32 we're going
to switch that around so that now we
have each of our images that stays the
same place
and then we have our 32 by 32 and then
by our three
our last are three different values for
the color and of course we want to go
ahead and they run this where we say
divide by 255 that was from earlier it
just brings all the data into zero to
one that's what this is doing so we're
turning this into a zero to one array
which is all the pictures 32 by 32 by
three and then we're going to take the
self training labels and we're going to
pump those through our one hot encoder
we just made and we're going to stack
them together and uh again we're
converting this into an array that goes
from uh instead of having horse equals
one dog equals two and then horse plus
dog would equal three which would be cat
now it's going to be uh you know an
array of 10 where each one is 0 to 1.
then we want to go ahead and set up our
test images and labels and when we're
doing this you're going to see it's the
same thing we just did with the rest of
them we just changed colors right here
this is no different than what we're
doing up here with our training set
we're going to stack the different
images
we're going to get the length of them so
we know how many images are in there you
certainly could add them by hand but
it's nice to let the computer do it
especially if it ever changes on the
other end and you're using other data
and again we reshape them and transpose
them and we also do the one hot encoder
same thing we just did on our training
images so now our test images are in the
same format so now we have a definition
which sets up all our images in there
and then the next step is to go ahead
and batch them or next batch and let's
do another breakout here for batches
because this is really important to
understand tends to throw me for a
little loop when i'm working with
tensorflow or cross or a lot of these we
have our data coming in if you remember
we had like 10 000 photos let me just
put 10 000 down here we don't want to
run all 10 000 at once so we want to
break this up into batch sizes and you
also remember that we had the number of
photos in this case length of test or
whatever number is in there we also have
32 by 32
by 3. so when we're looking at the batch
size we want to change this from 10 000
to
a batch of in this case i think we're
going to do batches of 100. so we want
to look at just 100 the first hundred of
the photos and if you remember we set
self i
equal to
zero uh so what we're looking at here is
we're going to create x we're gonna get
the next batch from the very initialize
we've already initialized it for zero so
we're going to look at x from zero to
batch size which we've set to 100 so
just the first hundred images and then
we're going to reshape that into uh and
this is important to let the data know
that we're looking at 100 by 32 by 32 by
3. now we've already formatted it to the
32 by 32 by 3. this just sets everything
up correctly so that x has the data in
there in the correct order in the
correct shape and then the y just like
the x uh is our labels so our training
labels again they go from zero to batch
size in this case they do sell five plus
batch size because the cell phi is going
to keep changing and then finally we
increment the self i because we have
zero so we so the next time we call it
we're going to get the next batch size
and so basically we have x and y x being
the photograph data coming in and y
being the label and that of course is
labeled through one hot encoder so if
you remember correctly if it was say
horse is equal to zero it would be uh
one for the zero position since this is
the horse and then everything else would
be zero in here let me just put lines
through there there we go there's our
array
hard to see that array so let's go ahead
and take that and uh we're gonna finish
loading it since this is our class and
now we're armed with all this um uh our
setup over here let's go ahead and load
that up and so we're gonna create a
variable ch with the cfar helper in it
and then we're going to do ch.setup
images
now we could have just put all the setup
images under the init but by breaking
this up into two parts it makes it much
more readable and also if you're doing
other work there's reasons to do that as
far as the setup let's go ahead and run
that and you can see where it says uh
setting up training images and labels
setting up test images and that's one of
the reasons we broke it up is so that if
you're testing this out you can actually
have print statements in there telling
you what's going on which is really nice
uh they did a good job with this setup i
like the way that it was broken up in
the back and then one quick note you
want to remember that batch to set up
the next batch because we have to run uh
batch equals ch next batch of 100
because we're going to use the 100 size
but we'll come back to that we're going
to use that just remember that that's
part of our code we're going to be using
in a minute from the definition we just
made so now we're ready to create our
model first thing we want to do is we
want to import our tensorflow as tf i'll
just go ahead and run that so it's
loaded up and you can see we got a
warning here
that's because they're making some
changes it's always growing and they're
going to be depreciating one of the
values from float 64 to float type or is
treated as an np float64 nothing to
really worry about this doesn't even
affect what we're working on because
we've set all of our stuff to a 255
value or zero to one and do keep in mind
that zero to one value that we converted
the 255 is still a float value but it'll
easily work with either the numpy float
64 or the numpy d type float it doesn't
matter which one it goes through so the
depreciation would not affect our code
as we have it and in our tensorflow
uh we'll go ahead and just increase the
size in there just a moment so you can
get a better view of the um what we're
typing in uh we're going to set a couple
placeholders here and so we have we're
going to set x equals tf placeholder tf
float 32 we just talked about the float
64 versus the numpy float we're actually
just going to keep this at float 32 more
than a significant number of decimals
for what we're working with and since
it's a placeholder we're going to set
the shape equal to and we've set it
equal to none
because at this point we're just holding
the place on there we'll be setting up
as we run the batches that's what that
first value is and then 32 by 32 by 3
that's what we've reshaped our data to
fit in and then we have our y true
equals placeholder tf float 32 and the
shape equals none comma 10. 10 is the 10
different labels we have so it's an
array of 10. and then let's create one
more placeholder we'll call this a hold
prob or hold probability and we're going
to use this we don't have to have a
shape or anything for this this
placeholder is for what we call drop out
if you remember from our theory before
we drop out so many nodes is looking at
or the different values going through
which helps decrease bias so we need to
go ahead and put a placeholder for that
also and we'll run this so it's all
loaded up in there so we have our three
different placeholders and since we're
in tensorflow when you use keras it does
some of this automatically but we're in
tensorflow direct cross sits on
tensorflow we're going to go ahead and
create some more helper functions we're
going to create something to help us
initialize the weights initialize our
bias if you remember that each layer has
to have a bias going in we're going to
go ahead and work on our conversional 2d
our max pool so we have our pooling
layer our convolutional layer and then
our normal full layer so we're going to
go ahead and put those all into
definitions and let's see what that
looks like in code and you can also grab
some of these helper functions from the
mnist the uh nist setup let me just put
that in there if you're under the
tensorflow so a lot of these already in
there but we're going to go ahead and do
our own and we're going to create our
init weights and one of the reasons
we're doing this is so that you can
actually start thinking about what's
going on in the back end so even though
there's ways to do this with an
automation sometimes these have to be
tweaked and you have to put in your own
setup in here now we're not going to be
doing that we're just going to recreate
them for our code and let's take a look
at this we have our weights and so what
comes in is going to be the shape and
what comes out is going to be a random
number so we're going to go ahead and
just knit some random numbers based on
the shape with a standard deviation of
0.1 kind of a fun way to do that and
then the tf variable uh init random
distribution so we're just creating a
random distribution on there that's all
that is for the weights now you might
change that you might have a higher
standard deviation in some cases you
actually load preset weights that's
pretty rare usually you're testing that
against another model or something like
that and you want to see how those
weights configure with each other now
remember we have our bias so we need to
go ahead and initialize the bias with a
constant
in this case we're using point one a lot
of times the bias is just put in as one
and then you have your weights add on to
that uh but we're gonna set this as
point one uh so we wanna return a
convolutional 2d in this case a neural
network this is uh would be a layer on
here what's going on with the con 2d is
we're taking our data coming in uh we're
going to filter it strides if you
remember correctly strides came from
here's our image and then we only look
at this picture here and then maybe we
have a stride of one so we look at this
picture here and we continue to look at
the different filters going on there the
other thing this does is that we have
our data coming in as 32 by
32
by
3 and we want to change this so that
it's just this is three dimensions and
it's going to reformat this as just two
dimensions so it's going to take this
number here and combine it with the 32
by 32 so this is a very important layer
here because it's reducing our data down
using different means and it connects
down i'm just going to jump down one
here
it goes with the convolutional layer so
you have your your kind of your
pre-formatting and the setup and then
you have your actual convolution layer
that goes through on there and you can
see here we have init weights by the
shape and knit bias shape of three
because we have the three different
here's our three again and then we
return the tfnn relu with the convention
2d so this convolutional
has this feeding into it right there
it's using that as part of it and of
course the input is the x y plus b the
bias so that's quite a mouthful but
these two are the are the keys here to
creating the convolutional layers there
the convolutional 2d coming in and then
the convolutional layer which then steps
through and creates all those filters we
saw then of course we have our pooling
so after each time we run it through the
convectional layer we want to pull the
data if you remember correctly on the on
the pool side and let me just get rid of
all my marks it's getting a little crazy
there and in fact let's go ahead and
jump back to that slide let's just take
a look at that slide over here
so we have our image coming in we create
our convolutional layer with all the
filters remember the filters go um you
know the filter is coming in here and it
looks at these four boxes and then if
it's a step let's say step two it then
goes to these four boxes and then the
next step and so on uh so we have our
convolutional layer that we generate or
convolutional layers they use the uh
relu function um there's other functions
out there for this though the relu is
the most the one that works the best at
least so far i'm sure that will change
then we have our pooling now if you
remember correctly the pooling was max
uh so if we had the filter coming in and
they did the multiplication on there and
we have a one and maybe a two here and
another one here and a three here three
is the max and so out of all of these
you then create an array that would be
three and if the max is over here two or
whatever it is that's what goes into the
pooling of what's going on in our
pooling uh so again we're reducing that
data down reducing it down as small as
we can and then finally we're going to
flatten it out into a single array and
that goes into our fully connected layer
and you can see that here in the code
right here we're going to create our
normal full layer so at some point we're
going to take from our pooling layer
this will go into some kind of
flattening process and then that will be
fed into the full the different layers
going in down here
and so we have our input size you'll see
our input layer get shape which is just
going to get the shape for whatever's
coming in
and then input size initial weights is
also based on the input layer coming in
and the input size down here is based on
the input layer shape so we're just
going to already use the shape and
already have our size coming in and of
course you have to make sure you knit
the bias always put your bias on there
and we'll do that based on the size so
this will return
tf.matmul
input layer w plus b this is just a
normal full layer that's what this means
right down here that's what we're going
to return so that was a lot of steps we
went through let's go ahead and run that
so those are all loaded in there and
let's go ahead and create the layers
let's see what that looks like now that
we've done all the heavy lifting and
everything uh we get to do all the easy
part let's go ahead and create our
layers we'll create a convolution layer
one and two two different convolutional
layers and then we'll take that and
we'll flatten that out and create a
reshape pooling in there for our reshape
and then we'll have our full uh layer at
the end so let's start by creating our
first convolutional layer then we come
in here and let me just run that real
quick and i want you to notice on here
the 3
and the 32 this is important because
coming into this convolutional layer we
have three different channels and 32
pixels each
so that has to be in there the four and
four you can play with this is your
filter size so if you remember you have
a filter and you have your image and the
filter slowly steps over and filters out
this image depending on what your step
is for this particular setup 4 4 is just
fine that should work pretty good for
what we're doing for the size of the
image and then of course at the end once
you have your convolutional layer set up
you also need to pull it and you'll see
that the pooling is automatically set up
so that it would see the different shape
based on what's coming in so here we
have max toolbar two by two and we put
in the convolutional one that we just
created the convolutional layer we just
created goes right back into it and that
right up here as you can see is the x
that's coming in from here so it knows
to look at the first model and set the
the data accordingly set that up
so it matches and we went ahead and ran
this already i think i read let me go
and run it again and if we're going to
do one layer let's go ahead and do a
second layer down here and it's uh we'll
call it convo 2.
it's also a convolutional layer on this
and you'll see that we're feeding
convolutional 1 in the pooling so it
goes from convolutional 1 into
convolutional one pooling from
convolutional one pooling into
convolutional two and then from
convolutional two into convolutional to
pooling and we'll go ahead and take this
and run this so these variables are all
loaded into memory and for our flattened
layer uh let's go ahead and we'll do uh
since we have 64 coming out of here and
we have a four by four going in let's do
eight by eight by 64. so let's do
4096. this is gonna be the flat layer so
that's how many bits are coming through
on the flat layer and we'll reshape this
so we'll reshape our uh convo two
pooling and that will feed into here the
convo two pulling and then we're gonna
set it up as a single layer that's
4096 in size that's what that means
there we'll go ahead and run this so
we've now created this variable the
convo to flat and then we have our first
full layer this is the final
neural network where the flat layer
going in and we're going to again use
the relu for our setup on there on a
neural network for evaluation and you'll
notice that we're going to create our
first full layer our normal full layer
that's our definition so we created that
that's creating the normal full layer
and our input for the data comes right
here from the this goes right into it uh
the convo too flat so this tells it how
big the data is and we're going to have
it come out it's going to have 10 24
that's how big the layer is coming out
we'll go ahead and run this so now we
have our full layer one and with the
full layer one we want to also define
the full one dropout to go with that so
our full layer one comes in
keep probability equals whole
probability remember we created that
earlier and the full layer one is what's
coming into it and this is going
backwards and training the data we're
not training every weight we're only
training a percentage of them each time
which helps get rid of the bias so let
me go ahead and run that and
finally we'll go ahead and create a y
predict which is going to equal the
normal full 1 drop out and 10 because we
have 10 labels in there now in this
neural network we could have added
additional layers that would be another
option to play with you can also play
with instead of 10 24 you can use other
numbers for the way that sets up and
what's coming out going into the next
one we're only going to do just the one
layer and the one layer drop out and you
can see if we did another layer it'd be
really easy just to feed in the full one
drop out into full layer two and then
full layer two dropout would have full
layer two feed into it and then you'd
switch that here for the y prediction
for right now this is great this
particular data set is tried and true
and we know that this will work on it
and if we just type in y predict and we
run that
we'll see that this is a tensor object
shape question mark 10 d type 32 a quick
way to double check what we're working
on so now we've got all of our we've
done a setup all the way to the y
predict which we just did we want to go
ahead and apply the loss function and
make sure that's set up in there
create the optimizer and then
trainer optimizer and create a variable
to initialize all the global tf
variables so before we dive in to the
loss function let me point out one quick
thing or just kind of a rehab over a
couple things and that is when we're
playing with this these setups
we pointed out up here we can change the
4 4 and use different numbers there the
change your outcome so depending on what
numbers you use here will have a huge
impact on how well your model fits and
that's the same here of the 1024 also
this is also another number that if you
continue to raise that number you'll get
possibly a better fit you might overfit
and if you lower that number you'll use
less resources and generally you want to
use this in
the exponential growth an exponential
being 2 4 8 16 and in this case the next
one down would be 5 12. you can use any
number there but those would be the
ideal numbers when you look at this data
so the next step in all this is we need
to also create a way of tracking how
good our model is and we're going to
call this a loss function and so we're
going to create a cross entropy loss
function and so before we discuss
exactly what that is let's take a look
and see what we're feeding it
we're going to feed it our labels and we
have our true labels and our prediction
labels so coming in here is where the
two different variables we're sending in
or the two different probability
distributions is one that we know is
true and what we think it's going to be
now this function right here when they
talk about cross entropy uh in
information theory the cross entropy
between two probability distributions
over the same underlying set of events
measures the average number of bits
needed to identify an event drawn from
the set that's a mouthful uh really
we're just looking at the amount of
error in here how many of these are
correct and how many of these um are
incorrect so how much of it matches and
we're going to look at that we're just
going to look at the average that's what
the mean the reduced to the mean means
here so we're looking at the average
error on this
and so the next step
is we're going to take the error we want
to know our cross entropy or our loss
function how much loss we have that's
going to be part of how we train the
model so when you know what the loss is
and we're training it we feed that back
into the back propagation setup and so
we want to go ahead and optimize that
here's our optimizer we're going to
create the optimizer using an atom
optimizer remember there's a lot of
different ways of optimizing the data
atoms are most popular used
so our optimizer is going to equal the
tf train atom optimizer if you don't
remember what the learning rate is let
me just pop this back into here here's
our learning rate when you have your
weights you have all your weights and
your different nodes that are coming out
here's our node coming out
it has all its weights and then the
error is being prop sent back through in
reverse on our neural network so we take
this error we adjust these weights based
on the different formulas in this case
the atom formula is what we're using we
don't want to just adjust them
completely we don't want to change this
weight so it exactly fits the data
coming through because if we made that
kind of adjustment it's going to be
biased to whatever the last data we sent
through is instead we're going to
multiply that by .001 and make a very
small shift in this weight so our delta
w is only 0.001 of the actual delta w of
the full change we're going to compute
from the atom and then we want to go
ahead and train it so our training or
set up a training
variable or function and this is going
to equal our optimizer minimize cross
entropy and we make sure we go ahead and
run this
so it's loaded in there and then we're
almost ready to train our model before
we do that we need to create one more
variable in here and we're going to
create a variable to initialize all the
global tf variables and when we look at
this
the tf global variable initializer this
is a tensorflow
object it goes through there and it
looks at all our different setup that we
have going under our tensor flow and
then initializes those variables
so it's kind of like a magic wand
because it's all hidden in the back end
of tensorflow all you need to know about
this is that you have to have the
initialization on there which is an
operation um and you have to run that
once you have your setup going so we'll
go ahead and run this piece of code and
then we're going to go ahead and train
our data so let me run this so it's
loaded up there and so now we're going
to go ahead and run the model by
creating a graph session graph session
is a tensorflow term so you'll see that
coming up it's one of the things that
throws me because i always think of
graphics and spark and graph as just
general graphing but they talk about a
graph session so we're going to go ahead
and run the model and let's go ahead and
walk through this
what's going on here and let's paste
this data in here and here we go so
we're going to start off with it with a
tf session as sess so that's our actual
tf session we've created uh so we're
right here with the tf uh
session our session we're creating we're
going to run tf global variable
initializer so right off the bat we're
initializing our variables here and then
we have for i in range 500 so what's
going on here remember 500 we're going
to break the date up and we're going to
batch it in at 500 points each we've
created our session run so we're going
to do with tf session as session
right here we've created our variable
session and then we're going to run
we're going to go ahead and initialize
it so we have our tf global variables
initializer that we created
that initializes our our session in here
the next thing we're going to do is
we're going to go for i in range of 500
batch equals ch.nextbatch
so if you remember correctly this is
loading up
100 pictures at a time and
this is going to loop through that 500
times so we are literally doing uh what
is that uh 500 times 100 is uh 50 000.
so that's 50 000 pictures we're going to
process right there and the first
process is we're going to do a session
run we're going to take our train we
created our train variable or optimizer
in there we're going to feed it the
dictionary we had our feed dictionary
that we created and we have x equals
batch 0 coming in y true batch 1
hold the probability 0.5 and then just
so that we can keep track of what's
going on we're going to every 100 steps
we're going to run a print so currently
on step format
accuracy is
and we're going to look at matches
equals tf.equal tf argument why
prediction one tf.org max y true comma
one so we're going to look at this as
how many matches it has and here our acc
uh all we're doing here is we're going
to take the matches how many matches
they have it creates it generates a
chart we're going to convert that to
float that's what the tfcast does and
then we just want to know the average we
just want to know the average of the
accuracy and then we'll go ahead and
print that out
print session run accuracy feed
dictionary so it takes all this and it
prints out our accuracy on there so
let's go ahead and take this oops
screen's there let's go ahead and take
this and let's run it and this is going
to take a little bit to run
so let's see what happens on my old
laptop and we'll see here that we have
our current we're currently on step zero
it takes a little bit to get through the
accuracy and this will take just a
moment to run we can see that on our
step 0 it has an accuracy of 0.1 or
0.1028
and as it's running we'll go ahead you
don't need to watch it run all the way
but this accuracy is going to change a
little bit up and down so we've actually
lost some accuracy during our step two
but we'll see how that comes out let's
come back after we run it all the way
through and see how the different steps
come out is actually reading that
backwards
the way this works is the closer we get
to one the more accuracy we have
so you can see here we've gone from a
0.1 to a 0.39
and we'll go ahead and pause this and
come back and see what happens when
we're done with the full run all right
now that we've
prepared the meal got it in the oven and
pulled out my finished dish here if
you've ever watched any of the old
cooking shows let's discuss a little bit
about this accuracy going on here and
how do you interpret that we've done a
couple things
first we've defined accuracy
the reason i got it backwards before is
you have
loss or accuracy and with loss you'll
get a graph that looks like this it goes
oops that's an s by the way there we go
you get a graph that curves down like
this and with accuracy you get a graph
that curves up this is how good it's
doing now in this case uh one is
supposed to be really good accuracy that
means it gets close to one but it never
crosses one so if you have an accuracy
of one that is phenomenal in fact that's
pretty much impos you know unheard of
and the same thing with loss if you have
a loss of zero that's also unheard of
zero's actually on this this axis right
here as we go in there so how do we
interpret that because you know if i was
looking at this and i go oh 0.51 that's
uh 51 you're doing 50 50. no this is not
percentage let me just put that in there
it is not percentage this is log
rhythmic what that means is that 0.2 is
twice as good as 0.1 and when we see 0.4
that's twice as good as 0.2 real way to
convert this into a percentage you
really can't say this is is a direct
percentage conversion what you can do
though is in your head if we were to
give this a percentage we might look at
this as
50
we're just guessing equals 0.1 and if 50
roughly equals 0.1 that's we started up
here at the top remember at the top here
here's our 0.1028 the accuracy of 50
then 75 percent is about 0.2 and so on
and so on don't quote those numbers
because that doesn't work that way they
say that if you have 0.95
that's pretty much saying a hundred
percent and if you have anywhere between
you'd have to go look this up let me go
and remove all my drawings there uh so
the magic number is 0.5 we really want
to be over a 0.5 in this whole thing and
we have
both 0.504 remember this is accuracy if
we were looking at loss then we would be
looking the other way but 0.05 you know
instead of how high it is we want how
low it is uh but with accuracy being
over 0.5 is pretty valid that means this
is pretty solid and if you get to a 0.95
then it's a direct correlation that's
what we're looking for here in these
numbers you can see we finished with
this model at
0.5135 so still good
and if we look at when they ran this in
the other end remember there's a lot of
randomness that goes into it when we see
the weights they got
.5251 so a little better than ours but
that's fine you'll find your own comes
up a little bit better or worse
depending on just that randomness and so
we've gone through the whole model we've
created we trained the model and we've
also gone through on every 100th run to
test the model to see how accurate it is
what's in it for you we will start with
the course of fundamentals what is a
neural network in popular neural
networks it's important to know the
framework we're in and what we're going
to be looking at specifically then we'll
touch on why a recurrent neural network
what is a recurrent neural network and
how does an rn in work
one of the big things about rnns is what
they call the vanishing and exploding
gradient problem so we'll look at that
and then we're going to be using a use
case
study that's going to be in keras on
tensorflow cross is a python module for
doing neural networks in deep learning
and in there there's the what they call
long short term memory lstm and then
we'll use the use case to implement our
lstm on the cross so when you see that
lstm that is basically the rnn network
and we'll get into that the use case is
always my favorite part before we dive
into any of this we're going to take a
look at what is an rnn or an
introduction to the rnn do you know how
google's auto complete feature predicts
the rest of the words a user is typing i
love that auto complete feature as i'm
typing away it saves me a lot of time i
can just kind of hit the enter key and
it auto fills everything and i don't
have to type as much well first there's
a collection of large volumes of most
frequently occurring consecutive words
this is fed into a recurrent neural
network analysis the data by finding the
sequence of words occurring frequently
and builds a model to predict the next
word in the sentence and then google
what is the best food to eat in loss i'm
guessing you're going to say loss mexico
no it's going to be las vegas
so the google search will take a look at
that and say hey the most common
autocomplete is going to be vegas in
there and usually it gives you three or
four different choices so it's a very
powerful tool it saves us a lot of time
especially when we're doing a google
search or even in microsoft words has a
some people get very mad at it auto
fills with the wrong stuff but you know
you're typing away and it helps you
autofill i have that in a lot of my
different packages it's just a standard
feature that we're all used to now so
before we dive into the rnn and getting
into the depths let's go ahead and talk
about what is a neural network neural
networks used in deep learning consist
of different layers connected to each
other and work on the structure and
functions of a human brain you're going
to see that thread human in human brain
and human thinking throughout deep
learning the only way we can evaluate an
artificial intelligence or anything like
that is to compare it to human function
very important note on there and it
learns from a huge volumes of data and
it uses complex algorithm to train a
neural net so in here we have image
pixels of two different breeds of dog
one looks like a nice floppy eared lab
and one a german shepherd you know both
wonderful breeds of animals that image
then goes into an input layer that input
layer might be formatted at some point
because you have to let it know like you
know different pictures are going to be
different sizes and different color
content then it'll feed into hidden
layers so each of those pixels or each
point of data goes in and then splits
into the hidden layer which then goes
into another hidden layer which then
goes to an output layer rnn there's some
changes in there which we're going to
get into so it's not just a
straightforward propagation of data like
we've covered in many other tutorials
and finally you have an output layer and
the output layer has two outputs it has
one that lights up if it's a german
shepherd and another that lights up it's
if it's a labrador so identify as a
dog's breed set networks do not require
memorizing the past output so our
forward propagation is just that it goes
forward and doesn't have to be memorized
stuff and you can see there that's not
actually me in the picture uh dressed up
in my suit
i haven't worn a suit in years so as
we're looking at this we're going to
change it up a little bit before we
cover that let's talk about popular
neural networks first there's the feed
forward neural network used in general
regression and classification problems
and we have the convolution neural
network used for image recognition deep
neural network used for acoustic
modeling deep belief network used for
cancer detection and recurrent neural
network used for speech recognition now
taken a lot of these and mixed them
around a little bit so just because it's
used for one thing doesn't mean it can't
be used for other modeling but generally
this is where the field is and this is
how those models are generally being
used right now so we talk about a feed
forward neural network in a feed forward
neural network information flows only in
the forward direction from the input
nodes through the hidden layers if any
and to the output nodes there are no
cycles or loops in the network and so
you can see here we have our input layer
i was talking about how it just goes
straight forward into the hidden layers
so each one of those connects and then
connects to the next hidden layer
connects to the output layer and of
course we have a nice simplified version
where it has a predicted output the
refer to the input is x a lot of times
and the output as y decisions are based
on current input no memory about the
past no future scope why recurrent
neural network issues in feed forward
neural network so one of the biggest
issues is because it doesn't have a
scope of memory or time a feed forward
neural network doesn't know how to
handle sequential data it only considers
only the current input so if you have a
series of things and because three
points back affects what's happening now
and what your output affects what's
happening that's very important so
whatever i put as an output is going to
affect the next one um a feed forward
doesn't look at any of that it just
looks at this is what's coming in and it
cannot memorize previous inputs so it
doesn't have that list of inputs coming
in solution to feed forward neural
network you'll see here where it says
recurrent neural network and we have our
x on the bottom going to h going to y
that's your feed forward but right in
the middle it has a value c so there's a
whole another process was memorizing
what's going on in the hidden layers and
the hidden layers as they produce data
feed into the next one so your hidden
layer might have an output that goes off
to y
but that output goes back into the next
prediction coming in what this does is
this allows it to handle sequential data
it considers the current input and also
the previously received inputs and if
we're going to look at general drawings
and solutions we should also look at
applications of the rnn image captioning
rnn is used to caption an image by
analyzing the activities present in it a
dog catching a ball in midair uh that's
very tough i mean you know we have a lot
of stuff that analyzes images of a dog
and the image of a ball but it's able to
add one more feature in there that's
actually catching the ball in midair
time series prediction any time series
problem like predicting the prices of
stocks in a particular month can be
solved using rnn and we'll dive into
that in our use case and actually take a
look at some stock one of the things you
should know about analyzing stock today
is that it is very difficult and if
you're analyzing the whole stock the
stock market at the new york stock
exchange in the u.s produces somewhere
in the neighborhood if you count all the
individual trades and fluctuations by
the second
it's like three terabytes a day of data
so we're going to look at one stock just
analyzing one stock is really tricky in
here we'll give you a little jump on
that so that's exciting but don't expect
to get rich off of it immediately
another application of the rnn is
natural language processing text mining
and sentiment analysis can be carried
out using rnn for natural language
processing and you can see right here
the term natural language processing
when you stream those three words
together is very different than i if i
said processing language natural leap so
the time series is very important when
we're analyzing sentiments it can change
the whole value of a sentence just by
switching the words around or if you're
just counting the words you may get one
sentiment where if you actually look at
the order they're in you get a
completely different sentiment when it
rains look for rainbows when it's dark
look for stars both of these are
positive sentiments and they're based
upon the order of which the sentence is
going in machine translation given an
input in one language rnn can be used to
translate the input into a different
languages as output i myself very
linguistically challenged but if you
study languages and you're good with
languages you know right away that if
you're speaking english you would say
big cat and if you're speaking spanish
he would say cat big so that translation
is really important to get the right
order to get there's all kinds of parts
of speech that are important to know by
the order of the words here this person
is speaking in english and getting
translated and you can see here a person
is speaking in english in this little
diagram i guess that's denoted by the
flags i have a flag i own it no um but
they're speaking in english and it's
getting translated into
chinese italian french german and
spanish languages some of the tools
coming out are just so cool so somebody
like myself who's very linguistically
challenged i can now travel into worlds
i would never think of because i can
have something translate my english back
and forth readily and i'm not stuck with
a communication gap so let's dive into
what is a recurrent neural network
recurrent neural network works on the
principle of saving the output of a
layer and feeding this back to the input
in order to predict the output of the
layer sounds a little confusing when we
start breaking it down it'll make more
sense and usually we have our
propagation forward neural network with
the input layers the hidden layers the
output layer with the recurrent neural
network we turn that on its side so here
it is and now our x comes up from the
bottom into the hidden layers into y and
they usually draw very simplified x to h
with c as a loop a to y where a b and c
are the perimeters a lot of times you'll
see this kind of drawing in here digging
closer and closer into the h and how it
works going from left to right you'll
see that the c goes in and then the x
goes in so the x is going upward down
and c is going to the right a is going
out and c is also going out that's what
gets a little confusing so here we have
xn
cn and then we have y out and c out and
c is based on ht minus 1. so our value
is based on the y and the h value are
connected to each other they're not
necessarily the same value because h can
be its own thing and usually we draw
this or we represent it as a function h
of t equals a function of c where h of t
minus 1 that's the last h output and x
of t going in so it's the last output of
h combined with the new input of x where
h t is the new state fc is a function
with the parameters c that's a common
way of denoting it h t minus one is the
old state coming out and then x of t is
an input vector at time of step t
well we need to cover types of recurrent
neural networks and so the first one is
the most common one which is a one to
one single output
one-to-one neural network is usually
known as a vanilla neural network used
for regular machine learning problems
why because vanilla is usually
considered kind of a just a real basic
flavor but because it's very basic a lot
of times they'll call it the vanilla
neural network which is not the common
term but it is you know kind of a slang
term people will know what you're
talking about usually if you say that
then we run one to many so you have a
single input and you might have a
multiple outputs in this case
image captioning as we looked at earlier
where we have not just looking at it as
a dog but a dog catching a ball in the
air and then you have mini to one
network takes in a sequence of inputs
examples sentiment analysis where a
given sentence can be classified as
expressing positive or negative
sentiments and we looked at that as we
were discussing if it rains look for a
rainbow so positive sentiment where rain
might be a negative sentiment if you're
just adding up the words in there and
then of course if you're going to do a
one-to-one mini to one one to many
there's many to many networks takes in a
sequence of inputs and generates a
sequence of outputs example machine
translation so we have a lengthy
sentence coming in in english and then
going out in all the different languages
you know just a wonderful tool very
complicated set of computations you know
if you're a translator you realize just
how difficult it is to translate into
different languages one of the biggest
things you need to understand when we're
working with this neural network is
what's called the vanishing gradient
problem while training an rnn your slope
can be either too small or very large
and this makes training difficult when
the slope is too small the problem is
known as vanishing gradient and you'll
see here they have a nice uh image loss
of information through time so if you're
pushing not enough information forward
that information is lost and then when
you go to train it you start losing the
third word in the sentence or something
like that or doesn't quite follow the
full logic of what you're working on
exploding gradient problem oh this is
one that runs into everybody when you're
working with this particular neural
network when the slope tends to grow
exponentially instead of decaying this
problem is called exploding gradient
issues and gradient problem long
training time poor performance bad
accuracy and i'll add one more in there
uh your computer if you're on a lower
end computer testing out a model will
lock up and give you the memory error
explaining gradient problem consider the
following two examples to understand
what should be the next word in the
sequence
the person who took my bike and blank a
thief the students who got into
engineering with blank from asia and you
can see in here we have our x value
going in we have the previous value
going forward and then you back
propagate the error like you do with any
neural network and as we're looking for
that missing word maybe we'll have the
person took my bike and blank was a
thief and the student who got into
engineering with a blank were from asia
consider the following example the
person who took the bike so we'll go
back to the person who took the bike was
blank a thief in order to understand
what would be the next word in the
sequence the rnn must memorize the
previous context whether the subject was
singular noun or a plural noun so was a
thief is singular the student who got
into engineering well in order to
understand what would be the next word
in the sequence the rnn must memorize
the previous context whether the subject
was singular noun or a plural noun and
so you can see here the students who got
into engineering with blank were from
asia it might be sometimes difficult for
the air to back propagate to the
beginning of the sequence to predict
what should be the output so when you
run into the gradient problem we need a
solution the solution to the gradient
problem first we're going to look at
exploding gradient where we have three
different solutions depending on what's
going on one is identity initialization
so the first thing we want to do is see
if we can find a way to minimize the
identities coming in instead of having
it identify everything just the
important information we're looking at
next is to truncate the back propagation
so instead of having whatever
information it's sending to the next
series we can truncate what it's sending
we can lower that particular set of
layers make those smaller and finally is
a gradient clipping so when we're
training it we can clip what that
gradient looks like and narrow the
training model that we're using when you
have a vanishing gradient the option
problem we can take a look at weight
initialization very similar to the
identity but we're going to add more
weights in there so it can identify
different aspects of what's coming in
better choosing the right activation
function that's huge so we might be
activating based on one thing and we
need to limit that we haven't talked too
much about activation function so we'll
look at that just minimally there's a
lot of choices out there and then
finally there's long short term memory
networks the lstms and we can make
adjustments to that so just like we can
clip the gradient as it comes out we can
also
expand on that we can increase the
memory network the size of it so it
handles more information and one of the
most common problems in today's
setup is what they call long-term
dependencies suppose we try to predict
the last word in the text the clouds are
in the and you probably said sky here we
do not need any further context it's
pretty clear that the last word is going
to be sky suppose we try to predict the
last word in the text i have been
staying in spain for the last 10 years i
can speak fluent maybe you said
portuguese or french no you probably
said spanish the word we predict will
depend on the previous few words in
context here we need the context of
spain to predict the last word in the
text it's possible that the gap between
the relevant information and the point
where it is needed to become very large
lstms help us solve this problem so the
lstms are a special kind of recurrent
neural network capable of learning
long-term dependencies remembering
information for long periods of time is
their default behavior all recurrent
neural networks have the form of a chain
of repeating modules of neural network
connections in standard rnns this
repeating module will have a very simple
structure such as a single tangent h
layer lstms's
also have a chain like structure but the
repeating module has a different
structure instead of having a single
neural network layer there are four
interacting layers communicating in a
very special way lstms are a special
kind of recurrent neural network capable
of learning long-term dependencies
remembering information for long periods
of time is their default behavior ls
tmss also have a chain like structure
but the repeating module has a different
structure instead of having a single
neural network layer there are four
interacting layers communicating in a
very special way as you can see the
deeper we dig into this the more
complicated the graphs get in here i
want you to note that you have x of t
minus 1 coming in you have x of t coming
in and you have x at t plus 1. and you
have h of t minus 1 and h of t coming in
and h of t plus 1 going out and of
course on the other side is the output a
in the middle we have our tangent h but
it occurs in two different places so not
only when we're computing the x a t plus
one are we getting the tangent h from x
to t but we're also getting that value
coming in from the x of t minus one so
the short of it is as you look at these
layers not only does it does the
propagate through the first layer goes
into the second layer back into itself
but it's also going into the third layer
so now we're kind of stacking those up
and this can get very complicated as you
grow that in size it also grows in
memory too and in the amount of
resources it takes but it's a very
powerful tool to help us address the
problem of complicated long sequential
information coming in like we're just
looking at in the sentence and when
we're looking at our long short term
memory network there's three steps of
processing
in the lstms we look at the first one is
we want to forget irrelevant parts of
the previous state you know a lot of
times like you know is as in a unless
we're trying to look at whether it's a
plural noun or not they don't really
play a huge part in the language so we
want to get rid of them then selectively
update cell state values so we only want
to update the cell state values that
reflect what we're working on and
finally we want to put only output
certain parts of the cell state so
whatever is coming out we want to limit
what's going out too and let's dig a
little deeper into this let's just see
what this really looks like uh so step
one decides how much of the past it
should remember first step in the lstm
is to decide which information to be
omitted in from the cell in that
particular time step it is decided by
the sigmoid function it looks at the
previous state h to t minus 1 and the
current input x t and computes the
function so you can see over here we
have a function of t
equals the sigmoid function of the
weight of f the h at t minus one and
then x a t plus of course you have a
bias in there with any of our neural
networks so we have a bias function so f
of t equals forget gate decides which
information to delete that is not
important from the previous time step
considering an stm is fed with the
following inputs from the previous and
present time step alice is good in
physics john on the other hand is good
in chemistry so previous output john
plays football well he told me yesterday
over the phone that he had served as a
captain of his college football team
that's our current input so as we look
at this the first step is the forget
gate realizes there might be a change in
context after encounting the first full
stop compares with the current input
sentence of x a t so we're looking at
that full stop and then compares it with
the input of the new sentence the next
sentence talks about john so the
information on alice is deleted okay
that's important to know so we have this
input coming in and if we're going to
continue on with john then that's going
to be the primary information we're
looking at the position of the subject
is vacated and is assigned to john and
so in this one we've seen that we've
weeded out a whole bunch of information
and we're only passing information on
john since that's now the new topic so
step two is then to decide how much
should this unit add to the current
state in the second layer there are two
parts one is a sigmoid function and the
other is the tangent h in the sigmoid
function it decides which values to let
through zero or one tangent h function
gives a weightage to the values which
are passed deciding their level of
importance minus one to one and you can
see the two formulas that come up the i
of t equals the sigmoid of the weight of
i h to t minus one x of t plus the bias
of i and the c of t equals the tangent
of h of the weight of c of h of t minus
1 x t plus the bias of c so our i of t
equals the input gate determines which
information to let through based on its
significance in the current time step if
this seems a little complicated don't
worry because a lot of the programming
is already done when we get to the case
study understanding though that this is
part of the program is important when
you're trying to figure out these what
to set your settings at you should also
note when you're looking at this it
should have some semblance to your
forward propagation neural networks
where we have a value assigned to a
weight plus a bias very important steps
than any of the neural network layers
whether we're propagating into them the
information from one to the next or
we're just doing a straightforward
neural network propagation let's take a
quick look at this what it looks like
from the human standpoint as i step out
of my suit again consider the current
input at x of t john plays football well
he told me yesterday over the phone that
he had served as a captain of his
college football team that's our input
input gate analysis the important
information john plays football and he
was a captain of his college team as
important he told me over the phone
yesterday is less important hence it is
forgotten this process of adding some
new information can be done via the
input gate now this example is as a
human form and we'll look at training
this stuff in just a minute
but as a human being if i wanted to get
this information from a conversation
maybe it's a google voice listening in
on you or something like that um how do
we weed out the information that he was
talking to me on the phone yesterday
well i don't want to memorize that he
talked to me on the phone yesterday or
maybe that is important but in this case
it's not i want to know that he was the
captain of the football team i want to
know that he served i want to know that
john plays football and he was the
captain of the college football team
those are the two things that i want to
take away as a human being again we
measure a lot of this from the human
viewpoint and that's also how we try to
train them so we can understand these
neural networks finally we get to step
three decides what part of the current
cell state makes it to the output the
third step is to decide what will be our
output first we run a sigmoid layer
which decides what parts of the cell
state make it to the output then we put
the cell state through the tangent h to
push the values to be between -1 and 1
and multiply it by the output of the
sigmoid gate so when we talk about the
output of t we set that equal to the
sigmoid of the weight of zero of the h
of t minus one and back one step in time
by the x of t plus of course the bias
the h of t equals the outer t times the
tangent of the tangent h of c at t so
our o t equals the output gate allows
the passed in information to impact the
output in the current time step let's
consider the example to predicting the
next word in the sentence john played
tremendously well against the opponent
and one for his team for his
contributions brave blank was awarded
player of the match there could be a lot
of choices for the empty space current
input brave is an adjective adjectives
describe a noun john could be the best
output after brave thumbs up for john
awarded player of the match and if you
were to pull just the nouns out of the
sentence team doesn't look right because
that's not really the subject we're
talking about contributions you know
brave contributions or brave teen brave
player brave match um so you look at
this and you can start to train this
these this neural network so starts
looking at and goes oh no john is what
we're talking about so brave is an
adjective
john's going to be the best output and
we give john a big thumbs up and then of
course we jump into my favorite part the
case study use case implementation of
lstm let's predict the prices of stocks
using the lstm network based on the
stock price data between 2012 2016.
we're going to try to predict the stock
prices of 2017
and this will be a narrow set of data
we're not going to do the whole stock
market it turns out that the new york
stock exchange generates roughly three
terabytes of data per day that's all the
different trades up and down of all the
different stocks going on and each
individual one
second to second or nanosecond to
nanosecond but we're going to limit that
to just some very basic fundamental
information so don't think you're going
to get rich off this today but you at
least you can give an eye you can give a
step forward in how to start processing
something like stock prices a very valid
use for machine learning in today's
markets
use case implementation of lstm let's
dive in we're going to import our
libraries we're going to import the
training set and get the scaling going
now if you watch any of our other
tutorials a lot of these pieces just
start to look very familiar because it's
very similar setup let's take a look at
that and just reminder we're going to be
using anaconda the jupiter notebook so
here i have my anaconda navigator when
we go under environments i've actually
set up a cross python 36 i'm in python36
and
nice thing about anaconda especially the
newer version i remember a year ago
messing with anaconda in different
versions of python in different
environments anaconda now has a nice
interface
and i have this installed both on a
ubuntu linux machine and on windows so
it works fine on there you can go in
here and open a terminal window and then
in here once you're in the terminal
window this is where you're going to
start
installing using pip to install your
different modules and everything now
we've already pre-installed them so we
don't need to do that in here but if you
don't have them installed in your
particular environment you'll need to do
that and of course you don't need to use
the anaconda or the jupiter you can use
whatever favorite python id you like i'm
just a big fan of this because it keeps
all my stuff separate you can see on
this machine i have specifically
installed one for cross since we're
going to be working with cross under
tensorflow we go back to home i've gone
up here to application and that's the
environment i've loaded on here and then
we'll click on the launch jupiter
notebook now i've already in my jupyter
notebook
have set up a lot of stuff so that we're
ready to go kind of like martha
stewart's in the old cooking show so we
want to make sure we have all our tools
for you so you're not waiting for them
to load and if we go up here to where it
says new you can see where you can
create a new python three that's what we
did here underneath the setup so it
already has all the modules installed on
it and i'm actually renamed this if you
go under file you can rename it we've
i'm calling it rnn stock and let's just
take a look and start diving into the
code let's get into the exciting part
now we've looked at the tool and of
course you might be using a different
tool which is fine let's start putting
that code in there and seeing what those
imports and uploading everything looks
like now first half is kind of boring
when we hit the run button because we're
going to be importing numpy as np that's
uh the number python which is your numpy
array and the matplot library because
we're going to do some plotting at the
end
and our pandas for our data set our
pandas is pd and when i hit run it
really doesn't do anything except for
load those modules just a quick note let
me just do a quick draw here oops shift
alt there we go you'll notice when we're
doing this setup if i was to divide this
up oops i'm going to actually let's
overlap these here we go
this first part that we're going to do
is
our data
prep
a lot of prepping involved
in fact depending on what your system
and since we're using karass i put an
overlap here
but you'll find that almost
maybe even half of the code we do is all
about the data prep and the reason i
overlapped this with uh cross let me
just put that down because that's what
we're working in uh is because cross has
like their own preset stuff so it's
already pre-built in which is really
nice so there's a couple steps a lot of
times that are in the kara setup we'll
take a look at that to see what comes up
in our code as we go through and look at
stock and the last part is to evaluate
and if you're working with
shareholders or
classroom whatever it is you're working
with the evaluate is the next biggest
piece um so the actual code here crosses
a little bit more but when you're
working with some of the other packages
you might have like three lines that
might be it all your stuff is in your
pre-processing and your data since cross
has is cutting edge and you load the
individual layers you'll see that
there's a few more lines here and cross
is a little bit more robust and then you
spend a lot of times like i said with
the evaluate you want to have something
you present to everybody else and say
hey this is what i did this is what it
looks like so let's go through those
steps this is like a kind of just
general overview and let's just take a
look and see what the next set of code
looks like and in here we have a data
set train and it's going to be read
using the pd or pandas dot read csv and
it's a google stock price train dot csv
and so under this we have training set
equals data set train dot ilocation and
we've kind of sorted out part of that so
what's going on here let's just take a
look at let's look at the actual file
and see what's going on there now if we
look at this
ignore all the extra files on this i
already have a train and a test set
where it's sorted out this is important
to notice because a lot of times we do
that as part of the pre-processing of
the data we take
20 percent of the data out so we can
test it and then we train the rest of it
that's what we use to create our neural
network that way we can find out how
good it is uh but let's go ahead and
just take a look and see what that looks
like as far as the file itself and i
went ahead and just opened this up in a
basic word pad and text editor just so
we can take a look at it certainly you
can open up an excel or any other kind
of spreadsheet
and we note that this is a comma
separated variables we have a date
open high low close volume this is the
standard stuff that we import into our
stock or the most basic set of
information you can look at in stock
it's all free to download in this case
we downloaded it from google that's why
we call it the google stock price
and it specifically is google this is
the google stock values from as you can
see here we started off at 1 3 2012.
so when we look at this first setup up
here
we have a data set train equals pd
underscore csv and if you noticed on the
original frame
let me just go back there
they had it set to home ubuntu downloads
google stock price train i went ahead
and changed that because we're in the
same file where i'm running the code so
i've saved this particular python code
and i don't need to go through any
special paths or have the full path on
there and then of course we want to take
out
certain values in here and you're going
to notice that we're using
our data set
and we're now in pandas
so pandas basically it looks like a
spreadsheet
and in this case we're going to do i
location which is going to get specific
locations the first value is going to
show us that we're pulling all the rows
in the data and the second one is we're
only going to look at columns 1 and 2.
and if you remember here from our data
as we switch back on over columns we so
we start with zero which is the date and
we're going to be looking at open
and high which would be one and two
we'll just label that right there so you
can see now when you go back and do this
you certainly can extrapolate and do
this on all the columns
but for the example let's just limit a
little bit here so that we can focus on
just some
key aspects of stock
and then we'll go up here and run the
code and again i said the first half is
very boring whenever we hit the run
button it doesn't do anything because
we're still just loading the data and
setting it up
now that we've loaded our data we want
to go ahead and scale it we want to do
what they call feature scaling and in
here we're going to pull it up from the
sk learn or the sk kit pre-processing
import min max scaler and when you look
at this you got to remember that biases
in our data we want to get rid of that
so if you have something that's like a
really high value let's just draw a
quick graph
and i have something here like the maybe
the stock has a value one stock has a
value of 100 and another stock has a
value of five
you start to get a bias between
different stocks and so when we do this
we go ahead and say okay 100 is going to
be the max
and 5 is going to be the min and then
everything else goes and then we change
this so we just squish it down
i like the word squish so it's between 1
and 0. so 100 equals 1 or 1 equals 100
and 0 equals five and you can just
multiply it's usually just a simple
multiplication we're using uh
multiplication so it's going to be uh
minus five and then a hundred divided or
ninety-five divided by one so or
whatever value is is divided by
and once we've actually created our
scale we've tolling is going to be from
zero to one we want to take our training
set and we're going to create a training
set scaled and we're going to use our
scalar sc we're going to fit we're going
to fit and transform the training set uh
so we can now use the sc this this
particular object will use it later on
our testing set because remember we have
to also scale that when we go to test
our model and see how it works and we'll
go ahead and click on the run again it's
not going to have any output yet because
we're just setting up all the variables
okay so we pasted the data in here and
we're going to create the data structure
with the 60 time steps and output
first note we're running 60 time steps
and that is where this value here also
comes in so the first thing we do is we
create our x train and y train variables
we set them to an empty python array
very important to remember what kind of
array we're in and what we're working
with and then we're going to come in
here we're going to go for i in range 60
to 1258 there's our 60 60 time steps and
the reason we want to do this is as
we're adding the data in there's nothing
below the 60s so if we're going to use
60 time steps uh we have to start at
point 60 because it includes everything
underneath of it otherwise you'll get a
pointer error and then we're going to
take our x train and we're going to
append training set scaled this is a
scaled value between 0 and 1. and then
as i is equal to 60 this value is going
to be
60 minus 60 is 0. so this actually is
0 to i so it's going to be 0 60 1 to one
let me just circle this part right here
one to sixty one uh two to sixty two and
so on and so on and if you remember i
said zero to sixty that's incorrect
because it does not count remember it
starts at zero so this is a count of 60
so it's actually 59. important to
remember that as we're looking at this
and then the second part of this that
we're looking at so if you remember
correctly here we go we go from 0 to 59
of i and then we have a comma a 0 right
here and so finally we're just going to
look at the open value now i know we did
put it in there for one to two
if you remember quickly it doesn't count
the second one so it's just the open
value we're looking at just open
and then finally we have y train dot
append training set i to zero and if you
remember correctly i2 or i comma zero if
you remember correctly this is zero to
59 so there's 60 values in it uh so we
do i down here this is number 60. so
we're going to do this is we're creating
an array and we have 0
to 59
and over here we have number 60 which is
going into the y train it's being
appended on there and then this just
goes all the way up so this is down here
is a
0 to 59 and we'll call it 60 since
that's the value over here and it goes
all the way up to 12
58. that's where this value here comes
in that's the length of the data we're
loading
so we've loaded two arrays we've loaded
one array that has
which is filled with arrays from zero to
59 and we loaded one array which is just
the value and what we're looking at you
want to think about this as a time
sequence uh here's my open open open
open open open what's the next one in
the series so we're looking at the
google stock and each time it opens we
want to know what the next one 0 through
59 what's 60 1 through 60 what's 61 2
through 62 what's 62 and so on and so on
going up and then once we've loaded
those in our for loop
we go ahead and take x train and y train
equals np.arrayxtrain.npraytrain
we're just converting this back into a
numpy array that way we can use all the
cool tools that we get with numpy array
including reshaping so if we take a look
and see what's going on here we're going
to take our x train
we're going to reshape it
wow what the heck does reshape mean
that means we have an array if you
remember correctly
so many numbers by 60.
that's how wide it is and so we're when
you when you do x train dot shape that
gets one of the shapes and you get um x
train dot shape of one gets the other
shape and we're just making sure the
data is formatted correctly and so you
use this to pull the fact that it's 60
by um
in this case where's that value
60 by
1199 1258 minus 60 11.99 and we're
making sure that that is shaped
correctly so the data is grouped into
11 99 by 60 different arrays and then
the one on the end just means at the end
because this when you're dealing with
shapes and numpy they look at this as
layers and so the end layer needs to be
one value that's like the leaf of a tree
where this is the branch and then it
branches out some more
and then you get the leaf np dot reshape
comes from and using the existing shapes
to form it we'll go ahead and run this
piece of code again there's no real
output and then we'll import our
different cross modules that we need so
from cross models we're going to import
the sequential model dealing with
sequential data we have our dense layers
we have actually three layers we're
going to bring in our dents our lstm
which is what we're focusing on and our
dropout and we'll discuss these three
layers more in just a moment but you do
need the with the lstm you do need the
dropout and then the final layer will be
the dents but let's go ahead and run
this and they'll bring port our modules
and you'll see we get an error on here
and if you read it closer it's not
actually an error it's a warning what
does this warning mean these things come
up all the time when you're working with
such cutting edge modules are complete
being updated all the time we're not
going to worry too much about the
warning all it's saying is that the h5py
module which is part of cross is going
to be updated at some point and if
you're running new stuff on cross and
you start updating your cross system you
better make sure that your h5 pi is
updated too otherwise you're going to
have an error later on but you can
actually just run an update on the h5 pi
now if you wanted to not a big deal
we're not going to worry about that
today and i said we were going to jump
in and start looking at what those
layers mean i meant that and we're going
to start off with initializing the rnn
and then we'll start adding those layers
in and you'll see that we have the lstm
and then the dropout lstm then dropout
lstm then dropout what the heck is that
doing so let's explore that we'll start
by initializing the rnn regressor equals
sequential because we're using the
sequential model and we'll run that and
load that up and then we're going to
start adding our lstm layer and some
dropout regularization and right there
should be the q dropout regularization
and if we go back here and remember our
exploding gradient well that's what
we're talking about the dropout drops
out unnecessary data so we're not just
shifting huge amounts of data through
the network so and so we go in here
let's just go ahead and add this in i'll
go ahead and run this and we had three
of them so let me go ahead and put all
three of them in and then we can go back
over them there's the second one and
let's put one more in let's put that in
and we'll go ahead and put two more in i
mean i said one more in but it's
actually two more in and then let's add
one more after that and as you can see
each time i run these they don't
actually have an output so let's take a
closer look and see what's going on here
so we're going to add our first lstm
layer in here we're going to have units
50. the units is the positive integer
and it's the dimensionality of the
output space this is what's going out
into the next layer so we might have 60
coming in but we have 50 going out we
have a return sequence because it is a
sequence data so we want to keep that
true and then you have to tell it what
shape it's in well we already know the
shape by just going in here and looking
at x train shape so input shape equals
the x train shape of one comma one it
makes it really easy you don't have to
remember all the numbers that put in 60
or whatever else is in there you just
let it tell the regressor what model to
use and so we follow our stm with a
dropout layer now understanding the
dropout layer is kind of exciting
because one of the things that happens
is we can over train our network that
means that our neural network will
memorize such specific data that it has
trouble predicting anything that's not
in that specific realm to fix for that
each time we run through the training
mode we're going to take point two or
twenty percent of our neurons and just
turn them off so we're only going to
train on the other ones and it's going
to be random that way each time we pass
through this we don't over train these
nodes come back in in the next training
cycle we randomly pick a different 20.
and finally i see a big difference as we
go from the first to the second and
third and fourth the first thing is we
don't have to input the shape because
the shapes already the output units is
50 here this item the next step
automatically knows this layer is
putting out 50 and because it's the next
layer it automatically sets that and
says oh 50 is coming out from our last
layer that's coming up you know goes
into the regressor and of course we have
our dropout and that's what's coming
into this one and so on and so on and so
the next three layers we don't have to
let it know what the shape is it
automatically understands that and we're
going to keep the units the same we're
still going to do 50 units it's still a
sequence coming through 50 units and a
sequence now the next piece of code is
what brings it all together let's go
ahead and take a look at that and we
come in here we put the output layer the
dense layer and if you remember up here
we had the three layers we had uh lstm
dropout and dents dents just says we're
going to bring this all down into one
output instead of putting out a sequence
we just know i want to know the answer
at this point and let's go ahead and run
that and so in here you notice all we're
doing is setting things up one step at a
time so far we've brought in our way up
here we brought in our data we bought in
our different modules we formatted the
data for training it we set it up you
know we have our white x train and our y
train we have our source of data and the
answers where we know so far that we're
going to put in there we've reshaped
that we've come in and built our cross
we've imported our different layers and
we have in here if you look we have what
uh five total layers now cross is a
little different than a lot of other
systems because a lot of other systems
put this all in one line and do it
automatic but they don't give you the
options of how those layers interface
and they don't give you the options of
how the data comes in cross is cutting
edge for this reason so even though
there's a lot of extra steps in building
the model this has a huge impact on the
output and what we can do with this
these new models from karass so we
brought in our dents we have our full
model put together a regressor so we
need to go ahead and compile it and then
we're going to go ahead and fit the data
we're going to compile the pieces so
they all come together and then we're
going to run our training data on there
and actually recreate our regressor so
it's ready to be used so let's go ahead
and compile that and i'd go ahead and
run that and if you've been looking at
any of our other tutorials on neural
networks you'll see we're going to use
the optimizer atom atom is optimized for
big data there's a couple other
optimizers out there beyond the scope of
this tutorial but certainly atom will
work pretty good for this and loss
equals mean squared value so when we're
training it this is what we want to base
the loss on how bad is our error well
we're going to use the mean squared
value for our error and the atom
optimizer for its differential equations
you don't have to know the math behind
them but certainly it helps to know what
they're doing and where they fit into
the bigger models and then finally we're
going to do our fit fitting the rn into
the training set we have the
regressor.fit x train y train epochs and
batch size so we know where this is this
is our data coming in for the x train
our y train is the answer we're looking
for of our data our sequential input
epics is how many times we're going to
go over the whole data set we created a
whole data set of x trains so this is
each each of those rows which includes a
time sequence of 60. and bad size
another one of those things where cross
really shines is if you were pulling the
save from a large file instead of trying
to load it all into ram it can now pick
smaller batches up and load those
indirectly we're not worried about
pulling them off a file today because
this isn't big enough to cause a
computer too much of a problem to run
not too straining on the resources but
as we run this you can imagine what
happened if i was doing a lot more than
just one column in one set of stock in
this case google stock imagine if i was
doing this across all the stocks and i
had instead of just the open i had open
close high low and you can actually find
yourself with about 13 different
variables times 60 because there's a
time sequence suddenly you find yourself
with a gig of memory you're loading into
your ram which will just completely you
know if it's just if you're not on
multiple computers or cluster you're
gonna start running into resource
problems but for this we don't have to
worry about that so let's go ahead and
run this and this will actually take a
little bit on my computer it's an older
laptop and give it a second to kick in
there there we go all right so we have
epic so this is going to tell me it's
running the first run through all the
data and as it's going through it's
batching them in 32 pieces so 32 lines
each time and there's 1198 i think i
said 11.99 earlier but it's 11.98 i was
off by one and each one of these is 13
seconds so you can imagine this is
roughly 20 to 30 minutes run time on
this computer like i said it's an older
laptop running at 0.9 gigahertz on a
dual processor and that's fine what
we'll do is i'll go ahead and stop go
get a drink of coffee and come back and
let's see what happens at the end and
where this takes us and like any good
cooking show
i've kind of gotten my latte i also have
some other stuff running in the
background so you'll see these numbers
jumped up to like 19 seconds 15 seconds
but you can scroll through and you can
see we've run it through 100 steps or
100 epics so the question is what does
all this mean one of the first things
you'll notice is that our loss can is
over here and it kind of stopped at
0.0014 but you can see it kind of goes
down until we hit about 0.014 three
times in a row so we guessed our epic
pretty close since our losses remain the
same on there so to find out we're
looking at we're going to go ahead and
load up our test data the test data that
we didn't process yet and a real stock
price data set test eye location this is
the same thing we did it when we prep
the data in the first place so let's go
ahead and go through this code and we
can see we've labeled it part three
making the predictions and visualizing
the results so the first thing that we
need to do is go ahead and read the data
in from our test csv you see i've
changed the path on it for my computer
and then we'll call it the real stock
price and again we're doing just the one
column here and the values from
ilocation so it's all the rows and just
the values from these that one location
that's the open stock open let's go
ahead and run that so that's loaded in
there and then let's go ahead and create
we have our inputs we're going to create
inputs here and this should all look
familiar because this is the same thing
we did before we're going to take our
data set total we're going to do a
little panda concat from the datastate
train now remember the end of the data
set train is part of the data going in
let's just visualize that just a little
bit here's our train data let me just
put tr for train and it went up to this
value here but each one of these values
generated a bunch of columns it was 60
across and this value here equals this
one and this value here equals this one
and this value here equals this one and
so we need these top 60 to go into our
new data so to find out we're looking at
we're going to go ahead and load up our
test data the test data that we didn't
process yet and a real stock price data
set test eye location this is the same
thing we did when we prepped the data in
the first place so let's go ahead and go
through this code and we can see we've
labeled it part three making the
predictions and visualizing the results
so the first thing that we need to do is
go ahead and read the data in from our
test csv you see i've changed the path
on it for my computer and then we'll
call it the real stock price and again
we're doing just the one column here and
the values from i location so it's all
the rows and just the values from these
that one location that's the open stock
open let's go ahead and run that so
that's loaded in there and then let's go
ahead and create we have our inputs
we're going to create inputs here and
this should all look familiar this is
the same thing we did before we're going
to take our data set total we're going
to do a little panda concat from the
datastate train now remember the end of
the dataset train is part of the data
going in let's just visualize that just
a little bit here's our train data let
me just put tr for train and it went up
to this value here but each one of these
values generated a bunch of columns it
was 60 across and this value here equals
this one and this value here equals this
one and this value here equals this one
and so we need these top 60 to go into
our new data because that's part of the
next data or it's actually the top 59.
so that's what this first setup is over
here is we're going in we're doing the
real stock price and we're going to just
take the data set test and we're going
to load that in and then the real stock
price is our data test.test location so
we're just looking at that first column
the open price and then our data set
total we're going to take pandas and
we're going to concat and we're going to
take our data set train for the open and
our dataset test open and this is one
way you can reference these columns
we've referenced them a couple different
ways we've referenced them up here with
the one two but we know it's labeled as
a panda set as open so pandas is great
that way lots of versatility there and
we'll go ahead and go back up here and
run this there we go and you'll notice
this is the same as what we did before
we have our open data set where pended
our two different or concatenated our
two data sets together we have our
inputs equals data set total length data
set total minus length data set minus
test minus 60 values so we're going to
run this over all of them and you'll see
why this works because normally when
you're running your test set versus your
training set you run them completely
separate but when we graph this you'll
see that we're just going to be we'll be
looking at the part that we didn't train
it with to see how well it graphs and we
have our inputs equals inputs dot uh
reshapes or reshaping like we did before
we're transforming our inputs so if you
remember from the transform between zero
and one and uh finally we want to go
ahead and take our x test and we're
going to create that x test and for i in
range 60 to 80. so here's our x test and
we're appending our inputs i to 60 which
remember is 0 to 59 and i comma 0 on the
other side so it's just the first column
which is our open column and once again
we take our x test we convert it to a
numpy array we do the same reshape we
did before and then we get down to the
final two lines and here we have
something new right here on these last
two lines let me just highlight those or
or mark them predicted stock price
equals regressor dot predicts x test so
we're predicting all the stock including
both the training and the testing model
here and then we want to take this
prediction and we want to inverse the
transform so remember we put them
between zero and one well that's not
going to mean very much to me to look at
a float number between zero and one i
want the dollar amount so i want to know
what the cash value is and we'll go
ahead and run this and you'll see it
runs much quicker than the training
that's what's so wonderful about these
neural networks once you put them
together it takes just a second to run
the same neural network that took us
what a half hour to train add and plot
the data we're going to plot what we
think it's going to be and we're going
to plot it against the real data what
the google stock actually did so let's
go ahead and take a look at that in code
and let's uh pull this code up so we
have our plt that's our oh if you
remember from the very beginning let me
just go back up to the top we have our
matplotlibrary.piplot as plt that's
where that comes in and we come down
here we're going to plot let me get my
drawing thing out again we're going to
go ahead and plt is basically kind of
like an object it's one of the things
that always through me when i'm doing
graphs in python because i always think
you have to create an object and then it
loads that class in there well in this
case plt is like a canvas you're putting
stuff on so if you've done html5 you'll
have the canvas object this is the
canvas so we're going to plot the real
stock price that's what it actually is
and we're going to give that color red
so it's going to be in bright red we're
going to label it real google stock
price and then we're going to do our
predicted stock and we're going to do it
in blue and it's going to be labeled
predicted and we'll give it a title
because it's always nice to give a title
to your graph especially if you're going
to present this to somebody you know to
your shareholders in the office and the
x label is going to be time because it's
a time series and we didn't actually put
the actual date and times on here but
that's fine we just know they're
incremented by time and then of course
the y label is the actual stock price
plt.legend tells us to build the legend
on here so that the color red and real
google stock price show up on there and
then the plot shows us that actual graph
so let's go ahead and run this and see
what that looks like and you can see
here we have a nice graph and let's talk
just a little bit about this graph
before we wrap it up here's our legend i
was telling you about that's why we have
the legend to show the prices we have
our title and everything and you'll
notice on the bottom we have a time
sequence we didn't put the actual time
in here now we could have we could have
gone ahead and
plotted the x since we know what the
dates are and plotted this to dates but
we also know that it's only the last
piece of data that we're looking at so
last piece of data which in somewhere
probably around here on the graph i
think it's like about 20 of the data
probably less than that we have the
google price and the google price has
this little up jump and then down and
you'll see that the actual google
instead of a turn down here just didn't
go up as high and didn't load go down so
our prediction has the same pattern but
the overall value is pretty far off as
far as stock but then again we're only
looking at one column we're only looking
at the open price we're not looking at
how many volumes were traded like i was
pointing out earlier we talk about stock
just right off the bat there's six
columns there's open high low close
volume then there's weather i mean
volume shares then there's the adjusted
open adjusted high adjusted low adjusted
close they have a special formula to
predict exactly what it would really be
worth based on the value of the stock
and then from there there's all kinds of
other stuff you can put in here so we're
only looking at one small aspect the
opening price of the stock and as you
can see here we did a pretty good job
this curve follows the curve pretty well
it has like a little jumps on it bins
they don't quite match up so this bin
here does not quite match up with that
bin there but it's pretty darn close we
have the basic shape of it and the
prediction isn't too far off and you can
imagine that as we add more data in and
look at different aspects in the
specific domain of stock we should be
able to get a better representation each
time we drill in deeper of course this
took a half hour for my program my
computer to train so you can imagine
that if i was running it across all
those different variables it might take
a little bit longer to train the data
not so good for doing a quick tutorial
like this so we covered a lot of really
cool things in this tutorial today
hopefully you'll be able to get rich
predicting stock or maybe get a job if
you are familiar with the stock domain
and business domain but you can see how
this can be applied to all kinds of
different tools and trades across
different domains so the biggest
takeaways are as we had an introduction
to the rnn and it's set up on there we
went over the popular neural networks
that are out there we discussed what is
a recurrent neural network we went into
one of the big problems with the rnn and
that's the exploding gradient problem we
also discussed the long short term
memory networks which is part of what
we're working on that's the the lst
which is part of the rnn setup and
finally we went through and we predicted
the google stock to see how it did so i
want to thank you for joining us today
again my name is richard kirchner one of
the simply learn team that's
www.simplylearn.com for more information
please visit our website feel free to
ask any questions and check out our
different courses we have you can also
place comments below in the youtube
video and we will keep you monitoring
those and try to address those again
thank you for joining us today
hi there if you like this video
subscribe to the simply learn youtube
channel and click here to watch similar
videos turn it up and get certified
click here