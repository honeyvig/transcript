hello everyone welcome to this video
tutorial on data science with our 2022
full course by simply loan
r is one of the best programming
languages to learn data science
this video covers all the crucial data
science concepts and algorithms that
will help you master data science using
r
before we look at the topics we'll be
covering in this video let's understand
the importance of using r for data
science
data science is a field of study that
has its usage in all domains to extract
meaningful insights from large volumes
of data in order to make business
decisions
r is a powerful programming language
that has excellent statistical and
visualization capabilities
r is an open source language so you can
contribute to r make new libraries for
different functionalities edit the code
and add modifications to it
it supports amazing packages such as
deep lyer tidier tideverse and h2o that
make your life easier as a data
scientist
statistical models can be written with
only a few lines of code using r
you can also make interactive web
applications in r using this signy
package
visualizing and presenting data in an
elegant manner is very important in
today's business savvy world
r is a powerhouse when it comes to
creating interesting graphs and visuals
our packages like ggplot plotly and
ggvis allow you to create detailed and
attractive visualizations
it also has a growing community of data
scientists and statisticians
another reason why r is so popular for
data science and analytics is because
some of the top companies across the
world use r to analyze manipulate and
visualize data as well as create
predictive models
facebook microsoft google jpmorgan ibm
and uber are some of the top companies
using r for data science now that you
have learnt about the importance of
using r for data science let's look at
the agenda for today's session
so we will start by getting an
introduction to data science after that
we'll understand how to import data in r
next we will learn about data
manipulation using dplyr and tidier
packages
after that
we learn to visualize data using ggplot
and plotly
moving further we learn about the
various algorithms used by data
scientists such as linear regression
logistic regression decision tree as
well as random forest algorithm in r
we will also learn about support vector
machine and
hierarchical clustering algorithms
after which we'll look at time series
analysis
we will end our 2022 full course video
by understanding who is a data science
engineer and look at the data science
engineer resume so let's get started
are you one of the many who dreams of
becoming a data scientist keep watching
this video if you're passionate about
data science because we will tell you
how does it really work under the hood
emma is a data scientist let's see how a
day in her life goes while she's working
on a data science project well it is
very important to understand the
business problem first in our meeting
with the clients emma asks relevant
questions understands and defines
objectives for the problem that needs to
be tackled she is a curious soul who
asks a lot of eyes one of the many
traits of a good data scientist
now she gears up for data acquisition to
gather and scrape data from multiple
sources like web servers logs databases
apis and online repositories oh it seems
like finding the right data takes both
time and effort after the data is
gathered comes data preparation this
step involves data cleaning and data
transformation data cleaning is the most
time consuming process as it involves
handling many complex scenarios here
emma deals with inconsistent data types
misspelled attributes missing values
duplicate values and what not then in
data transformation she modifies the
data based on defined mapping rules in a
project etl tools like talent and
informatica are used to perform complex
transformations that helps the team to
understand the data structure better
then understanding what you actually can
do with your data is very crucial for
that emma does exploratory data analysis
with the help of eda she defines and
refines the selection of feature
variables that will be used in the model
development but what if emma skips this
step she might end up choosing the wrong
variables which will produce an
inaccurate model thus exploratory data
analysis becomes the most important step
now she proceeds to the core activity of
a data science project such as data
modelling she repetitively applies
diverse machine learning techniques like
canon decision tree knife base to the
data to identify the model that best
fits the business requirement she trains
the models on the training data set and
test them to select the best performing
model emma prefers python for modeling
the data however it can also be done
using r and sas well the trickiest part
is not yet over visualization and
communication emma meets the clients
again to communicate the business
findings in a simple and effective
manner to convince the stakeholders she
uses tools like tableau power bi and
qlikview that can help her in creating
powerful reports and dashboards and then
finally she deploys and maintains the
model she tests the selected model in a
pre-production environment before
deploying it in the production
environment which is the best practice
right after successfully deploying it
she uses reports and dashboards to get
real-time analytics further she also
monitors and maintains the project's
performance well that's how emma
completes the data science project we
have seen the daily routine of a data
scientist is a whole lot of fun has a
lot of interesting aspects and comes
with its own share of challenges now
let's see how data science is changing
the world data science techniques along
with genomic data provides a deeper
understanding of genetic issues in
reaction to particular drugs and
diseases logistic companies like dhl
fedex have discovered the best rules to
ship the best suited time to deliver the
best mode of transport to choose thus
leading to cost efficiency with data
science it is possible to not only
predict employee attrition but to also
understand the key variables that
influence employee turnover also the
airline companies can now easily predict
flight delay and notify the passengers
beforehand to enhance their travel
experience well if you're wondering
there are various roles offered to a
data scientist like data analyst machine
learning engineer deep learning engineer
data engineer and of course data
scientist the median base salaries of a
data scientist can range from 95 000 to
165 000
so that was about the data science are
you ready to be a data scientist if yes
then start today the world of data needs
you that's all from my side today thank
you for watching comment below the next
topic that you want to learn and
subscribe to simply learn to get the
latest updates on more such interesting
videos data science with r sponsored by
simplylearn that's www.simplylearn.com
get certified get ahead my name is
richard kirschner and i'm with the
simply learn team what's in it for you
we're going to go through an
introduction to our yr
cran comprehensive r archive network and
cover installing r then we'll get into
simple linear regression using r line of
best fit using error summation
correlation analysis in r and then we'll
get into classification using r use case
predict the class of a flower let's
start with an introduction to r
first it's an open source r is
completely free and open source with
active community members
extensible it offers various statistical
and graphical techniques compatible r is
compatible across all platforms linux
windows and mac library r has an
extensive library of packages for
machine learning easy integration it can
be easily integrated with popular
softwares like tableau sql server etc
this only touches a little bit on these
different aspects of r the fact that
it's a free it's very extensive it
probably has one of the most extensive
set of data analysis packages currently
out it has a compatibility is
continually growing so it's integrated
with everything from cluster computing
to python integration which is now
coming out and its extension of
libraries allows you to import the
different libraries to use for whatever
your needs are makes it a very diverse
and easy to use coding source for
analyzing data r is more than just a
programming language as i just touched
upon it it has a worldwide repository
system comprehensive r archive network
and it can be accessed at https colon
slash
cran.rproject.org
it provides up-to-date versions of code
and documentation for r cran hosts
around 10 000 packages of r that is a
huge repository focused on just data
analytics let's install r to get started
you can easily download the executable
file for r and install it from cran
website if you go under the downloads in
this case since i'm on a windows machine
we'll walk through it from the r 3.5
version for windows
so here we are at https colon slash
cran dot r dash project dot org and you
can see down here we have the different
options for the downloads if we go under
windows which is i'm on a windows
machine if you're on a mac machine you
do the mac or linux there's install r
for the first time just click on that
and open and run it the installation is
pretty simple follow the default options
to finish the installation after the
installation is complete you'll see the
r icon on your desktop alternatively
there are mirror sites and so we can go
into our studio let's just take a quick
look at that if you're on the r studio
website that's www.rstudio.com
you can go down under products and
rstudio download you'll see a number of
options here the first one is the
rstudio desktop open source license
that's the same thing you just
downloaded so if you download that
that's what you're getting there's also
an open source server setup which is
also free you can download from here and
then rstudio offers a number of paid
packages which includes all kinds of
different supports for a company and
individual level setup whether you go
through the cran site or the rstudio
site the installation is pretty simple
just a quick note if you're a debian
distribution including ubuntu you can
install r using as regular package
management tools and when you're using a
linux system that is preferred because
then it registers it properly on the
setup system let's go ahead and open up
our rstudio and take a look here we are
and i've opened this up in the r studio
version which automatically opens up
some extra windows which is nice let's
go and take a look at that we have our
console on the left this is your main
workspace so if i go in here and i do
click the mouse and i do a four plus
four i'll come up and say the answer is
eight and you have some environmental
information and over on the right you
have plots usually when you're working
in here let's do a script we'll go up
here into the plus sign in the upper
left-hand corner and just add some
script in here in this case it showed up
on the top which you can move these
windows around unless i do y
equals 3
plus 4
x equals y plus 2 and then i'm going to
do just x and let's do a plot and throw
a plot in there c
sees a notation that these are going to
be cartesian points so we got 1 comma 2
comma 3. i'd be like your x and then y
three comma four comma five for just a
standard call scatter plot um you don't
have to memorize this we'll go into some
of this later on and then i can take all
of this if i go under code since i'm
working in the code console and i go
down to
run region and just run all it takes
this code and just runs it through my
console and you can see down here it's
executed y equals three plus four x
equals x plus two and if you add three
plus four plus two you get nine that's
what the x does and then i did a plot i
threw the plot in there where i'm
plotting one two three three four five
and if you use just a straight plot it's
a scatter plot and you can see that this
appears on the bottom right where you
have your plots coming in so it's a very
quick way to show data and this one the
wonderful things about r is it's very
easy and quick to go through
and do different functions on the data
and analyze it so it's a very popular
package before you start programming an
arm you should install packages and its
dependencies packages provide
pre-assembled collections of functions
and objects each package is hosted on
the cran repository not all packages are
loaded by default but they can be
installed on demand remember earlier
we're talking about all the different
packages available you don't want to
install everything from r it would just
be a huge waste of space you want to
just install those packages you need so
to install packages in rstudio you go
under tools and install packages when
you click on the install packages you'll
get a dialog box you'll see where as a
repository cran because there's other
repositories and you can even download
and install your own packages you can
build and then you'll pick out the
packages you want and separate multiple
spaces or commas so you can install
numerous packages at the same time in
this case we've
installing the forecast forecast package
and you can see down here i just type in
forecast all the install all the
dependencies uses other packages to
build on it has those then just click on
install and it's done before we cover
the basic linear regression model let's
just take a quick look at some of the
different parts of our or parts of
programming you'll need or scripting
first you know there are various data
structures in r we have vectors we have
matrixes we have arrays data frames and
lists
vectors is the most basic data structure
if you remember vectors are a location
and a direction it's how they're
generally defined although a vectors
you're talking about scripting can
contain numerous different values you
could have vectors with four five six
seven eight different values in it for
example a picture on the computer might
have the location of the pixel and the
number for the red queue and the green
hue and the blue hue so now you have
something with five different numbers in
it consisting of that vector matrixes
allow you to move stuff around so you
might have a two by three matrix that
you switch to a three by two we looked
at an xy plot earlier it might be that
you have everything uh is you have 10
different numbers and each one has two
values x y and you need to switch that
matrix so then you have two arrays of
five numbers arrays are just at a
collection and you can have arrays of
arrays of arrays data frames have labels
on them which makes them easier to use
so usually we use a lot of data frames
when we're working with data because
they're just easy you can have a column
and you can have a row so think of rows
and columns when you see the term data
frames and then lists are usually
homogeneous groups so an r
you're usually looking at similar data
that's connected in the list so the
first thing we do is before we even
importing the data you should have the
data ready so we need to look at the
data and see what's going on you can
import data from various sources
including excel minitab csv table text
files
csv is usually a text file comma
separated variables tabs separated
variables there's all kinds of different
options here and importing the table is
very simple we have table read table
file equals data table enter equals true
and let's just take a closer look at
what we're talking about here scratch
the closer look i didn't realize it was
just for example so importing a table
file is pretty simple you read.table
file equals data.table so whatever the
name of the file is comma header equals
true so if there's a header to it in
this quick flash they have name age
gender we'll actually do this in r with
another data set in just a minute but
you can see it's very easy to read a lot
of people use are even in other
programming languages so they can
quickly look at the data before they
even start analyzing it csev file same
thing read.csv file equals data csv
header equals true separation equals
space so in this case even though csv
stands for comma separated variable this
one is separated by spaces and they have
the header name age gender so it's the
same file saved as a csv file and
there's also excel um excel has its own
issues as far as making sure you know
what the tables are and the headers are
but you can see that each one of these
is easy to import so and just like it's
easy to import the data you can also
export tables in r so you can see here
uh right dot table my file see my file
dot text comma separated and the scoop t
just means it's tab separated so if
you're using tabbed files on there
example like excel so you can write a
dot xls to my file in this case it is a
text separation equals scoop t so it's a
tab separated for excel file csv same
thing very easy to write a csv file to
your computer once you've changed the
data or alternate depending on what
you're doing with it and once we have
our data imported and we can save it
afterwards and export it graphing
visualization in r is very powerful and
it's quick love doing this before even
exploring the data sometimes you just
want to graph it to see what it is there
so you have an idea of what you're
looking for so graphics and r cover a
huge amount of different things r
includes powerful package of graphics
that help in data visualization these
graphics can be viewed on screen saved
in various formats including pdf png
jpeg wmf and ps can be customized
according to very graphic needs you can
copy and paste in word or powerpoint
files r supports various types of
graphics including bar chart pie chart
histogram kernel density plots line
chart box plot heat map word cloud
there's even some more obscure ones but
these are the main ones that most people
use i know i use a lot of heat maps but
word clouds are a lot of fun if you're
doing websites and data and word
analysis and histogram is very popular
all of these are very widely used let's
look at the box plots also known as
whisker diagrams box plots display the
distribution of data based on minimum
first quartile medium third quartile and
maximum so right off the bat we can use
a box plot to explore our data with very
little work to create a box plot we
simply give box plot and the data very
straightforward so we might have
passenger numbers in the thousands i
guess this is exploring data dealing
with airplanes and you can see here they
just have a simple plot if you break it
down you have your if we go back one
you'll notice we have like a minimum a
maximum our medium first quartile and
third quartile let's just break that
apart you can see the line on the bottom
is your minimum your line on the top is
the maximum you have your medium and
your first quartile right there first
quartile and third corridor and the way
a lot of times you read this is things
that are above the box are outliers and
things below the box or outliers and
what's in the middle of the box is use
the data you're looking at welcome to
today's video tutorial on importing data
in r by simply loan in this video we
will learn how to read data from various
file types such as text file csv excel
file as well as a sas 7b data file that
stands for sas version 7 binary data
file
we will learn the important packages you
need to install before importing the
data sets we will also see how to write
data to an excel file
using the r programming language
so let's get started with our demo first
and foremost we need to install the
important packages
the first package we have is our java
you can see it here
so the r java is a low level r to java
interface it allows creation of objects
calling methods and accessing fields
then we have
xlsx jars
that collects all the external jars
required for the excel xx package
it is used for importing excel files and
then we have the
excel
sx package that provides our functions
to read write and format excel data now
let's load the packages to the library
so first of all you need to install all
the packages now i have already
installed all the packages so before a
package can be used it has to be loaded
to the current r environment
you also need to load a package that is
already installed previously but not
available in the current environment
now let me just
load the packages using the library
function i'll hit ctrl enter to
run these packages
okay
now to get the list of all the packages
installed you just write the library
function with parenthesis
if i
run this line
it will open in a new window saying our
package is available and these are the
packages that are already installed
let me close this window
okay
so let's get started with importing a
text file onto our now we will import a
text file that has some credit data let
me show you the file first
here
i have a folder on my desktop that says
data files and here you can see i have a
text file called credit if i open that
it is a text file that has some
information now we are going to import
this information on to r so let's see
how to do it
i'll go to my r studio
okay
now to read a text file
we use the read.table function and
passing the location of the file
so let me show you how to do it i'll
write
credit underscore data which is going to
be my variable name that will store the
file information then i am going to use
read
dot
table function
and inside this function i am going to
pass in the location of the file
followed by the file name and the
extension of the file
so
i'll go to my location
which is this folder i'll just
copy the location
and i'll paste the location here
make sure these are all
double backslash
and it should be within quotes
and now
with double backslash i'll give the file
name which is
credit
so i'll write
credit
dot txt which is the extension of the
file and close the double quotes let's
run this
all right
if you see the result we have
successfully
loaded our
text data to this variable called credit
underscore data
now to
view the
data you can use the view function
and i'll pass in the variable which is
credit underscore data
let's run this using control enter
this will open in a
new tab you can see i have
5 columns from v1 to v5 and there are
total 10 rows of information
now you can see that the rows and
columns are ok but
the column name
has no meaning at all so let's assign
some proper names to the columns and
convert it into a data frame
let me just scroll down
so here
i'll
write my
variable name that is credit underscore
data
and then
i'll create a data frame using data dot
frame function
and inside this i am going to write my
column names
so the first column will be the id
column and the id column i am going to
get from the same variable which is
credit underscore data and this time i
am going to slice the
result
so i will give a comma which means i
need all the rows and the
first column
that is going to be my id so if i go to
my
table here
i'm going to consider the first column
instead of v1 i'm going to assign the
column name as id
okay
now
i'll give a comma and then
we'll write the second column name as
name which is the name of the person
and i'm going to use the paste function
so that
we can
merge
these two columns v2 and v3
so the way to do is i'll give my paste
function
followed by
i'll give the
variable name that is credit
and this will have
all the rows from the
second column
i'll give a comma and then i'll write
credit underscore data and i'll merge it
or paste it
with all the rows from the third column
i'll give another comma let me come to
the next line
okay
now
the third column i'm going to create is
called type
i'll write credit
underscore data
i'll slice it
all the rows
from the
fourth column which is b4 here
let's give another comma
and my final column is going to be
let's say transaction
i'll give equal to
then write credit underscore data it
will have all the information for all
the rows from the fifth column or v5
let's run this
you can hit the run button here
okay
now let me just display the data again
i'll write credit
underscore
data again and run it
there you go let me just expand this
you have
the columns renamed the first column is
id column then we have the name then we
have type and last column we created was
transaction that has
dollar values or some
currency values
okay
now
moving ahead
to the next example where we will read a
csv file so csv file has comma separated
values
so we are going to import a salary csv
file let me show you the salary file
first
so on my data files folder i have a
csv file called salaries let me open it
okay so this is my
salary file which has information about
id the employee name job title we have
bsp over time pay other pay benefits
total pay we have the agency
and other information so we are going to
import the csp file data onto r now
let's do it
now in order to
read a csv file we are going to use the
function that is read.csv
so i will first create a variable called
salary
that will have
my csv file stored in it and the
function i am going to use is read.csv
and again
inside this i'm going to pass my file
location let me copy the file location
from the top which is here
i'll just copy this location and i'll
change the
file name and the extension of the file
let's copy this
scroll down and i'll
paste it here
okay we'll give a double quote here as
well
and instead of credit dot txt i'm going
to write
salaries dot csv
since it is a csv file
all right
now let's run this
okay it has successfully
loaded our csv file onto this variable
called salary
now let me just display the head of the
salary
variable
now the head function displays the
top
six rows
from your table you can see here we have
the top six rows from our table
which is this
okay
now if you want to view it you can use
the view function
so if i write
view
followed by the variable name which is
salary i'll run it this will open in a
new window you can see it
i'll close the credit underscore data
window
now you can view the entire file here
we have the id the employee name job
title base p
and if i scroll down you can see all the
rows loaded
now here it tells you the total number
of entries and the total number of
columns
all right
now moving ahead you can also check the
structure of the csv data set and the
function is str so the str function will
help you see the structure of the data
set which is the internal structure
if i run this
let's expand the output window
and you have information about all the
columns you see here id column is of
type integer these are the values we
have the employee name as character
column and these are some of the
employee names
we have the job title benefits total pay
these are the data types
okay
now
if you want you can check the column
names as well using the column or the
call names function
if i write call names and i'll pass in
my variable that is salary
let's give a space here
i'll scroll down a bit and hit enter
there you go you can see all the column
names so there are total 13 columns id
employee name job title agency total pay
and others
cool
now
let's go ahead and read data from an
excel file that has pokemon data
so let me first show you the pokemon
excel file so here in my data files
folder we have a file called
pokemon which is an excel file let me
just open it
okay you can see here there are total
four sheets in it one is pokemon then we
have moves evolution and type chart and
there are certain specific information
present in
these sheets so we are going to import
all this data on to r now
so for reading and writing data
using
an excel file
you need to install the packages read
excel and write excel i've already
installed the packages let me just run
the
library commands so that i have these
packages loaded to my library
and let me run library write excel as
well
okay
next if you want to
display all the excel sheet names in the
workbook
you can do this using the excel
underscore sheets function and pass in
the location of the file followed by the
file name
and the
type of file
so i'll give my function name as
excel underscore
sheets
and within this i'm going to pass in my
location let me just copy from the top
i'll just copy this location and we'll
change the file name and the type of
file or the extension of the file
so here i'm going to paste the location
and instead of salaries.csv
i'm going to give
pokemon
dot
x
l s x which is for an excel file i'll go
ahead and run this line
there you go if you see the output here
we have the different sheet names
printed the first sheet was pokemon then
we have moves evolution and chart type
all right
now to read data from the pokemon excel
file we'll use the read underscore excel
function
so let's read
data from one of the sheets let's say i
want to read data from
moves sheet let me show you how to do it
i'll create a variable called df
this will store my
moves sheet data i'll use the function
read underscore excel
and then
within double quotes i'll give my
location let me just paste it here
all right and then instead of
salary start csv i'll write
pokemon
dot xlsx
which is
for an excel file
and then i'm going to give
another parameter that is the sheet name
i'll give sheet equal to
and within single quotes i'll pass in my
sheet name that is moves
let's run it
okay we have
imported the move sheet data to r now
now let's say
we'll print the
head of the data set
there you go so we have printed the
first six rows if i
maximize this you can see the first six
rows from the moves
sheet is printed now
and you can also
view this data using the view function
i'll run it
this is how the
entire data set looks like
close it
okay now if you want you can also
perform certain operations
on this
data set let's say i want to
sum the power column
so there is a column here which is
called power if you can see this we have
a column called power i want to find the
total power
so i'll use the sum function
inside the sum function i'll give my
variable name which is df and then
i'll write
power
and then i'll give a comma
i'm going to omit or remove all the any
values that are present so that there
are no discrepancies while calculating
the sum i'll say n a dot rm which is to
remove dna values equal to t which means
true
let's run it and check the sum this is
the sum you can see it here 25 460.
you can also print the
structure of the data frame
by using
sdr function
and then
we'll give the variable name that is df
if i expand this you can see the
structure you have all the data types
and their columns
also if you want some summary statistics
you can use the summary function that
will return the
statistical summary of all the columns
if i write summary
and within brackets i'll give my
data frame name that is tf and run it
you see it returns us
some
very significant
statistical information regarding
minimum value
the first quadrant value median mean you
have the third quartile value the
maximum value
and other information
okay now moving ahead
now so far we have seen operations on
reading data from text csv and excel
files now let's see how to write data to
an excel file
now to
write data to an excel file
i'll first create a variable called df
and then i'll say
data dot
frame
and
i'm going to create a matrix
and convert that matrix into a data
frame
so my matrix will have values from 1 to
50
and then i'm going to give my second
parameter which is
n row basically i am specifying the
number of rows that i want lets say my
matrix would have ten rows and then i
will give
the third parameter which says n
call which means
n column equal to 5
so this is how you create a matrix and
convert it into a data frame let me just
go ahead and print the data frame now
okay so first i'll run my
matrix and then let's print it
okay let me expand this so this is my
matrix that has
10 rows and 5 columns from x1 to x5
all
right now
we need to
write the information of this matrix
that we created onto an excel file for
that
we are going to use the
write
underscore
excel sx function
within this function i am going to pass
in my data frame name that is df
and then
i am going to give the location of the
file
let me paste it here
and instead of
salaries.csv i'll give a
new file name which will be output
and it will be of type
excel
sx
so what this command will do is
it is going to write all the information
present inside the data frame
into a new excel file called output and
it will store it in the location which i
have passed here
now let me just show you here now
currently we don't have any output excel
file here once i
run this statement it will create an
output file
let's run it and see the difference
okay so i've created my output file let
me check the data files here
there you go you can see here we have an
output excel file if i open this
you will find it has the same matrix
that i created using r
with the columns x12
x5 and the 10 rows so it has values from
1 to 50.
let me close it
okay now you can also
read this using the read underscore
excel function
will pass in the
location and i'll change this from
salaries.csv to
output
dot xlsx
make sure to have the double quotations
now
let's run it
there you go you can see here even in
our r output you can see we have the
matrix ready
all right let me just minimize this
now finally
we will do
a demo on reading a sas file
so if i show my data files folder we
have a sas file you can see here sas 7b
dat file that has information regarding
movies so we are going to import this
sas file onto r now
so i'll go to my r studio let's scroll
this and i'll hit enter all right
now
to read a sas file you need to
install a package
so i'll write install dot packages
that is sas 7 bdad
this stands for
sas version 7 binary data file
and then you are going to call the
library function that will have the
sas 7
be that package now i've already
installed this package so you need to
install it first now i'm going to just
call this package
okay
now to
read a sas file
i'll create a
variable called movies that will store
the information
and then i'll use the function
read
dot sas 7 b dat
and then
within double quotes i'm going to pass
the
location
it's the same location i'm only going to
change the file name and the extension
of the file so my file name is movies
and the extension or the type of file is
sas7
bdat
let me run this
okay so we have successfully imported
the sas file onto r now
now to check whether it was
ok or not you can use the view function
and pass in the variable name that is
movies if i hit ctrl enter
there you go so we have successfully
imported our
movies data set which was of type sas 7b
that you can see we have the movie name
the type of movie which is basically the
genre
rating the year in which the movie was
released
and then you have information about the
numbers it got and the director if i
scroll down you can see there are total
277 entries and there are total
seven columns in it
all right so let me now scroll to the
top and show you what we did in this
demo
so first we installed all the important
packages that were necessary for
importing the different
data sets or reading the data sets and
writing the
data sets onto r
so first we saw how to read a text file
using the
read dot table
and then we
assigned
names to the columns using
id name type and transaction
you also saw how to use the paste
function in
r
then the next example we saw was to read
a csv file using
read.csv function wherein we saw how you
could give the file location and the
file name followed by the extension of
the file
we saw how to use the head function and
the
view function
we checked the structure of the csp data
set we looked at the
unique columns present
then we
moved ahead and saw how to read data
from excel files
then we installed
two packages namely read excel and write
excel
then we displayed all the sheets present
in our excel file finally we saw how to
write to excel files wherein we created
a matrix and converted that into a data
frame and that matrix we
route it to an excel file which was
output dot xls
and in the end we saw how to read a
sas 7 beta file
so that brings us to the end of this
video on importing data in r
let's learn about data manipulation in r
and here we will learn about
d player package
and when we talk about this d player
package it is much faster and much
easier to read than base r so d player
package is used to transform and
summarize tabular data with rows and
columns you might be working on a data
frame or you might be getting in a
inbuilt r data set which can then be
converted into a data frame so we can
get this package the plier by just
calling in library function
and this can be used for grouping by
data summarizing the data adding new
variables selecting different set of
columns filtering our data sets sorting
it selecting it arranging it or even
mutating that is basically creating new
columns using functions
on existing variables so let's see how
we work with d player now here
i can basically get the package here so
i can just say
install dot packages d plier now we
already see the the package here which
is showing up so i will just select this
one i can do a control enter and that
will basically set up the package
package d player successfully unpacked
so that is done now you can start using
this package by just doing a library d
plier and this was built it shows me my
version of our so let's also use a
inbuilt data set that is new york
flights 13 so we can do install.packages
and that will search and get that
relevant data set
i can again call it by using library
function now once that is done we can
look at some sample data here by just
doing view flights and that shows me the
data in a neat and a tabular format
which shows me year month day
departure time schedule departure time
and so on
now we can also do a head to look at
some initial data
which can help us in understanding the
data better
so what is this data about how many
columns we have what are the data types
or object types here it shows me how
many variables we have
so this is fine now we can start using
deployer and
in that we can use
say filter function if we would want
to look in for specific value now here
we have the column as month so i will do
a filter now i'm creating a variable f1
i'm using the filter function
on flights
which we already have
and then what we can do is we can
basically
look at the month where the month value
is 0 7
so let's look at that
and this one
you can do a view on f1 which shows me
the data wherein you have filtered out
all the data based on month being seven
so this is a simple usage of filter we
can take some other example we may want
to include multiple columns so we can
say f2 filter
flights and here we will say month
is equal to 7 day is 3
and then look at the value of f2 if you
are interested in seeing this
and that tells you the month is 7 and
days 3 you could also look into a more
readable format by using view on f2 and
that gives me my selected results so we
are just extracting in some specific
value we can keep extending this so here
we can say flights
is what we would want to work on i'm
using the filter function so i can
straight away
instead of creating a variable then then
doing a view i can also do a view in
this way i can just pass in my filter
within the view and within this i am
saying filter
i would want to look at the flights
month being 0 9 day being 2 and origin
being lga
and then that shows me the value here
and obviously you can scroll and look at
all the columns and if you see the
origin column it shows the selected
value so now we have filtered out our
data based on values
in three different columns
now what we can also do is we can use
and or we can use or operators so i
could have done this
in a a little different way so i could
have said head which shows me initial
result i will do a flight so within my
head function i am passing in this
and what does that contain so you are
saying flights and in this flights data
set you would want to pick up the month
being the column so we use the dollar
symbol here we given a value and i'll
say and and i'll again say flights
wherein i will select the day being two
and and and remember when you talk about
and it is going to
check if all the values are met true so
then you say flights origin
lge a and you look at the value so in
this way i can
filter out
specifically multiple values
by specifying columns now we could have
done it in this way we could have
created a view or we could have assigned
this to a variable and then done a view
on that where we could have selected
month being day and origin or you can be
more
specific
in specifying all the columns it makes
the code more readable so let's look at
the values and here you are looking at
head which shows me based on month
day and then you can look for further
columns for other variables that is
origin being lga
now what we can also do is we can do
some slicing here to select rows by
particular position
so i can say slice and i would want to
look at
rows one two five and i can do this
so you can always assign or look at the
view of this
i can just do
here so when i did a slide one is to
five it shows me
my entries
for one to five
now similarly we can do is slice 5 to 10
and now you are looking at
5 to 10 values
so you can always look at the complete
data and then you can slice out
particular data now mutate is usually a
function which is used when you would
want to apply some variable on a
particular data set
and then you would want to
add it to
your
existing data frame or you would want to
add a new column so this is where you
use
mutate which is mainly used to add new
variables so let's see how you work on
mutate
so
it's pretty simple so you create a
variable over delay now i would want to
do a mutate so that it adds a new column
so i'm selecting my data which is flight
i will call the new column as overall
delay
and then basically
i can look at overall delay being
arrival delay minus departure delay so
let's create this and let's look at view
of this which shows me
or which should show me my new column
which is overall delay which was not in
my original data set so you can anytime
do a head on this one to compare the
value so this one shows me arrival delay
and then there are many other variables
what you can also do is you can do a
view
and you could have just look at flights
if you would want to compare
so you can look at the flights and this
one would not have any
overall delay column so it basically
shows me 19 columns only
what we see here
and if you
do a view
on overall delay then that basically
shows me 20 columns so we know that the
new column has been added
to
this overall delay so if you would want
to work with 20 columns you will use
overall delay if you would want to work
with your original data set you will use
flights now you can also use a transmute
function which is used to show only the
new column so we can do an overall delay
and at this time we will say transmute
we will say flights overall delay
the computation remains same but at this
time if i look at view on overall delay
it only shows me the new column so
sometimes we may want to compute result
based on two variables or two columns
and just look at the new value and then
we can decide if we would want to add it
to our existing structure
now you can also use summarize
and summarize basically helps us in
getting a summary
based on certain criteria so we can
always do a
summarize
and
what we can do is we can look at our
data
and we can say on what basis we would
want to summarize this particular data
so we can do a summarize function now
summarize on flights i will say average
a time and i would want to calculate an
average so for that i am using a inbuilt
function called mean
i will do that on airtime column
so
let's look at flights once again and
here we can see there is
arrival time not a time sorry arrival
time and we would want to do some
average on this particular data we would
want to summarize this so what i'll do
is i will use the summarize function
i will say average airtime and this one
i will look at mean of a time so let's
see if there is a a time column i might
be
let's look at this one and i will delay
and yes we have an airtime so we were
actually looking at
summarizing based on a time not the
arrival time
so air time is how much time it takes in
air for this particular fight and we
will want to use the trans summarize
function not the transmute so summarize
flights average a time and this one we
will calculate the mean of average a
time
and
i will also do a any removal which is i
am saying true so let's do this and that
basically shows me the average a time is
151
i can also do a total a time where i am
doing a summation of values or i can get
the standard deviation
or
i can basically
get multiple values such as mean
i can say total airtime where i'm doing
a summation
and then i can look at other values
which is if you would want to put in
standard deviation here you could do
that so let's look at the result of this
summarize and this basically allows me
to get some useful information which is
summarized based on a particular
function such as mean sum standard
deviation
or
all three of them
now
let's look at grouping by so sometimes
we may be interested in summarizing the
data by groups and that's where we use
the group by function
so we can always
use
the group by clause
now
here we are taking a different data set
so we will say for example let's look at
head of mt cars
and that is basically my data set on
empty cars now that shows me the model
of the car
it shows me my lathe cylinder this and
your horsepower and various other
characteristics or variables in this
particular data set
so here
we can say let's do a grouping by gear
so there is a column called gear so i
will call it by gear i will look at my
data set and then what i am using here
which you see with these percentage and
greater symbol
is called
piping
so that basically
feeds your previous data frame into next
one so this is sometimes useful and you
can get this by just saying control
shift and m and you can then use this so
we are going to
have
piping so i am saying empty cars now
this is my original data set
where i did a head
or i could have done a view on this one
if you would want to see it in a more
readable format and that basically shows
me the data so we are using a different
data set so i want to group it by the
gear column so i'm going to call it by
gear
and
this one takes my data that is empty
cars i'm using the piping and then i'm
saying group the data based on gear
column that's done now let's look at the
value of by gear
or you can always do a view so remember
whenever you are doing a group by it is
giving you a
internal object where your data is
grouped based on a particular column
so we can look at the values here you
can do a view that shows you
your data grouped based on a particular
column
now i can again use the summarize
function
where i would want to now work on the
new one where it was grouped based on
gear so i am doing a summarize and here
i am going to say gear 1 which will be
having the value of summation on the
gear column
and then i am saying gear 2 which is
mean well you could give some meaningful
names to this
and let's look at the value of this one
where we are basically now looking at
the values which is sum and mean values
based on the gear
similarly we can use look at different
example so we can say by gear
and i am again using piping
but earlier we had taken gear
we had grouped the data
and we called it by gear so we took our
original data set empty cars but now
within this particular data which was
grouped by gear
i will take this data set i will use the
piping and i will summarize it where i
am saying
within this particular data set i would
want to get the sum or i would want to
get the mean and then you can look at
the values
so
what you are doing is
you are
either looking at your original data set
or you're looking at the data which was
already grouped and then you can look at
the values
now here what we can do is we can group
by cylinder say might be you are
interested in looking at data which is
summarized based on the cylinder column
you can do that and then for this by
cylinder i'm doing a piping where i'm
using the summarize function and
summarizing will then be done based on
the mean values of the gear column or
the horsepower
so let's do this
and then you can basically look at the
value at any point you may want to look
at the data set again so just go ahead
and you can look at what does the value
contain
and
by cylinder or by gear and do a head and
it gives you the value
so you can always do some summarizing or
grouping in these ways
now here we are going to use sample
underscore n function and sample
underscore
fraction for creating samples
so for this
let's take the flights data set again
and we would want to
get 15 random values now that is done
and it shows me 15 rows with some random
values from the data what you can also
do is you can do a portion of data by
using sample underscore
fraction and here i'll say flights i'll
say 0.4 which will return 40 percent of
the total data so this can be useful
when you are building your machine
learning where you would want to split
your data into training and test might
be you are interested in some portion of
the data so you can do this
which is very useful function
and then you can look at the value of
that now
what we can also do is we can use a
range function so like we were doing a
grouping by or we were
trying to pull out a particular column
so in the same way we can use arrange
which is a convenient way of sorting
than your base are sorting so for a
range function
let's do a view
based on a range so we will work on the
flights data set which we have
and here
what we would want to do is we would
want to arrange the flights data set
which is based on year and departure
time and we are doing a view out of it
so that basically
gives me the data
which is arranged based on
your year and departure time now i can
do a head to give me
some highlighting of that data
now
the piping operator what we are using
can be used in these ways also so here i
will say df i will just assign the data
set empty cars to it let's look at the
df which has basically your different
models you can obviously
look at the head or view of it to look
at useful information we can also go for
nesting options which can be useful
so we are
creating a variable called result here
now that has the arrange function
so what does this arrange function do so
when we would want to use arrange to
sort the data so i would want to sort
the data but what data would i sort so i
will use sample n
which will give me some portion of the
data or some sample data now what is
that sample data so here we are using
nesting that is
earlier when we did a sample we just
said data and how many random samples we
want but instead of giving that what we
are going to do is we are going to use
filter here
now this filter will work on df
so filtering will happen based on the
mileage which is greater than 20
i will say size is 5 and i would want to
basically arrange this in a descending
order so i'm using that this
on this particular mileage column by
default it is always ascending
so let's get the result out of this
which will basically show me the mileage
details in a descending order so this is
my data frame and now
we can look at the result what we have
created
so just do a view or do a head
and look at the view so here you see
mileage
where the highest value is on the top
and we were only interested in five
values in a random sample so that's why
when you did a view it shows your five
values and it shows in a descending
order based on mileage so we have
not only used an inbuilt function we
have not only arranged the data that is
we have sorted the data but we have
sorted the data based on a descending
order on a particular column we have
said the value should be greater than 20
and we have also said we just need five
random samples
now let's look at some other examples so
you can always do a multi assignment
so i can say filter
wherein i am going to use
df which was assigned empty cars i am
going to say mileage should be greater
than 20
then i say b which is going to get a
sample out of a
and i just want 5 random values
so let's look at that so we have b which
is
going to get a
set of 5 values from a now i will create
a result variable which will arrange b
which is sample data in a descending
order now let's look at the result of
this and that basically shows me what we
were seeing earlier so you can do a
multi assignment where you can create a
variable get a sample out of it and then
basically whatever is that result you
can arrange that or sort that in a
descending or by default ascending order
so same thing we can do it using pipe
operator
so piping so here i will say result
i'm passing in my df that's the data set
i'm using piping and which basically
tells what you need to do on this
particular data set so i'm going to
filter out the data based on mileage 50
sorry mileage 20 then i'm going to push
that
or forward it to
get the random sample and whatever is
this random sample is going to be pushed
so you are arranging this in a
descending order so this is one more way
of doing it and then basically you can
look at the result so these are some
simple examples where you can use your
dplyer with multiple assignments or
using your nesting to filter out the
data
you can also do a
arrange which is to sort the data you
can get some random samples out of it
you can summarize the data
you can also
summarize the data based on one or two
or multiple columns and you can use some
inbuilt functions to summarize the data
based on some
functions which are applied on the
variables or on the columns
you can transmute it
where you would be interested in only
looking at one column
you can mutate it where you want to add
a new column
you can slice it
and you can give the conditions where
you can say and on or
to filter out the data
so what we can also do is on this
particular data set which we have say
for example df
where i have my data let's look at this
one
and if i just do a df at this point it
shows me my data set
and if you would be interested only in
particular column
then your d player also allows you to
either we can do a filter or we can
simply do a select
now for selecting we can choose
uh our data so for example i'll say df
underscore
i'm interested in mileage i am
interested in horsepower might be i am
interested in
your cylinders in this
and for this one what i can do is when i
would want to do a select
i can basically say
selected df
let's call it some name
i can say
control shift m
which is for piping and then basically
what you can do is you can do a select
and you can choose your columns so i was
interested in mileage i was interested
in
horsepower
i was interested in cylinder and here
what i'm doing is i'm using a select
where i can look at the new data frame
so let's do this
and
i'm sorry here we will have to give it
df
this is where you are passing in your
data
yeah now this one is done and we can
look at the value of this one by just
doing a df
or
head
on df
underscore
mileage horsepower cylinder and look at
the selected result so you can be
looking at selective columns i could
have done this filter but filter will
always look for
a condition say your mileage is greater
than 20 or might be your cylinders are
more than 4 or something else but when
you do a select you are selecting
specific columns so view always gives
you all the columns head gives you
highlight but then select can be useful
when we are interested in looking at
only specific data so this is how you
can use the plier for manipulation
for your data transformation for
basically filtering out the data
by selecting particular data and then
working on it so similarly there is one
more package called tidr and we'll see
how we can use data manipulation
done using your tidr package
let's learn about that idr package it
makes it easy to tidy your data
and this basically helps you creating a
more cleaner data
so
which is easy to visualize and model now
this comes with mainly four functions so
you have gather which makes
your data wide or it makes white data
longer so that is basically used to
stack up multiple columns
you have spread function which makes
long data wider that is stacking the
data together or stack
if you would want to unstack the data to
data
and you are talking about data which has
same attributes
and then your spread can spread the data
across multiple columns
you have separate which is function
which splits single column into multiple
columns
and to complement that you have one more
function which is unite and that
combines multiple columns into single
columns so these are four main functions
which are used in your idr package so
let's look how we work with this
so let me bring up my r studio here now
for this first is let me just clean up
my screen here doing a control l
so i will install the package it is
already installed but we can just do a
control enter
and then
i can say do you want to restart our
prior to reinstall to install i'll say
okay
and it is basically going to get the
package
now it says package tid tidy r the rest
idrs has been successfully unpacked
let's use that package with using our
library function
and that was built under our version 3.6
now i can basically start using these
functions so for example here we are
creating a data frame so let's say n is
10
and then we basically would say
we will call it white now that's the
variable name i'm using the data.frame
function
i'm saying id which will be
1 to n so that will take the values from
1 to 10 and then these are the values
which have
10 entries so this is a vector phase 1
phase 2 phase 3 let's create a data
frame out of it now that's done we can
have a look at our data frame by just
doing a view wide and that shows me the
id column and it has face dot one phase
dot two and face dot three now we can
use our function so for example we can
work with gather that is reshaping the
data from wide format to long format and
basically you can say stacking up
multiple columns
so let's see how we do that here i'll
call it long i'm working on white i'm
using the piping
functionality and then i'm using gather
so this one i will say what will be the
data which i will use
so we are using wide as a data frame
then i am saying response time so that
will be basically one more column and
then you have your columns which you
would want to basically stack so i am
saying from phase one to phase three so
let's do this
and once this is done let's have a look
at our variable long so this one shows
me that i have an id column i have the
response time column and i have the face
column which we mentioned and that
basically has all the values stacked in
so you have face dot one phase dot two
and face dot three so all the columns
are being stacked here so all my data so
now i have totally 30 entries in this
one so this is basically using your
gather function now sometimes we may
want to use
a separate function now separate
function is basically splitting a single
column
into multiple columns so which we
would want to use when multiple
variables are captured in a single
variable column okay so let's look at an
example of this one so let's say long
separate that's what we will call we
will work on this long which has all the
data stacked in
as the columns we selected then i am
saying separate i want the face column
and then i would say when i separate the
columns what are my column names now i
could also give a separator by giving a
comma and then mentioning the separator
if that is required so let's do this
now once this is done let's have a look
at our long separate so what we see here
is the
column which we used so we were doing a
face column and that was to be
split and we wanted to split it into
target and number so that's what we see
here so you have face being split into
target and number and then you have the
response time so this is how you use the
separate function now there is also
something called as unite function which
is basically a complementing of separate
function so it takes multiple columns
and combines the elements to a single
column so for example here
we will
call it long unite and we will take long
separate which was separating the data
we want to unite so we will take phase
target
number
and we want to have a separator between
them so let's basically do this
and now let's look at the result of this
unite
so you see you have the face and target
merged together so you have face dot one
the separator is dot as we have
mentioned and we have united multiple
columns
so
this is one more function of your ti dr
which helps you
basically
uh tidy up your data or put it in a
particular way
now then you have your spread function
and this is basically for unstacking so
that is
if you have if you would want to convert
a stack to data or if you would want to
unstack the data which is of same
attributes spread can be used so that
you can spread the data across multiple
columns so it will take two columns say
key and value and spread it into
multiple columns so it makes long data
wider so we can look at this one we will
say long unite i'm using the piping i
will use the spread function i'll work
on the face column and response time and
let's do this and then let's do a view
on this
so it tells me our data is back in the
shape as it was in the beginning so
these are four functions
which are very helpful when we work with
tidr package
so let's learn about visualization
and here we will learn about
r
which can be used for your visualization
now
one thing which we need to understand is
because of our ability to see patterns
which is highly developed we
can understand
the data better if we can visualize it
so the efficient way or effective way to
understand what is in our data or what
we have understood in our data we should
or we can use graphical displays that is
your data visualization so there are
actually two types of data
visualizations so you have exploratory
data visualization which helps us to
understand the data and then you have
explanatory visualization which helps us
to share our understanding with others
so when you talk about r
r provides various tools and packages to
create data visualizations
and which can be used for both kind of
data analysis or both kind of
visualizations
so when you talk about exploratory data
and visualization the key is to keep all
the potentially relevant details
together
now the objective when we talk about
exploratory data analysis is to
help you see what is in your data
and the main question is how much
details can
we interpret
now when you talk about different
functions which we see here such as plot
which is more for a generic
plotting you have bar plot which is used
to plot data using rectangular bar so
you can say creating bar charts you have
histogram or hist function to create
histograms where you look at the
frequency of
the data are basically used to look at
the central tendency of the data you
have box plot which is used to represent
data in the form of quartiles you have
gg plot which is a package which enables
the user to create sophisticated
visualizations with the little code
using the grammar of graphics
and then you have plot lee or plot l y
it creates interactive
web based graphs via the open source
javascript graphing library now before
we see some examples here let's also
talk about when you talk about plotting
let's also try to understand what kind
of
plots you can have and what kind of
techniques you have so let me open up my
r studio here
now for example i can pull out a
particular data set
and let's look at this one so
here i can look at
all the panes and that shows me the
information now what i can do is
i can install
and get the inbuilt data sets and then i
can simply do a plot
wherein i am doing a plot on jquery data
set so let's see what does that show it
summarizes the relationship between four
variables in check weight data frame
which is
in our's built-in data set package
now from these plots we can see for
example weight
varies systematically over time
you can also see that chicks were
assigned to four different diets
now when we talk about explanatory data
analysis
or visualization that shows others what
we found in the data this means we need
to make some editorial decisions
what features we would want to highlight
for emphasis
what features are distracting or
confusing and you want them to be
eliminated right so there are different
ways of doing it now when you talk about
your graphics or visualizations you have
i would say
three different types or you can say
four so you have the base graphics which
is easiest to learn now here we are
having an example of base graphics
where i can use the base graphics
i can get a
data set using library
then i can simply create using plot
function to
generate a simple scatter plot of
calories with sugar from u.s serial data
frame in the mass package
and then i can give it a title so this
is basically a simple example of base
graphics now you also have what we call
as grid graphics which is powerful set
of modules for building other tools
now you also have latest graphics which
is general purpose system based on grid
graphics and then you have your gg plot
2 which implements grammar of graphics
and is based on grid graphics so you
have different ways now here since i
already have used library and i have the
data set i can just do a x so i can
assign the
sugar related values to x and calories
related value to y
then i can use one more which is
library function and calling in grid now
i can basically use functions such as
push view port if i would want to create
a plot
using your grid graphics to
create the similar kind of plot which we
created using base graphics but this
will give you much more
power than base graphics
it will
have a
steep learning curve but
it is usually useful so i can do this
where i'm saying push view port
then i can basically say i would want to
have a data viewport
i would say different functions of your
grid package so i'm saying rectangle you
have x-axis y-axis given some points
here
and then basically you can add details
to the graph by giving the names to the
columns and you can
basically create a simple grid graphics
based plot here
now there are different other options
which we can use to create plots now
before we go into understanding how you
create plots let me just give you a
brief on what are the different kind of
plots and how they can be used so here
we will look at these different plots
now for example
we have a bar chart which is a graph
which shows comparisons across
discrete categories
so you have x-axis which will show the
categories being compared and y-axis
which represents a measured value and
height of the bars are proportional to
measured values
now
to create different kind of charts you
can use ggplot which is a package for
creating graphs in r
it is basically a method of thinking
about and decomposing complex graphs
into logical subunits
and that is a part of tidy works
ecosystem so it takes each component of
graph accesses you can give scales you
can give colors you can give the objects
and you can build graphs on particular
data you can modify each of those
components in a way that's more flexible
and user friendly you can if you are not
providing details for the components
then ggplot will use sensible defaults
and this basically makes it a powerful
and flexible tool now here
are
different options when you use your
ggplot such as you can use geom or what
we call as geometric objects
to form the basis of different type of
graphs for bar charts you have for line
graphs you have scatter plots that is
underscore point you have underscore box
plot for box plots
you have quartile for continuous x
violin for richer display of
distribution and jitter for small data
so here is some simple example
where i would not go into too many
details here but you can just have a
look at this one where we are
using library function to get the
ggplot2 package
then basically we would want to look
into the mileage data we would want to
look at the structure of it
and then we can basically get the tidy
words package finally we can create a
bar chart
using jio underscore bar
and we can basically also mention what
would be in x-axis now you can also give
different colors to basically add more
meaning to your data
you could also go for stacked bar charts
so here we are actually
telling ggplot to map the data in the
drive column to fill the aesthetic so
here i am giving
[Music]
aesthetic access class
and i'm saying what is the data we need
to have and then we are using geom
underscore bar
so you can also have dodged bar
in your gg plot that is not bar charts
which are stacked but next to each other
and you can create that by using
your position as position underscore
dodge okay now you can obviously use
your different packages which are
inbuilt and you can create your bar
charts
and you have other kind of graphs such
as line graph which is basically a type
of graph that displays information as a
series of data points connected by
straight line segments such as this one
and for this one we are using if you see
jio underscore line
now you can also create a scatter plot
which is a two dimensional
data visualization that uses points
to graph the values of two different
variables one in an x axis one on y axis
like what we saw in base graphics
example and they are mainly used if you
would want to assess the relationship or
lack of relationship between two
variables
and you also have histogram which i
mentioned is mainly to look at the
distribution of a data to look at the
central tendency of the data
basically looking at
your
large amount of data for a single
variable
you would be interested in saying where
is
more data found in terms of frequency
whereas lesser data found in the graph
how close the data is towards its
mid point or what we call as mean median
mode
so
you can use histogram where you can
categorize the data in what we call as
bins so these are some basics on
different kind of graphs now we can look
at some examples and see how that works
so what we were seeing is some quick
examples of base graphics or grid
graphics now here
let's do
an example of pie chart for different
products and units sold so you want to
create a graph for this first let's
create a vector and pass in the value
here
now i can also create labels which i
would want to assign to these values
and then basically i can plot the chart
by saying pi so that's the kind of chart
which i would want to create
and i would say the data would be x
and labels
so let's do this and that shows me a
simple pie chart now i can also give
main details here so instead of just
doing a pi x comma labels i can say what
is the main and then
what kind of coloring it should follow
so this is the way you can create a
simple
plot now i can also find out what is the
percentage
and then basically
i would be interested in plotting the
pie chart which takes x
which takes the labels which will be the
percentage which we are calculating here
by doing a round function
and then you can basically give details
to your
graph you can say what color it follows
you can basically look at the legend
where it needs to be
in your chart
what are the values
and then basically fill up the colors so
let's run this one and that shows me the
percentage which was calculated and it
gives me the details
and we can always have a look at our
plot now if you would want to go for a
3d pie chart
then
you can get the package which is plotrix
let's use that by calling in the library
function let's pass in some data to x
and let's give some values or labels
which will make more meaning to the data
and then let's plot the 3d graph so i'm
saying pi 3d here where i'm using x and
labels
then i'm basically doing an explode
which will basically control how your
graph looks like
and basically give the values so it also
takes the title when you say main and by
chart of countries
now let's create
data for graph so again we are having a
variable here we are create using the c
function creating a vector
and then let's create a histogram for
this one
where i would say x lab what would be
your data around x x-axis what is the
color what is the border and here i am
creating a simple histogram
which as i discussed earlier will always
show
your values on the x-axis and y-axis is
more of frequency and then you can look
at the set of values and what is their
frequency
and we can basically use this histogram
for exploratory data analysis look at
the data try to understand what is the
central tendency of your data values
now we can also give some limits by
using the x limb and y limb and then i
can also specify what is the limit so we
have given some values here wherein we
have said your x limit is 0 to 40
and y limit is 0 to 5. now if you
compare this with the previous one which
we had created
this one based on the frequency had
taken the limits but we can assign
limits
explicitly by giving this and then
create a histogram which makes more
meaning
now let's take
another data set that is air quality
let's view this to see what does that
data contain so you have
ozone solar wind temperature month and
the day so this is the kind of
information we have in the air quality
now let's use the plot function to draw
a scatter plot where as i mentioned you
would be interested in analyzing
variables
and see
what is the relationship between them so
to plot a graph between ozone and wind
values
so we will say plot we will say the data
which is air quality from that i would
be interested in the ozone column or
ozone field and the wind field i can
create a plot based on this
now i can also be saying what should be
the color what is the type of the data
which you would want to create and you
can look at the info information so you
can create a histogram you can create a
scatter plot to basically understand the
data better and then infer some
information from that data so let's take
the
air quality data set itself without
specifying any particular column and you
can create a plot which shows me
all the different values which you have
in the data and it basically shows you
the difference this is more of an
example like what we did for chickweight
where we did a base graphics now you can
assign labels to the plot so that is
when you are creating a plot you can say
air quality you will say ozone
and then that's your ozone concentration
you have your y lab which is the number
of instances
you have what is the title ozone levels
in new york city what is the color so
these are the details what we have given
with our plot function and let's look at
the data so it just tells me that this
is the ozone concentration
uh the number of instances what you have
and you are looking at the data now we
could also create a histogram by picking
up a particular column that is such as
solar
from your air quality and that basically
shows me the frequency of solar values
and we can then try to find out what is
the mid
what is the mean what is the standard
deviation and so on you can also look at
your histogram and try to understand if
it is left skewed and right skewed
so we can do that
now here let's get the temperature out
from this particular data set
let's create a histogram on temperature
and that basically shows me the
frequency of the temperature values
and
what values have the most frequency or
most occurrence
now you can create a histogram
with
labels
so let's do that with the limit
and then let's also use text
to basically given the values which also
takes the values and for each set of
frequency or each set of values it gives
me the labels now you can have a
histogram with non-uniform width so you
could do that by doing a hist function
and then
passing in your temperature you can say
what will be the main what is the title
what will be your x lab
it will tell you a limit around x axis
what is the color what is the border
what are the breaks you would want to
have
for your bars and you can simply create
a histogram using this so this basically
takes the breaks which we have given
such as 55 to 60
60 to 70 70 to 75 and so on so this is
basically creating a histogram with
non-uniform width
and it purely depends on the kind of
values what you have
now you can also create a box plot which
sometimes helps us in understanding the
the data quartiles also understanding
are outliers so you can create multiple
box plots based on the data from air
quality so we'll select all the data and
then we'll do some slicing on the data
so let's create a box plot which tells
me the values and if you look at these
points here
like single dots these are basically
your outliers
we can learn about that more
in later sections
so you can use
your gg plot two library to analyze
a particular data set so for that we
will first
use the install dot packages and get
ggplot2
so it says do you want to restart r and
i can say yes so let it get the package
i think the package was already there
and now
let's look at
using ggplot2 so for that i have the
library function
and let's do a attach where i'm getting
a data set which is empty cars
now then i will create a variable p1 i
will use gg plot i will pass in my data
i'll give the aesthetics
what is the columns which you would be
interested in
and then you are using geom underscore
box plot to basically create a plot
which gives me the box plot for the
values here and this is based on
the cylinders which is there in your
data
so we can always look at what does our
data contain
and what kind of values or features are
available in the data now let's create a
box plot we will also use the coordinate
function and that basically gives me
based on the data so i've changed the
coordinates now if you
look at the previous one where we
created a plot we had mileage on the
y-axis and cylinders
on the x-axis
now i did a coordinate flip and that's
like your transpose function so you have
created the box plot but you have just
flipped the coordinates you can
create a box plot and then say fill
which is the factor of cylinder so that
can be used to fill up the values in
your box plot
now what we can also do is we can create
factors so we have learnt about factors
earlier which is usually used to work on
categorical variables
so here let's create a factor
which is empty cars care you have am you
have cylinder and if you look at the
factors which we have created we have
passed our data
what is the field or the column we are
interested in what is the level of
values there and what are the labels for
those values right so we have learnt
about factors
you can always look into the previous
section and learn more about factors
now let's create a scatter plot by using
the ggplot function again we will use
the data as empty cars i will go for
mapping option and then i will give my
aesthetics that is what would be x what
would be your y
and you also would want to use what kind
of function you are using so let's go
for geom pawn point and that basically
helps me in creating a scatter plot now
you can create a scatter plot by factors
so here we will say gg plot so notice in
all of these cases depending on the kind
of data you have
depending on the kind of plot you are
interested in you will use the ggplot
and then basically a function with that
or the inbuilt package so here i'm
saying data is empty cars i am going for
mapping which basically will take the
values for your x and y
what is the color
and the coloring will be done based on
the factor values now if you remember
factors will obviously have some levels
and
those levels will basically help you in
differentiating between your categorical
variables so i'm saying as dot factor on
cylinder and then i'm using geom point
to basically create this scatter plot so
let's do this
and
i can
look at the values of this one so it
says
must be there is an error which says
must at least one color from the hue
palette so let's look at that one so the
error which we were facing when we gave
color as the factor values was because
when you look at these factors which
were created with some labels if we look
at the values of these it tells me there
are any values in that particular column
similarly your gear
or similarly you can completely look at
the complete data set it tells me
cylinder you have am you have care now
these have some
we have created some labels but these
have n a values
so what we can do is we can create a
scatter plot as we did earlier by giving
the aesthetics and that's a simple
scatter plot
wherein i'm also using geom point so
that i can have these points by defaults
or with defaults
you can also
give a color specific basically if you
would want to have different kind of
data in the same plot or i can
create scatter plots by different sizes
by giving a size or
i can give a color and size and that's
again one way in which you can create
your scatter plots now let's also see
how you can visualize
one more data set which is mpg
so i can also do it in this way where i
set ggplot2
and then pass and look at the data set
what we have here
you can just do a view on this to see
what my data contains if the fields have
any any values if that's going to affect
your plotting so now what we can do is
we can create a bar plot or a bar chart
so i am saying gg plot the data would be
as we have given in previous lines that
is ggplot2 mpg then i will say what
should be in my aesthetics and what kind
of
chart are you going to create so i'm
saying geom underscore bar so that's my
bar chart and that has basically your
class and count now you can create a
stacked bar chart where your information
is stacked
in the same bars
and we are still using the same data
we are going for aesthetics which is
class
and then when you say geom bar which
creates your stack bar we will use fill
which is drive
and we can always go back and look at
our data for example
you can always look into this so you
have the drive column here
and you are also working on this
complete data set so let's go ahead and
create a stacked bar chart and that
basically gives me the information where
you have the drive information which is
stacked
here now you can do a dodge
by giving the position as dodge
so we are still going to go for a stack
chart but this time the bars will be
next to each other and that can also be
done which is very useful
you can use this by using geom point
where you are mapping and you are
specifying what are your aesthetics so
we were creating a scatter plot
now you can also use
or give more details where you can say
color can be based on the class
and we have different classes and based
on that my points have been colored
now you can also use a plot
lyu or plotly library so let's install
this one
i will say yes for example let it
basically restart so that all my
packages are updated
then i can access that package using
library function
and then
create a variable
to which you are assigning your plot
underscore ly plot so data is empty cars
what will be your x axis what will be
your y axis and details on your marker
which we have given
wherein i will give a list which is size
color which is a combination
and then you have your line
what kind of color it will have and what
will be the width so this is where i'm
going to use plot ly
and let's look at this plot so it
basically gives me some information now
we see some warnings which are getting
generated but there is
you don't need to worry about that so
you can look at the packages what you
have
and what options you are using so
similarly we can create one more plot
using plot ly and look at the values of
those so that's a plot with a trend
which explains me about my data
so this is a simple small tutorial
on
understanding or
how you can have your graphics or
visualization
used to understand your data obviously
there are much more examples
much more ways in which you can pass
into your plot functions
or your gg plot
and the inbuilt
packages which are available in r for
your visualization now that could be for
exploratory data analysis or explanatory
data analysis so try these graphs and
see if
you can change these options and try or
create new visualizations let's dive
into why linear regression now that we
have come this far i'm planning to take
a closer look at our cells so that we
can estimate cells in the future how
about we hire a data scientist you can
see our two uh corporate individuals
looks more like they should be agents or
secret agents someplace discussing how
to better forward their company and so
they're going to come in and ask the
data scientist to come in good idea this
will help us to keep a constant track of
our cells so why do we need linear
regression let's assume that we need to
predict the number of skiers based on
snowfall so this happens to be we've
kind of jumped one business to the next
so we're looking at the skiing business
very popular in a lot of areas and it's
based on snowfall a lot of times you
figure you don't have snow you don't
have skiers but can we actually use
something more specific instead of look
at snowing and instead of saying hey
look it's snowing we can actually start
drawing graphs and the graph shows that
with an increase in snowfall the
frequency of skiers also increases so
and there's a pretty direct correlation
if you've ever been up to ski areas when
there's a lot of snowfalls the skiers
all show up because they know it's going
to be better skiing right afterwards so
it's kind of easy to see why skiers and
snowfall would go together and usually
draws a nice straight line and you can
easily predict how many skiers and that
way you can also predict how many people
you need to service them how many lifts
to have up and running and all the stuff
that goes with running a ski area thus
we see that the number of skiers are
directly proportional to the amount of
snowfall regression models a relation
between a dependent y and an independent
x variable this is real important to
understand regression because when we
talk about linear regression it's the
basis of almost all our machine learning
algorithms out there and it's usually an
underlying part of the math in our deep
learning so it all starts here with
linear regression and we talk about a
dependent y and an independent x
variable these are numbers we're usually
talking about floats so we want to have
an actual number value coming out and
that's different than something that's
categorical or we want to know yes or no
true false so regression means we're
looking for a number the independent
variable is known as predictor variable
and dependent variable is known as the
response variable so we have one
predictor variable the amount of
snowfall and one response variable how
many skewers are going to show up and
then we can take this relationship and
it can be expressed as y equals beta
we're going to use the greek letter beta
and we have beta naught plus beta 1 x 1
and if you continue out it'd be plus
beta 2 x 2 and so on till the nth degree
in this case snowfall is an independent
variable and skiers is a dependent
variable so we kind of have a little
quick overview let's go ahead and talk a
little bit more about what is linear
regression so we're going to go from y
predicting a number of skiers for
snowfall and we've looked at the formula
a little bit but let's look a bit closer
at what exactly is going on with linear
regression and the question is going to
come up in an interview what is linear
regression linear regression is a type
of statistical analysis that attempts to
show the relationship between two
variables linear regression creates a
predictive model on any data showing
trends in data the model is found by
using the least square method there are
other methods the least square method
happens to be the most commonly used out
there and we usually start with the
least square method since it's the most
common and usually works the best on
most models and we're already looking at
how linear regression works but let's
dig deeper into it and let's take a
closer look how linear regression works
i will provide you with a data set which
has rent area other information in it
looks like our secret agents have uh put
on their casual wear for this one you
need to predict rent accordingly we are
given area and rent here so you can do
linear regression with more variables
but we're just going to look at the area
and then from that try to figure out
what the rent should be we plot the
graph and if you look at it here you can
see that the graph kind of a linear
pattern with a little dip in it so the
area seems the rent seems to be based on
the area in most of our work as data
scientists we usually try to start with
a physical graph if we can so we can
actually look at and say hey does what
i'm fitting to this graph look right you
can solve a lot of problems that arise
by looking at the graph and saying no
that doesn't look right at all or oh i
should be looking over here for this
then we find the mean of area and rent
so the mean just means average and if
you take one two three four five and add
them all together and divide by in this
case there's five areas you get three
and the rent is two four six five and
eight if you add them all together and
divide by five you get five and we plot
the mean on the graph so you can see
right here here's the mean of the rent
and the area it's kind of a very central
point which is what we're looking for
the average of everything in there the
best fit line passes through the mean
this is real important when you're
eyeballing it as humans we can do this
fairly easy if it's a straight line
through a bunch of data it gets more
complicated mathematically when you
start adding multiple variables and
instead of a line you have a curve but
for basic linear regression we're going
to draw a line through the data and the
lines should go through the means that's
going to be part of the best fit for
that line but we see there are multiple
lines that can pass through the means so
depending on how you look at it you can
kind of wiggle the line around back and
forth so we keep moving the line to
ensure that the best fit line has the
least square distance from the data
points and this is you can see right
here residual we have our data point and
then we look at this distance between
them and we square that distance and
that's what they mean by the least
squared distance we want all those
distances to add up to the smallest
amount we can we talk about that
distance we call it the residual it
equals the y actual minus the y
predicted very straightforward just
looking at the distance you can
accidentally switch these and it'll come
out the same because we're going to
square it but when you're working with
other sets of linear regression you want
to make sure you do the y actual minus
the y predicted the value of m and c for
the best fit line y equals mx plus c can
be calculated using the mentioned
formula and you can see here we have m
equals the number of points that's what
n stands for then we have the sigma the
greek symbol is there which is a
summation of x times y minus the
summation of x times the summation of y
over the number of the summation of x
squared minus the summation of x squared
of x all of it squared
that can be a little confusing trying to
say that and then of course your c value
is going to be the summation of y times
the summation of x squared minus the
summation of x times the summation of x
times y over the total count of the
summation x squared minus the summation
of x squared that's a mouthful normally
we don't worry too much about these
formulas other than to know they're
there now if you're a mathematician you
might go back in there and work on those
and those people who originally started
to put together the different models in
r of course had to know all this math to
build them and had to test it out that's
one of the nice things about r what is
important is that you know about these
formulas so that you know where it's
coming from so if you're using one
linear regression model this is how this
one works there's other linear
regression models based on different
math and so you'd have to be aware when
that math changes and how that changes
the model we find the corresponding
values so in this case if you're going
to break this up and you're building
your own model instead of using the one
in r you would have your rent you would
have your area you would find your rent
squared your area squared and your rent
times area and if we go back one we can
see that that's all the different little
pieces in this model we have x times y
we have x squared we have the sum of x
which is going to be squared we have the
sum of x the sum of y this is the same
these are all the little pieces in here
that we see for m equals and c equals
and so once we compute all these we have
15 25 55 145 88 very easy to do on a
computer thankfully you could have
millions of these points and it would do
it in a very short period we can then
plug in those values very easily into
this formula and get a final answer and
we'll see that m equals 1.3 and c equals
1.1 and then when we take this for the y
predicted equals m of i x of i plus c we
can take these and actually run the data
we have so we're going to see how it
predicts so now we find the y the value
of y predicted so we have our x in and
we want to know what y what we think y
will be based on our formula we
generated the linear regression formula
and so we come in here we have our x we
have our actual y then we have our y
predict what we think it's going to be
and then we have we take y minus y
predict and we get those values minus
0.4 it's off by .3 this one's off by one
this one's on off by minus 1.3 by 0.4
and we go ahead and square that and then
we can sum that square and average it
out and we'll get a value this is just a
summation down here of y minus y
predicted square is 3.1 and we call that
the least square value for this line is
3.1 and we go ahead and divide that by
5. so when we plot the y predict and
this is the best fit line and you can
see that it does a pretty good job going
right through the middle of lines and
it's something like rent versus area if
you're trying to figure out how much
rent to charge or how many people are
allowed to be in the area that's going
to work but if you're looking for the
rent value compared to the area this
gives you a good idea based on the area
of what you should rent the place for
it's close enough that in the business
world this would work you're not
computing something down to the
millimeters or micrometers or nuclear
physics we're just charging people and
so if you're off by a couple dollars
it's not a big deal you'll be pretty
close to what you think it's worth and
get the right value for your property
use case predicting the revenue using
linear regression now that you have a
good idea of how linear regression works
and we're kicking back on our lounging
sofa today let's work on a real life
scenario where you have to predict the
revenue so we're going to take a data
set and we'll pull this up in just a
minute we'll have paid organic social
and revenue there are three attributes
we'll be working on the first one is our
paid traffic so all the traffic through
the advertisement that comes in the
non-paid traffic from search engines so
these are people coming to the website
and are doing so because they did a
search for something specific so it's
pulling it from your website all the
traffic coming from social networking
sites so we want to know what's coming
from twitter and facebook and somehow
they've linked into your website and
coming into your marketing area to buy
something and we'll use this to predict
the revenue so we want to know how all
this traffic comes together and how it's
going to affect our cells if the
traffic's coming in we're going to make
use of the multiple linear regression
model so you'll see that this is very
similar that we had before where we had
y equals m times x plus c but instead we
have y equals m1 x1 plus m2 x2 plus m3
x3 plus c m is the slope so you have m1
m2 m3 the three different slopes of
three different lines y is the revenue
so we're only looking for one variable
and remember this is regression so we're
looking for a number x1 is going to be
the paid traffic and of course m1 the
corresponding slope x2 the organic
traffic and x3 the social traffic we
will follow these steps to create the
regression model we'll start by
generating inputs using the csv files
we'll then import the libraries
splitting the data into train and test
very important whenever you're doing any
kind of data science applying regression
on paid traffic organic traffic social
traffic so we're going to apply our
regression model on them and then we're
going to validate the model as a whole
reason we split it to a training and
testing is so we can now validate to see
just how good our model is now before we
open up r i always like to just take a
quick look at the data itself and we're
using the rev.csv file and this is a dot
cs file or comma separated variable file
we'll just open that with word pad any
kind of text editor would be nice to
show you what's going on and you can see
we have paid organic social and revenue
across the top says your titles for your
columns and the values going down and
this is the same as what we saw on the
slide just a minute ago paid organic
social and then revenue from them they
probably have it in more of a
spreadsheet than just a word pad and i'm
using the newer version of our studio
which already has a nice layout
automatically opens up with your console
terminal window down and left you have
your script and we'll run it in script
and then when i run it line by line in
script you'll see it show up on the
console bottom left i could type it
straight into the console and it would
do the same thing and then on the right
we have our environment and under the
environment we'll see plots if you're in
an older version the plots will pop up
and you usually are running just in the
console and then you have to open up a
different window to run the script and
you can run the script from there this
opens up everything at the same time
when you use the new rstudio setup and
the first thing we want to do is we want
to go ahead and generate a variable
we'll call it cells and we want to load
this with our data set and so
in r we use the less than minus sign
that's the same as assigned to it's like
an arrow pointing at cells it says
whatever it goes past here is going to
be assigned to cells and we want to read
in our csv file and then i went ahead
and fixed the csv file up a little bit
when i say the csv file i mean the path
to it
and we'll go ahead and just paste that
in there and this is my edited version
and you'll see i took wherever we had
the
backward slash i switched it for a
forward slash because a backward slash
is an escape character and whenever
you're programming in just about any
scripting language you want to do the
forward slash instead so like a good
chef i prepped this and copied it into
my clipboard and we can see the full
path here and the full path ends in our
rev.csv files that's the path on my
machine and then i'm going to run that
and now we have down here you see it
appear right here oh must have an error
in my path it says object rev not found
let me fix that real quick and run it
again and this time it does assign the
cells the data set and then we'll just
type in cells and this is the same as
printing it so if you work in other
languages you'll do like print and then
cells or maybe print sales dot head if
you're in pandas in python or something
like that but in r you just type in the
variable we'll run this and you can see
it printed out all those columns so
that's the same thing we just looked at
and it says uh max print so it emitted
750 rows so it has a total of a thousand
rows and we can scroll up here on our
console where the answer came in and if
i scroll to the top
way up to the top there we go you'll see
paid organic social and revenue so
everything looks good on this and
there's a lot of cool things we can do
with it one of the things we can do is i
can summary it so let's do a summary of
cells and we'll run that and the summary
of cells comes in it tells you your min
first quarter median mean third quarter
max for paid organic social breaks it up
and you can actually get kind of a quick
view as to what the data's doing you can
take more time to study this a lot of
people look at this and you can see from
the media and the mean that tells you a
lot and it's divided into quarters so
you have your first quarter value or in
max it just gives you a quick idea
what's going on in the data summary is a
really wonderful tool one of the reasons
i like to jump into
r before i might do a major project in
something else or you can run everything
in r and let's take a look at the head
head of cells and let me run this and
you can see it just shows the head of
our data the first six rows in this case
and it starts with one important to know
other different systems start with zero
and go up r is one that starts with the
first is one and then if we want to do a
plot i'll go ahead and do a plot cells
another really powerful tool that you
can just jump right into with r once you
have your data in here and you'll see
that the plot comes over on the right
hand side and what it does is it
compares them so if i look at my paid
and my organic and you cross the two
you'll see there's a nice line down the
middle big black line where it plots the
two together let me just widen this a
little bit so we can see more of it
there we go same thing with paid and
social paid and revenue so each one of
these has a clear connection to each
other so that's a good sign we have a
nice correlation between the data and
you can just eyeball that that's a good
way to jump in and explore your data
rather quickly and see what's going on
so we'll go ahead and i'll put a pound
sign there
hashtag and uh splitting the data into
training and test data we want to split
it so that we have something to work
with to train our data set and when we
do our data set you do this with any
data set because you want to make sure
you have a good model and then once
we've trained it we're going to take the
other set of data we pulled off to the
side and we'll test it to see how good
it is if you test it with the training
data that's already got that memorized
and should give you a really good answer
in fact when you get into more advanced
machine learning tools you watch those
very closely you watch your training set
and how it does versus your testing set
and if your training set starts doing
better than your testing set then you're
starting to over train it now that's
more with deep learning and some other
packages that are beyond what we're
working with today but you know this is
important it's very important to
understand that that test and that
training correlate with each other gives
you an idea what's going on and make
sure that you're set up correctly and
we'll go ahead and set a seed of two and
that's for our random generators so that
when we run them you could see them with
just a red there's a ways to randomize a
number you could do like date and stuff
like that but that way we always use the
same random numbers so we'll seat it
with two and then we're going to need a
library in this case we're going to
import the library and we want to import
ca tools and ca tools has our split
function so we're going to be able to
split our data along with many other
tools we're going to need for this
example so they're all built into the ca
tools that's why we need to import that
a lot of times they call them library or
sometimes you can refer to as packages
so let's go ahead and run that line and
that brings the library in so now we can
use all those different tools that are
in there and then i'm going to do a
split and we're going to assign to the
split that's going to be our variable
sample split okay so that's one of the
tools we just brought in is to sample as
the actual keyword or the function word
sample.split and we're going to take our
cells and we're going to go ahead and do
a split ratio equals to 0.7 and let's do
0.7 so we make sure it knows it's a
decimal point or float and what this is
saying is that we're going to take our
cells data the variable we created
called cells that has the data in there
and i want to split it and i'm going to
split it so that 70 of it goes in one
side we'll use that for training and 30
will go into the other side which we'll
use then for testing out our model let
me go ahead and run that and let's hold
on one second got an error there spit is
definitely very different than split i
don't think we want to spit our data out
we which is actually kind of what we're
doing is spitting it out but we want to
split we want to split the data let me
get that spelled correctly and then when
i type in split we can just run that
because that's now a variable you'll see
that it has true false false true so it
generates a number of interesting
different statistics going across there
as far as the way it splits the data and
right here if you have not used r in a
while or if you're new to r completely
that line one that's what the little one
means down there and then true false
false true that means that's how we're
looking at the data we're splitting all
those different pieces of data in line
one different directions and so we now
want to create our train set and we're
going to assign that and when we take it
and we assign to the train set a subset
of cells and then the subset of cells
that's going to be based on split equal
and put this in brackets true so you can
see the values down here is capital t
capital r capital u capital e so i just
want to reflect that on the train and
this is going to take everything where
our split variable is equal to true and
it's going to set cells equal to that
and we'll go ahead and run that and then
we're going to set our test variable as
a subset of cells and if we assign the
true to the train set then we want to
assign the false
to our test set so now we'll have this
will have 30 of the variables in it and
train will have 70 percent we'll go
ahead and run that so we've now created
both our training set and our test set
and we can just real quickly type in
train hit the run on there and you can
see our train set if we scroll way up to
the top we'll have the column names on
there which should match what we had
before paid organic social and revenue
and then we'll type in test when i hit
run on there that's going to do the same
thing it'll spread out all the test
variables going out so now that we have
the test and train variables we want to
go ahead and create the model and you'll
see this in any of the machine learning
tutorials they always refer to them as
models we're modeling a function of some
kind on the data to fit the data and
then we're going to put the test data
through there to see how well it does
and so we're going to create the
variable called models and i'm going to
assign that
lm that is our linear regression model i
love how simplified r makes it just lm
linear regression and then model m
um i guess it dates back because the
linear regression model is like one of
the first major models used so they kept
it easy on there and uh revenue happens
to be the main variable that we want to
track so if you remember correctly from
our formula y is the revenue and then we
have x is our paid traffic x2 organic
traffic and x3 our social traffic so y
is what we want to predict in here and
then you'll see this notation where we
have our squiggle and the period which
means we're going to match up the
revenue lines with the lines of the data
we're putting in here and then i'll put
comma and then our data equals
train and of course that's the training
data that we created so when i hit the
run remember we did all that discussion
about all those different functions and
formulas to compute our model and how
that's set up when it comes down to it
we spend all our time setting up our
data and then we hit the run button on
the single line and our model's been
trained so we now have a trained model
here and we can do a summary of the
model summary is such a wonderful
command because you can do that on r
that works on all kinds of different
things on there oops and of course it
does help if i remember that my model
has a capital n when i run it and you'll
see right here it tells you a little bit
about the model use a summary on there
comes down here has our residuals this
is all the information on there as far
as like the minimum the median the max
has all our coefficients in there if you
remember correctly from our formula
let's just go ahead and switch back over
to that we have m1 x1 m2 x2 m3 x3 plus c
well that's right here here's our
intercept and then we have our different
values for each of these so we have our
intercept our paid organic and social
and then it also shows us error in
information on the error and one of the
really nice things about when you're
working with r you can come down here
and you see um where we have our stars
down here and it says uh three stars
really good two stars maybe one star
probably no correlation and we can see
with all of these it has three stars on
them so out of three stars we get three
stars on all of these there's no four
stars four stars would mean that you
have the exact same data going in as
coming out and then if we're going to do
this we want to actually run a
prediction on here and that's what we
saved our test data for so let's come
down here we'll do our prediction we'll
take this variable and we'll assign it
predict
model remember the predict comes from
when we imported that package the ca
tools that's all part of the ca tools in
there so we're going to predict we're
going to use the model and then we're
going to use our test data pretty
straightforward quick and easy we'll run
this and then if we go ahead and type in
the predict it'll print out what's in
that variable and you'll see down here
the predicted values expects to come out
it's going to see our revenue is and it
goes through and it gives it both the
line number and the actual revenue value
so that's quick and easy boy we got a
prediction really fast and this is also
how you would do it if you had new data
coming in after this you could predict
the revenue based on these other factors
so now that we have a training model
which or we train the data with our
training data or we trained the model
with our training data and we've done a
prediction on our model with our test
data we want to look this up and we took
a quick glance from our training thing
our training said it trained really well
but that's not the final word on it the
final word is going to be comparing it
so we want to go ahead and do comparing
predicted versus actual values what i'm
going to do is i'm going to do a plot
and i'm going to do our test and we're
going to do test
revenue dot type equals and we can put
this in as a type
type 1 or l and then we have lty equals
1.8 and we're going to set up for column
blue that's a lot of stuff in there
let's go ahead and just run that so you
can see what that looks like and what's
that's doing it's going to generate this
nice graph we see on the right there's
our graph you can see the data comes up
and down
we're plotting the prediction on there
so this is the values we predicted and
then let's go ahead and do
lines and i actually did this backwards
so let me try that again and we'll start
with the red one we're going to do the
first one in red and i want to start
with the actual test revenue so here's
our test revenue and we'll go ahead and
run this and so we have our red plot
over here in red go ahead and take a
look at that pull that over you can see
how that looks goes up and down hard to
track there and then we're going to do a
couple things here we'll plot our test
revenue
and then we'll make it a little prettier
though it's kind of hard to say see the
way i was scrunched up here on the right
as far as size and sizing and everything
but we'll go ahead and have our columns
in blue and run our prediction on there
too you see they overlap the two graphs
so we have our test revenue and our
prediction and then finally what i
started with is we'll go ahead and plot
the prediction fully on the bottom and
run that and if you look over here on
the graph we put the blue lines over the
red lines and so you'll see a couple
spots where there's red underneath there
but for the most part our prediction is
pretty right on so it looks really tight
looks like a good set of predictions for
what we're working on and this is what
we were looking at the slide right
before we started diving into the r
studio we can see in the slide here
here's the red that's a little bit
better spread out than what i have on my
screen from my r studio and the graph
shows a predicted revenue we see that
the two lines are very close so again
they're tight it's right on this is what
we're looking for in our prediction
model but it's not good enough just
having a graph and you always do both
and the reason we do both is you want to
have a visual of it because sometimes
you look at these you get to the graph
you're like oh my gosh what is that and
then sometimes you look at the graph you
go that's right on how come my accuracy
doesn't look right and you realize that
your accuracy might be off or you're
plotting it incorrectly so let's go
ahead and look at the accuracy and we'll
use rmse that's going to be the variable
name we're going to give it and sqrt the
square root of the mean and mean just
means average and then we want
prediction minus
cells revenue and then we're going to
take this whole thing and we're going to
square it so what we've got here is
we're going to go through let's just
explain this formula we're using is i'm
going to look at the what i predicted
for to be and what the actual sales
what's our prediction versus our sales
comparison to the revenue and when we
compare those two we're going to find
the average and then i'm going or we're
going to square each one of those values
and then we're going to average the
square and then find the square root of
that and that's quite a mouthful the
reason we do it this way the reason we
square the value and then we find the
means is to generate an answer based on
doesn't matter whether it's plus or
minus there's a lot of other formulas
you can use there to check your accuracy
and all kinds of other things you can do
but a quick straightforward way of doing
is just like this and then let's go
ahead and run this
and then we type in the rmse and that'll
give us an actual printed value out
let's just see what that looks like and
so we have
866.6338 for our accuracy and so we look
at the
866.63 and we compare that to say 50 000
that's pretty close that's actually
pretty good accuracy given this model
takes a little bit more when you're
playing with accuracies and start
dividing out other different aspects of
it to get something you can communicate
better with but you always start with
the square root means that's always a
good place to start use case we'll start
with linear regression and then we'll
jump in and also do a decision tree so
you can see how they are same and how
they differ in the way they function and
what they're used for and while we're
going through the use case you can start
linking the theory behind it connected
to the actual use and as you connect
those dots you can see that by making
changes in the model based on the theory
you can also fine-tune it we'll talk
about that briefly as we go through this
for this analysis we'll use the default
cars data set to find the correlation
between variables cars is a standard
built-in data set that makes it
convenient to show linear regression in
a simple and easy to understand fashion
and we'll start with head cars and
stream cars now i've gone ahead and
removed the script on here to make this
easier to see and we don't have to do
any kind of importing of data because
it's already in there i'm just going to
type in head and cars and hit enter and
what you're going to notice is it's
going to display the first six rows one
two three four five six and you'll see
speed and distance now this is a little
bit like a spreadsheet so if you're
using an excel spreadsheet it should
look the same and this is a standard
data frame kind of setup
in this case whenever you say head it
usually lists the first number of rows
depending on your package in r it starts
with one and then it does the first six
rows string when we do a string of cars
it's going to break it down and show us
that it's a data frame it has 50 objects
that's with the 50 obs of two variables
and then we have speed the first
variable is going to be a number is what
any um stands for four four seven seven
it just starts listing the data there
and distance now i'm assuming the speed
is the speed of the engine otherwise if
it's the speed of the car then the
distance should always be the same and
that would need to be kind of silly uh
but this is the speed of the engine and
based on the speed of the engine can we
correlate that to the distance let us
visualize the data using scatter
scatterplot to understand the
relationship between predictor and
response and i want you to notice that
in this slide a line has been drawn
through the data now when we first start
with data you don't have that line so as
we plot it i want you to kind of
visualize that line there and just kind
of say hey does this stuff kind of
scatter around a line or not before we
actually create the model and we simply
do plot and i'm going to do a shorthand
here cars and i hit enter and it comes
over here to the right and plots our
data for us now because there's only two
variables i don't have to do anything
special i'm going to expand this over
environment so we can see it and you can
see you can just visualize a line right
through the data and it's all clumped
along that line which makes it really
nice for doing a linear regression model
now i did something a little tricky here
because i'm very lazy i typed in plot
cars a lot of times you don't want to do
that because you don't want to plot all
the data we could do plot in this case
cars dollar sign speed the first
variable cars dollar sign distance when
i do that i get the same plot all i'm
doing is telling it to use just these
two columns of data and although the
first one is quick and easy if you only
have two columns of data the second one
is what you really want to do when
you're plotting data like this you want
to control which columns you're using
and then we want to do a correlation
correlation analysis studies the
strength of relationship between two
continuous variables it involves
computing the correlation coefficient
between the two variables if one
variable consistently increases with
increasing value of the other then they
have a strong positive correlation a
value close to one and again i can just
type in correlation cars the short form
and we can see we have a nice
correlation going on there between the
two what we really want to do though is
we want to do we want to correlate cars
and then we're going to do the speed
comma cars and the distance and so when
you look at this actually hit the enter
key in the middle of that which is fine
you can in r do it line by line so if
you're trying to do a bunch of different
columns you might want to do that to
make it easier to read and you can see
here i get the 0.806894
the closer this is to one the more these
correlate so i'm saying there's a pretty
good correlation positive correlation
between the two variables and if you
look at it when i did just cars which
did the correlation over all the
variables you'll see that speed
correlates with speed 100 because
they're the same that's just the nature
of that but when you look at speed
versus distance we get the point 806 849
and if you do distance to distance you
get also one since they're identical
variables so now it's time to build our
linear regression model on the entire
data set to build the coefficients let's
just take a look and see what that looks
like back in our console i'm going to
type in linear
mod and in r we want to do the arrow
kind of like an arrow and a line or in
this case the uh
less than minus sign that's the same as
assigning whatever we're going to put
after it to this variable so we're
creating the linear variable lm stands
for linear module and we're going to
look at speed and distance so we put the
little uh squiggly bracket between them
this lets r know we're going to deal
with these two columns comma data equals
cars and when i hit enter on here we've
generated our linear mod now we want to
go ahead and summarize it and we simply
do a summary and then brackets
linear mod and it generates all kinds of
information on our model so we can
explore just how well this model is
fitting the data we have right now now
if you've done linear regression in
other packages and scripts you're going
to see that this is so easy to explore
data in r so even somebody who's working
in say python or spark hadoop coming
back to the r console to do some basic
exploration of the data is very
beneficial and if you use just r r goes
into all kinds of different packages and
you can even do cluster computations
through h2or there's a lot of cool
things this is what makes r so wonderful
is how easily we can summarize uh
something like a linear model and to
explore this data let's go back to the
slide and we'll see here where it says
the residuals the first one q stands for
first quadrant median third quadrant max
coefficients and we first want to do is
uh look at the estimate standard
deviation on here and you'll see the
intercept is at minus 17.5791
and speed
3.9324 so we can compute the distance
equals to the intercept plus the beta
times the speed which just means that we
can assign a distance equal to minus
17.579
plus the
3.932 times the speed the value of p
should be less than 0.05 for the model
to be statistically significant and this
is one of the things i love about r you
can see right down here we punched in
the numbers and it even tells you the
significant coats so if you're trying to
guess or remember it tells you right
away uh hey this 1.49 to the minus 12 is
way below 0.001 on there and even puts 3
stars next to it so you can see that
that is a very high correlation and you
can see the 0.0123 gets 1 star because
it is greater than the 0.05 or it's less
than the 0.05 so this shows that there's
a very high correlation statistically is
very significant so now we're going to
go ahead and create a training and a
test data set and the first thing we
want to do is we want to set the seed so
the sample can be recreated for future
use remember in all these different
regression models they usually have some
randomization going on in the back to
kind of fit it and guess where it's
going to go and if you want everything
to match you want to start with the same
seed in that randomizer that way it
recreates it identical every time you
run it on a different computer and just
like working with data in any package
we're going to create indices for the
training data and we're going to model
the training data and then we're also
going to do the test data and build our
model on the training data let's walk
through that and see what that looks
like in r
and i've actually mistyped that one
let's type in set dot seed to set our
seat for the 100 let's go
training
row index and that's our name we're
making up
for our variable and we're going to set
that to
sample now sample is a command in r so
we're going to take a random sample of
some of the data and then we're going to
do one
colon
in rows
cars so this is the number of rows and
cars and we're going to start with one
and sample them so row one of the data
we're skipping any of the titles on the
top and then in the sample there's two
variables that go in the first one is
the in rows of cars and then we want to
do 0.8
times in rows of cars this basically
says that we're going to take 80 percent
of all the rows in the car sample we'll
hit enter to go and run that and we see
here that i had an error and that is
because it's in row actually put an s on
there that's something i do a lot on
we'll just fix that real quick brings up
a point that if you use your up down
arrow you can quickly paste through the
last things you typed in and then you
can reuse them so there we go now i have
my training row index which is basically
a list of all the rows we're going to
use and now that we have a training row
index we'll go ahead and create our
training data variable and we'll assign
that
cars
and we'll put that as a training
row
index
and this is supposed to be brackets
because we're dealing with a
array type format we want to do the
trending row index
comma so we just need the index of the
row and as far as the columns we want
all the columns so here we have our
training data and now we set up a
training data we also need our test data
and with our test data we'll assign that
also to cars and because we use the
training row index and we use the sample
to create it we can simply do a minus
sign training
index comma so it's identical to the
training data but the test data is going
to be the ones the rows that were not
included in the random sample of 80 so
it's going to be 20 of the data and of
course it's very easy to do mistakes on
here so we'll come back into this bring
back up the error and then we have
training in the training index training
row index there we go very easy correct
errors in r so now we have our training
data and our test data so we have 80 in
training data and 20 in test data and
let's go ahead and create our model
we'll call it
lm
mod and this is identical to what we did
earlier here's our linear model we're
going to have our two different columns
if you remember above we had to tell it
which columns we wanted i'm going to do
distance and speed and then the data
we're going to set equal to
training
data so as opposed to doing all of cars
we want to use just the training data on
here and now that we've created a model
off of the training data we need to go
ahead and run our prediction off of our
test data we want to see how the test
data which has nothing to do with the
model so far how well it fits and we'll
go ahead and call it distance predict
there's our assignment so we're going to
assign the value to it and with r we
type in predict and then we called our
model we go and pull up our model lm mod
and so we're going to use that model and
then we want to put in the test data
here's our test data and so now we're
creating the distance predict variable
and we're going to put all the
information as far as predicting on our
tested data and we'll run that just hit
the enter key and then if you remember
from before we want to review the model
diagnostic measures and so we're going
to go in here and do a summary and
you'll see in here we have a couple
different things going on but let's go
ahead and take a summary of this and
just walk through what this means so
summary
and then we called it lm mod and we hit
enter and it pulls it up and we have a
nice summary of our lm mod and the first
thing we're going to note is that the
data that's right now in our lm mod that
we're summarizing is our training data
and let's go back to the slide and
highlight this so the data equals the
training data that's where the lm mod
comes in residuals we have our minimum
our first quarter medium third quarter
and the max or quadrant and a quarter
you'd think i was doing a business
analysis as opposed
into the year taxes in first quarter
versus quadrant we look at our
coefficient we can see where our
intercept is and our speed is and we
note over here are significance codes so
they all match up so a simple
correlation between the actuals and the
predicted values can be used as a form
of accuracy measurement so we take a
look at the model we created and now we
need to take our predictions and just
see how accurate it is on the data that
we didn't use to program the linear
model so let's create a variable called
actuals underscore
predicts
and assign it data frame this is how we
create a data frame as we just assign
data.frame and we'll do a c bind and in
our c bind we're going to take our data
and we're going to create a column
called actuals so this is our choice the
name actuals this is going to equal test
data and we're going to use a dollar
sign and distance
and we're running out a little room
there so let me pull this over so we can
see it better there we go distance and
then we also want to take and compare
that to what we predicted for the our
other data and we'll call this column
predicted
and that's going to equal our distance
predict remember distance predict is set
to the predicted values of our test data
so what this is is we're saying hey
here's our test data the actual data and
the distance and then we have what we
predicted that distance to be and we'll
go ahead and assign that and we can do a
quick head
actuals
predicts there we go remember head shows
us it's a data frame so it's going to
show us the labels and the first set of
data in this case you'll notice that the
rows have a different count they're not
one two three four five six well we
randomly picked twenty percent of the
data so this is the first six rows of
that random selection which comes out as
one four eight twenty twenty six thirty
one and we have the actual so the actual
value is two and the predicted value in
this first one is minus five so they're
way off uh 22 7 26 20 26 37 54 to 42 50
to 50. so one of these is actually
pretty right on where a lot of them are
really off at the beginning let's just
see what that looks like though because
just eyeballing the first six rows does
not tell us what's really going on so
let's create another variable
correlation
accuracy and we're going to assign this
correlation cor remember we do a cor and
we have our actual predictions so let's
just see how our actual values versus
the prediction correlate with each other
and we assign that one actuals
underscore predicts there we go let it
auto finish for me and we can just go
ahead and type in correlation
accuracy and see what that looks like
so we have an actual one a predicts 0.8
and you can see how they kind of
correlate we have the 0.82 and if you
remember
from before we're looking for um either
0.001 0.01
0.05 0.1 that is the p-value which we're
not looking at here this is not based on
the p-value this is based on we looked
at earlier with correlation that the
closer to one the closer the values are
and so 0.82 gets upwards of one so you
see that there's a normal pretty much a
correlation in here but we want to dig a
little deeper because this doesn't
really tell us how accurate it is for
that we're going to use another tool for
that we're going to calculate the min
max accuracy and mape and the min max
accuracy equals the mean value of the
minimal actuals and predicts over the
max value of the actuals and predicts
and then the mean absolute percentage
error or map as it's called equals the
mean of the absolutes predicteds minus
actuals over the actuals that's a lot to
follow as far as
remembering all these different math and
theory behind it cool thing though is
it's all done pretty much for you so you
can compute all the error matrixes in
one go using the regress eval function
in dmwr package since this is the first
time i've used this install i'm going to
have to install the dmwr package
so we can actually install it two
different ways if you remember we talked
about installing package earlier we can
go under tools and install packages and
i certainly could type in
dmwr it even comes up and lets me know
that's one of the main packages up there
well that's a tool of the rstudio setup
you can also install this using the
console we simply type in install dot
you can even see here it comes up
install packages and then dm wr and when
i hit enter it should do the same thing
as the other one does it goes through
and installs a dmwr package and it takes
just a moment to zip through all the
different package setup and then the
format for dmwr is dmwr
we're going to go colon colon we are
specifically looking at the
regression.eval
and then we're going to do actuals
underscore predicts
and we'll go ahead and put in the
individual columns on this one so we
have actuals i love the auto typing and
then again actuals predicts
and this time we want to do it against
the predictance i'll go ahead and hit
enter on here and you'll see in here it
comes up with our mae mse rmse and the
mape values today we're going to cover
logistic regression in the r programming
language logistic regression is kind of
a misnomer in that when most people
think of regression they think of linear
regression which is a machine learning
algorithm for continuous variables
however logistic regression is a
classification algorithm not a
continuous variable prediction algorithm
logistic regression is also sometimes
called logit regression in this video we
are going to learn why we would use
regression as a predictive algorithm
what is regression the different types
of regression as i've already mentioned
some regression algorithms are
classification algorithms and some are
continuous variable algorithms why use
logistic regression what is logistic
regression and then we'll look at a use
case a college admission
using logistic regression so why would
we use regression well let's say we had
a website and our revenue was based on
the traffic that we could drive to that
website whether through r d or marketing
and we wanted to predict the revenue
based on site traffic and the website
traffic would be related to the revenue
we could generate the more traffic
driven to our website then the higher
our revenue or at least that's what we
would intuitively assume and so we need
to predict the revenue based on our
website traffic here we have the plot of
revenue versus website traffic traffic
would be considered the independent
variable and revenue would be the
dependent variable often the independent
variable or variables if we had more
than one could be called the explanatory
variables and the dependent variable
would be called the response variable
however we typically refer to them as
independent and dependent variables so
our intuition tells us that the
independent variable drives the
dependent variable and if there is some
relationship between the two variables
then we would say that there is a
correlation between the two variables
and then we may be able to use the
independent variable to make predictions
about the dependent variable if you look
at the chart on the right you'll see
there is a clear trend between website
traffic and revenue as website traffic
increases the revenue increases and we
could draw a line to show that
relationship and then we could use that
line as a predictor line so for example
what will revenue be if our traffic is
4.5 k and we see that when the traffic
is around 4 500 the revenue is around 13
000 and if we could draw a perpendicular
line from 4.5 k on the axis on the x
axis the traffic axis up to the orange
regression line sometimes called the
line of best fit then we could draw
another line over to the y-axis the
revenue axis and we could see where it
lands and that would be the prediction
so in this example when the traffic is
around 45
000
hits the predicted revenue is around 13
000. normally we wouldn't actually draw
those lines we would generate an
equation and we would call that equation
a model and we could plug the
independent variable into the equation
to generate the dependent variable
output which we would call our
prediction so what is regression
regression is a statistical relationship
between two or more variables where a
change in the independent variable is
associated with a change in the
dependent variable it's important to
note that all variables are related to
each other for example a person's
favorite color may not be related to
revenue from a website here the change
in one variable height is closely
associated with the change in the other
variable age this makes intuitive sense
as you get older from when you're born
you get taller and it seems to make
sense if we plot that data we would see
those green points on the graph up to
some certain age where growth would
taper off in the plot in the middle of
the screen we see the clear linear
relationship between age and height
which is indicated by the solid red line
we sometimes call that line a trend line
or a regression line or the line of best
fit and we see that the height is the
dependent variable and age is the
independent variable of course you might
ask doesn't height depend on other
factors of course it does but here we're
looking at the relationship between two
variables one independent and one
dependent age and height there are
various types of regression linear
regression logistic regression multiple
linear regression polynomial regression
and there are others decision tree
regression random forest regression but
linear regression is probably the most
well-known and by definition when there
is a linear relationship between a
dependent variable which is continuous
and an independent variable which is
continuous or discrete we would use
linear regression when the y value in
the graph is categorical such as yes or
no two or false they voted they did not
vote they purchased something they did
not purchase something it depends on the
x variable logistic regression is when
the y value in the graph is categorical
in nature for example yes no purchased
don't purchase voted did not vote and it
depends on the x variable notice the
trend line for linear regression and the
line for logistic regression they're
different more on that later and there's
polynomial regression as well when the
relation between the dependent variable
y and the independent variable x is in
the nth degree of x sometimes we say an
nth degree polynomial of x in this
picture you can see that the
relationship is not linear there's a
curve to that best fit trend line so why
would we use logistic regression and we
need to understand why we would use
logistic regression and that linear
regression picking the machine learning
algorithm
for
your problem is no small task and it
really behooves us to understand the
difference between these machine
learning algorithms linear regression
answers the question how much so in our
earlier example if website traffic grows
then how much would revenue grow whereas
logistic regression would answer what
will happen or not happen linear
regression in general is used to predict
a continuous variable like height and
weight logistic regression is used when
a response variable only has two
outcomes yes or no two or faults often
we refer to logistic regression as a
binary classifier since there are only
two outcomes so let's try to understand
this with an example let's say we have a
startup company and we are trying to
figure out whether the startup will be
profitable or not that's binary with two
possible outcomes profitable or not
profitable and let's use initial funding
to be the independent variable here we
have a graph that shows funding versus
profit and it appears linear once again
our intuition tells us the more funding
a startup has the more profitable it
will be but of course data science
doesn't depend on intuition it depends
on data but this graph does not tell
whether the startup will be profitable
yes or no it only states that with an
increase in funding the profit also
increases that's not binary if we wanted
to predict how much profit then linear
regression would be useful but that's
not what we are being asked hence we
need to make use of logistic regression
which has two outcomes in our case
profitable and not profitable so let's
draw a graph again the x-axis will be
our independent variable funding and the
y-axis will no longer be the dependent
variable profit but it will be the
probability of profit for example if you
look at a company with a funding of say
40 then the probability that that
company will be profitable is around
point eight or eighty percent based on
the best fit line called a sigmoid curve
in the example we plotted several
companies some with 10 15 20 some with
50 60 or 70 or 65 and we indicated
whether they were zero not profitable or
a one profitable on the graph and this
is how we should think of logistic
regression in this example given the
amount of funding we calculate the
probability that a company will be
profitable or not profitable and if we
use the threshold line of 50 then we
have our classifier if the probability
is 50 or higher the company is
profitable if the probability is lower
than 50 percent it's not profitable so
what is logistic regression this is a
linear regression graph so let's compare
linear regression to logistic regression
and take a look at the trend line that
describes the model it's a straight line
which is why we call it linear
regression but using linear regression
we can't really divide the output into
two distinct categories yes or no to
divide our results into two categories
we would have to clip the line between
zero and one if you recall probabilities
can only be between zero and one and if
we're going to use probability on the y
axis then we can't have anything that is
below zero or above 1. thus we would
have to clip the line and once we clip
the line we see that the resulting curve
cannot be represented in a linear
equation so for logistic regression we
will make use of a sigmoid function and
the sigmoid curve is the line of best
fit notice that it's not linear but it
does satisfy our requirement of using a
single line that does not need to be
clipped for linear regression we would
use an equation of a straight line y
equals b sub 0 plus b sub 1 times x x
being the independent variable y being
the dependent variable but as we've said
we cannot use a linear equation for
binary predictions so we need to use the
sigmoid function which is represented by
the equation p equals 1 divided by the
quantity 1 plus e to the minus y e being
the base of the natural logs then taking
the log of both sides and solving we get
our sigmoid function and graphing it we
get our logistic regression line of best
fit the applications for logistic
regression are endless here we might
have a couple of gardeners or farmers
and one farmer says i am planning to
grow plants in my backyard and i want to
know whether my trees will get infested
with bugs which i'm sure is a huge
problem and these days the fewer
pesticides we use the better off we are
for health reasons the young lady says i
can use logistic regression in a model
to predict it for you and the other
farmer says well how is logistic
regression going to help but we know
that healthy versus not healthy is a
binary classification and we can use a
binary classifier like logistic
regression to solve this problem which
is what the young lady has in mind if we
have the data we can model the data and
generate a sigmoid function that best
fits our data and then use the
probability of a healthy tree versus a
not healthy tree or an infested tree
once again we see our sigmoid line of
best fit and some points that represent
healthy trees and not healthy trees once
again we plot our sigmoid curve scratch
that and once again once our sigmoid
curve is plotted and our threshold is
set almost always at 50 percent we can
use the sigmoid curve and calculate the
probabilities and hear the independent
variable of width which i assume is an
application of pesticides or no
pesticides to determine whether or not
the probability of healthy is greater
than 50 percent the threshold or less
than 50 percent the threshold and now
let's take a use case of a college
admission problem of course we can't
solve anything using data science if we
don't know what the problem is so we
need a problem statement and here the
problem statement is simple we are given
a data set and we need to predict
whether a candidate will get admission
in a desired college or not based on
their gpa and college rank so the very
first thing we need to do is import the
data set and if it's a small enough data
set and it can fit into our computer
memory then we are golden otherwise we
have some work to do the data set that
we were given is in csv format comma
separated values csv files are easily
imported into our environment once we
import the data we next need to select
and import the libraries that we will
need although r is a great programming
language with a lot of built-in
functions it is easily and powerfully
extended by the use of libraries and
packages then we need to split the data
set into a training set and a test set
it's important to note that the data set
that we've imported that we were given
has the gpas and college ranks for
several students but it also has a
column that indicates whether those
students were admitted or not so what
we're saying is that the data set has
the answers and if you think about it it
must how could we possibly train a model
have a model learn if we didn't know the
answers already that's why it's called
machine learning so we train the model
and then we test the model once we split
the data into training and test sets we
will apply our regression on the two
independent variables gpa and rank
generate the model and then run the test
set through the model once that is
complete we will validate the model to
see how well it performed so now it's
demo time so here we have our studio my
favorite ide or integrated development
environment and i have a script already
composed to run our logistic regression
on the problem at hand which is our
college admissions problem so let's just
walk down the script before i run it and
talk about the various points in the
script so first define the problem well
the problem was defined and you really
can't do anything in data science
without a clear problem definition once
the problem is defined you can load your
libraries acquire your data set the
working directory in my case i'm on
windows explore the data
munch the data if necessary and then
prepare the data scale the data if
necessary split the data into train and
test data sets then train the model
using the training data and run the test
data through the model and then validate
the model for accuracy and precision etc
so here we are going to load our
libraries i'm going to use a package
called ca tools and now that the library
is loaded i'm going to set my working
directory and if i come over here to the
files tab you'll see that there is my
working directory in that working
directory there's a file called
binary.csv
and that's the csv the comma separated
value file that the college gave me i'm
going to ingest and then look at that
data and as you can see it has four
columns gre gpa rank and then the answer
column admit whether or not someone was
admitted which would be a 1 or not
admitted which would be a 0. and if we
come over to the right and we can look
and see some of the first few values of
each of those four columns now let's
split the data we're going to take this
my data frame and split it into two a
training set and a test set and the
ratio we're going to use is 80 20. so 80
of the data will go into the training
set and 20 will go into the test set now
that ratio could be 60 40 it could be 70
30. typically it really depends on the
size of your data how much data you have
but this is for our example and for our
purposes 80 20 is perfect next we'll do
a little data munching in general you
munge the data early on after ingestion
and you really have to be careful in
this case we don't have any missing
values we don't have any real outliers
our data was pretty clean when we got it
and ingested it but in general that's
not the case and a lot of work and a lot
of attention needs to be paid to the
munging process here we're just going to
take our data the admission column and
the rank column and convert them to
categorical variables right now they're
integers as you can see on the right
hand side and once i run these two lines
they will be converted to factors and
all the fun stuff we're going to use the
glm function the general linear model
function to train our logistic
regression model and the dependent
variable is admit and the independent
variables are gpa and rank and the
little tilde sign here says the
dependent variable will be a function of
gpa and rank the two independent
variables the data will be the training
set and the family will be binomial and
binomial indicates that it's a binary
classifier it's a logistic regression
problem there it is we ran our model and
there's a summary of our model you can
see that there is some statistical
significance in gpa and in rank by the
coefficients and output of the model so
next let's run the test data through the
model
and once we have done all that we can
now set up a confusion matrix and look
at our predictions versus the actual
values again this is important we had
the answers and now we took and we
predicted some answers so hopefully our
predicted answers match up with the
actual answers we'll run a confusion
matrix as you can see the predicted
values versus the actual values and it's
important here to note that if it was
predicted false and it was actually
false and we see 189 or if it was
predicted true and it was actually true
which is 27 we did well on those but
there are few that were predicted
incorrectly some were predicted true but
were actually false some were predicted
false that were actually true and now
we'll confirm what our percentage
accuracy is 72 not bad but we could
probably do better how we'll leave that
to another video we'll be doing part one
of a two-part series decision tree
followed by random forest because
decision tree is part of the random
forest
what's in it for you first we'll cover
what is a decision tree what problems
can be solved using decision trees how
does a decision tree work and then we'll
dive into a use case survival prediction
in r so i'll be using the r platform on
this one so what is a decision tree and
here we have kind of a new looking front
panel of a car driving down the street
and we have left or right so you're
coming to an intersection you've got to
decide which way to go you're going to
go left or you're going to go right wait
or go so do you go right away we have a
green light so probably want to go right
away after you check for cross traffic
make sure no one is asleep at the wheel
or talking on their cell phone decision
tree is a tree shaped algorithm used to
determine a course of action each branch
of the tree represents a possible
decision occurrence or reaction so
another example is we have a shopkeeper
very nondescript shopkeeper
kind of a little spooky without the eyes
so we have a shopkeeper and he wants to
organize his stall so i must organize my
stall and just looking around trying to
figure out what the best way to stack
all his goods are and he has some
broccoli some oranges and some carrots
this is kind of a nice example because
we can see where some of the colors and
shapes overlap where others don't and so
you can create a tree out of this very
easily and say is it colored orange if
it's not well it goes into one stack
well that happens to be all their
broccoli and if it is colored orange
then it goes into another stack and
you're like well that's still kind of
chaotic you know people are looking at
carrots and oranges a very strange
combination to put in a box so the next
question might be is it round and if it
is then yes you put the oranges into one
box and no that means it's carrots we
put in the other box and you can see
that a lot of stuff we do in life easily
can fall into like a decision tree how
do you decide something you know does it
go left does it go right yes or no and
even with this we even have a number of
different end choices broccoli carrots
and oranges so what kind of problems
does this solve what problems can be
solved by using a decision tree
classification we're dealing with a
classification object belongs and from
our example we just looked at a carrot
is orange while broccoli is green i
guess it's nondescript day
without the faces i love it because a
lot of times you're looking at data you
really don't want to know what exactly
the data is but you're sorting it so you
know like this kind of represents data
that you really don't know exactly where
it's coming from just kind of a fun
thing regression regression is another
problem the biggest can be solved
regression problems have continuous or
numerical valued output variables
example predicting the profits of a
company so we have two different primary
problems we solve for we solve for
classification is it green or is it
orange and we solve for regression which
is a continual number so you know if
you're doing stocks this would be a
dollar value that ranges anywhere from
zero to thousands of dollars
how does a decision tree work terms you
must know first so before we dive into
the decision tree let's look at some of
the terminology for the decision tree we
have notes each internal node in a
decision tree is a test which splits the
objects into different categories and
continuing with our vegetable stand
example we have broccoli no it's not
orange and
oranges and carrots are and so on so you
can see right there where it splits
that's a note where the orange is split
in the carrot split into each of their
own groups and each one of the splits is
a node the very top node of the decision
tree is called the root node so the very
first one we started off where we split
it is a colored orange and you can see
that pointed out nice and neatly there
this is a root note at the top
each external node in a decision tree is
called the leaf node the leaf node is
the output and you can see down here we
have our broccoli and we have our
carrots and our oranges each one of
those is a leaf node so to quickly
rehash our terms we have anywhere there
is a split that's a node and if it's at
the very top of the tree that's the root
node and if you're at the very bottom of
the tree where you have a final answer
that is the leaf node so you have a root
node a leaf node and then everything
else is just a generic node so one of
the most important terms or concepts
that powers a decision tree is entropy
entropy is a measure of the messiness of
your data collection the messier or more
random your data higher will be the
entropy so we can see here we have
broccoli oranges and carrots are all
mixed together and it's very chaotic and
you could say this collection has high
entropy
and if we sort out the carrots out of
here in i guess just one of the broccoli
i don't know where the other broccoli
went you could say if you have just the
oranges and the broccoli this is a lower
entropy they say this collection has low
entropy but really it's higher and lower
one is higher or lower than the other
one and from entropy we get information
gain information is the decrease
obtained in entropy by splitting the
data set you based on some conditioning
so we have our information one which we
can actually there's actual computations
and so we have our e1 which is put in
there for our entropy value at the base
node or root node and we have 2 a2 which
is a leaf node and so we can actually
calculate just calculations for entropy
and we actually look at those two values
the entropy and we take that we have e1
is greater than e2 therefore the
information gain equals e1 minus e2 so
here we have another example we're
actually going to attach some values to
this one in just a moment so we're going
to move away from the fruit and we're
going to go to everybody's favorite
thing organizing a child's closet and
here we have a young lady waving at us
hi my cupboard is a mess i must organize
my stuff and we're going to classify the
objects based on their attribute set
using a decision tree we can look at it
we have a shape a size and a label and a
number and i'm going to go ahead and
look at the labels because that's really
what's key on this one is we have a
couple balls in here some books cards
and blocks and if you look at that when
you have something that's a ball it's
round so it's a very different shape the
book card and blocks are all rectangular
but the books are a different size so
the books have like a size six i guess
you think of them as very large cards a
size four so they're not quite as big
and the blocks are little lego blocks it
looks like kind of fun to stack on there
and then we can actually quantify this
and we can actually say hey we got we
have different numbers we have two balls
two books one card four blocks and we
split at each level based on certain
conditions on the attributes so
splitting aims at reducing the entropy
and we have a formula our first actual
formula you'll see happens in the
background when we're running these
different modules in r it looks a little
complicated but we can break that up and
simplify that for you so we see this
formula the negative sum of i of x
equals the probability of 1
value of x times the log of 2
probability value of x the term
probability value of x it's probably
easiest to understand if we go ahead and
put the numbers in there so you can
actually see what we're looking at and
so we plug the numbers in there we look
at the probability of the value of x
well if we do the number of balls
there's two balls and the total number
of items is nine so we have a
probability of any one object being two
out of nine so it's pretty
straightforward math when you look at
the numbers you can actually see that
very easily and so we can calculate our
entropy of two over nine times the
squared log of two over nine plus
one ninth the log squared of one over
nine plus four over nine times the log
squared of four over nine so our whole
group is the sum of all the entropies
and so when we look at that we have the
entropy of the balls which is the two
over nine rectangles two over nine i
mean books two over nine cards one over
nine and the blocks four over nine and
we just plug those numbers in there and
we get an actual value of
1.8282 so what the heck does a module do
with that number well now we must find
the conditions for our split every split
must give us the maximum achievable
information gain and so we have our
first entropy value 1.8282
and it turns out when you compare all
the different individual entropy values
for the balls for the books for the
cards for the blocks rectangles
you find out that the biggest split is
going to be in the shape and it's going
to be our first split will be on the
shape it will directly segregate the
balls out so they'll split up a huge
group from one from the other and so we
compute that we end up with e2 which is
all of our rectangles and then we have a
leaf node which is the balls and so with
our e2 we see that
1.3784 is much less than the
1.8282 and so our second split will be
on the size because that will directly
sacrifice and so we do that we can see
that if the size is greater than 5 all
the books go to one side while the cards
and the blocks go to the other and again
we have our e3 which equals point seven
one six which because it hits zero we
can say all our objects are now
classified with a hundred percent
accuracy if it was only that easy to
clean the kids closets and you can see
we have a nice drawing that shows you
where all the leaf nodes are on this in
which case we have the balls the books
the blocks and the cards are part of the
leaf note and the root node is our first
split we have all the object and we're
splitting them by rectangle by shape so
we're going to dive into the use case
and in this case we're going to get a
little morbid there i am behind my desk
that's definitely not me i don't have
that big of a chin but
i do usually have a cup of coffee and it
looks kind of like that it's sitting
next to me on my desk so the use case is
going to be survival prediction in r and
let's implement a classification of data
set based on information gain
this is going to use the id3 algorithm
don't let that scare you that's the most
common algorithm they use to calculate
the decision tree there's a couple
different ways they can calculate the
entropy but the formula is all the same
behind it so id3 is most common in r and
then we'll be using the r studio for our
interface for our ide so our studio is
free to download there's the free
version of course there's an enterprise
version and there's all kinds of
different choices in there but one of
the wonderful things about r is that
it's been an open source for a very long
time and is very well developed it's
also very quick and easy to use on a lot
of things so this is going to be in the
r studio ide there's some other
interfaces but this is i would suggest
using the r studio it's the most common
one and if you remember i said this is
going to be a little morbid because
we're talking about how many people
survived and died on these ships so in
this case we have a nice vacation cruise
ship going on there or i'm not sure what
that is with the tugboat so we have a
ship and it had 20 lifeboats the
lifeboats were distributed based on the
class gender and age of the passengers
we will develop a model that recognizes
the relationship between these factors
and predicts the survival of a passenger
accordingly so we want to predict
whether the person is going to survive
or die in a shipwreck and we'll be using
a data set which specifies if a
passenger on a ship survived its wreck
or not
so we're going to look at this data and
if you open it up into a spreadsheet and
don't forget you can always put a note
on the youtube video down below and we
will be monitoring that you can request
the data set sent to you or more
correspondence you can also go to
www.simplylearn.com
and request this data set too it's just
a simple csv file we'll send out and you
can see right here we have survived so
one indicates a person survived the
wreck
zero they didn't make it they were under
the water and the luxury class of the
cabin so if they were in you know what
what values like one two three depending
on what luxury class they were and then
we have this is kind of interesting how
many siblings they had so it's kind of
an interesting added statistic in there
number of parents on board and disembark
location so where the destination is
so here we've opened up rstudio and if
this is your first time in our studio
you'll notice that it opens up
standardly in three panes uh the upper
left is for text documents you're
bringing in and then the bottom left is
the actual commands as they're processed
and then on the right hand side you have
your environment set up on there now
i've zoomed in with my font a little
oversized in fact we can even take that
up one more size and see if we can get a
larger setup view here we go so i've set
the font for 18 probably a lot larger
than you need to work on a project and
the first thing i'm going to do is on
the right hand panel we're going to be
working with plots so i'm going to click
on the little plot tab up there and i'm
just going to collapse that over to the
side for right now and then i'm going to
open up the bottom one and you'll see on
the top i actually have
two different tabs team in the back was
really nice to set up the whole package
for what we're working on here and we'll
go through this line by line and then
you'll see as we execute the different
lines they're going to appear down below
so i'll be flipping between these two
left panels and let me go backwards just
for a second here we're going to do
this part
so let's take a look at the installs
and if you look at the installs and when
i'm not going to actually run the
installs because i've already run them
on my computer so these are already
installed in my r packages and i don't
have to re-run them
we have a number of packages we're
bringing into our because that's how our
functions and the first number of
packages are all part of our decision
tree so our f selector our r part our
carrot with the dependencies equal true
and our
dplyr and our r part plot oh that's a
mouthful you'll need to import all of
these in your setup and the first one
which is your f selector compute your
chi squared your information gain your
entropy all of those formulas are part
of the you bring it in that's what that
package is the r part is your
partitioning of your decision tree so
these are your primary two decision tree
modules that we're bringing in
the carrot is a part for actually
splitting apart the data because we
always want to do a test set a train set
so they built a package carrot to kind
of handle all those you'll see carrot
used a lot of other machine learning
packages when you're doing the r
the
dplyr is for mutating your data so you
can sum it you can classify it you can
filter it
that's all part of the dplyr
and then our part has another piece
called plot and that's for helping us
plot the data as we sort out the tree
because the r part creates a tree but
you still want to be able to plot it and
then we come down here and we have some
additional tools we're going to work
with a spreadsheet this is a microsoft
access spreadsheet so we're going to use
the xlsx package to import it and you
might think it's a little outdated
because i know when i'm doing any new
projects i try to save my data into a
comma separate variable file or
something more standardized but a lot of
times you'll end up with these old
databases where they're exporting data
and their export comes in an excel
spreadsheet and guess what this is a
time saver instead of opening up the
spreadsheet re-saving it as a comma
separated variable file you can just
open it right up in r which is really
nice and then we have our data tree
which goes right along with displaying
the data so we have our rpart.plot and
we're going to put that into the data
trees have a nice visual and finally you
need to install the package ca tools
and ca tools is another package used for
general manipulation of your data and
your vectors so it also has a split
function for training data i know we
have another package for that but both
those packages fit in there as far as
the working with the data so a lot of
times you'll see these loaded in there
together and you'll see in a moment
we'll import the elementary stat learn
package also which is very generic
working with data setup as far as
statistics and data processing now
because my java setup is a little
different because i use a developer
package well the latest update of the
developer package and the r are
struggling to see each other so i need
to go ahead and add this in chances are
you don't need to worry about this but
if you do get an r java error you'll
need to go ahead and add an align to set
your environment and i'm just going to
set the java home i went ahead and
looked up the last solid jre folder and
you'll see under program files java you
can look them all up to make sure it's
the jre and not the jdk the difference
between those is one is a standard
executable that's a jre folder it is a
folder not an actual file and the jdk
deals more with developer site on things
and i'm going to go ahead and run this
and if you're in r you can run these
lines individually by holding your
control key down and hitting the enter
button so i've now loaded my workspace
and set my environment here and we can
actually look and just make sure a
system dot
get environment is the opposite one
and i can just look real quick and say
hey what's my java
home set to
and if i run that
whoops i forgot to put brackets around
it
there we go
and if i do that you'll see that it's
set to the jdk18025
and again we're looking at the split
screen and let me just move this up here
a little bit and let me hit the enter
key a bunch of times it comes up on the
screen
when i run it and i'm running it line by
line the run text then appears down here
and this is what's actually being
executed so i could just as easily type
it in down here as putting it up here on
the top part
that's just a little bit of r studio and
working with our studio
so i've shown you the installs and again
i've already installed it on my computer
so it takes a minute or two to install
it
we need to go ahead and then bring those
libraries in to work with
so
basically we're going through all the
different libraries we're using
elements.learn is a standard r library
so it should already be installed you
don't have to install that one and i can
just go right down them and you'll see
as i install them that they show up on
the bottom and every time you go to run
a new setup
you need to go ahead and install these
pull the libraries in you got to tell
your r studio these are the libraries
you're using
and you can see right down here that
as it executes them
it has all the different things one of
them had like couple different parts to
it so loaded the lattice and the gg plot
to
really nothing to worry too much unless
you see errors down here so if you start
getting error codes
then you might have to go troubleshoot
and find out why it's not installing
correctly or did it install correctly
but it's pretty straightforward we've
covered all the libraries
and again if you want a copy of this
code simply let a comment down below on
the youtube video or visit
www.simplylearn.com
and they'll be happy to send you a copy
of the base code
next step is we want to go ahead and
load the excel file
after setting the working directory
so let's go ahead and set the working
directory scwd is a command for that in
r
and then this is wherever you have
downloaded your data to and your files
to a lot of times if you get the zipped
framework sent to you it will unzip all
into one location
and then you gotta look that location up
and put that in here a lot of times it
automatically sets up for you if you're
if you're working in the same directory
in this case you can see i've pasted in
a very lengthy
path because i have this as part of my
documents my business docs and simply
learn and marketing videos and so on
i'll go ahead and do the control enter
and run that so now my working directory
is set
and you can see down below it's executed
this on the
console down here
and if i open up that working directory
you can see i have a ship xls file
that's the data we're going to look at
and we looked at that on the slide
you could also open this up and i want
to go ahead and open it up with a
notepad
by default it'll open up as a
spreadsheet oh it is an excel file so
i'd have to open it up as an actual
spreadsheet wasn't thinking that one
through this is one of the problems with
saving stuff as a proprietary ship soft
file even something as widely used as
microsoft
excel spreadsheet
but when we open up the spreadsheet you
can see here the same data we looked at
before
this was coming up in this file
and so i take this file we can go ahead
and let's um
create a variable we'll call it add
and we're going to assign it
that's what the lesser than dash means
if you've never used r before we're just
going to assign it a ship xlsx
and then i'll do my control enter and
you'll see it run down here ship
add is assign shipped.xls
our excel spreadsheet on there
and then let's have a another variable
we'll call it df for data frame and
we're going to assign this
read
xlsx and that's from the library slx
that we just imported
and we want to read the file add
because we just created that variable
with the ship.xls
and i didn't catch this right away but
you'll see when we open it up in a
spreadsheet
it might be hard to see in your video
screen it has a the first sheet is
called ship one so this data is on the
page ship one in the excel spreadsheet
and we gotta make sure that we tell the
reader to look for that
and so we can simply do that in our
command line
sheet name
equals
and it was ship one on there
so we go ahead and hit the control enter
and run this line and it brings it down
below and executes it it's now reading
in that data from that spreadsheet
that's what's going on and it takes just
a moment to go ahead and read it in
and then with any data processing we
need to select the meaningful columns
for the prediction so this is important
that we sort through our data and set it
correctly so that whatever we're feeding
it into can read it correctly
we can do this with
df
we're going to reassign it and we're
going to select
and from there we're going to select df
as our main data frame going in
and then list the columns that we want
if we look at the
data we have survived we have a class we
have a name
sex and age
and then there's siblings and parents
we really don't want to know the name
because we don't need the name to run
this on here and then we got to decide
which one of these we want to work with
and the guys in the back played with the
data a little bit and they came up with
the following columns
survived which we need definitely
because that's what we're actually
trying to figure out is whether we're
going to survive or not
what class they're which which class of
the boat they're writing in with their
first class second class
sex and age we went ahead and dropped
the rest of them at least for this
example
and so i go ahead and do my control
enter and you'll see it execute down
below
and now df
equals
these three columns or
four columns
plus our answer is one of those so i
always think of it as three columns plus
one but it is four columns
and then for the next step we want to
make sure the data is in the right
format so again i'm going to resign the
df
and we're going to mutate the data
and that means we're going to take each
one of these columns and we're going to
let it know what kind of data that is
so our first one is let me put the
database in or the data frame in there
df
and then we want to go ahead and do
survived equals a factor of survived so
this is a yes no it's letting us know
that this is a 0 or a 1 on here
and then for the class and the
age those are as numeric
there's other ways to process class
since it is a very restricted number on
there but for this we're just going to
do it as a numeric value same thing with
age and you see we didn't do anything
with sex we're just going to leave that
as male or female and that's fine for
this because it'll pick up on that when
we run the decision tree
and we'll do our control enter so now
i've assigned the new df and you can see
it comes down below
as we run it on there so this is now
loaded and df now is a mutated data
frame that we started with and we've set
it up with our specific columns we want
to work with in this case survived class
sex and age and we've formatted three of
those columns to a new setup so one of
them is factor that's what we use for
survive so it knows it's a zero and one
and then we switch to class and age to
make sure that it knows that those are
numeric values
now that we have our data formatted we
need to go ahead and split the data into
a training and testing data set
and by now if you've done any of our
other machine learning tutorials you
know this is standard you need to have
your data that we train it with and then
you got to make sure it works on data
that it hasn't trained on
so we'll go ahead and do the splitting
let's take a look and see what that
looks like
and when you're dealing with splitting
the data we're doing a little randomness
so we'll just go ahead and set the seed
to one two three you can set that to
whatever number you want that's just
setting the random variable seed
and we'll go ahead and hit enter on
there
and to do this we're going to create
sample and we're going to go equals
sample dot
split
so we're splitting up the data
this is actually creating a whole new
column
trues and falses
it's all stored in sample and we want
our data frame and we use a dollar sign
to denote that we're going to be working
with
one of the different columns a nice
thing you can do an r
and it actually shows my survived so df
survived
and then our split
ratio
split ratio i want to add a period in
there which it doesn't have
our split ratio is going to be equal to
and we'll do 0.70 so we're going to take
70 percent of the data
of those that have survived and we're
going to just randomly select which one
of those we're going to use for training
and which one we're going to use for
testing it out later
we'll hit our enter and load that sample
in there
and then we'll go ahead and create our
train and our test data set so i'm going
to set train equal to
and this is where it kind of gets
interesting because we're going to do
subset
and then we're going to do a subset of
our df
where sample
and then we're going to use equals since
this is a logic gate so wherever it sets
sample equal to true
we're going to use that line in df
and this is of course
r so it's all capitalized for true
so here's our trained data frame we're
creating so when i execute this and it
shows up down below
it goes up and says hey this sample
created a whole bunch of trues and false
and we're going to take those trues and
falses and wherever we've created a true
which is 70 of the data randomly
selected that's what we're going to
train the data with and then we need our
test set
and that's a subset just the same thing
we just did
we need our data frame and this is where
the sample equals
false
and again when i hit ctrl enter it goes
down this line of code and it runs it
down here in our terminal window
so now we've nicely split our data up we
have our training data set and our test
data set
and this is of course we get to the
point of what we're actually trying to
do which is create our decision tree
and we're going to do that by training
the decision tree as a classifier
this always gets me you spend all this
work sorting through the data
reformatting it in this case mutating it
and filtering it and creating a train
set and a test set
so we're going to go ahead and create
our tree and we're going to assign to it
the r part
that's the module we imported
and for the our part we need to go ahead
and
do what's called the survived and this
is kind of a little complicated to look
at so let's just take a quick look at
this i'm going to go backward on it the
first part is we're importing the data
so data equals train
and then we want to know is who survived
so
the first part we put in here is the
column we're looking for is survived
and you'll see that we have added the
squiggly mark in the period because
we're looking just for this prediction
we just want to know who survived from
these boats
for programming our decision tree and
i'll go ahead and hit ctrl enter and
we've now created the tree that tree
with all those values is now in there
and if you remember from before we've
taken from when we did the closet full
of clutter we've done the same thing
where we have our entropy and that one
line computes the entropy and the
increase in information and it brings
entropy down to zero so it's the same
thing but instead of applying it here as
we did visually to the closet it's now
applied it to the data on the ship it's
now looked at the survival of zero or
one based on female male what age they
are where the best split of the age is
and what class they were in you can see
we have first class if we go down there
i think there's
third yeah third second so i guess it's
class one two and three but you can see
that's how i split it up so it's use
these different variables to minimize
the entropy and maximize the information
gain and that's all done in that one
line of code
boy we worked on all that theory and uh
computer doesn't fear in one line of
code it's important to understand the
theory behind it so you know what's
happening because as they make changes
to these or if you had special settings
on it you have to know what they're
doing
but in this case you can see that we
once we get it formatted the computer
does all the heavy lifting for us
and it never ceases to amaze me just how
advanced machine learning has become
these days and how easy it is for people
to get into it and learn the basic
setups and be able to compute your
different machine learning models
once we've computed our decision tree
and we built it on there we want to go
ahead and start doing predictions with
it
and what are we going to predict well
you remember that we created our data
with the train and test so we have our
training data which we programmed the
tree for
now we want to see what it does with the
test data so let's see what that looks
like and we can wrap that into tree dot
survived dot predicted
we'll assign that to our prediction
this is nice because we have the command
predict from one of the modules the r
part module that we brought in
i can simply put in here that we're
going to use the tree
we have our test data
and type
this happens to be classification or
class
and this would also be if you were to
distribute this tree you could save the
tree and send it someplace else to be
used on new data this would be the
command line you would use to predict on
here predict you have your tree saved
you have whatever data you're putting
through and you let it know it's doing a
classification on the tree
and we'll go ahead and run that so now
that's loaded up
and now that we've gone through we've
trained our tree we've run a prediction
on our test data remember the tree had
never seen the test data till now so
we've already built the tree and we're
just testing it out
and to do that we're going to use a
confusion matrix for evaluating the
model and let's see what that looks like
that is simply confusion
matrix so once we have the confusion
matrix
which came in from our carrot we need to
compare the two different pieces of data
and the first one is going to be the
value we generated up here we have tree
dot survived dot predicted
so i'm just going to copy that and paste
it down here
and then the second one is the tree
we're comparing that to
let's see survived
it's not the tree it's easy to get
confused sometimes when you're moving
these things around
we're going to compare it to test
the test group of data specifically
the survived column
and let's go ahead and run this and see
what that looks like
and so let's go ahead and scroll up a
little bit here to where it first
executed the confusion matrix
and the most important part of this is
that we have our prediction
and our reference and so zero means they
didn't survive one means they survived
we had our prediction on here
versus the actual and that's what these
two columns mean
and so when you read this little chart
we have 0 0
so both the prediction and the actual
reference said agreed on 218 values for
people who did not survive a little grim
there for data remember i'm talking this
is morbid data talking about death rates
of people on boats it says that they
both agreed on 93 so 93 of them where we
predicted the value that they would
survive and the reference came up the
same
likewise
the predicted were predicted one
and the reference said no these people
didn't survive well our prediction got
25 wrong
and where it says they didn't survive
and they actually did it got 57 wrong
and we can come on down with the
confusion matrix and we can look at the
accuracy comes up with the 0.7913
or 95 percent ci
so you're looking at pretty high
accuracy
information ratio p-value when you're
getting statistics of p-value is one of
the things we look at remember that
there are certain values if it's over
0.95
it's considered highly valuable variable
0.5 means maybe anything less than that
means it's probably not connected
you have to look up the p-value a little
bit more on that
then they just have more information
down here
which you can go through and
which you can go through but the
important part is that we have a very
high accuracy
that's what we're looking at this is a
very accurate model and again always
remember the domain you're working with
certainly this is an example and this is
a good little piece of data you can play
with but if i was say betting money and
my accuracy came up as 79 or over 95 ci
i'd be like yes let's bet on that that's
a good bet but if i was betting on a
life or death situation i might kind of
rethink that a little bit if that is if
it's going to affect my life that way
you can also look at this as if you're
trying to figure out where the boats go
in this particular example and where to
put the lifeboats that's actually pretty
good be able to say hey we can increase
the chances of people surviving this
accident by putting the lifeboats in the
appropriate places
and then this is always fun let's
visualize the decision tree
one of the cool things you can do inside
of r real easily
and if you remember one of the modules
we brought in has a rpart.plot
we're going to take that and we're going
to dump that into the prp for the tree
and let's go ahead and hit enter
and this is fine let me move this over
so you can get a nice view you can see
how it actually split it up and let me
just rerun that again now they expanded
the right you'll see right here where
one of the biggest splits was whether
the person was male or female
didn't know that was probably one of the
biggest breaks as far as lowering the
entropy and increasing the information
gain
and then once you know sex is male it
came down here and it said you're not
going to survive i guess the men were
throwing themselves overboard on the
boat i don't know maybe they're being
very gallant and letting the women and
children survive which is what i hope
that means so once you know that it's
not a male and it's a female passenger
if you're in class three or above you
had a high chance of surviving if you
were less in class three class one or
two then it was based on age and so you
can see where age was greater than 19
so anybody over 19 had a high chance of
surviving and then if they were or less
than 19 i'm sorry if the age was a
little confusing on that one but if the
age was one sided 19 they probably
survived if not then i said hey if your
age is less than 60 you probably
survived and if not
people who are older probably didn't
make it out of the boat in time so
elderly have a much higher risk which
you would expect you'd expect that to be
the case that elderly would have a hard
time getting off into the lifeboats and
they would probably step aside for the
younger generation i know i would you
know so that might be more of a social
norm statistic as opposed to survival
rate let's go and put this back on over
here
and see what else we can do with this
data
you know if you're working with the
shareholders it's good to have a lot of
tools as far as how to display you know
they'll want to see this number here
where you say hey this is what the
confusion matrix looks like and we got a
0.79 accuracy that would be like the
number one thing you would talk to the
shareholders about other data scientists
care about the p-value and the
information rate and information like
that but this is really the core right
here this is how much data and this also
shows them how much data was looked at
so you have a you can actually just add
these numbers together and you can see
how many different people were in the
this particular data set i mean lines in
the data set but we go ahead and
visualize the 2d decision boundaries and
let's take a look at that that's kind of
fun
it's a little bit more complicated so
i'm not going to go into it in too much
detail today now the guys in the back
put this code together for me which is
great so i'm just going to paste that in
because like i said we're not going to
go into it into too much detail i want
to show you that there's some other ways
to display the data that is not just
this nice little tree over here which
gives us a little bit more maybe view of
what's going on as you dig deeper into
data that becomes very important it
might not be something so simple where
you can look at in this case
one two three four nodes so they have
four major nodes that they split the
data in
so we're looking at this we're going to
go ahead and create a set test
equal to test comma c241
we're just assigning color column to go
with the data
okay that's what that is
and then we're going to take set and
have it equal to test
and we'll do x1
and we're going to set that equal to
minimum set of one minus one max set of
one plus one
by equals point zero one
they had to play with these numbers a
little bit to get this to look right so
we're creating a sequence on here from
our the set we created so our test set
and then x2 is also creating a sequence
based on different values
and then we're going to put that all
into a grid so let's go ahead and run
that through all the way down to the
grid
and they'll make a little more sense
when we add the column names to the grid
okay
so we'll set our test we'll create our
x1 sequence and our x2 sequence
remember a sequence is just like in this
case starting with 0.1 it's like 0.01
0.02 0.03 0.04
so we're going to create a grid with all
these values separated by 0.01 so that's
all we're doing right now
let's go ahead and set that up there's
our grid and then we want to go ahead
and create some column names and for our
grid set we're going to have our c class
and age
so we're just doing the class and the
age on here remember we did sex also
before so it's just class and age on
this chart that we're pulling out
and we're going to create a y grid and
the y grid is going to equal tree
survived predicted
okay so here's our y grid and our
predicted values on there
now we're actually going to do something
we're going to plot and we're going to
plot the y grid oops hold on let me see
what's in here
and let me go ahead and just run the
plot it's got to be the
there we go so let's take a look at this
and see if we can make a little sense of
this code and what's going on here
because there's a couple things going on
what we've done is we've plotted a
decision tree classification data set
we've done our plot and we have our set
so our sets equal to our test data
and then we have our main we went ahead
and give it a name decision tree
and then we have our x lab
and our x limit and we've set those up
as class with our y lab equals age and
our x limit equals range x1 comma y
limit equals range of x2
and when you look over here you can see
that we have age on the left and class
on the bottom and so based on age and
class
and so once we put in this nice graph
here we want to go ahead and add our
contours and our points so let's go
ahead and run that and i'm going to run
all three of these so we can see
all the colors come in on the graph and
this is actually going to take a little
bit because remember you can make it go
a little faster by changing a couple of
the notes up here well we did 0.01 from
the min to max
by changing that to 0.1 or other values
you can actually it'll run a little
quicker and we're going to create a nice
little decision tree classification test
set just to see what that looks like and
how that data displays
now that was taking so long to run with
0.01 i went ahead and just did a 0.1 so
instead of filling the whole page with
different colors for a really pretty
view now i just have these different
colored lines coming across it gives us
another model that we can show for
whoever you're working with in this case
your shareholders of the company or the
individuals who you're posting to the
web for and you can see where we have
the green and the red and we've set
those up to identify from the decision
who's going to make it and who's not
based on age and class
now my favorite one on this and i'm
going to go back and just highlight this
really quick was let me just rerun that
go back to prp tree where we display the
tree
and that's where we split it in part
because this information
gives us some keys that we can then use
in other models so it's good to know
that sometimes what you're taking comes
out and maybe you create a bucket for
people under 19 between 19 and 60 and
over 60 and different things like that
so you can actually put people in
buckets in a class age split just a
quick note on there again you know lots
of different things from which you can
display the data and you can see how we
use the decision tree to figure out
chances of somebody surviving based on
their class and their age let's go ahead
and take a look at the decision trees so
let's go ahead and talk about another
algorithm decision tree decision tree is
a tree shaped algorithm used to
determine a course of action each branch
of the tree represents a possible
decision occurrence or reaction there's
a tree which helps us by assisting us in
decision making let's look at the basic
terminologies to understand decision
trees we have a root node we have a
splitting we have decision node decision
node and then those split into terminal
node decision node terminal node
terminal node and each decision node
continues to split until it ends in a
terminal node note a is the parent node
of b and c also note we call the
terminal nodes leaf nodes you can also
see that we have a branch or a sub tree
so when you have a split everything
under that split under one side is
called a subtree you cannot build your
decision tree without knowing entropy
and information gain entropy entropy is
the measure of randomness of impurity in
the data set so we have here a bunch of
fruit and you can see we have it looks
like apples oranges and bananas
it's very chaotic so it's a very random
data set has a very high entropy and if
we take out one group let's say we take
out the apples it's a little less random
now we only have bananas and oranges so
it's a less random data set and we have
lower amount of entropy entropy is a
measure of randomness or purity in the
data set so when you have a homogeneous
data set we will have entropy equals
zero and here you can see it's all
oranges an equal divided data set will
have an entropy equal to one so here we
have a half or um what is it looks like
a couple bananas a couple oranges and a
couple apples so everything is equal so
we have an entropy of one on there since
there's two of everything information
gain it is a measure of decrease in
entropy after the data set is split also
known as entropy reduction so we look
over here we have an entropy equals e1
and the information gain as we split the
bananas out
the size becomes smaller and you'll see
that e1 is going to be greater than e2
or we measure e2 of the apples and
oranges in this case if you love your
fruit it's probably getting you hungry
right about now and then the information
gain from our level 2 equals e2 minus e1
where e2 is greater than e3 as we come
down here we'll see that the e3 is now
the third level we split the oranges and
apples out and each time that entropy
becomes less and less and less until in
this case we have an entropy of zero
since they're all homogeneous in their
order all oranges all apples all bananas
so let's take a look at the use case
decision tree and actually put this into
code and see what that looks like as we
go from theory to script and to predict
the class of flower based on the petal
length and width using r and you'll see
here we have these beautiful irises
probably the most popular data set for
beginning data analysis and statistics
we have the setosa the virginica and the
versa color let's install the packages
that will help us in the use case uh so
because we're doing decision tree we
have our part our part.plot and then we
have the library our part and the
library our part dot plot let's go ahead
and take a look at that and if you
remember we have two ways of installing
it we can do the install packages we did
that earlier here we go install packages
using my up arrow to get to it or
we can go up to tools install packages
and i can do our part and our part dot
plot and then go ahead and hit the
install badges it will be updated this
is our currently loaded oh it looks like
some of them already loaded that's fine
we'll just go ahead and update those and
once we have those updated let's take a
look and see what we have going on here
hope still loading let's go ahead and
type in library
our part and library
our part dot plot so we want to bring it
into our console with we're working in
so up and now we just download the
packages and now we want those libraries
available to what we're doing
so the iris although it's built into the
r package download on the basic download
we actually have to install the data or
bring the data in so we're going to do
the data iris and then we're going to do
the string iris and let's just flip back
on over to our r studio and set up our
data iris
and then we're gonna do string
before we do that let's do head head
iris
there we go and you can see the head has
sepal length sepal widths pedal length
pedal width species in this case the top
part's all setosa and each row gets a
number and then we did the string
iris and that comes up with information
and if we flip back to our slide you can
see here the structure of the database
under string is it's a data frame as 150
objects five variables each so each one
has five variables all features are
numerical except species which is
categorical as it is our target variable
which we want to predict so we want to
predict whether it's going to be a
varicosa versus color setosa so you can
see here we want to predict all the
features are numerical except species
which is categorical it is our target
variable which we want to predict and
remember that's the setosa the
versacolor the virginica the three
different categories we saw earlier in
the beautiful pictures of the flower and
so we're going to go ahead and use the
set seed to decide the starting point
used in the generation of sequence of
random numbers remember we set the seed
so that if we ever want to
reproduce what we're doing it will
reproduce the same thing each time
because using the same randomizer seed
bring slightly different results but
depending on what you need it for and
then we're going to generate random
numbers using run if and the question is
why do we want to
create random numbers on here that's
kind of an unusual thing to do for in
rows iris let's go back and just take a
quick look here actually we want to go
back to our r studio and you're going to
notice when i did the head of iris what
did i have i had setosa satosis at
setosa setosa setosa so the data is
organized by species and it's already
grouped them together in the data set
well we want to randomize that so that
doesn't affect our output and then we'll
go ahead and take and create our iris
ran
where we recreate the data frame based
on a random order instead of setosa all
being grouped together let's go ahead
and put that into our rstudio and set
dot seed and we'll match their number
9850 this way we'll have identical
results to what they're doing on the
slides then we're going to set the
variable
g
equal to run if and base this on in row
of iris
and finally we're going to create iris
underscore ran
and we're going to assign our variable
we're creating iris underscore ran and
we'll set that equal to we're gonna do
that the original database we're gonna
set that equal to iris and we need
brackets because we're assigning the
valuables and we want to change the
order so there's an actual
command order we can do it's going to be
order
g
so this means all the rows in the order
that g is so we just randomized all
those rows and then we want to keep all
the columns we'll just put a comma and
we'll leave it blank for the columns so
now iris ran we'll have some randomized
rows coming in and if we do head let's
just do this real quick
iris
ran we can see there's no longer setosa
setosa we now have virginica setosa
versicolor setosa and you can see the
row numbers at the beginning we have 103
20 63 17 so they're randomized on there
pretty well now that we've done a little
data manipulation we can go ahead and
build our model using our part as
follows and one of the things i want you
to notice here is we're going to build
our model for our r part on the first
hundred because our data equals iris ran
one to a hundred there's 150 objects so
we're keeping a third of them off to the
side that way we have our training set
and our test set already pre-built
because we randomized the rows going in
from the beginning and then once we've
done that we want to go ahead and print
our model out and take a look at it
let's go ahead and put this into our r
studio and take a look and so we have
our model we're going to call it model
that's our variable name that we're
giving it we're going to assign that our
part and in brackets we'll start with
species and we want to correlate species
to all the other columns so how do we do
that and the shorthand is just to put a
period there and then we're going to put
a comma and we want to set our data
equal to iris
rand that's the data we put in there and
remember i said we're going to do just
the first 100 rows so we can save the
last 50 for testing so we can do that
simply by doing row one to one hundred
so in r you start with one for your rows
and because we want all the columns we
just put a comma and that denotes that
we're going to use all the columns in
here and in the
our part there's different methodologies
we're using the method equals
class
and you realize that we're doing a
classification between the three
different types of flowers and so this
is what we want to go ahead and set up
our model with and when we run that
we've now created a nice model for us we
can go and just type in model hit enter
and you'll see it'll print out a bunch
of information about our model so let us
know what's going on in there and let's
just break that down and see what that
looks like so we have our target our
predictors that species and it's going
to relate to we used a period to denote
all the columns we're going to take the
first 100 rows to train the data and we
have the defining method to be used as
classification that's what the method
equals class is and what we want to do
next is we're going to do a plot and
take a look and see what this actually
looks like a little visual here which is
nice and we simply do that with our plot
for the model and let's break this down
a little bit and see what that looks
like in the r setup and we do this
our part dot plot a little bit of a
mouthful there sometimes that's part of
the r part package or module so when you
imported this in it also included its
own plotting format and we're going to
send it our variable model and there are
actually a number of types of plots
listed for our part but we're just going
to do type equals 4 fallen dot leaves
equals t
and
extra equals 104. let's go ahead and hit
enter on there and you'll see right over
here on the right hand side it goes into
our plots let me just enlarge the large
that
we have our versa color our setosa
versacolor virginica let's go ahead and
break that down as you can see down here
it represents the tree as it splits it
looks at it and says hey if the petal
dot length is less than 2.6 it's going
to be a setosa
so 34 percent of them fall into that
category and if the petal length is
greater than 2.6 then looks at that and
says okay now i have uh three different
variables we're going to split it again
pedal width is less than 1.7 it's going
to be a versa color and if it's greater
than 1.7 it's a virginica and so you can
see here it's very simple tree it forms
it's only got a couple levels you can
see a branch off to the right very nice
depiction of what's going on as far as
the model now that it's created but once
we create the model we need to figure
out is this model any good so let me go
back over here and let's go back to our
slides and the first thing we're going
to do is we're going to take our test
data remember we saved 50 rows for the
end to go ahead and run a predict on
there so let's go ahead and run that
prediction so we'll call it model
underscore predict and we're going to
hit a wrong button there to predict and
we're going to what are we going to
predict we're going to predict our
initial model and we need to send the
data so data equals and we don't
actually have to put the data part and
we just put in iris underscore
ran and then we want to do just the last
50 rows and we denote that by 101 colon
150 and then comma and we'll leave that
part blank since we want all the columns
to show up and then finally this is a
classification so we need to let it know
that type equals class there we go and
let's just see what that looks like
model predict and so this is what the
data is put out as is it says whatever
this is 131 is a virginica number 10 is
a setosa line number 95 is a verse of
color and so on all the way through what
we really want to know is how good our
model is so we're going to go ahead and
install a couple more packages
we're going to install the packages
required to use the confusion matrix and
to do that we're going to install carrot
and e1071
and then of course we want to set them
to our project by using the library
command and so we can simply install
packages and we want carrot and we'll go
ahead and just do that right there
there's carrot installed and we went
ahead and did the install for packages
e1071
let's see it goes through there's really
quick install compared to the carrot and
then once we've installed the packages
we need to load it into our workspace so
there's our library carrot
there it goes and once that's loaded
we'll go ahead and also load the or set
the library to the
e1071 so that we have both those
libraries available to us and here we go
library and then this one is
e1071 and now these two libraries are
fully available to us and then we're
going to use these tools to create the
confusion matrix and this one confusion
matrix this is the iris
ran and then we'll go ahead and put the
brackets here and we're only going to do
101 to
150 and then we only want the first five
columns of the last five column the
first five columns so five in there and
we'll set that to our model predict and
then when we hit enter it's gonna head
and just print out all this information
so right here we're looking at our
confusion matrix to see how everything
kind of balances out let's go over that
and see a little closer what that
actually means so the first part we see
is a reference and in here we have our
prediction because that's what we call
the column if you remember correctly uh
and we have it for setosa versus color
virginica and then we have setosa
versicolor virginica going down each
side and the way you read this is if you
look at this we predicted uh 16 out of
16 setosa
and then
versacolor we predicted 13 of the verse
of color and then two of those we
predicted as virginica so we were wrong
on two of those and with the virginica
we predicted 17 correct and we had two
wrong that were versa color so we look
at the overall statistics on here and we
have an accuracy of 0.92 and a 95
ci that's pretty good as we're going
through here those are good deals on
that and there's a lot more information
as you go down this the one thing i want
to remind you on this as we're looking
at these predictions and this is also
true for the linear regression model we
looked at earlier when we talk about the
accuracy that is the accuracy on this
data and we say that as a programmer to
make that a very clear distinction
because if you're in data analysis you
should cringe if i say oh this is a 95
accurate model without that put in there
because bad data in bad data out we
won't go into detail right now on that
but that is a very important to note
that whenever you quote any of the
accuracies on here or you discuss any of
these values that come up today we're
going to cover the random forest in r if
you remember from part one we did the
decision tree and no one wants a poor
lone standing tree out in the field we
want a forest and so we're going to
cover random forest it's a little play
on wards there i'm sure the people who
designed it also knew that too when they
were thinking about it so what's in it
for you what is a random forest how does
a random forest work we're going to
cover applications of a random forest
some of the terms you need to know when
using a random forest and then we'll go
over the use case in our predicting the
quality of wine so let's start a little
bit with exploring what is a random
forest and welcome to raphael's vineyard
and i don't even know how to pronounce
that in alsace france the largest wine
production region in the world so they
have
centuries of wine production and data
they've collected on how to make the
best how to make their grapes and grow
them and how to properly get the right
grapes and ferment them like raphael
wine makers around the world are facing
a grave problem so let's meet the man
himself and try helping him out what is
a random forest we're going to come back
to the vineyard but we're going to look
at some other examples along the way too
open up the door and of course uh here
comes raphael that probably i'd probably
look closer to raphael than i do to the
young lad with the beard hey raphael so
tell us how it's going not as good as
before not many people work in this
profession anymore so every stage of
production has gone slower if only we
could somehow speed up the process well
lucky for you i'm a machine learning
engineer how about we automate the
quality prediction process for wine let
me begin by telling you what machine
learning in random forest is machine
learning is the science of applying
algorithms that allow a computer to
predict the outcome without being
explicitly programmed random force is an
ensemble machine learning algorithm so
we're bringing in different pieces and
then putting those pieces together it
operates by building multiple decision
trees remember i told you we don't want
just a lone tree out there in the we
want a forest they work for both
classification if you remember from part
one we had our vendor with his broccoli
and oranges i think he also had carrots
and it also the output given by the
majority of his decision trees is the
forest final output so if you have a
number of decision trees you have five
decision trees and four of them say it's
broccoli then it's probably broccoli and
you can also use it as a regression
model the output given by the majority
of decision trees is a forced final
output just like you do in
classification you can also come up with
actual values and numbers as a
regression model could you explain how a
random force works in layman terms it's
always a good thing to explore the
information on something a little bit
easier to understand and see
sure let's consider the case of sam a
high school student this so reminds me
of my own high school student who's just
started his first year in college asking
all his friends so sam is confused on
which course to take up so he's trying
to figure out what his next class is
going to be at school should he take
music lessons dance lessons math science
maybe social studies so he decides to
ask a couple of his friends for
suggestions and sam approaches jane
first so he comes up to jane and says
hey which of these classes should i take
and jane asked sam a few questions based
on which she can suggest
theoretical do you like theory no he
doesn't like the theory part calculative
do you like calculative because yes i
like doing calculations and so jane
based on a couple question goes she has
a little decision tree there going she
goes mathematics sounds good so james
forms a decision tree based on sam's
response and gives her suggestion and of
course this is based on jane's views and
her history and the data that she's
collected over her years in high school
next sam approaches bella bella asks him
hey do you like the field of science and
sam goes yes and then bella goes do you
like
new in industry he like new things
coming up in industry and sam goes yes
and bella goes you know you should
similar to jane bella also forms a
decision tree based on sam's response
and gives her suggestion and says hey
how about artificial intelligence sounds
like oh artificial intelligence so sam
next asks terry for his suggestion
and sam goes do you like scoring things
and he goes yes
and he goes do you like theoretical and
he goes no and he comes up and says
mathematics so you have two people
saying mathematics and once a person
suggests an artificial intelligence
since two out of three friends suggested
math sam decides to take mathematics and
there's a lot of subtleties in this
example it's actually a really good
example some of the things we run into
in data science is when we collect data
and we collect from different sources we
might have data from one source that
measures one thing or one set of
instruments and we might have data from
another source that measures something
else we saw that with each of his
friends are measuring different things
and asking different questions that's a
little bit beyond the scope of this
particular tutorial so let's go back to
our wine cellar so you see more the
number of decision trees more accurate
will be the prediction random forests
have a number of applications in market
already if you've been watching any of
our other tutorials you'll see that
these are very common we have in banking
it can be used to predict fraudulent
customers it is used to analyze symptoms
of patients in detecting the disease
in e-commerce the recommendations are
predicted based on customers activity
stock market trends can be analyzed to
predict profit or loss
most recently i was watching a review on
weather here in the united states and
the data they pull in for that and one
of the top weather places actually uses
five different models random forest was
one of them i believe they use five
different models to predict the weather
and depending on what the models come up
with and what's going on some of the
models do better than others i thought
that was really interesting it's
important to note so when you see us
repeat the same things over and over and
you're looking at the different machine
learning tools you realize that we use
multiple tools to solve the same problem
and we can find the best one that fits
for that particular situation so let's
see how that fits back in our wine
production to help speed up the process
of wine production we will automate the
prediction of wine quality suppose our
random forest builds three decision
trees we have the first decision tree
which is going to divide everything by
chlorides less than 0.08 and it's either
yes or no and then it's going to look at
alcohol greater than six yes or no
quality equals high quality not so good
and if the chlorides were greater than
0.08 quality equals low so there's our
first tree and then our second tree is
going to split it on sulfates less than
0.6 and ph ph less than 3.5 and again we
have it coming down here that if the ph
is low and the sulfites are low the
quality equals high if the ph is a
little higher but the sulfides are low
it's not so good and if the sulfides are
greater than 0.6 we have a quality
equals low and then we'll do a third
decision tree we'll split it based on ph
and we can see below how that tree looks
like with our sugar less than 2.5 if not
it's a quad equals low it's real low
quality and if it is less than 2.5 then
the ph decide whether it's going to be a
quality 5 or not so good and if we track
one of our wines down the list we'll
look at it we'll say oh two out of three
decision trees indicate the quality of
our wine to be five the forest predicts
the same so this sounds promising so
let's go ahead and dive in and see what
that actually looks like so this is the
data set that holds all the attribute
values required to predict the wine's
quality and i'll pull this up in a
spreadsheet in just a moment but you can
see here we have fixed acidity volatile
acidity citric acid all the way to
quality and then quality is like a four
five six or seven i believe and this
will be done in r so let's go ahead and
get started in our coding we'll be using
r studio before we dive into our studio
i always like to open up the data in the
spreadsheet and look at the actual data
called it uh wine quality dash
red.xls so it's a spreadsheet a
microsoft spreadsheet
and when we open it up we'll see that it
matches what we had before as far as the
slide was showing us with the we have 12
different columns the first 11 are our
features our fixed acidity volatile acid
citric acid things you always want to
look at when you're looking at your data
and we've done this before in the part
one but just a reminder we want to note
that we have decimal places here float
values
and a lot of these things especially the
first number of columns
our free sulfur dioxide looks to be
an integer but it varies a lot so we
have you know i see some values all the
way up to 51
7 4.
so we'd probably keep this as a linear
model kind of thing same thing with the
sulfur dioxides densities again decimal
places sulfides alcohol quality i want
you to note in the quality
that we only have a few qualities here
this isn't
like a spread between zero and a hundred
it looks to be three four five six and
seven
and so when we start looking at the
processes we can have the option of
either doing this as a
series of integers or float values going
from
3 to 7 or we can convert that to a
categorical 3 4 5 6 and 7 being the
categories but we'll look at that in the
coding here in just a minute and finally
you'd want to go ahead and scroll down
through the data and just look for
missing data or anything weird
obviously you're working with big data
you wouldn't be able to do that if this
was spread across numerous computers and
you had millions of
rows we have what 1 687 rows here so
it's a nice size spreadsheet still
considered a small data set
let's go ahead and switch over and let's
take a look at this in the r
so here we are in our r studio
and if you remember it's uh comes up
automatically in three panels i have my
font on fairly large to help out
the upper left panel is a
file that's open you can have multiple
files and tabs open as you can see on
mine right now i'll probably close most
of those in just a minute
and this is where you can put in your
commands
then you can execute these commands by
either clicking on the run box or
control enter is the hotkeys for that
and all that does is then puts it down
in the console and executes that line of
command you could just paste these
directly in the console i've certainly
been known to do that if i'm loading up
some data and i just want to take a
quick look at it in r because i'm doing
the coding in something else or if i
just want to do a quick explore before i
dive in and figure out what needs to be
done that's one of the tricks you can do
and then on the right hand side we have
our environment history connections
we mainly are going to just be sitting
on plots and i'm just going to minimize
this for right now out of the way while
we dive in if you haven't yet you do
need to go ahead and install the package
random forest i'm not actually going to
do that because i've already run that we
don't need to rerun that but i am going
to go ahead and import it into our
project with the library and then random
forest and we'll do the control enter
you can always hit the run button up
there too
and then now the library has been
uploaded in there and you can see down
below it takes us and just copies it and
pastes it down below and runs it in our
console
and then just a quick troubleshooting
thing my java happens to be the
developer's job the latest release of
the developers java from last week is
clashing with my r and rerouting my
java files so your chances are you don't
need to to look this up or find out
where your java is but there's a jre
folder if you do have an error that
comes up next in the next library you
look that up it's usually under program
files java and you'll see jdk depending
on what version you have on there and
then find that jre folder and i'm just
going to set the environment on there
you have to do that every time
you run the script so with r there is a
place in r where you can set it
permanently but generally it's set to
whatever your system variables are set
to so you don't want to mess with that
so you just want to put this in your
code if you're having problems you can
always take it out then we need to go
ahead and bring in another library
and we're going to be importing a the
spreadsheet so it's an x the
xlsx since it's a microsoft spreadsheet
and i'll go ahead and run that
let's go ahead and create a variable
path we're going to assign to variable
path the location of our folder so this
is a very lengthy one you'll have to
copy and paste it off your computer my
name is richard and you can see under
it's under c column users richer's
documents docs business simply learn
marketing
random forest and then the actual file
which is wine quality dash red.xls
so i put in the whole path in here you
could also set the computer path to
whatever folder you have it in and then
just use just the name of the file but i
went ahead and did this because it a lot
of times it's good to be aware where
your paths are going where you're
pulling your data from
and again i went the control enter so i
could you'll see that that pushed the
line down below and ran that line so our
path is now set
and then let's create a variable called
data and we'll assign that to a read
and we're going to be reading let's see
where is it there it is and spreadsheet
and before i put in
we need the path
but we also need one more thing on the
spreadsheet because it is an excel
spreadsheet we need to know
which page we're on so let's go look
that up
and in the spreadsheet this might be a
little hard to see because it's a very
tiny font down there but you'll see the
page tabs on the bottom this actually
has two different pages and we want page
one for this project so let's flip back
over and put that in our code
and the term they use is sheet
index i always have to look that stuff
up
and it was sheet index of one we'll go
ahead and execute this
so at this point we've now loaded the
data into
the variable data
and it's reading those and i think there
was like a couple thousand rows of data
in there so it takes it just a moment to
pull it in
once i have my data in there i always
like to do something like this
data
and in
always forgetting r it's head data
and we can run that
and you'll see it prints out that what
we looked at in the spreadsheet so we
have our fixed
acidity volatile acidity citric acid
residual sugar chlorides free sulfur all
the different things
we're actually looking for quality so
you want to keep that in mind that
quality is the final setting is it a
three four five six what is the quality
on there
and the first thing we need to do is
we're actually going to take the data
quality
and let me see quality there it is
in r the dollar sign means that you're
looking at one of the
columns in here or features as it is so
we see data dollar sign quality and r
that means that's what we're going to be
editing
and what we want to do is we want to
change the quality just a little bit
and so we're going to set it equal to
now we're not assigning a variable we're
altering it so data
dollar sign quality equals
and we want to do as is one of the
functions in r we're going to do as a
factor
and then data
quality because that's what we're
actually editing on there and this stop
just for a second to look at this
factor takes this
and looks at the data
as a
categorically so instead of it being on
a continual scale of you know
0 to 100 or something like that it
actually looks at it as category of five
category of six category of a category
of cat category of dog
there are yes or no categories versus a
continual now there's a lot of different
things in here that we can do the same
thing with
but all of these really are
linear numbers 7.4 0.70
sulfur dioxide 34 67 so we really don't
need to mess with any of the other
variables in there but we do want to go
ahead and set this up and we'll come up
here and do our control enter and it'll
run it down below
and you'll see data quality equals as
factor data quality so we've now turned
that into a factor or into a categorical
data
and so let me just bring this down a
little bit the guys in the back did the
rest of this next piece of code
there are so many different ways and if
you have been
looking at any of our tutorials
this should look kind of familiar this
is one way to do it you'll see we have
our training data and our testing data
and so we've done here is we've got it
we have a data set size equals
and we're just going to do by the row
and so we want point eight or eighty
percent of the data is going to be our
training set to train our forest and if
you train a forest or any
machine learning model of course want to
test it so we're going to save 20 of the
data for testing
and when you look at this we have the
index
which we set at 80 percent comma and
then a negative index so that's the
remaining setup so it goes from that
index on this is our notation for
sorting out that data
and then we create the index and we use
the term sample
and this is just going to go in there
and set our different samples for our n
row size data set size and everything so
we have an index now so the data is
indexed and we're just going to pull off
the first set of index and the end of
the index for our data
and we can run all of this in one shot
we'll go under code
and we'll run
selected lines and so now we have our
data in here we have our training set
and our testing set that's what we
wanted to get to
so once we've formatted all the data
then we get to the next real easy step
which is a single line to bring in our
random forest
so let's go ahead and we'll use a
variable rf standing for random forest
and we're going to assign
if you remember correctly we brought in
our library random
if you wonder why i go up here all the
time it's because i'll forget which
letters are capitalized in some of these
because they switch between packages so
much but it's random with a capital f
for 4s random forest so the rf is going
to equal random forest
and of course the
we start by telling it which column
we're trying to predict
and of course we can always go down here
and double check we're running this on
the
quality
there we go quality and we're using just
that column all the way to the end
and then we've added our team in the
back has gone in and played with this a
little bit and we'll go a little bit
over what they did but i'm going to put
in the values that they have set for the
different options in the random forest
before i do that it would help if i tell
it what data to use data is going to
equal
our training set
and then we have the variables that they
put together and let's just talk a
little bit about what these different
variables mean
so
m try and entry these are some of the
variables you play with with the random
forest the m try not so much
when they first put it out i think m try
was very common four is a good value i
can't remember what the default is but
it's probably three four or five or
something like that depending i think it
depends on how many features you have
going in there so you could even just
leave this as a default without too much
troubles
the entry this is where it gets
important and i'm going to make this
2001.
one of the suggestions is to always make
this odd like a tiebreaker just in case
the default is like 500 or 501.
and then finally we have importance
equals true i can't remember what it
defaults to
but generally you want to set the
importance equals to true it actually
looks at the correlation between the
different features to make sure that
there is some kind of correlation as
it's doing its calculations
so our one line of code is going to
assign our random forest and go ahead
and train our random force all in one
execution so this whole thing has been
centered around getting to rf set to the
random forest and let's go ahead and
execute this variable or this line of
code and you'll see it takes a little
bit down here to execute as it goes
through and sets it up
and you'll see it probably takes a
minute or so to execute the code
depending on the size of the data and
how good your machine is
and then with all of our data we can
start looking at the answers we'll just
do rf we'll execute that
and you'll see that the random forest
does a nice job you can see right here
it has our date equals training it
basically goes over everything we talked
about
and you come down here and it has a
confusion matrix which is nice we always
like our confusion matrix when we're
talking to our shareholders and
explaining
what the data comes out with and how it
trains
and we can go ahead and plot our rf
let's see what that looks like
and
as the number of forests grows remember
we set it to 2001 forest you can also
see how the error comes in how the
different error on the different
variables
this is kind of a nice graph that gives
you an idea it might help you adjust
whether you really want to do 2000
different
forests you might actually truncate this
off at around
maybe 800 because you notice the lines
kind of continue on
again this is just still on the plot
and of course the next step is we need
to go ahead and predict remember we
split our data into two groups
so let's go ahead and get a result we'll
call it result and we'll assign this one
there we go
we'll set this to a data frame
there we go there's our data frame
and then oh let's call this we have our
test our test
quality because that's what we're we're
setting up on our test group with the
quality
on there
then we wanted to have a predict
nice little keyword there to go and run
our random for us
and then we tell it that we're going to
predict using our rf our random forest
our testing group
1 to 11 is what we're using type equals
response
so let's go ahead and execute it that's
a mouthful on there
somewhere i left out a comma in there so
it gave me an error but you can see
right here we've set in there now we
have a result and just real quick we
could do the head of the result
result comma 5 and just take a look at
the first five lines
and you'll see we have the actual value
and what we predicted over here
and we could also just do all of the
results and just see what that looks
like oops
and print that out
and so it has all the different data
we're going through on there
and finally let's just go ahead and plot
there we go we'll put plot up here plot
and let's plot the result
and just see what that looks like
maybe not the best one to show to the
shareholders but it does give you an
idea on the left on the y axis you'll
see that it predicted eight
and this tells you that it a lot of the
eight and the eight scale was pretty
probably gave us
about fifty percent came up there at
eight
really maybe fifty percent it didn't
guess right but it did come up there and
say that it was at least a four five or
six even where it was considered an
eight and so on you can see these
different blocks coming in so that if
you line up the
three with the three
four with the four and you kind of cross
index them a heat map would probably do
better but for this example we'll just
do a quick plot of the data
as far as how it works and you can again
see the results that we predicted over
here and you can look those over
of course back in the wine cellar we're
talking to our vineyard owner and if you
remember from before we looked at this
chart right here the error rate is 29.05
percent therefore our accuracy is
70.95 percent if you're doing a vineyard
that's probably pretty good accuracy and
always check your domain when you're
sharing this stuff because if you're
doing something that's life or death
maybe 70 accuracy might not be so good
but if you're looking at where you're
going to distribute the bottles to for
the wine and who's going to drink it you
know 70 percent accuracy is pretty good
for that that's probably a lot better
than having no accuracy whatsoever and
not even knowing what wine is going out
to who so we have automated the process
of predicting the quality of wine i
myself prefer to predict it by sampling
it that's great thank you i'd like to
visit this guy and sample his wine and
find out just how good his different
vineyards are so now let's understand
what an svm is and if you want to
understand support vector machine you
really need a good example and i know
that cricket is a wonderful game i don't
know a lot about it but i know it's very
popular around the world and we're going
to use qriket as an example of svm so
we'll classify qriket players into
batsmen or bowlers using the runs to
wicket ratio so a player with more runs
would be considered a batsman and a
player with more wickets would be
considered a bowler and if we could take
a data set say of people cricket players
with runs and wickets in columns next to
their names we could create a plot a
two-dimensional plot in this example
clear separation between qriket players
considered bowlers and those cricket
players considered batsmen and you don't
always have a data set this clean where
there is a clear segregation of data
bowlers versus batsman but for the sake
of understanding svm this example does
well so before we do any separation
before we apply any high-level
mathematics let's just take a look at an
unknown value a new data coming into our
data set and that datum we don't know
what class it belongs to that datum is a
bowler or a batsman we know that much
but we don't know should we consider
that cricket player a batsman or a
bowler and so we want to draw a decision
boundary some type of line separating
the two classes so that we can use that
decision boundary to classify the new
data point of course we could draw lines
there's a yellow line a green line a
blue line there's actually not just
three lines but an infinite number of
lines that we could draw between those
two classes so which one do we pick and
this is kind of reminiscent of simple
linear regression where we find the line
of best fit we want the line of best
separation we want a line that clearly
separates those two groups as much as
possible once we have the correct line
we would be able to classify the new
data point but in this example you can
see if we pick yellow the new data point
would be a bowler but if we picked green
or blue the new data point would be a
batsman so we need the one that best
separates the data what line best
separates the data we'll find the best
line by computing the maximum margin
from equidistant support vectors now
support vectors in this context simply
means the two points one from each class
that are closest together but that
maximize the distance between them or
the margin so the word vector here you
may think well he really means points
data points well in two-dimensional
space yes and maybe even in
three-dimensional space but once you get
into higher dimensional spaces when you
get more and more features in your data
set you have to consider these as
vectors you can no longer really
describe them as points so we call them
vectors and the reason they're support
vectors is because the two
vectors that are closest together that
maximize the distance between the two
groups support the algorithm so that's
why we call them support vectors so if
you look at our example there are a
couple of points at the top that are
pretty close to one another and a couple
of points at the bottom of that graph
that are pretty close to each other and
i'm not really sure looking at the graph
which two are closest but clearly those
points are the ones we need to consider
the rest of the other points are too far
away from the rest of the other points
in other words the bowler points are far
to the right and the batsman data points
are far to the left so we'll look at
these four points to begin with
mathematically we would calculate the
distance among all of these points and
we would minimize that distance once we
pick the support vectors we'll draw a
dividing line and then measure the
distance from each support vector to the
line the best line will always have the
greatest margin or distance between the
support vectors if we consider the
yellow line as a decision boundary the
player with the new data point would be
considered a bowler the margins don't
appear to be maximum though so maybe we
can come up with a better line so let's
take two other support vectors and we'll
draw the decision boundary between those
and then we will calculate the margin
and notice now
that the unknown data point the new
value would be considered a batsman we
would continue doing this and obviously
a computer does it much quicker than a
human being over and over and over again
until we found the correct decision
boundary with a greatest margin and in
this case if you look at the green
decision boundary the green line appears
to have the maximum margin compared to
the other two and so let's consider this
the boundary of greatest margin and now
classify our unknown data value and now
clearly it belongs in the batsman's
class the green line divides the data
perfectly because it has the greatest
margin or the maximum margin between the
support vectors and now we can have
confidence in our classification that
the new data point is indeed a batsman
technically this dividing line is called
a hyperplane and the reason it's called
a hyperplane will become a little bit
more evident in a few minutes generally
in two-dimensional space we consider
those
lines but in three-dimensional space and
higher dimensional spaces they're
considered planes or hyperplanes so we
always tend to refer to them as
hyperplanes and the hyperplane that has
the maximum distance from the support
vectors is the one we want
so sometimes called the positive
hyperplane d plus is the shortest
distance to the closest positive point
and d minus sometimes called a negative
hyperplane has the closest shortest
distance to the closest negative point
and the sum of d positive and d negative
is called the distance margin so if you
calculate those two distances and add
them up that's the distance margin and
we always want to maximize that if we
don't maximize it we can have a
misclassification and you can see the
yellow margin is much smaller than the
green margin so this problem set is
two-dimensional because the
classification is only between two
classes and so we would call this a
linear svm now we're going to take a
look at kernel svm and if you notice in
this picture this is a great depiction
of a plane not a line this is
three-dimensional space so that's the
plane that separates those two classes
so what if our two-dimensional data
looks like this what if there's no clear
linear separation between the two
classes and machine learning parlance we
would say that these are not linearly
separable how could we get support
vector machine to work on data that
looks like this since we can't separate
it into two classes using a line what
we're going to do and clearly there's no
line i mean convince yourself there's no
line that goes and separates those two
classes so what we would do is we would
apply some type of transformation to a
higher dimension we would apply a
function called a kernel function to the
data set such that the data set would be
transformed into a higher dimension a
dimension high enough where we could
clearly separate the two groups the two
classes in this case with a plane much
the same way you saw the picture a few
slides ago where the plane was
separating the two classes here we
clearly could draw some planes between
the green dots and the red dots and of
course we could draw an infinite number
of planes separating those two classes
but we would draw the one that would
maximize the margin so if we let r be
the number of dimensions then the kernel
function would convert a given
two-dimensional space to a
three-dimensional space and as i said
once the data is separated in three
dimensions we can apply svm and separate
the two groups using a two-dimensional
plane and this is analogous in the
higher dimensions and that last picture
on the right hand side would be some
type of depiction of a higher dimension
hyperplane and there are more than one
type of kernel function there's gaussian
rbf kernel sigmoid kernel polynomial
kernel depending on the dimension and
how you want to transform the data there
are more than one choice for kernel
functions so now let's go through a use
case let's take a look at horses and
mules and see if we can use svm to
classify some new data so the problem
statement is classifying horses and
mules and we're going to use height and
weight as the two features and obviously
horses and mules typically in general
tend to weigh differently and tend to
stand taller so we'll take a data set
we'll import the data set we'll make
sure we have our libraries the
e1071 library has support vector machine
algorithms built in we'll compute the
support vectors using the library once
the data is used to train the algorithm
we'll plot the hyperplane get a visual
sense of how the data is separated and
if it's two-dimensional or
three-dimensional that's great remember
if it's higher dimensional it's tough to
plot those and then we'll use the new
model the trained model to classify new
values in general we would have a
training set a test set and then ingest
the new data but for our example we're
just going to use the whole data set to
train the algorithm and then see how it
performs and once we see how it performs
we'll see did we get a horse did we
predict a horse when we had a horse did
we predict a mule when we had a mule so
here's the r code of course
if you want to run this r code you'll
have to install r and then our studio
then you'll have to ingest the data and
then you'll have to create an indicator
variable to help us with our plots we'll
install our libraries install the
package if you don't already have it
installed i do and then call the library
up into the r session that you're
working we'll create the data frame from
the data that was ingested we will then
view the data frame and of course we
have to explore the data frame hoping
that we don't have missing values or we
don't have outliers if we do have
missing values or outliers we'll have to
either impute those values or delete
those values but in this case let's hope
we don't have any and now for a quick
demo on support vector machines so we
have a working directory where my data
is stored and i'm going to set the
working directory to that data source
and read the data file into r and now
i'm going to create a vector that really
is just an indicator variable it's the
negative ones and the ones that indicate
whether it's a mule or a horse negative
one for a mule positive one for a horse
so i'll create that vector and svm
there's a great package e1071
great library that has the support
vector machine functions that we're
going to use and now i'm going to create
the data frame i'm going to take my data
and i'm going to add to it the is horse
vector
and now let's just take a peek and as
you can see there's the height weight
and whether or not the animal was a mule
or a horse so remember machine learning
we take a data source with known
outcomes and we apply our algorithm to
that data source in this case a data
frame of heights weights and the
indicator variable and then we use the
machine learning algorithm to learn from
that data with those known results so
now let's plot the data the height
versus weight and as you can see the
horses tend to be a lot taller and weigh
a lot more than the mules so now let's
run our algorithm our svm model against
the data and the key here is that it's a
linear model and we've run it and great
now we have a model let's look at the
summary of the model and as you can see
there's the formula that it ran and some
of the information about the vectors in
the classes and there are two support
vectors so now let's find those two
support vectors on our graph and this
might be a little hard to see the
orange outline around this point and
this point indicate that those are the
two vectors that will support this model
and thus we call them the support
vectors now let's get the parameters of
the hyperplane from the model and let's
plot a line that will be our hyperplane
there's our plane line and therefore to
the left and below our mules and to the
right and above our horses above that
line let's take a few new observations
and i've encoded these so that it would
be illustrative we'll take these new
observations and we'll plot them
and i'm going to generate a new plot red
being the horses black dots being the
mules i'll create an x and y axis i'm
going to plot
three
dots and you can see one of them the
green dot the first one kind of falls
near the mules the blue one kind of
falls near the horses and visually we
might conclude hey one's a mule one's a
horse but what about the orange dot well
for the orange dot we really need our
hyperplane we really need something to
clearly indicate which side it's on and
of course by looking at this data
visualization you can tell that the
orange dot is probably a mule and let's
verify those results by sticking those
observations into our model and predict
some outcomes and down below you can see
that for the three data points one two
and three the first one the green one
was negative one that's a mule the
second one was a positive one that's
blue that's the blue point which was a
horse and the third one which we weren't
sure about but clearly the svm model
correctly predicted that that third
point would be a negative one would be a
mule and we can visually see that by the
hyper plane on the graph so that's a
quick example of svm in general support
vector machines are binary classifiers
we don't use them for higher level
classification if you had three or more
classes you might want to use something
like random forest what applications you
can use it for face detection text
categorization image classification and
bioinformatics really anywhere that you
have the need to classify things into
two groups so let's dive into what is
clustering what is clustering so
we'll have a group of people here
hanging out at a table or local coffee
shop i'm guessing coffee shop since they
all have the same mugs and one of them
is uh trying to figure out what they're
gonna do i have 20 places to cover in
four days i guess he's going traveling
that sounds fun one of those breeze
through costa rica or visiting the us or
going to to europe to go visit all the
highlights he's got 20 places he wants
to go and he wants to hit him in four
days very ambitious
and uh how will i manage to cover all of
them that's the question that he's
coming up how am i going to get to all
these different places in the short time
i have maybe he's a sales team so he has
20 places he's got to do demos for you
can make use of clustering by grouping
the places into four clusters
each of these clusters will have places
which are close by so we're going to
cluster them together by what is closest
to the other one then each day you can
visit one cluster and cover all the
places in the cluster
great that's a great idea you know if
you're going to hit uh four different
places maybe you're going to cluster
whatever streets are closest together so
you don't have to travel spend all your
time traveling the method of dividing
the objects into clusters which are
similar between them and are dissimilar
to the objects belonging to another
cluster so this is a formal definition
of clustering and we have here what we
call hierarchical clustering and partial
clustering and we're going to go more
into detail on hierarchical clustering
and you'll see the difference between
the two where partial clustering doesn't
have the descending graph it just has
groups of things
and under hierarchical clustering we're
going to go over agglomerative and
divisive and you can think of
agglomerative of bringing things
together and divisive as dividing them
apart and then partial clustering the
two most common ones are k-means that's
probably the most common used for
partial clustering and there's also
fuzzy c means and there's other ways of
clustering and other algorithms out
there but these are the two big ones k
means really being the most common one
used but we're not going to dig too deep
into partial clustering because we're
studying hierarchical clustering today
so what is clustering well the
applications of clustering are pretty
numerous we have things like customer
segmentation what people kind of belong
together how do we group them together
we have social network analysis
so social networks we might look at
sentiments what group of people like
something what group of people don't
like something and then we can use that
sentiments to suggest new cells for them
you know this whole group is into harley
harley-davidson motorcycles so we cater
to them to sell them harley-davidson
motorcycle parts and we find out that
people in that group also like leather
jackets they also like motorcycle boots
and they like bandanas my brother's into
harley-davidson so
that's why i picked that kind of funny
example but you can kind of see we look
for similarities between these people
and then we group them together
accordingly and that's a good
sentimental clustering is very common in
so many things today we want to know the
sentiments on stocks who is interested
in stocks who's not sentimental analysis
we talked about customer segmentation in
cells well we want to know what kind of
sentimental view people have on
different restaurants if we're going to
open up a new restaurant or a new
putting stores in and of course city
planning a lot of this i just kind of
aimed at the city planning side city
planning is big with clustering we want
to cluster things together so that they
work we don't want to put a industrial
zone in the middle of somebody's
neighborhood where they're not going to
enjoy it or have a commercial zone right
in the middle of the industrial zone
where no one's going to want to go next
to a factory to go eat a high-end meal
or dinner so it's very big in city
planning it's also very big in just
pre-processing data into other models so
when we're exploring data being able to
cluster things together reveal things in
the data we never thought about and then
once we have it clustered we can put
that into another model to help us
figure out what it is we want for our
solution so let's go a little deeper
into hierarchical clustering let's
consider we have a set of cars and we
have a group similar we want to group
similar ones together so below we have
you'll see four different cars down
there and we get two clusters of car
types sedan and suv so if you're just
looking at it you can probably think oh
yeah we'll put the sedans together and
the suvs together and then at last we
can group everything into one cluster so
we just have just cars so you can see as
we have this we make a nice little tree
this is very common when you see anybody
talks about hierarchical clustering this
is usually what you see and what comes
out of it we terminate when we are left
with only one cluster so we have as you
can see we bring them all together we
have one cluster we can't bring it
together anymore because they're all
together hierarchical clustering is
separating data into different groups
based on some measure of similarity so
we have to find a way to measure what
makes them alike and what makes them
different agglomerative clustering is
known as a bottom up approach remember i
said think of that as bringing things
together you see i think the latin term
aglow is together because you have your
glomerate rocks where all the different
pieces of rocks are in there so we want
to bring everything together that's a
bottom up
and then divisive is we're going to go
from the top down so we take one huge
cluster and we start dividing it up into
two clusters into three four five and so
on digging even deeper into how
hierarchical clustering works
let's consider we have a few points on a
plane so this plane is 2d so we have an
xy coordinates kind of makes it easy
we're going to start with measure the
distance so we want to figure a way to
compute the distance between each point
each data point is a cluster of its own
remember if we're going from the bottom
up agglomerative then we have each point
being its own cluster we try to find the
least distance between two data points
to form a cluster and then once we find
those with the least distance between
them we start grouping them together so
we start forming clusters of multiple
points this is represented in a
tree-like structure called dendogram so
this is another key word dendogram and
you can see it is it's just a branch
we've looked at before and we do the
second group the same so it gets its own
dendogram and the third gets its own
dendogram and then we might group two
groups together so now those two groups
are all under one dendogram because
they're closer together than the p1 and
p2 and then we terminate when we are
left with one cluster so we finally
bring it all together you can see on the
right how we've come up all the way up
to the top whoops and we have the gray
hierarchical box coming in there and
connecting them so we have just one
cluster and that's a good place to
terminate because there is no way we can
bring them together any further so how
do we measure the distance between the
data points
i mean this is really where it starts
getting interesting up until now you can
kind of eyeball and say hey these look
together but when you have thousands of
data points how are we going to measure
those distances and there is a lot of
ways to get the distance measure so
let's go and take a look at that
distance measures will determine the
similarity between two elements and it
will influence the shape of the clusters
and we have euclidean distance measure
we have squared euclidean distance
measure which is almost the same thing
but with less computations and we have
the manhattan distance measure which
will give you slightly different results
and we have the cosine distance measure
which again is very similar to the
euclidean playing with triangles and
sometimes it can compute faster
depending on what kind of data you're
looking at so let's start with euclidean
distance measure the most common is we
want to know the distance between the
two points so if we have point p and
point q the euclidean distance is the
ordinary straight line it is the
distance between the two points in
euclidean space and you should recognize
d equals in this case we're going to sum
all the points so if there was more than
one point we could figure out the
distance to the not more than one points
this is the sum of more than two
dimensions
so we can have the distance between each
of the different dimensions squared and
that will give us and then take the
square root of that and that gives us
the actual distance between them and
they should look familiar from euclidean
geometry maybe you haven't played too
much with multiple dimensions so the
summation symbol might not look familiar
to you but it's pretty straightforward
is you add the distance between each of
the two different points squared so if
your y difference was 2 minus 1 squared
would be 2 and then you take the
difference between the x again squared
and if there was a z coordinates it
would be you know z1 minus z2 squared
and then take the square root of that
and sum it all or sum it all together
and then take the square root of it so
to make it compute faster since the
difference in distances whether one is
farther apart or closer together than
the other we can do what's called the
squared euclidean distance measurement
this is identical to the euclidean
measurement but we don't take the square
root at the end there's no reason to it
certainly gives us the exact distance
but as far as doing calculations as to
which one is bigger or smaller than the
other one it won't make a difference so
we'll just go with the so we just get
rid of that that final square root
computes faster and it gives us the
pretty much euclidean squared distance
on there now the manhattan distance
measurement is a simple sum of
horizontal and vertical components or
the distance between two points measured
along axes at right angles now this is
different because you're not looking at
the direct line between them and in
certain cases the individual distances
measured will give you a better result
now generally that's not true most times
you go with the euclidean squared method
because that's very fast and easy to see
but the manhattan distance is you
measure just the y value and you take
the absolute value of it and you measure
just the x difference you take the
absolute value of it and just the z and
if you had more you know different
dimensions in there a b c d e f however
many dimensions you would just take the
absolute value of the difference of
those dimensions
and then we have the cosine distance
similarity measures the angle between
the two vectors and as you can see as
the two vectors get further and further
apart the cosine distance gets larger so
it's another way to measure the distance
very similar to the euclidean so you're
still looking at the same kind of
measurement so should have a similar
result as the first two but keep in mind
the manhattan will have a very different
result and you can end up with a bias
with the manhattan if your data is very
skewed if one set of values is very
large and another set of values is very
small but that's a little bit beyond the
scope of this it's just important to
know that about the manhattan distance
so let's dig into the agglomerative
clustering and agglometric clustering
begins with each element as a separate
cluster and then we merge them into a
larger cluster how do we represent a
cluster of more than one point so we're
going to kind of mix the distance
together with the actual agglomerative
and see what that looks like and we're
actually going to have three key
questions that are going to be answered
here so how do we represent a cluster
more than one point so we want to look
at the math what it looks like
mathematically and geometrically how do
we determine the nearness of clusters
when to stop combining clusters always
important to have your computer script
or your whatever you're working on have
a termination point so it's not in going
on internally
we've all done that if you do any kind
of computer programming or writing of
script let's assume that we have six
data points in a euclidean space so
again we're dealing with x y and z in
this case just x and y
so how do we represent a cluster more
than one point let's take a look at that
and first we're going to make use of
centroids very common terminology in a
lot of machine learning languages when
we're grouping things together so we're
going to make use of centroids which is
the average of its points
and you can see here we're going to take
the 1 2 and the 2 1 and we're going to
group them together because they're
close and if we were looking at all the
points we look for those that are
closest and start with those and we're
going to take those two we're going to
compute a point in the middle and we'll
give that point 1 5 1.5 1.5 and that's
going to be the centroid of those two
points and next we start measuring like
another group of points we got 4 1 zero
when they're pretty close together so
we'll go ahead and set up a centroid of
those two points in this case it would
be the
4.5 and
0.5 would be the measurements on those
two points and once we have the centroid
of the two groups we find out that the
next closest point to a centroid is over
on the left and so we're going to take
this and say oh 0 0 is closest to the
1.5 1.5 centroid so let's go ahead and
group that together and we compute a new
centroid based on those three points so
now we have a centroid of one point one
or one comma one and then we also do
this again with the last point the five
three and it computes into the first
group and you can see our dendrogram on
the right is growing so we have each of
these points are become connected and we
start grouping them together and finally
we get a centroid of that group two and
then finally the last thing we want to
do is combine the two groups by their
centroids and you can see here we end up
with one large group and it'll have its
own centroid although usually they don't
compute the last centroid we just put
them all together so when do we stop
combining clusters well hopefully it's
pretty obvious to you in this case when
they all got to be one but there are
actually many approaches to it
so first pick a number of clusters k up
front and this is done in the fact that
we don't want to look at 200 in clusters
we only want to look at the top five
clusters or something like that so we
decide the number of clusters required
in the beginning and we terminate when
we reach the value k
so if you look back on our clustering
let me just go back a couple screens
you'll see how we clustered these all
together and we might want just the two
clusters and so we look at just the top
two or maybe we only want three clusters
and so we would compute which one of
these has a wider spread to it or
something like that there's other
computations to know how to connect them
and we'll look at that in just a minute
but to note that when we pick the k
value we want to limit the information
that's coming in so that can be very
important especially if you're fitting
it into another algorithm that requires
three values or you set it to four
values and you need to know that value
coming in so we might take the
clustering and say okay only three
clusters that's all we want for k so the
possible challenges this only makes
sense when we know the data well so when
you're clustering with k clusters you
might already know that domain and know
that that makes sense but if you're
exploring brand new data you might have
no idea how many clusters you really
need to explore that data with let's
consider the value of k to be two so in
this case in our previous example we
stop and we're left with two clusters
and you can see here that this is where
they came together the best while still
keeping separate the data the second
approach is stop when the next merge
would create a cluster with low cohesion
so we keep clustering until the next
merge of clusters creates a bad cluster
low cohesion set up on there that means
the point is so close to being between
two clusters it doesn't make sense to
bring them together but how is cohesion
defined oh let's dig a little deeper
into cohesion the diameter of a cluster
so we're looking at the actual diameter
of our cluster and the diameter is the
maximum distance between any pair of
points in the cluster we terminate when
the diameter of a new cluster exceeds
the threshold so as that diameter gets
bigger and bigger we don't want the two
circles or clusters to overlap
we have radius of a cluster radius is
the maximum distance of a point from
centroid we terminate when the radius of
a new cluster exceeds the threshold
again we're not we don't want things to
overlap so when it crosses that
threshold and is overlapping with other
data we stop so let's look at divisive
clustering remember we went from the
bottom up now we want to go from the top
down divisive clustering approach begins
with a whole set and proceeds to divide
it into smaller clusters so we start
with a single cluster composed of all
the data points we split it into
different clusters this can be done
using monotheistic divisive methods
what is a monothetic divisive method and
we'll go backwards and let's consider
the example we took in the agglomerative
clustering to understand this so we
consider a space with six points in it
just like we did before same points we
had before and we name each point in the
cluster so we have in this case we just
gave it a letter value a b
c d e f
since we follow top down approach and
divisive clustering obtain all possible
splits into two columns so we want to
know where you could split it here and
we could do like a a b split and a cdef
split we could do bce adf and you can
see this starts generating a huge amount
of data
abcdef
and so for each split we can compute
cluster sum of squares
and we can see here the actually have
the formula out for us bj 12 equals n1
of absolute value of x minus absolute
value of x squared so again we're
computing all the different distances in
there squared back to your kind of
euclidean distances on that and so we
can actually compute b and j between
clusters one and two and we have the
mean of the cluster and the grand mean
depending on the number of members in
the cluster and we select the cluster
with the largest sum of squares
let's assume that the sum of squared
distance is largest for the third split
we had up above and that's where we
split the abc out and if we split the
abc out we're left with the def on the
other side
we again find the sum of square
distances and split it into clusters so
we go from a b c we might find that the
a splits into b c
and d into e f
and again you start to see that
hierarchical dendrogram coming down as
we start splitting everything apart and
finally we might have a splits in b and
c and then each one gets their own def
and it continues to divide until we get
little nodes at the end and every data
has its own point or until we get to k
if we have said a k value so we've kind
of learned a little bit about the
background and some of the math and
hierarchical clustering let's go ahead
and dive into a demo and our demo today
is going to be
for the problem statement we're going to
look at u.s oil so a u.s oil
organization needs to know it cells in
various states in u.s and cluster the
states based on the sales so what are
the steps involved in setting this
problem up so the steps we're going to
look at and this is really useful for
just about any processing of data
although i believe we're going to be
doing this in r today we're going to
import the data set so we'll explore our
data a little bit there create a scatter
plot it's always good to have a visual
if you can once you have a visual you
can know if you're really far off in the
model you choose to cluster the data in
or how many splits you need and then
we're going to normalize the data so
we're going to fix the data up so it
processes correctly we'll talk more
detail about normalization when we get
there and then calculate the euclidean
distance
and finally we'll create our dendogram
so it looks nice and pretty and we have
something we can show to our
shareholders so that they have something
to go on and know why they gave us all
that money and salary for the year so we
go ahead and open up r and we're
actually using our studio which is the
really it has some nice features in it
it automatically sets up the three
windows where we have our script file on
the upper left
and then we can execute that script and
it'll come down and put it into the
console bottom left and execute it and
then we have our plots off to the right
and i've got it zoomed in hopefully not
too large a font but large enough that
you can see it
and let's just go ahead and take a look
at some of the script going in here
it's clustering analysis and we're going
to work we'll call it mydata
and we're going to assign it in r this
is a symbol for assigning and we're
going to go read csv
read csv file
and put that in brackets
and let's before we go any further let's
just look at the data outside of r it's
always nice to do if you can
and the file is going to be called
utilities.csv
this would also be the time to get the
full path so you have the right path to
your file
and remember that you can always post in
the comments down below
and when you post down there just let us
know you want to connect up with simply
learn so that they can get you this file
so you can get the same file we're
working on you can repeat it and see how
this works this is utilities.csv it's a
comma separated variable file so it's
pretty straightforward and you can see
here they have the city fixed charge and
a number of different features to the
data and so we have our ror cost load
demand cells
nuclear fuel cost on here and then going
down the other side we have u.s cities
arizona boston they have central u.s so
i guess they're grouping a number of
areas together
the commonwealth area you can see down
here in nevada new england northern us
oklahoma the pacific region
and so on so we have a nice little chart
of different data they brought in
and so i'm going to take that complete
path that ends in the utilities.csv
and we're going to import that file let
me just enlarge this all the way
upside an extra set of brackets here
somehow maybe i missed a set of brackets
this can happen if you're not careful
you can get brackets on one side and not
the other or in this case i've got
double brackets on each side there we go
and then the magic hotkeys in this case
are your control enter
which will let me go ahead and run the
script
and so i've now loaded the data and as
you can see i went ahead and shrunk the
plot since we're going to be looking at
the window down below
and we can simply convert the data to a
string
now all of us do this automatically the
first time we say hey just print it all
out as a string and then we get this
huge mess of stuff
that doesn't make a whole lot of sense
so
when you can see here they have you can
probably kind of pull it together as
looking at it but let's go ahead and
just do the head
i will do the head of my data
there we go and ctrl enter on that and
the head shows the first five rows
you'll see this in a lot of different
scripts in r it's you type in head and
then in brackets you put your data and
it comes through in this the first five
rows as you can see below
and it shows arizona boston central and
it has the same columns we just looked
at so we have the fixed charge the ror
the cost the load the
d demand i'm not an expert in oil so i'm
not even sure what d demand is
cells i'm guessing nuclear how much of
it supplied by nuclear
and the actual fuel cost
and then the different states that it's
in are different areas
and one of the wonders of r is all these
cool easy to use tools that are so quick
so we'll do pairs
and pairs creates a nice graph let me go
ahead and run this
whoops the reason it gave me an error is
because i forgot to resize it so let me
bring my plot way out so we can see it
and let's run that again
and you'll see here that we have a nice
graph of the different data and how it
plots together how the different points
kind of come together
this is neat because if you look at this
i would look at this and say hey this is
a great
candidate for
some kind of clustering and the reason
is is when i look at any two pairs let's
go down to say cells and fuel costs
towards the bottom right
and when you look at them where they
cross over you sort of see things how
they group together but it's a little
confusing you can't really pinpoint how
they group together you could probably
look at these two and say yeah there's
pretty good commonalities there and if
you look at any of the other pairs
you'll start seeing some patterns there
also and so we really want to know what
are the patterns on all of them put
together not just any two of them but
the whole setup
let me go ahead and shrink my um this
down for just a second just a notch here
and let's create a
scatter plot
and this is simply just use the term
plot in brackets
and then which values do we want to plot
and if we remember when we looked at the
data earlier let me just go back this
way
in this case let's go ahead and compare
two just two values to see how they look
and we'll do field cost and cells
and it's in my data
so we'll let it know which two columns
we're looking at next to each other and
it will open up our plot thing and then
go ahead and execute that
and we can see on those
close up of what we were just looking at
in the pairs
and if i was eyeballing this i would say
oh look there's a kind of a cluster up
here of five items and
this one it's hard to tell which cluster
to be with but maybe it's six you go in
the top one and you have a middle
cluster and a bottom cluster maybe two
different clusters so you can sort of
group them together fuel cost and the
cells and see how they connect with each
other
and again that's only two different
values we're looking at so in the long
run we want to look at all of them
and then the people in the back
they sent me the script so we can go
ahead and add labels so with my data
text fuel cost cells the labels equal
city
position four
these are numbers you can kind of play
with till they look nice
and oops again i forgot to resize my
plot it doesn't like having it too small
and we'll run that
i mistyped something in here
oh i did a lowercase d and they did a
capital d there we go
so now we can go in here and do this
with my data and you can see a little
hard to see on my screen with all the
different things in there it plots the
actual cities so we can now see where
all the cities are in connection with in
this case fuel cost and cells
so you have a nice label to go with the
graph
and then we can also go ahead and plot
in this case let's do
oh the r o r oops
and we'll do that
also with the
cells
and do my data
remember to leave it lowercase this time
so we plot those two
we'll come over here and it's gonna i'm
surprised it didn't give me an error
and then we'll also add in the
with statement so we put some nice
labels in there
and it's gonna be the same as before
except instead of doing the fuel cost
cells we want the r cells and we'll
execute that
oops and of course it gives me an error
because i shrunk it down
so let's redo those two again
there we go and we can see now we have
the ror with cells and we'd probably get
a slightly different clustering here if
i was looking at these cities they're
probably different than we had before
but you could probably look at this and
say these kind of go together and those
kind of go together but again we're
going to be looking at all of them
instead of just one or two
and so at this point we want to dive
into the next step we're going to start
looking at a little bit of coding or
scripting here
this is very important because we're
going to be looking at normalization
we put that in there normalization
and if you've done any of the other
machine learning
skills and setup this should start to
look very normal in your pre-processing
of data whether you're in r or python
or any of these scripts we really want
to make sure you normalize your data so
it's not biased
remember we're dealing with distances
and if i have let's say the rors even
look at this graph here on the right
you can see where my ror varies between
eight and fourteen
that's a very small variable and our
cells varies between four thousand and
sixteen thousand so you can imagine the
distance between four thousand and eight
thousand which is a distance of four
thousand versus eight to ten versus two
the cells is going to dominate so if we
do any kind of special work on this it's
going to look at cells and it's going to
cluster them just by the cells alone and
then ror might have a little tiny effect
of 2 versus 4 000
we want to level the playing field
turns out there's actually a number of
ways in script to normalize
so i'm just going to put in
the code that they put together in the
back for me and let's talk about it a
little bit so we have z we're going to
assign it to my data
and let's go ahead and
we're going to do a little reshaping
across all rows
or i mean across all columns so each of
the rows are going to have a little
reshaping there
and then we're going to get m which
stands for means and we're going to
apply it to
my data so again we want to go ahead and
create a
the most
common variable in there
and then s is going to be sd stands for
standard deviation
so instead of just doing a lot of times
what they do with normalization of data
is we just reshape the data everything
between zero and one
so that if the lower end is eight that
now becomes zero and the upper end is 14
that now becomes 1.
that doesn't help if it is not a linear
set of data
so with this we're going to look for the
means and the standard deviation for
reshaping the data and that way the most
common values now become the kind of
like the center point
and then the standard deviation is how
big the spread so we want the standard
deviation
to be equal amongst all of them and then
finally we go ahead and take z and with
the z we're going to reassign it and
we're going to scale the original my
data which we re shaped
based on m and based on the standard
deviation
and the 2 in here that just means we're
looking at everything in kind of a xy
kind of plot
and we can quickly run these control
enter control enter control enter
control enter so now we have z which is
a scaled version of my data
and now we can go ahead and calculate
the euclidean distance
oops calcu
there we go
and in
r this is so easy once you've gotten to
here we've done all that pre data
processing
we'll call it distance
and we'll assign this to
dist
so d-i-s-t
is
the computation for getting the
euclidean distance
and we can just put z in there because
we've already reformatted and scaled z
to fit what we want let me go ahead and
just hit enter on that
and i'm going to widen my left hand side
again
i'm always curious what does this data
look like so let's just type in distance
which will print the variable down below
oops
you have to hit ctrl enter
and this prints out a huge amount of
information
as you can see just kind of streams down
there
and let's go ahead and large this
and i don't know about you but when i
look at something like this it doesn't
mean a whole lot to me other than i see
two three four five six
and then you kind of have the top part 6
17 18.
so imagine this is like a huge chart
is what we're looking at
and we can go ahead and use print oh
distance
digits
equal three and let's run that
also keep forgetting that it
has to go through the graph on the right
and we see a different slightly
different output in here let me just
open this up so we can see we're looking
at
and by cutting down the distance you can
start to see the patterns here if it's
looking at the different distances
so if i go to the top we know the
distance between 1 and 2
1 and three one and four one and five
one and six and then two and three
and so on obviously distance between
itself is zero and it doesn't repeat the
data so we don't care to see two versus
one again because we already know the
distance between one and two
and so we have a nice view all the
distances in the chart and that's what
we're looking at right here and it's a
little easier to read that's why we did
the print statement up here to do digits
equals three make it a little bit
smaller we could even just do digits
oh let's just do two see what that looks
like we might lose some data on this one
if it's uh if something's way off
but we have a nice setup and we can see
the different distances and that's what
we were computed here between each of
the points
and then the whole reason we're doing
this is to get ourselves a nice
dendogram going a nice clustering
dendogram we'll do a couple of these
looking at different things
we'll take a variable hc.l and we're
going to assign it
h cluster
and then distance
that easy we've already computed the
distances
so the h clustering does all the work
for us
and then we hit enter on there so now we
have our hcl which is assigned the h
clustering
computation based on distances and apart
and then i'm going to expand my graph
because we would like to go ahead and
see what this looks like
and we can simply plot that
and hit the control enter so it runs
and look at that we have a really nice
clustering dendogram
except when i look at it the first thing
i notice is it really shows like numbers
down below
now if you were a shareholder and some
data scientist came up to you and said
look at this this is what it means you'd
be looking at that going what the heck
does 3 9 14 19 1 18 mean
so let's give it some words there
so let's do the same plot with our hcl
hc there we go
and let's add in labels and this is just
one of the commands and plots
so we have labels equals my data
and then under my data we want to know
the city
and we'll have it hang minus one that's
just the instructions to make it pretty
so we'll run
that oops i accidentally ran just hang
my one let me try that again
there we go okay so now you can see what
it's done and the hang my one turns it
sideways that way we can see uh central
which is central us and kentucky
and we start to actually get some
information off our clustering setup
and the information you start looking at
is that
when we put all the information together
you probably want to look at central
american kentucky together
oklahoma and texas has a lot of
commonality as does arizona and southern
u.s
and you can even group all five of those
florida oklahoma texas arizona and
southern u.s
these regions for some reason share a
lot of similarities and so we want to
start asking what those similarities are
but this gives us a place to look it
says hey these things really go together
you should be grouping these together as
far as your cells and what's what you're
doing
and then one of the things you might
want to do is there's also we can do the
dendrogram average
this changes how we do the clustering
so it looks very similar like we had
before
there's our hcl we're going to assign it
we're going to do an h cluster
we're still doing it on distance oops
distance
and this time we're going to set the
method to average so we can change the
methodology in which it computes the
values
and before if you remember correctly we
did median median's a little bit
different than means we did the most
common one and then we want the average
of the median
and let's go ahead and run that
and then we can plot
and here's our hcl
oops there we go here's our hcl and i
can run that plot
and you can see this changed a little
bit
so our way it computes and groups things
looks a little different than it did
before and let's go ahead and put the
cities back in there and do the hang
control copy let me just real quickly
copy that down here
because we want the same labels
and again you can see nevada
idaho pujet i remember we're looking at
southern u.s and arizona
texas and oklahoma florida so the
grouping really hasn't changed too much
so we still have a very similar grouping
it's almost kind of flipped it as far as
the distance based on average has
but this is something you could actually
take to the shareholders and say hey
look these things are connected and at
which point you want to explore a little
deeper as to why they're connected
because they're going to ask you okay
how are they connected and why do we
care
that's a little bit beyond the actual
scope of what we're working on today
but we are going to cover membership
what's called a clustering membership on
there
and let's create a member we'll just
call it number one
oops there remember.1
and we're going to assign to this
we're going to do cut tree
and
cut tree
it limits it so what that means is i
take my
hc.l in here
oops there we go dot l
and so i'm taking the cluster i just
built and we want to take that cluster
and we want to limit it to just a depth
of three so we'll go ahead and do that
and run that one oops let me go run
there we go so now i've created my
member one
and then oops let me just move this out
of the way
we're going to do an aggregate and we're
going to use z remember z from above
and we're going to turn member 1 into a
list
and then we're going to aggregate that
together based on the mean let me go
ahead and enter run that
so i did member l it's actually member
one
there we go
and if we take a look at this
we now have our group one fixed charge
and then all your different
columns listed there and most of them
should come up kind of looking between
zero and one but you'll see a lot of
variation because we're
varying it based on the means so it's a
means the standard deviation not just
forcing it in between zero and one
which is a much better way usually to
normalize your data than just doing the
zero one setup
and finally we can actually look at the
actual values
and the same chart we just did
oops
i made a mistake on there with my data
there we go okay
and again we now have our actual data
instead of looking at just the if you
looked up here it's all between zero and
one
and when we look down here we now have
some actual connections and how far
distance this different data is
again because more of a domain issue and
understanding the oil company what these
different values means and you can look
at these as being the distances between
different items
so a little bit different view and you
have to really dig deep into this data
we really want you to take away from
this is the dendogram
and the charts that we did earlier
and that is the cluster output and our
nice dendogram
so this would be stage one in data
analysis of the cells again you'd have
to have a lot more domain experience to
find out what all the individual numbers
we looked at mean and what the distance
is and what's difference between central
america and kentucky and why they're
similar and why it groups all of central
kentucky florida oklahoma texas arizona
and southern us together into one larger
group so i'd be way beyond the scope of
this but you can see how we start to
explore data and we start to see things
in here where things are grouped
together in ways we might not have seen
before and this is a good start for
understanding and giving advice for
cells and marketing maybe logistics city
development there's all kinds of things
that kind of come together in the
hierarchical clustering as you begin to
explore data and we just want to point
out that we get three clusters of
regions with the highest cells region
with average cells region with the
lowest cells again those are some of the
things that they clustered it around and
you could actually see where things are
going on or lacking you know it's this
case if you're the lowest cells no one
wants to be in the region of the lowest
sales welcome to this session where we
will learn on time series analysis using
our programming language so this is
basically a mini project
where we will look at time series data
and how we can analyze it
visualize it to basically find some
important information or gather insights
from the data now when you talk about
time series analysis time series is
basically any data set where your values
are measured
at different points in time
so when you talk about time series data
data is usually
uniformly spaced at a specific frequency
for example hourly weather measurements
you have daily counts of website visits
monthly sales total and so on so when
you talk about time series that can also
be irregularly spaced and sporadic for
example timestamp data in computer
systems event log or history of 9 11
emergency calls
now when we work with time series data
for example here i am taking a energy
data set we can see how techniques such
as time based indexing resampling
rolling windows can help us explore
variations in electricity demand and
renewable energy supply over time now
here we will look at some aspects of
this data set which i am considering so
there is this is open power systems data
set and here is the data set i have we
can look at the data set now this is in
a simple format it has time
it basically has values for consumption
and then you have data for wind and
solar and wind plus solar so in certain
cases you have only the date and the
consumption but then if we scroll down
we will also find
data for wind solar wind plus solar and
so on so this is a time series data set
which we would want to work on
sometimes you may also have the data
collected which just does not have the
time but it may also have
time stamp that is it would have say
hour minutes and seconds and that can
also be worked upon so let's consider
this data set and let's work on this
project where we will analyze this time
series data set
now here we can work on this time series
data we can basically create some data
structures out of it such as data frames
we can do some time based indexing we
can visualize the data we can look at
the seasonality in the data look at some
frequencies and also do some trend
detection
now when you talk about this data set it
has electricity production and
consumption which is reported as daily
totals in gigawatt hours
and here are the columns of the data
which i was just showing you so you have
data you have consumption you have wind
you have solar and wind plus solar so
this is the data we have and we will
basically explore say electricity
consumption and production in germany
which has varied over time so some of
the questions which we can answer here
is when is electricity consumption
typically highest and lowest how do wind
and solar power production vary with
seasons of the year
what are the long term trends in
electricity consumption solar power and
wind power how do wind and solar power
production compare with electricity
consumption and how has this ratio
changed over time
we can also do wrangling or cleaning of
this data or pre-processing of data and
create a data frame and then we can
visualize this
now let's see how do we do that so i
will open up my r studio and let's look
at the data set so here is the data set
now i'm picking it up from my machine
you can also pick it up from github so
all the data sets or similar data sets
can be find in my github repository and
here
i can look in the data sets you will
find
a lot of different data sets here there
are some time series data sets such as
power
i can search for power or you have
basically
coal
or you have this
opsd
germany daily data set and there are
many other data sets which you can work
on
now to
get the documentation on this project
you can also look in my github
repository and you can search for
repositories
and then basically you can look in data
science and r
and here there is a project folder where
i have given the documentation sample
data set and also
your time series analysis related
document this is also the code which you
can directly import in your r studio and
you can practice or work on this project
so let's see how does that work so first
thing is we will create a data frame
from this data set now here if you see
i am using header as true so that it
understands the heading of each column i
am also giving row dot names and i'm
specifying date so there is this date
column in the data set as i showed you
earlier let's look at it again so you
have date consumption wind solar wind
plus solar so you can suggest that date
should become the index column which can
be useful so you can do this now let's
just
create this
let's look at what does this data frame
contain
and here if you see it shows me some
data which
has been
now as a part of this data frame
structure it starts with consumption
wind solar wind plus solar and if you
see this one is becoming my index column
so i can always do a head and look at
part of the data frame using head or
tail so look at the first records so
let's see this now that shows me the
head data i can also do a tail and look
at the
ending values so if you closely see here
we have wind
solar
wind
dot solar and that basically has n a
values so there are missing values but
let's look at the tail and that tells me
that there is some data available for
wind and solar and wind solar
now we can always look in a tabular
format using view
and we can look at the data so this
shows me that there are values in these
columns we see any values but if i
really scroll down
i can see
some values which would be available for
wind and solar and wind solar so i can
just use view now i can look at the
dimensions of this particular object
and that tells me there are 400
4384
rows and four columns you can always
look at the structure that is check the
data type of each column which can be
very useful so if i see here i don't see
the date column because date column was
considered as an index which can be
useful but i also look at my other
columns they are of the num types so
that's the data type for each
attribute or each column here
now we would be interested in looking at
this date column so let's look at the
data type of this date column
now if i try to do this this will show
me that this is null because date as a
column does not exist because we created
it as an index so if i look at row names
and then i search
for my data show me the index column or
row.names it tells me these are the
values that's the date column
which we are seeing here now we can
access a specific row by just doing a my
data and give the index value or row
name value so let's look at that and
that shows me based on this index you're
looking at the value
you can obviously search for a different
date
something like this you can also pass in
a vector and you can give
range of values so that is 0 1 2006 to 4
of january and we can look at this one
so it shows me
these are the values so here actually
i'm not giving a range but i'm just
selecting multiple values from row.names
now we already know that in r you have a
summary function
so you can always do a summary and that
gives you
for each column it gives you minimum
first quartile median mean third
quartile and maximum values so we are
looking at consumption we are looking at
wind solar and wind dot solar
now this is good but then if i would
want to really visualize the data access
the data do some analysis then it would
be good to
take all the columns and then we can
later decide to change the data type of
say date column if we want to use it so
earlier i was using date as row.names or
the name of the rows or index what you
call in any other programming language
so here i will just use my data set and
i will say header is true i'm calling it
mydata2 let's look at the data
and this one shows me five columns where
in my first column is the date
consumption wind solar and so on now
looking at the structure
so let's look at the data type
so it tells me
that if now i'm interested in looking at
the date column from my data to data
frame it tells me it is a factor
with four 384 levels and these are the
values
so
it is not in a date time format it's a
factor now what we can do is we can
convert this into a date format how do
we do that so let's have a variable x
and i'm going to use as dot date
function and i'm going to pass in my
date column so that's assigned to x now
let's look at the head of x and it shows
me the values we will also see what kind
of class it is
and we will look at the structure of x
so class already says it is date type
and look at the structure so it shows me
the format
now we have converted this column or
column related value into x now how do i
basically
extract values out of it or make it a
part of data frame so first i will use
so all once it has been converted in
date format i will go for as dot numeric
and here i will create a variable called
year and i will just do a format on x
which is basically of date type and then
i am saying
percentage y so that will get me the ear
component out of this let's look at the
values
that shows me year component
now similarly we can get the month out
of this and then basically look at the
month values we can get the day out of
it and we can get the day component now
if i look at my data 2 which we had
created earlier this basically had date
consumption wind solar wind solar so
what i can do is i can add these
extracted columns such as year month day
to my data frame using a
c byte that is column bind and i will
assign it to my data to again so let's
do this and now if you look at head it
shows me
date so that should be date format
consumption now this one might not be
date format but we'll see you have
consumption wind solar and we have
extracted the year month and day which
can help us for group buy we can do some
aggregations we can do a plotting and we
can do various things by these
additional columns now let's look at
first three rows here so i'll say one is
to three for my data two and that shows
me some data here you can always do a
ahead and look at the sample of data so
that basically shows me month
day
your columns and then you have your date
now what we can do is we would want to
visualize this data we would want to
basically understand the consumption now
as i said
if we want to visualize the data say for
example i want this which is consumption
of data over years and this one is in
terms of gigawatts per hour as we were
mentioning here gigawatt hours so if i
would want to create this visual to
basically understand the pattern of the
data how do we do it so we can you
create a line plot of full time series
of germany's
electricity consumption using the plot
method now how do we do that
so here
one of the option is i can straight away
use the plot method
i can then say what would be in my
x-axis what would be on my y-axis
what would be the type of
graph i would want to plot what is my
name on x-axis y-axis and this is the
simplest way so i'm saying my data 2 i'm
extracting the year column
and here i'm taking the consumption so
let's create a plot
and here if you see we are looking at a
plot we do see some tick times and we
see that the data has been divided with
every two years so from 2006 onwards to
2016
but then really this data does not give
me
a you know a very useful way of looking
at the rate or understanding it might be
what i can do is i can use the same way
but i can give apart from x-axis and
y-axis i can say
that limits that is x limit is 2006 to
2018 and y limit is from 800 to 1700 so
we can do this and let's look at this
again this is a plot but it really does
not help me in visualizing and
understanding the data so what are the
better options
i can go for multiple plots in a window
as of now we are just sticking to one
plot in window so if you would want to
have multiple plots you can always
change the value here and make it two or
three that will say how many rows and
how many columns as of now we will just
keep it as it is par
mf row now
if i would want to plot i can straight
away give the column name so i am
interested in getting the consumption
now i can just do a plot i'll say
mydata2 and i will choose the second
column which is consumption which we saw
here
from our data so consumption was the
second column so i can just do a plot in
a straight away way without mentioning
your x-axis y-axis limits and so on and
if you look at this this one is giving
me
a pattern now here i am looking at
x-axis y-axis which is not really named
we do not have a name to this graph
and we are looking at the data it does
show me some kind of pattern but
might be we can make it more meaningful
so i can do it this way where i say my
data second column let's give access as
year x axis y axis is consumption
now that has changed
the x-axis and y-axis now i can also
give some more details i can say type
should be line
i have the line width i am saying color
is blue
and let's do this so this looks more
meaningful might be shows a wavering
pattern of consumption over years
i can also give a
limit of x that is 0 to 2018 and that
basically shows me the range now we can
change that and we can be more specific
and saying x limit should be 2006 to
2018
and let's look at this now this one once
you have given a proper limit it shows
the line graph and it shows what was the
consumption in 2006 and over a period
till 2018.
i can then
use any of these options are fine but it
depends on what and whom you are
presenting the data or what kind of
analysis you are doing so i can do a
plot i can choose column second x lab
which is x axis y axis type is line
width giving x limit y limit and then
i'm giving a title to this which is
consumption graph
and then basically you are looking at
the line graph
now those are the options which you can
do either you could be very specific or
you could just
give
your column which you want to plot or
obviously make it more meaningful by
giving all the details
now what we can do is if we would want
to look at
this data and understand it better
rather than just looking at a simple
line i can take the log values so here
i'm saying log of
my data to second column so i'm taking
log values of consumption and i'm taking
the difference of logs so i can say
difference and then you can basically
increase or decrease this by multiplying
it by some number so rest remains the
same i'm changing the color and let's
look at this plot and you see this
basically is giving me
a better pattern which makes meaning
here we see the log values so this is
you are using a simple plot function
in r you can also use gg plot now for
that we can install the ggplot package
it's already there in my machine so i'll
say no
i will access this by using the library
vgg plot 2
and now i can use gg plot to plot so the
way you specify here you can say mydata2
that's the data frame i am saying type
as o and when i am saying line i am
basically going to
use x axis which is here y is
consumption and let's look at this plot
so again we are back to the one which we
were doing earlier really does not make
any sense
gives us some data but then really does
not give me enough information
i can
in my aesthetics i can say x as year y
is consumption i can do a grouping and
then i can give line and plot so again
we have some information but really does
not help me right now let's look at
other example so i'm just doing the same
thing here and i'm looking at line type
being tasked i'm using the gg plots
other methods such as geom line and gm
point to give me more information and if
i look at the
plot it does give me data it tells me
what are the different values it gives
me some kind of pattern but i would
still prefer the way we were doing with
plot now
we can change the color and obviously
add details to it so what we see is when
you use the plot method which i did
earlier it was choosing pretty good tick
locations that is every two years
and labels the years for the x axis
which was helpful
right but with these data points which
we were seeing here
or say for example this one
or say this one
or say this one
we are looking at some data but then
that
really is quite crowded
and it is hard to read you can look at
the values but then it really does not
give you enough information so we can go
for plot method but then we will see how
we can consider different data now if i
would want to plot the solar and wind
time series so let's see how do we do
that
so
wind column is what i'm interested in so
first thing is it was always good to
find out the minimum and the maximum
values in every column so i'm saying
minimum i am saying let's put in here my
data 2
and then let's look at the values so we
are looking at the columns
we know consumption is the second column
wind is the third column
and
you have solar as the fourth and this
one is the fifth so let's say let's find
out the minimum of each of these columns
which we would want to plot so let's say
minimum of data third column and here
i'm also saying remove the n a values
because we do not want to consider the n
a values so let's let look at the
minimum that shows me 5.7757
what is the maximum value it is 826 so
that also helps me in giving a limit if
i want to plot wind on y axis i can give
a y limit from 5 to 850
consumption wise let's find out the
minimum from second column and maximum
and similarly for solar find the minimum
and maximum and wind plus solar minimum
and maximum so this will be helpful when
you would want to plot multiple graphs
or
give some limits so that's fine now for
multiple plots as i said
instead of having one plot let's plot
consumption and wind and solar and try
to see a pattern so i can say par
function and i will say three rows and
one column
so
now when i start plotting you will see
you will have multiple plots in one
single window so let's see how we do it
so here
let's look at plot one so this one is
consumption as we did earlier
and let's look at the data so that gives
me some data you can always do a zoom
and you can look at the data you can
basically expand this graph or you can
reduce this graph to see
what kind of pattern we have in
consumption similarly we can
basically choose
date being
x axis
my consumption being y-axis right so
this is being more specific because here
we have a range but it really does not
give me enough information so i will
basically give
x-axis y-axis i will give the name that
is daily totals and then i will
basically give consumption color and y
limit based on my minimum and maximum
limits so let's do this
and now we can
look at the data here so let's see this
data
makes a little more meaning because we
are looking at the dates
and let me do a zoom so it shows me all
the dates it shows me the data points it
shows me
how the data
pattern is changing for consumption
now
this is for consumption so what we can
do is
we can also extract specific data so if
you see here i have done some testing
where i am saying okay i would want to
get
a date specifically
i would want to extract some value so we
are looking at the date column but if
you remember we did not change the data
type we just changed the data type of
date column we extracted year month out
of it
it would be good if we can
convert a column into date time format
and put that in our data frame now
let's look at the plot2
this is mainly for
your
column
which should be consumption and wind and
solar so here i see it is solar data
and i can plot this one to see how it
looks like
and that tells me from 2006
onwards we have some pattern
i can
be more specific where i say
i would be giving date and then
the
column for solar x-axis y-axis what is
the type what is the y limit and what is
the color
it is always good to specify your x and
y axis give a name rather than let it
automatically pick up now this makes
more meaning because it shows me some
dates
similarly we can do for wind
so either you do it just by giving the
column
or you give your x and y axis so let's
look at this one
and this shows me the data so we can
choose plot three
this one we can choose plot two
we can choose plot one and we can put
all that data in one graph
so that's when you are putting in multi
plots in one particular graph you can
always do a zoom
you can always look at the data right
and this is usually useful to look at
the pattern what kind of pattern we see
what data we have and so on now moving
forward so we have seen how you are
creating these plots all in one window
let me reset this back to one plot per
window
and let's basically plot time series in
a single year so what we have seen is
that when you look at the plot method it
was quite crowded then we looked at
solar and wind and if you compare that
you will see your consumption pattern
your solar pattern your wind pattern and
basically we can see from this
particular data some kind of pattern
so electricity consumption is highest in
the winter
where we will see what is the
consumption
is it highest in winter or is it in
summer we can see that by breaking a
year
further into months we can see that but
we see a pattern which goes for every
year or every two years being highest at
a particular point of time and then it
drops down
so electricity consumption is highest in
winter and that might be due to
electrical heating
and increased lighting usage and lowest
in summer
now when you look at electricity
consumption appears to split into two
clusters we can always look at the
consumption one with oscillation
centered roundly around 1400 gigawatts
so you can always look at fourteen
hundred gigawatts and you see all the
values here which are in that particular
consumption
another with fewer and more scattered
data points centrally roughed around one
one five zero so if you really expand
this you can see you will have lot of
data points at this point
now
we might guess that these clusters
correspond with weekdays and weekends
which we can see if you break that data
into yearly monthly weekly and so on now
if you look at solar production
that is highest in summer when sunlight
is most evident and lowest in winter so
obviously when you are making or
gathering some insights when you're
looking at the data you are also using
your domain knowledge your business
knowledge your
you know knowledge of business to
understand how this goes
if you look at wind power production
that's again highest in winters and
drops down in summer
so due to stronger winds and more
frequent storms and lowest in summer
so there is some kind of increasing
trend in wind power production over
years which we can see here
over the years and
all the time series data what we are
looking at
is
referring or showing us some kind of
seasonality that is we are looking at
seasonality in which a pattern is
repeating again and again at regular
times
at regular intervals so if you look at
consumption solar and wind time series
that oscillates between high and low
values on a yearly time scale which we
can break down and see i'll show you
that
it corresponds with the seasonal changes
in weather over the year
so seasonality
does not have to correspond with
meteorological reasons for example if
you look at retail stale sales data
that will show you yearly seasonality
with increased sales in particular
months
so seasonality when we say can occur on
other time scales so the plots what we
are seeing here
they are fine but if you look at those
plots they might
show some kind of weekly seasonality
also
so in your consumption corresponding to
weekdays and weekend so let's plot for
one single year now how do i do that
so first is i will look at mydata2
that shows me the structure it shows me
date which is factor other columns which
are all numerics
now like we did earlier i'll repeat this
step where i'm going to convert the date
column into date type
look at head of it look at class of it
look at the structure of it right and
then what i want to do is i want to add
this
as to my data frame so i will create a
variable called mod data
and this one will have as data and i'm
formatting
the value of x which is date time into
month day and year so let's do that
and now you look at the mod data which i
created like modified data so this is
the format i have it is in date type if
you carefully see here
and then i can look at the head of it
so it saves me mod data
now
we are what we did here is when i said
my data 3
so
my data three
we did a
c bind and i did a mod data which is
going to add this column to my
other columns of my data too so my new
data frame is my data three let's look
at the structure of it and you see there
is this date column i can delete it i
can remove it i can let it be right so
that depends on our choice might be we
want to once our analysis done we want
to remove the mod data right so we can
keep both of them
now
let's
basically extract data for a particular
year now how do you do that so this is
some wrangling so i will say mydata4
let's call it mydata4 and i will use
subset function so subset will work on
mydata3 that's the data and what i'll do
is i will do a subset how do how is the
subset found so i'll say take the mod
data
column the value should be
greater than or equal to 2017 and should
be less than
2017 december 31st so i'm getting data
for one year and i'm storing it as my
data
4. let's get the head of it and you see
we are specifically looking at 2017
related data
now let's do a plotting of this where i
will only
create a plot for one year so i am
saying my data 4 that's my new
data what we got
so
here i am going to take the first column
which is mod data
i am going to take the third column
which is consumption so i am looking at
the date format for one year consumption
values for it and then rest of the
things as we have done earlier let's
look at the plot and this makes more
meaning right so when you look at this
plot it tells me jan to jan it shows me
some kind of pattern where i have
divided the year into months
right and it is broken down into say two
months so jan and march and may and july
and so on but we still see a pattern and
that gives me good understanding of
pattern where i've broken it down into
months
so this is where you have taken time
series in a single year to investigate
further and this is what we see
right now we can clearly see there are
some weekly
oscillations
what one more interesting feature is
that at this level of granularity that
is when you're looking at yearly data
there is a drastic decrease in
electricity consumption in early january
and late december during the holidays or
probably we can assume that this is
holidays now i can zoom in further and
look at just jan and feb data
let's see how we do that and let's see
how we work by zooming in the data
further
so to
zoom in the data further let's see how
we do it now here we have this mydata4
which is basically having a subset right
so let's work on this one so i will say
my data 4 which earlier i was taking
data 3 i was doing a subset and i was
giving the date but this time i will
make it more
narrower so i will say my data 4 i will
say subset from my data 3
and i will choose mod data column which
we have modified with the date format i
will choose the starting date as
1701
that is jan and then let's go till feb
and let's create this
now let's look at the head of this so it
shows me we have the data which is jan
and then you you can basically look at
more on this
now again as i said earlier let's find
out the minimum of this from the
first
column so that is basically your mod
data so let's look into this one
and that basically
will give me minimum and maximum let's
look at the values so this one tells me
jan 17 january 1
and maximum is
your
feb 28th second month 2017. so we are
actually looking at two months data here
let's look at the y minimum so this is i
will look at
column three now what is column three
consumption so let's look at the minimum
value for consumption maximum value of
consumption let's look at the values
which can be given as our limits
now this is the minimum and maximum now
let's do a plotting for this data which
has been narrowed down for consumption
based on my data so i am saying
my first column which is mod data and
then third column which is consumption i
am giving some
naming convention for sorry namings for
your x-axis y-axis
what is my
consumption
or what is my title here what is the
color and then you see i'm using x limit
to give the minimum and maximum limit
and y limit so let's look at this data
and if you
look at this data
it is specifically for two months and
again i can look at the pattern here
what i can also do is i can add some
grid here
so i can basically look at this data and
make more meaning out of it so it is
bi-weekly data you can see now i can add
a line here using ab line and then i can
basically choose what lines i would want
to add horizontally
so that basically allows me to dissect
the data and look at data in a more
meaningful way i can also
add vertical lines so vertical lines is
i'm saying sequence will be minimum
maximum and i'm saying an interval of
seven
so let's do this and
this basically has added some lines
every week and you can see at the end of
week it is dropping and then it is
starting again it peaks somewhere in the
mid of the week and again it
drops down so this is you're looking at
your consumption data right now what we
can also do is we can create some box
plots so when we looked at zooming in
data for jan and feb you can add some
data points like this so consumption is
highest on the weekdays as i showed you
here and lowest on the weekends so this
is what we are seeing when we are
breaking the data or zooming it further
for a couple of months so we have
vertical grid lines and we have nicely
formatted tick labels that is jan first
and 15th feb first and so on so we can
easily tell which days are weekdays and
weekends with use of these grid lines
and basically breaking it down so there
are many other ways to actually
visualize your time series data
depending on what patterns you are
trying to explore you can use scatter
plots you can use heat maps you can just
use histograms and so on
now moving further we would want to
explore the seasonality right so when
you further explore the seasonality of
our data we can use box plots basically
to group the data by different time
periods and display the distribution for
each group now how do we do that
let's come here and let's see how box
plot works so i can just do a simple box
plot and i can choose my consumption
column and that gives me just the
consumption data but this really does
not give me any meaning i can look at
solar data i can look at the wind data
and we can also see some outliers here
so we can create box plots but
if we would want to do a box plot what
is box plot it is basically a visual
display of your phi number summary that
is you want to look at your mean
median
you want to look at your 25th percentile
50 percentile
or 75th percentile so we can use a
quantile function use the consumption
column and then you basically give
a vector which shows you find number
summary so that's your quantile and then
let's do a box plot so if you are
looking at quantile it tells me what is
the minimum what is 25th percentile 50
75 100 that's from my consumption column
so let's create a box plot for
consumption
let's give it a name as consumption
let's give y axis as consumption and a
limit
for
y-axis now that's my consumption graph
so i can look at yearly data now that
will make more meaning rather than just
looking at the complete consumption data
so how do we do it early so we will say
consumption
and then i will say the year column so
it is consumption but grouped based on
year
so here i can give x axis y axis and i
can give y limit so let's create this
and this makes more meaning we can give
some coloring scheme here but now i'm
looking at 2006 2007 8 9 and so on and
we can look at the data what is the
range right it gives me five percentile
or sorry five number summary of the data
per year and it basically allows me to
look at the seasonality of this
similarly we can create box plot
by just giving consumption early group
and here i am giving the title as
consumption y-axis
x-axis and y-limit
wherein i can also use less so this is
one more feature which you can do and
that basically will give me
the tick points if you compare this one
to the previous graph
so when i created this previous graph i
had 2006 2008 and i had from 600 to 1800
and if i go for the next one
i am basically seeing more useful
information now let's look at monthly
data
so
i would want to group it based on months
and let's create that so this gives me
the monthly data where i'm looking at
months
and i could select a particular year or
i can just do a grouping based on months
so
i can have multiple plots to see a
difference here so let's do this
now let's create a box plot for
consumption which is monthly data and
let's give it a color let's look at the
wind data which is again grouped monthly
and let's look at the solar data which
is grouped monthly
now if i zoom in it basically gives me
the seasonality of the data
for your wind for your consumption for
your solar so what we are doing is we
are creating these box plots
which are giving us
values now what i can also do is i could
look at the day wise also but before we
look into this how do i
infer some information from these box
plots which are being created so this is
what we have done where we are looking
at the data for month and these box
plots give me ear seasonality
which we were seeing in earlier plots
but give some additional insights so if
i look at the data here it tells me the
electricity consumption is generally
higher in winter
now this is based on months so we can
see consumption is higher in winters
and lower in summer so we can obviously
look at our plot we can see where it is
lower where it is higher
and then
we can
look at the median and lower two
quartiles are lower in december and
january compared to november and
february so that is you look at the
quartiles and you will see
that
the median and lower two quartiles are
lower in december and january
here jan and december so you can look it
from my plot
now
this is giving you some idea on
seasonality
now
that might be due to business being
closed over holidays now this one we
were also seeing when we looked at time
series for 2017 only and box plot
basically confirms that there is this
consistent pattern throughout the years
now when you look at
your
solar and wind power production both
will give you a year seasonality
what we are seeing here
and
if basically i look at the data so
it depends on what parameters you are
choosing but if you look at solar it
will reflect the effect of occasional
extreme wind speeds associated with
storms and other transient and since we
are grouping it based on months we can
see this pattern is quite evident every
year
now what we can do is we can group the
data day wise so here let me again reset
this to
one plot per graph
now i'll say box plot i'll say
consumption which is group based on day
now we know that there is a day column
and let's give a while limit and let's
look at the data so this is where i'm
grouping the data day wise
so you look at 31 days and you look at
the box plot so this is where you are
plotting it on a daily basis
so you can look at the data you can
break it down to a particular week so
here i have given
a day and i have chosen all the 31 days
but i can break it down to a week and i
can look at the data so
if we look at the data per week or per
day we can basically infer that
electricity consumption
where i'm doing a consumption group by
day
is higher on weekdays than on weekends
so
time series with strong seasonality
can
often be represented with models that
can decompose signal into seasonality
and long trend
now this is
an easy way now how do we look at the
frequency of the data that could be
interesting to see
so let me
look at
say the yearly data
which we were seeing here
now let's go further and here
we have looked at data so what we will
do is we look at the frequency now when
you look at the frequency
when you talk about frequency in your
data so we have the modified date column
which gives me a frequency and if we
really look into the data that will tell
me
that the data is
on a daily basis so for that let's look
at my data three again which gives me
data and you can just see
all the data's data or dates are in
sequence so your 22 23 24 25 26 and so
on
i can look at i can access a d player
package
that is basically
allowing me to work in a better way now
i can look at the summary of this and
for all my columns
i am seeing
what is the minimum phi number summary
date and consumption so date does not
show me anything because this is not in
a date format it is just a factor but
other things have the fine number
summary so we are looking at wind plus
solar we are looking at year and month
and day and all these columns now what
we will do is we will
want to find out the sum of each
column how many entries does it have and
we will say the value should any value
should not be considered so let's look
at this one so it tells me for my
particular columns
so let me run this again
and that shows me
for each column how many values you have
and
these
counts
do not include the n a values
now similarly i can find out
specifically for consumption i can find
out is there any n a value so i'm saying
is dot n a and let's find out if there
is any n a value or missing value in
consumption it says 0
okay that's good if you look in wind
it tells me there are
1463 entries which are any
similarly solar
similarly
wind dot solar or wind plus solar so it
gives me a count of
n a values that is missing values
and also values which are not missing so
to understand frequency what we can do
is we can find out the minimum
on the date that is the first column and
i'm saying
rm
na.rm is true that is get rid of na
values and find out the minimum
and let's look at the minimum value
this is the minimum from my modified
date
now if i would want to get the frequency
i can basically use sequence function so
i can say
from x minimum that is the minimum value
i want to look at the frequency that is
day wise and let's just look at five
entries and see if there is a
day by day frequency
so let's look at the value of this and
obviously it tells me there is device
frequency so that allows me to look at
the frequency look at the type of it it
is an integer class is a date
so similarly we can say from x minimum
we can basically look at the frequency
month-wise
and i can again look at five records so
that shows me monthly data
right so i can
extract the data for frequency similarly
yearly data and that's also very useful
now
we can select data which has n a values
for wind
so how do i do it i would want to find
out
the wind column and i want to find out
where the values are in a so
i will create a variable
and here i will say mydata3
and then i give a conditional where i
say is na
in the column so let's do this
now once i've done this
once i've done this i have said that my
selected wind data from my data 3 where
we set any values
and i will give the names to this so
name should be in my data 3 i'm
interested in mod data consumption wind
and solar so these are the four columns
i'm interested in let's look at first 10
records here or first 10 rows so that
tells me these are the values where wind
has n a
or missing values
i can always do a view and that gives me
the complete data so it basically shows
me
1463 entries
and here it shows me all n a values so
you can look at all the way to the end
and it shows me wind has n a solar does
have some value here
in the last row but then also if you see
the numbers have
a difference so you have one four six
one and then you have two one seven four
so there is a difference so there is
some data in between where wind has some
values so we have found out n a values
now
what we will do is we will select data
which does not have any values so i will
call it cell selected wind 2
i'll again use my data 3 i will say
which but now i am saying not any
from this column and i will select the
data for the columns so i am interested
in looking at
10 records and this shows me not any
value so no more missing values so if i
really look at this data as i saw
earlier which has n a and if i look at
these values which are not any for the
wind column so looking at these two
result we will know that in year 2011
wind column
has some missing values
so let's focus on year 2011. so how do i
do that let's call it a different
variable i'll say mydata3 i will say
here when i say which where we were
saying na
here i will say the year should have a
value of 2011 and i want all these
columns
let's look at the data here
and this is showing me 2011 but
we
are not seeing all the values so there
are some values but then there are some
missing values also for 2011 based on
whatever analysis we have done so let's
look at the class of this it is
basically a data frame do a view
and this one will help me in finding out
where are the any values so if you just
scroll down
looking at all the data let's search if
wind column has a n a or a missing value
and i will see
if there is any missing value in which
column or which row it is for the wind
column so we have all the values which
are existing
i could select and search for one
specific value and i'll show you how we
can do that so here let's scroll all the
way down so it's like you're exploring
your data and seeing is
wind column having n a or missing value
for a particular row
and let's scroll here and here you see
there is a missing value for one
particular row so
13th december 2011 has wind value
15 december has wind value but
your 14th december does not have right
similarly we can search so there was
only one entry which was missing now
that could be for some reason might be
it was not calculated might be it was
not tabulated so we have a missing value
and that
can affect my plotting that can affect
my analysis so let's look at the number
of rows
in this which will tell me how many rows
we have
for 2011. so it tells me 365 so that is
basically the number of days in a year
now we will find out if
there were any values so we earlier
checked total number of na values per
column
that is
in your row number 265 to 269
we can see here 265 to 269.
so this is where we were seeing
are there any n a values right so let's
go back here
and
we want to find out the number of n a
values for a particular year how do i do
it so i can just do a sum i will say is
n a
now i am interested in my data 3
wind column and i am saying my year has
to be 2011 but i am finding out the n a
values
so let's do this and it tells me one and
that's right that's what we saw when we
did a view let's see
how many non-na values you have and that
is 364. so that basically
satisfies my logic so it's 364 plus 1
missing so there are 365 let's look at
the structure of this it tells me you
have modified date and date format you
have consumption wind and solar now
let's create a variable
selected wind 4
i will save in 3 that is which was
having all my n a and
non n a values for 2011
i will say let's find out the n a value
because i'm interested in finding out
that particular row so i'm saying find
out where the value is n a and i want
all the columns
let's look at this one and this is my
specific
row which has a n a value
now
we know that data follows a device
frequency which we have clearly seen now
let's select data which has any and non
na values
so
let's say let's call it test1 i will use
win3 which has
any non-na values but now i will say i
want the modified date which should be
greater than 12 12 2001. now remember we
had when we were doing a view we saw
that one particular day or what we see
here 14th of december there is no date
so i will select a subset of data
which includes this n a and non n a that
is might be i can take 13th of december
and 15th of december so let's start from
12 12
so the date should be greater than 12 12
that means 13th and it should be less
than 16 so that is 15th
and the columns
right so
now we have some data let's look at this
so i have
a i've selected a subset of data i could
have done this using subset also so i
have any and non-any values now
why are we doing this so sometimes you
might have some data for a particular
column and you may want to find out if
there are any missing values might be
you want to fill them up or replace them
with something so that is usually useful
when you are doing a trend detection so
say for example you have data for every
month and might be in one one of the
months you have missed or might be you
have data for every year collected
monthly and then in one of the years for
couple of months you don't have the data
like i can say 2016 i have data for all
12 months 2017 all 12 months 2018 might
be i don't have data from march and june
2019 i don't have data for same months
so i can forward fill or backward fill
them using the previous years same month
data so we can do that so here i have
test data where i've extracted a subset
of data
i can look at the
class of this it is a data frame
structure of this it has the columns now
let's use that library
and
function and use the tidy r package
and what we will do is we will fill it
up so i will use test1 i will fill the
wind column which has a missing value
now once you do this if you notice it
has done a forward fill so it has taken
the previous value and it has just
filled up that so you can
fill up the data using different
directions such as up and down left and
right and so on so we can take care of
missing values
in our frequency data which allows us to
basically
analyze the data in a better way now
here we will want to also look at some
more data so this is to deal with
frequencies of fill column
wherein you can take care of missing
values forward filled so filling values
can be done in different directions as i
said and you may want to first convert
your time series to specified frequency
if
your data does not have a frequency but
we had now if you do not have a
frequency might be you can convert it
into a frequency such as weekly daily
monthly as i showed you and then
basically you can
do a forward fill
for the value so for example if i have
my data i can break it down into weekly
and then look at the values and if there
are any values missing for weekly data i
can use a forward fill so that can take
care of my frequency data
then
let's look at the trends of the data
which is the last part of this project
so basically let's look at the trend so
when you say trend what does that mean
so in time series data
you always have some kind of trend so
that will exhibit some slow gradual
variability
in addition to higher frequency
variability such as seasonality and
noise
now
to visualize these trends what we do is
we use what we call as rolling means so
we know how our data is spread over year
or month or day but how about looking at
a rolling average and see what is the
difference so a rolling mean
will tend to smooth a time series by
averaging out the variations and
frequencies
so this can be higher than the window
size so there is something called as
windowing where you can choose a set of
time frame you can also average out any
seasonality on a time scale equal to
window size so
this will allow you to look at lower
frequency variation in the data
so when we are looking at electricity
consumption time series we already saw
there is a weekly pattern there is a
yearly seasonality which we saw using
box plots so we can also look at the
rolling means of the time scales how do
we do that so for this you can use some
package like zoo and then you can
basically use a rolling mean
using this zoo package
and you can say what
is the
frequency with which you want to
calculate the rolling mean
now how do we do this
let's look at this data so here i'm
going to my look at my data 3 which we
have been using so far now let's call it
a 3 day test you can give it any name
i'm going to use my data 3
i'm using the pipe in function
now i will use dplyer and i will arrange
the data descending in here now you can
always break it down step by step and
you can see the result of this
so i'm going to arrange this data in
descending order of year
so obviously my last one 2017 or 2018
will be on the top you want to group the
data by year so it depends on how many
years we have we'll see so you can group
the data by year now this data is then
used to basically mutate so mutate
function is going to allow me to use
this rolling mean so i will call it as
says 0 3
day so i'm going to calculate a rolling
mean every 3 days
for my consumption column
and
basically let's ungroup this so let's
see how this works
sorry yeah let's look at this and
here when i'm doing a three-day test
let's look at the result of this and
then i'll explain this so if you see
here we have the test three day column
now this has the rolling average now
what does that mean so first value here
what we see is one three six seven
is the average consumption in 2017
from the first date with the data point
on either side of it that is you can
look at
this
date so 1 1 3 0
then you look at
you are looking at the value 1 3 6 7
here so you look at 1 1 3 0 1 4 4 1 1 5
3 0 if i take a mean of these so for
example if i would just do this part
and that
is giving me
mean okay because i have a comment so
let's basically add anything as comment
and then let's do this so it saves me
one three six seven that's what we are
seeing here right so you've got getting
a rolling average every three days
similarly if you want every five days it
takes the five values and it gets the
mid value right so you can always find
out the mean
rolling mean
for a particular frequency now let's do
that for seven days that is weekly data
and yearly data that is 365 days so how
do i do it same logic my data test
now i am using my data 3 i am arranging
it in a descending order i am grouping
by year so
when you do a group by year so earlier
when we did a grouping by and when we
looked at the data it was telling me how
many rows we had
right so let's do a grouping by year and
let's say test 0 seven so that's a
rolling
average every seven days and i'm also
saying take care of the n a values
similarly i'm getting rolling average
every 365 days might be you can do
quarterly might be you can do half
yearly and let's do this so let's
create this my data test and let's look
at the result of this so i will use my
data test i will say arrange
based on modified date now we know there
is a column called modified date i want
to just look at 2017 data so i'm doing a
filter
right and then i will choose what are
the columns i'm interested in so i will
look at the 7 and 365 day and let's look
at say first seven records
so let's do this
and that basically gives me the
consumption value modified date year and
my rolling seven day average order of
seven day mean
which is for first 7 days and then 365
you will not see the data here but if i
do a view on this i can basically see
the values
so you can always select a particular
column to see the values these are the
values for every 7 day rolling average
this is for 365 days every 365 days so
you see all the values are missing but
every 365 entry you will have basically
some data
now
let's do a plotting of this and
basically visualize this data which we
are seeing rolling average so let me
first do a plotting one plot per graph
and let's do a plotting i will take
consumption data
x-axis y-axis
color and give a title to this so let's
create this and that's my consumption
data which is
spread over a period of time and that's
fair enough but now let's add some more
plot to this so i will add the seven day
rolling average to this
so for second plot to be added in the
same one in r you can use points
so i will say points i will choose 7
data column
type is line width
x limit y limit and color so let's do
this
and that's my
pattern seven day rolling average which
basically gives me some kind of trend
similarly i can add one more here and
this time i will choose the 365 day
and look at the pattern lines so now you
see some dots here well you could do it
in a different way so i can just add
legend to this and i can say legend will
be
where in x-axis and y-axis so i am
saying it will be 2500
and y is 1800 so my legend will come in
somewhere in here i am saying my legend
will have consumption
test
and this one i can give some names i can
give what is the color
i can say
what kind of
legend it explains what is
for each color and then basically a
vector so let's add a legend to this and
i've added a legend now you can do a
zoom and look at the data
and
here i see that
my x axis is fine but y axis is going a
little
out of my plotting area so i can
actually change that so here i have 1800
how about making it 1600
and let's look at this one
so
we can basically
uh go for this one and start again here
plot and points and line and then add a
legend right and you can basically place
your legend anywhere in the plot so this
basically is giving
me the trend what i'm looking at
my
rolling average
so similarly you can look at the trend
for wind and solar data so what we are
seeing here is when you look at trend
this is one more way of looking at it
you can always create plots in different
ways so seven day rolling mean has
smoothed out all weekly seasonality
which we were seeing here in my graph
where you look at every seven day
preserving the yearly seasonality so
seven day will tell
that electricity consumption is
typically higher in winter and lower in
summer so better is you break it down
uh yearly so here if you look at every
year you can see when is winter when is
summer what is the seasonality what your
trend what you are seeing here and if
there is a decrease or increase
for a few weeks
every winter
so similarly if you look at 365 now as
you said as i said rolling average
basically
reduces the variation so if i look at
365 rolling mean we can see long-term
trend
in electricity consumption is pretty
flat now that's what we are seeing it's
kind of pretty flat there is not much
variation over the years if you really
join these dots
so
we can basically see some highs and lows
and that gives me a trend now this is
how you can do a trend detection and
similarly we can do plotting for wind
and solar so this is a
small project which i demonstrated using
r
now all this code
which you have here in the form of a
project dot r file you can find here in
my github page this is a document which
explains some things feel free to
download this and you can add details to
it this is the sample data set which you
can also find in my repository in the
data sets folder so continue learning
and continue practicing r so who is a
data science engineer are you a data
science engineer or are you going to be
looking for a different field what
exactly is a data science engineer well
a data science engineer is someone who
has
programming experience in python and r
expert level knowledge ability to write
proficient codes and they we have python
and r i'm going to say python or r once
you become really proficient at one
language transferring those skills into
another one is usually fairly easy now r
is a little different because it is an
analytics platform so going from python
to r if you already know the data
analytics and python moving to r is
pretty easy uh now r doesn't have all
the other code that you can access with
python there's so many things you can do
with python that are not data analytics
so it's important to keep in mind that
you should probably be pretty
well-rounded in python and really have a
solid foundation in r or have a solid
foundation in r and be well-rounded in
another programming language where you
really get the programming side of it
strong sql and big data experience so
you have to have a strong coding skill
with hands-on big data experience and of
course we're showing sql here most
commonly used whether you're using a
microsoft sql server or mysql server you
can also start thinking hadoop and spark
in big data access hadoop file system is
not a huge jump if you've already
learned your sql and you've already
learned your basics in coding hadoop
sits on top of all that and does a
wonderful job creating huge clusters of
data but you really need to know your
sql because that is so common in most of
the large companies now accessing their
data and an ability to visualize models
and troubleshoot code of the models this
is a kind of an interesting one because
these models are can get very
complicated so you have to be able to
break them down into something that you
can put the pieces together and digest
each of the pieces so being able to
visualize these models is very important
and then being able to drill down and
troubleshoot the different models or the
pieces in those models they need to be a
versatile problem solver equipped with
strong analytical and quantitative
skills a self-starter with a strong
sense of personal responsibility and
technical orientation this is an
interesting one because when you talk
about data science engineer it's such a
new field that most companies don't have
it well defined they don't know what
they're looking for you might not even
know what you're looking for you might
have an idea and you're looking for
patterns but where do those patterns
lead you so you really need that
self-starter side to jump in there and
figure out where to go and be able to
communicate that back to the team and
there we go we have a strong product
intuition data analysis skills and
business presentation skills and this is
what's talking about is you gotta bring
that back to the team so if you don't
have a strong product in tuition if you
don't know what's going on with the
company what they need and where you're
going with your analysis it's going to
be a dead end and you've got to be able
to present that to the shareholders to
your co-workers which leads us to great
teammate with excellent interpersonal
skills and this is kind of a strange one
because you spend so much time behind
your desk so you have to be able to kind
of float between you're studying all the
data and being able to explain this to
people in simple terms that they can
understand uh no one wants somebody to
come up and say yeah the p score and the
f score is this that and the other thing
and you might have two people in the
room understand you you have to be able
to sit down and explain what that means
and why so let's take a look at the data
science skill set we talk about data
science engineer skill set we're talking
about database knowledge statistics
programming tools data wrangling
probably the least favorite and most
used machine learning data visualization
and then the touch on big data and so
we'll start with database knowledge
the most common database is of course
your sql structured query language
think rows and columns it's an essential
language for extracting a large amount
of data from data sets so knowledge of
the sql is mandatory for data science
engineers and you can see there's tools
required there's the oracle database i
mentioned my sql server microsoft sql
server teradata there are so many
different forms of sql and it'll just
keep coming back and coming back so if
you don't have a solid basic
understanding of sql go get it very
important because it will come up you
know it's if you don't know it it's
going to bite you and statistics of
course we are you know that's what this
is all about is figuring out and
predicting things so you need to know
your statistics statistics is a subset
of mathematics that deals with
collecting analyzing and interpreting
data therefore data scientists needs to
know statistics so you need to
understand your probabilities and what
that means and what the p-score means in
the f-score and means and mode and
median all that information standard
deviation all of those you need to be
aware of and then we get into the
programming tools and i mentioned this
earlier a little bit but you need to
master any one of these specific
programming languages programming tools
such as r python sas are essential to
perform analytics in data and again you
know you can move in there and you can
be an expert in just r you can be an
expert in just sas and a little bit you
can be an expert in python because
most of these when we're talking about
data science and data wrangling you
really kind of get down to the point
where you really need to have a full
programming language at least the basics
and if you're going to be working in
data science python right now is the
main one but you know there's also java
c plus so you need to know a solid
language at least the basics of it so
you can understand how to do basic
iterations and things like that i always
find that interesting that uh my sister
who runs a university data analytics
center that's the first question she
asks for people candidates to come and
work for is how do you iterate through
data you'd be amazed at how many people
don't understand that very basic concept
in programming so when we look at r r is
a free software environment for
statistical computing and graphs
supports most machine learning
algorithms for data analytics like
regression association clustering and
etc python python is an open source
general purpose programming language and
there's our general purpose i was just
talking about you need to know a little
bit about python libraries like numpy
and scipy are used in data science scipy
yeah numpy and scipy are very central to
python and i'd throw pandas in there
also that module those are very central
to working with python and data
analytics and then sas sas can mine
alter manage and retrieve data from a
variety of sources it can perform
statistical analysis on the data so
where r is the open source platform sas
is more like the paid for platform and
so it has some things more automated
because people are paying for it
developing it but it's also if you
really start using it as a company
you're going to have to pay for it where
r is free store is a free software
python's also open source and free to
use so we're going to talk a little bit
about data wrangling and i mentioned
earlier that is both the probably
people's least favorite aspects of data
science and also probably one of the
places you spend the most time so we
talk about data wrangling it is a
process of transforming raw data into an
appropriate format to make it useful for
analytics and it involves cleaning raw
data structuring raw data and enriching
raw data and this gets interesting
because you'll get stuck on something
like
in python you might get stuck on
something that is a date time based on
an integer 64 numpy set you might not
know what all those terms mean because
you work in r or something like that but
you get the point that it's all one
particular format and then you have to
figure out how to switch it so that the
computer can see it correctly in
whatever format you're using or whatever
analytics platform you're using
knowledge of machine learning techniques
such as supervised machine learning
decision trees linear regression k n etc
is useful for few job roles so this is
kind of interesting because sometimes
that's the center of the job role of a
data scientist and sometimes you just
have to be able to apply the programming
skills to it so you can get the answer
and let somebody else fine-tune your
machine learning techniques so it really
depends on what it is you're working
with and you can see here we have our
nearest neighbor set up where it groups
things together that look alike linear
regression you're drawing a line through
the data or curve so you can predict
what the next value is going to be and
then our nice decision tree which starts
splitting things up and says you know
it's sunny out yes let's go swimming no
it's raining we're not going to go
swimming yes john we might go swim it's
raining uh we'll stay indoors yes we'll
walk the dog no so you have a decision
tree that helps figure out how you get
to the end result so decision trees can
be pretty powerful and data
visualization data visualization is the
study and creation of visual
representation of data data
visualization uses algorithms
statistical graphs
plots information graphics and other
tools to communicate information clearly
and effectively this one you really have
to master as a data science this one you
really have to master as a data
scientist and there certainly are so
many different tools to use to visualize
it but when you're communicating a
picture is worth a thousand words when
you can put a picture up and and people
can look at it and go oh i see what's
going on that's worth a lot more than
saying yeah yeah this is good people
hear that and they just kind of go okay
it's good but why and you might say you
know okay these numbers make it look
good no people want something they can
see and there certainly is uh tabloo is
probably um one of the most popular ones
out there caleb view
power bi google data studio
in python there's
the pi kit and seaborn there's all kinds
of different options for visualization
you need to master
probably at least one or two of them so
you understand how they work and how
they're different
and then big data uh big data is a
massive amount of data which cannot be
stored and processed using traditional
methods big data has various benefits
like access to social data can enable
organizations to tune their business
strategies big data can improve customer
experience uh and so we're usually when
you say big data we're almost
always talking about hadoop and apache
spark it used to be hadoop as your file
system so that's how you store all your
data going across the nodes and there
certainly are other ways to store it
when we talk about hadoop we're usually
talking about at least 10 terabytes of
data when you're dealing with a hadoop
file structure and then you can use
spark where hadoop processes on and off
the hard drive so it's continually
reading and writing to the hard drive
spark is running in your ram and so when
you're doing a very intensive data
process you'll run a spark setup which
will spread those processes over
multiple computers and of course now the
two
combine and spark sits on top of hadoop
and you get your big data there's also
uh talent uh tabloo has its own server
set up there's splunk uh cassandra
you'll hear the cassandra database
pentaho i'm not even sure how to
pronounce that one it's not one i've
worked with certainly there's a lot of
options for big data the two big players
are of course hadoop and spark and even
your sql servers and mysql servers will
spread across five different servers and
you can be talking about big data across
five servers with those pulling them
into say apache spark to do your
high-end processing and then there's
non-technical skills probably the most
important one in data science and any of
our data analytics is intellectual
curiosity updating knowledge by reading
contents and relevant books on trends in
data science this is such there's so
much going on in this field and it's
exploding now that it's hard to keep
track of it all
so you really need to have that
curiosity
in the data science probably even more
so than a lot of other fields but i
would say in just today's world you need
intellectual curiosity because things
are changing so rapidly in today's world
the way that it's going with our
technology understanding how the problem
solved can impact the business business
acumen this is the one where a lot of
people kind of skip over you have to
have a little business sense if you
don't know how it's going to impact the
business you're going to be in trouble
why do they want to even pay you to be
there unless they know what's going on
and they're getting something in return
so this is your bottom line for your
paycheck is what's going on with the
business how is this going to impact the
business what are you doing for them and
communication skills companies look for
someone who can clearly and fluently
translate technical findings to a
non-technical team and i've talked about
this a bunch and i i can't even iterate
this enough if you are struggling with
your communication skills work on
building them they don't have to be you
know this isn't rocket science where you
have to know all these complicated terms
what you need to do is take the
complicated terms you know from data
science and reduce that down to
something that anybody can understand so
when you come up and you have uh you
know knn no one knows what k n is you
know unless they're in data science
nearest neighbor looking for things that
match together you got to explain that
to people who do not have the technical
skill in the data science arena and a
data science engineer needs to work with
everyone in the organization including
customers
so it's teamwork and that's always one
that throws people for a loop but
ultimately
you are analyzing your customers
information and so sometimes getting in
there and rolling up your sleeves if
you're let's say you're doing data
analysis for the sales team and you're
backing a sales team you might have to
get in there and go start making some
sales and making some phone calls and
see what it's like what is that team
doing when they call uh 15 people in
cold calls and then they do a follow up
two weeks later or three weeks later
which one works better two or three does
it work better to
call and send a fax call and send an
email you know when you should you show
up in person if it's that kind of
business so working with everybody
including the customer is very important
with the data and with this kind of
setup and non-technical skills for a
data scientist so let's take a look at
some of the roles a data science plays
in the job market let's drill in there
just a little bit here so when you have
a data scientist they're going to
perform predictive analysis and identify
trend and patterns that can help in
better decision making companies hiring
data scientists include apple adobe
google microsoft i would say that's even
expanding down to smaller companies you
know where you're talking about only 100
employees or something like that they're
starting to look at data scientists
because they need them to come in and be
a part of that team and the role is
understanding challenges of a system and
offer best solutions this should always
be the goal of a data scientist is to be
looking at the whole system and then
trying to find those patterns that are
going to best enhance the company or the
business or the whatever group maybe
you're working with the sales team or
the marketing team and you can see
languages um pretty important to have a
these are a number of different
languages and again it depends on your
own specialty there's all kinds of
different variations but we have r sas
python matlab sql hive pig spark you
should have at least a knowledge of all
of these maybe a python programmer never
used r download it go through the basics
so at least you know what's going on in
r versus python because r is pretty
powerful for doing a very quick display
and then of course a lot of things you
can do in python you have to be able to
do queries on database from time to time
create and modify algorithms which can
be used to reduce information from large
databases and usually this is one of
those things you do at the beginning you
set it up and then you kind of let it go
until it breaks and you have to go back
and fix it because your query isn't
working that certainly is a very common
thing to to have happen especially if
you're pulling data off the web and the
internet and then we have some more
companies that are hiring data analysis
like ibm dhl hp and so we're talking
about data analyst the role is
responsible for a variety of tasks such
as visualization optimization and
processing large amount of data and so
we're talking about a data analyst we're
also we're still talking a lot of the
same stuff but you'll see we've now
thrown in javascript and html in here
because that's pretty common with the
data analyst versus a data scientist and
you can see that sql is still pretty
solid and also
cc plus is pretty big so some companies
also use java so i'd put java in the
languages also on that for a data
analyst and then you have a data
architect ensuring that data engineers
have best tools and systems to work with
and so companies hiring a data architect
are like visa logitech coca-cola they
create blueprints for data management
with the best security measures i cannot
highlight that a data architect really
spends a lot of time with security
measures that's big how many hacks have
we seen in the last year on large
companies have lost data for millions of
their customers and how badly that
affects them and we're looking at
languages as you see that sql is pretty
solid across all of these under a data
architect xml that's one that a lot of
the data science jobs don't really you
don't see as much but you're going to
see more xml with a data architect
because there's a lot of xml files that
are that have your security measures
embedded in them and of course your hive
pig those are hadoop systems we're doing
some basic queries uh so instead of
doing the high end queries you now have
these simplified query systems built
into hadoop and spark so if you're using
big data you're going to want to know
your spark also and then we have our
data engineer the data engineer updates
the existing systems with better version
of the current technologies to improve
the efficiency of the databases think
admin here a lot of work tracking down
what version you're in of the different
programs what version of python are you
using what version of r are you using
what version of c plus plus so companies
hiring data engineers are like amazon
spotify facebook and they develop
construct test and maintain
architectures such as databases and
long-scale processing systems and here
we have our languages our sql our matlab
sas spss python java ruby c plus pearl
hive pig pretty much as a data engineer
at this level when you talk about admin
and updating all these databases need to
know all the different stuff that's
being used in the company and you're
testing out these structures to make
sure they work and so you really are
talking admin level uh kind of setup so
if you like spinning up an admin onto a
google cloud service or amazon cloud
service now you start to get an idea
what we're talking about because then
you can test it out up there and then
you can bring it back and update it to
the company statition creates new
methodologies for engineers to apply
and so when we look at this this is
where we're digging into the math of
which models to use what setups going to
work what kind of activation do we have
on our neural network these little
tweaks are going to make a big
difference and we look for methodologies
at a large level what kind of data are
we going to be looking at how we're
going to pull it together companies
hiring a data engineer or we're talking
about stations we're talking about
linkedin pepsico johnson and johnson and
of course these companies probably hire
a little bit of everything but they have
a lot of stations working for them and
so we look at the role extract and offer
valuable reports from the data clusters
through statistical theories and data
organization and the languages across
the board we're still seeing sql r
matlab sas spss data that's a new one
that i've seen in some of the other
roles python perl an older version and
that shouldn't be a surprise because a
lot of these companies probably built a
lot of the original packages on perl if
you go look at johnson and johnson and
of course your big data hive pig and
spark and then our database
administrator i mentioned the other one
is an admin they're more an admin level
for the software itself so administrator
is another level some of the tasks
involved are monitoring operating and
maintaining databases installation
configuration defining schemas training
users etc and this is interesting
training users because when you build a
database and you define all the
different
tables that are embedded in that
database and how they're all connected
the end users got to have that
information otherwise you're in trouble
when we have companies hiring data
engineers
of course tabloo twitter reddit
the role ensures that all the databases
are available to all relevant users
again a little security in there what
role do they have is allowed in and not
and is performing correctly and is being
kept safe and you better be ready to
spend some time in security because
again that's a very big thing nowadays
with the number of people who hack into
these databases and the languages are
going to be covered you have your c
sharp your java sql ruby on rails xml
again there's that xml because a lot of
your security is embedded in xml files
um python all the main players depending
on what the company's using data and
analytics manager so when you see
manager that means that somebody's going
to sit on top and organize the people
underneath so they're going to improve
the business process as an intermediary
between business and i t oh you better
have some really good communication
skills for this one some of the
companies hiring a data engineers are
coursera motorola slack and of course
these companies are hiring data
scientists in almost all of these fields
so the role oversees the data science
operations and assigns the duties to the
team according to skills and expertise
so that's a big one you better be able
to communicate what's going on help your
team members come out and be able to
communicate with other people so
sometimes your job is just being able to
get people to talk to each other the big
thing on that is is the whole
communication line going from one end to
the other because you can't communicate
all the information that everybody's
working on you have to pretty much come
in there and and help people communicate
what there's going on and also know what
they're up to without being
micromanagement because that will crash
a company if you micromanage everybody
but if you just let them do whatever
they want and they never talk to each
other that will also destroy the company
you gotta find that nice middle ground
and you really have to have an overview
of everything as a data and analytics
manager one of the things i mean you
know it's good to have at least one or
two solid bases of programming under
your belt for a data analytics manager
but you do need to have a very general
understanding of all the different tools
being used so that you know what's going
on with your different team members of
course
r sas java python matlab all of those
are big business analytics uh so this is
when you're talking about your bi or
your business intelligence and your
business analytics very important to
know this is kind of a subdivision of a
lot of the other stuff we've talked
about a lot of the other roles we've
looked at
so you possess specialized knowledge of
their business domain and apply that
knowledge and analysis specifically to
the operation of the business uh you
might be a specialist in banking you
might be a
genetics engineer so you know about
genetics you might just be general
business
helping the marketing so a lot of
companies hiring data engineer oracle
uber dell and the role act as a link
between the data engineers and the
management executives so it's very
similar to when you go back up here we
look at data analytics manager a
business and analytics is going to also
kind of fill that role in between but
they're more specialized in business and
you better know sql because we're
talking about a business specifically
most of the large companies are storing
their data that's up and running right
now that people are accessing to
purchase something on the home depot
website or the uber they're logging in
you better know your sql it's very
central to business analytics so let's
go ahead and take a look at the data
science engineer salary trends
and you'll see that our source is glass
door that's we're pulling a lot of our
data from average base pay is around 117
000 in the us the average salary and we
look at india we're talking about 950
000 a year and those are based on some
of them you need to go ahead and dig
deeper to find out what education level
versus entry level but those are pretty
solid base once you're in the industry
and once you created your career in
there we look at the job titles the most
common job title is data scientist
business intelligence manager is the
second greatest one
that's kind of good to note that's why
they put that up there as being its own
little subcategory of data science
managers a business intelligence manager
because it's such a high level of
jobs and then we looked at some of the
other ones data architect business
intelligence architect again your bi
your business intelligence gets its own
ranking because it's such a
high level you know it's a business
people want their businesses to make
money so that's who's hiring of course
data engineer business intelligence
developer business intelligence
consultant business intelligence analyst
business analyst and data analyst so you
can look at the data analysis being the
general one that doesn't go specific to
business so if you're looking just for
your career as far as getting a job
probably should have some kind of
business understanding in the background
and again you know your companies want
to make money so they're going to hire
someone who knows business if banking
business if you specialize in banking or
retail if you specialize in retail and
marketing and you can see here on the
data science salary trends and the
growth and data science job listings
it's continually going up it's um since
2014 to 2012 we've gone from 400 to 600
that's a pretty big increase so you know
a huge growth in this market it's one of
the biggest growing markets right now
for jobs and careers so let's go ahead
and take a look at uh building a resume
always exciting putting ourselves out
selling ourselves and if you looked at
some of our other videos dealing with
resumes you'll see a trend here
the top part is so different than what
resumes were in the 90s
in the 2000 to 2010 they've evolved
they've really evolved it used to be
2000 2010 maybe linkedin
maybe one other reference now you're
going to see that we want those
references this is a sales tactic which
now has come into resumes used to be
that if you're a real estate agent every
real estate agent i knew i used to deal
with real estate software for the real
estate industry back in the 90s every
real estate agent won their picture on
their business card they wanted them
their picture if they could put on their
contracts they want people to see the
face so that's really a big change is to
make sure that it stands out you stand
out here's a picture of somebody so
you're more than just a couple letters
and a name but of course you need your
contact information should always be at
the top uh you have your summary what
are you focusing on be a little careful
with your summary because if you have
everything in your summary and then they
scroll down to experience and education
and skills they're going to stop the
second you repeat yourself in your
resume that usually means to the reader
hey this person doesn't have anything
more to offer me i'm done so be a little
careful how you word your summary most
companies appreciate when you come in
here and you've adjusted this summary to
be both about you and how you're going
to serve that company so it's worth
researching that company to find out how
those connect and put that in the
summary take some time to do that that's
actually a pretty big deal and the
references are huge also especially in
data science when you're talking about
any of the programming or data science
data analytics having a place to go
where they can look it up and scroll
down and see different things you're
doing whether it's linkedin in this case
which is the business profile most
commonly used github where you have
stuff published facebook i'm always
hesitant because that tends to push more
towards uh social media type jobs and
other jobs but certainly there's people
who have facebook
who do marketing and stuff like that but
these links having these basic links
here is important uh people are starting
to look for that for some other uh setup
maybe you have a personal website this
is a good place to put that so that they
now have a multitude of links that go
back to you and highlight who you are
and then the next part or next four
parts so for the next four parts we have
a combination of experience education
skills certifications and you can see
they're organized if you have
you know a lot of people like to see
what kind of degree you have they want
to know where it came from and if you
just got out of college you're going to
put education at the top and then maybe
you'll put skills after that and then
your experience at the bottom if you've
been in the field for years
you know my degree just to give you my
age it goes back to the nine early 90s
so i usually put education at the very
bottom and then because a lot of the
stuff i'm trying to sell myself on right
now is my skills i actually put that at
the top and i'll put my education my
certifications at the bottom my skills
and then my experience is since it's a
huge part of my resume goes next you can
organize these in whatever order you
want that's going to work best for you
and sell you so remember you're selling
yourself this is your image probably
don't wear
an old tie-dye t-shirt with holes in it
you know something nice because it is
professional and of course your summary
your and then what do you have to offer
the company and again when i put out
resumes and i haven't done a resume in a
while you go in there and you can take
this and reorganize this so if the
company's looking for something specific
you might put the experiences specific
to that company you might even take
experience if you have like a long job
history like i do i've gone into a lot
of different things you might leave out
those companies or those experiences
that had nothing to do with data science
because it just becomes overwhelming
resume should only take about 30 seconds
to glance over maybe a minute tops
because after that point you've lost the
person's interest and if they want to
dig deeper they now have links they have
your website they have linkedin and they
can now take this and they come back to
it and they go okay let's look at this
person a little closer so quick overview
this is your sell sheet selling you to
the company so always tie it to the
company so that you have that what am i
going to give this company what are they
going to get from me we've covered a lot
on data science engineer in general and
we've gone over a basic resume uh
remember keep it simple short and direct
that is so important with that resume
thank you all for watching this video
tutorial on data science with r i hope
you liked it if you have any questions
then please feel free to put them in the
comments section our team will help you
solve your queries thanks again stay
tuned for more from simply learn
hi there if you like this video
subscribe to the simply learn youtube
channel and click here to watch similar
videos turn it up and get certified
click here