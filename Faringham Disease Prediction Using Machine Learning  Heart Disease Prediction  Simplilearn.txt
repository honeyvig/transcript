foreign
despite years of research more people
die from heart disease than any of the
combined deaths for cancer
and so fortunately we live in a time
when data science is being used to help
us model heart disease and to be able to
predict and to prevent and it is saving
lives and in this video we are going to
learn how to perform those very same
machine learning type activity my name
is Dr you want to listen and this is a
new machine learning series conducted by
simply learn and if you want to be
notified when we upload other videos
please like And subscribe alright so
machine learning and
cardiovascular disease now
cardiovascular disease or heart disease
is very common and so there have been
several projects that
um you know whether from private
organizations or the government where
data has been collected that's lots and
lots of data Health Data on thousands
maybe even millions of patients and we
can use said data to model heart disease
so for the video project what we're
going to do we're going to share we're
going to combine some slides to explain
the concepts and then we're actually
going to go to the Jupiter notebook
where we will use Python Programming to
perform these data science data analysis
activities we will do all of the steps
and eventually we will get to building a
few models including a decision tree and
a logistic regression and maybe even as
a support Vector machine but we're going
to take each step and it's going to be
divided in sections so that way you can
come back to the sections that you may
want to review
so here I have my lovely acronym that is
supposed to help you remember the steps
involved in machine learning and any
machine learning project you need to
have an acquisition of data so acquire
you need to filter set data then you
transform the data and you explore the
data this is where you get an
understanding of your data and finally
you're going to split the data there's
going to be two sets of splitting and
then the data will be ready to be put
into the algorithms for modeling so the
first step is the data acquisition of
course you can't do data science without
data so you need data but you don't just
need any data you need quality data and
you need data that is specific to the
project to the phenomenon the question
the business problem at hand and so
there are several ways that you can
obtain that data but it is important
that the data is accurate that it is
quality data because the quality of your
model will be a reflection of the
quality of your data so you can scrape
it you can extract it you can query it
you can collect it however you get it
you just want to make sure that the data
that you get is the data that you need
for your project
so once you have acquired your data then
you're going to filter your data because
data always comes messy it's uh from
real people in real lives and so there's
always going to be some outliers
there'll be noise there'll be errors
there'll be duplications
um there'll be missing numbers missing
values all of this you have to look for
in your data set before you start doing
any kind of data analysis and you want
to eradicate get rid of those those um
issues because you want to make sure
that your data is clean that it is
organized and that it is accurate so
those are the things that you're you're
doing as you process the data so after
you have filtered your data now this is
where you're going to do the
transformation right and so you are
going to either be transforming the date
from text to a python or a panda's date
time or you may be transforming text
values into numbers and integers because
that's what you have will need for the
algorithm so you may need to create some
dummy variables as such or you may just
need to be transforming some
values continuous values to along
because it's not normally distributed so
these are the things there it's a host
of Transformations that you can perform
on your data set but you have to know
what is in the data and what it is
you're going to require offset data and
that's where this uh these
transformation steps come in sometimes
there are missing values that you need
to impute but you will have to look at
the data now there's some basic steps
that you take and there's you know when
we do the programming you'll see how we
can identify some of the these issues
and and rectify them
so once we have transformed the data in
terms of making sure that the format is
appropriate for our data science
activities then we do the exploration
and this is my favorite part I love data
exploration this is where you get to get
your hands dirty and get an
understanding of the data and this is
where all of the data inside comes from
right this is where you find out what
are the trends and the patterns and the
anomalies that are in the data this is
the Insight that informs the data
decision making as it were and so you
want to make sure you have a very good
understanding of your data you're not
necessarily going to be sharing all of
this with the shareholders this is for
you so that you can be better at your
data science machine learning activity
right so you want to find out where the
correlations are you want to develop and
test some hypotheses
um and of course you want to generate
some visualizations the visualizations
are extremely important when we start
doing the coding I'll show you how when
you visualize your data there are things
that you can see only when you generate
a chart or a graph right so looking at
Raw numbers are in a table you will miss
quite a bit if you don't
um you know create some charts and
graphs so once we have a good
understanding of our data and so we
think that we are ready so sometimes
with the exploration you may need to go
back to the transformation or maybe even
back to the filtering stage you know
there is some type of a cyclic uh
process here with these three but
finally when you do get to the the stage
of modeling the data this is where we do
the splitting and there are two splits
you split
X's from the Y's or the variables or the
predictors from the Target and then the
second split is where you split it into
a training test and then you're going to
use the train to model the data and then
use the tests to evaluate the
performance of your model
so let's get to some coding now so I
like to think of machine learning as a
process like baking a cake or making a
pie or making pizza or something so the
first stage of course is always
assembling all of the ingredients and
then all of the utensils that you would
need and then the second stage is doing
the filtering of the ingredients so
you're getting all all the lumps and
everything you know the flour is fine
and you have everything it's accurate
and it's organized and then you're
transforming some of uh the ingredients
so for example like when you have to
melt the butter you're transforming it
from a solid to a liquid simulator you
may need to transform your data from
text to integers and then once you have
all of that now you're going to mix
everything together you're going to get
your hands dirty and that's where the
data exploration comes in and finally
when the flower is all needed now you're
going to divide it into the individual
you know balls or rolls or pies or
whatever and that's what the I like to
think of as the uh you know that's
splitting into
um X and Y and then train and test so
let's gather all our ingredients
ingredients together and this is where
we're going to move into our notebook
and I have decided that I'm going to use
the Google collab for a number of
reasons but you can do this you know in
your own Jupiter notebook directly on
your
um your system or you can do it through
Anaconda whichever way you choose but
Jupiter uh sorry
Google collab it's free you know and
I'll put a link to this actual notebook
in the comment sections to this video
all right so this is my Jupiter notebook
in Google collab and here you see I have
the first tab
um I'm importing the module so this is
like assembling all of the tools that
I'm going to need so I'm going to have
to have pandas you know for the data
frame numpies for all my numerical
python work and then I have matplotlib
for all of my basic charts and graphs
then I also have a Seaborn if I want to
get fancy and I also have SK learn which
is what I'm going to use eventually for
the modeling and then I have sci-fi for
some any type of statistical analysis so
I have already loaded those and I've
already I'm loading in this pie reads
that data and that is because my data
set is in the form of a Sav file but you
will probably get that yours in CSV or
Excel and so instead of saying read
underscore SSP like I do you would
simply say read underscore CSV or
whatever format your your data set is in
so here I'm loading in the data set and
I'm giving it the name I'm giving I'm
using this uh function that is part of
the pandas module read underscore SPSS
and so when I do load in the data set
i'm assigning it to the variable name DF
underscore FHS and you could name it
whatever you would like I try to give it
an informative name
um because sometimes I will save this as
the you know the original version of my
data frame and then I'll create another
working data frame such that if I make
any changes that
um I don't want to keep I can go back to
the original data frame so now that's
this is the data acquisition State now
this is a very quick short data
acquisition state
um there are times when this this is
going to be a much longer process
because you may have to scrape the data
set off of the internet or you may have
to get it from a pipeline or you may
have to get uh survey data from Excel
and then you have to do you know a whole
host of things
um to be able to get it in a form such
as you can then load it into your your
notebook and use Python Programming but
we'll start with the simple there are
lots of C uh CSV files based simple
files that you can use to get started
and so you would load that into your
notebook so that would be the first step
and now we want to look in line to make
sure everything is as it says and as is
expected and so the first thing you
usually do when you load your data set
is to take a look at said data set right
and so we use the dot head method right
and so if you have the name of your data
frame whatever it is dot head
um with that you call that you can see
the first five rows of your data frame
and so here we see our data frame we can
see all of the columns well here it's um
truncated but we'll we'll print out an
actual list of the columns but this is
just giving you an idea of the structure
of our data frame it is you know it
looks like an Excel table there are rows
and then there are columns right so we
first have age then we have LDL
cholesterol we have smoking fasting
blood sugar
HDL systemic blood sugar total and so on
and so on and so now we have an idea of
what is in the data frame and so we see
that we have 34 columns in all 34
columns meaning 34 features and here
we're looking only at five rows but
that's not things that all that the data
frame contains if we want to know how
large our data set is this is where we
would use and
we would use our
data dot we would use the name of our
data data frame
so we use the dot head method and we see
the first five rows of our data set
along with
um a truncated version of The Columns
and if we wanted to get the actual
dimensions of our data set that is where
we would use the shape attribute so I
click here that opens up a new cell and
I can type in here so I want to get the
dimensions on I'm writing that as some
kind of
message to myself or my fellow data
scientists should they use this notebook
and I use the name of the data frame
which is DF does FHS for data frame
Framing and heart studies data set and
then I do dot shape
and that gives me the size of my data
set so the first number is going to tell
you how many rows there are four
thousand four thousand five hundred and
seventy eight rows and 34 columns and we
knew there were 34 columns here from uh
our DOT head but now we know that there
are 4 500 rows in total so that gives us
an idea of the size of our data set now
if we want to know more uh get more
information
about our data set about these features
then we use the dot info and the dot
info what that does it gives you a
printout of all of the column names
along with a count of how many actual
non-null values so here it says that
there are 45
178 non null values in the age column so
that means there are none missing right
and they have all of the floats data
type but if we look at the LDL
cholesterol we see that it only has 4
511
non-null so there are missing values
here
um similarly with the smoking there's
some missing but just about three so not
a lot and we can go down the list and we
see all of the features and we can see
here that most of them are floats with
the exception of diabetic status and the
10-year update so there are mainly
floats in here but there are still some
categorical type variables right so the
floats these are the the decimal points
type data type
numbers and the categories are
classifications and so we see what how
what a data set is made up and so here
it gives us a summary and there are nine
categorical variables and 25 uh float or
these numerical type variables in the
data set so now we we have an idea of
what our data set is look looks like one
of the other things that I always do
with my data set as far as getting a
preliminary look at what is contained is
to get some basic descriptive statistics
on the data set and fortunately python
has a very simple code for that so for
basic descriptive that's where I would
use the
dot this scribe method
so if I do not describe
and I printed that out so here it's
giving me the count
the mean or the average the standard
deviation the minimum value the 25th
percentile 50th percentile the 75th
percentile as well as the maximum value
for each of the columns but you notice
here that it's just 25 columns in this
uh Matrix for the dot described right
and if we go back to our data set when
we did that info we we remember that uh
nine of the columns were categorical and
60 sorry 25 of them were numerical and
so when we applied the dot to describe
it only but well I should say by default
it would only give the basic
descriptives of the numerical values and
then I like to transpose that so if I do
the dot transpose on that and I it would
just transpose that table so now you can
kind of compare the means of each of the
features
um it can give you an idea of whether
there are some extraneous values and I
noticed that all of my
values are in the scientific notation so
let's repair that
so I ran this code here PD dot set
underscore option and the first
parameter is the display format and I'm
telling python that instead of the
scientific notion notation and all of
those zeros and you know the tens of the
power and all that just give me a
maximum of three numbers after the
decimal point so that's what this code
does here and so now if I can look at my
table now I can see that you know I get
a better idea the mean or the average
age of the sample in data set is 40 then
the minimum is the youngest person is 19
and the oldest person is
83 and then if I look at the median
which is the 50th percentile that is 41.
so not uh too much skew going on there
the mean and the mode are the median are
almost the same but if we look at some
of the other values for example LDL
cholesterol the mean here is 111.
but uh
well we have a Max of 281 and a Min of
22. yeah so that's quite a range and
that's probably contributing to a larger
standard deviation but again here the
median is not too far from
the uh the moan the mean sorry but you
know so just uh this this gives you some
kind of a very brief view of what's
happening so it may or may not allow you
to see if there are any crazy values but
of course you want to go beyond just
this chart and and do your due diligence
as far as the distribution of these
values are concerned but so this would
be the basic steps that you would
perform when you load your data set in
you would first do the dot head to get
the first five rows you would do that
shape to get the dimensions of the data
set then you do that info to see what
the data types are for each of the
variables and here you know we're
predicting heart disease and so we want
that to you know that would be the
target variable so we want to know what
data type that is and we're going to
look more closely but so you want to
just kind of do those first and then
um like I did with
the dot describe to get an idea of the
descriptive statistics so that was our
uh look at the data set
um the preliminary look at it as it were
so we have acquired our data set and we
have done a preliminary uh scan of the
data set we know the size of the data
set and we know what types of values are
in there so now we're going to do some
filtering of the data set right this is
where we're going to be looking for
outliers and noise and you know any kind
of extraneous type of values that should
not be there so let's move back to our
notebook here and we're going to go back
to the top where I was so we did our
data acquisition
steps and now we're going to do our
filtering and so the first thing that we
want to do with the filtering is when I
did look at the data set so if we go
back to the first five rows we notice
that there's this PID number here and in
in the when we did info this PID is uh
showing up as a float when it shouldn't
be it's just an ID number for you know
the hospital kind of thing or whatever
system it's used and those types of
numbers are usually of no value to our
modeling activity and so those are kind
those are the invalid type of data that
we want to get rid of so I'm going to
remove that from my data frame and so
I'm using the dot drop method and here I
put the column in question and I let the
function know that this column is you
know it is a column so therefore it will
be found in the x-axis hence X is equal
to y
and when I do that I'm removing it from
the data frame and reassigning that uh
filtered data frame back to my original
now I could create a working data frame
here at this point but I don't think I
will ever want to you know get that PID
back again if I do need it I can re-uh
you know bring the data back in like I
did originally but once I've done that
and if I were to then
do a a look at the dimensions for my
data frame now I do that shape again
I you will notice that it has gone from
33 34 to 33 columns because I removed
one now the next thing that you want to
do is you want to look for missing
values right this is important missing
values can cause big problems for your
analysis so we want to find the missing
values and we want to deal with the
missing values so we use the is null
method to find the missing values and
what that is going to do is going to
look through the data frame and wherever
there is a value it's going to say true
wherever there is well whatever there is
is a value it's going to be false that
means it is not null it's kind of a
counter logic and when there is a
missing value that's when it's going to
stay true
so this is what it looks like so all
these falses mean that there are values
there and so we would get that but what
we really want to know is the total of
all of these uh missing values and so
that's where we do the is null dot sum
right and so they just commented that
out in the meantime and if I did that
here so that is going to tell me for
each of the columns in my data set how
many missing values and we had kind of
seen that the LDL had some issues before
smoking only had three values missing
then total cholesterol five and diabetes
and so on and so the extra code that I'm
adding here is now going to take these
numbers and return a percentage of or a
proportion of uh what is missing so here
we see that for the they are cholesterol
that
1.46 percent of the values are missing
similarly for HDL 0.153 and so on and so
on and so that's what that does now when
it comes to missing values there are a
number of ways that you can deal with
the missing values and they first depend
on the quantity of missing the
proportion of missing now normally if
it's below five or even sometimes below
10 depending on the size of your data
set you can simply delete the rows that
contain the missing values but one thing
you must be pay particular attention to
is when removing missing values you do
not want to create any unwanted bias in
your data set so for example if we had
more cholesterol values missing say 10
20 percent
and we were to determine that
you know a large proportion of those
were women if we deleted that then we've
introduced bias in our data set right
and so we want to make sure when we are
treating our missingness
we understand what type of missingness
it is and there are types to missingness
right because you can have missing uh
you can have different types of missing
as far as these data frames are
concerned and so you have different ways
of dealing with the missing values
because it will have different effects
on your data sets depending on what
these missing values are correlated with
and so there are m n a r and m
hopefully this should help bring it up
to me if I see the type that I'm looking
for and I guess it was outlined here
missing not at random is one type of
value and then you have missing
completely at random and then you have
missing at random so missing at random
is probably the most benign of the type
of missingness so there's no relations
the correlation to the missingness then
missing not at random meaning that it
there is some pattern to the missingness
um and then missing completely at random
so this you would have no consequences
uh for deleting the missing not at
random you don't want to delete that you
want to treat that missing at random
meaning that there is a relationship
some correlation with other variables
not necessarily the target variable
but we have such a low proportion of
missing values that it may not be
um you know cause any issues as far as
dealing with that is concerned so what
we have I want to introduce you to this
missing go no module is just one of my
favorites because it helps kids
visualization to your missingness right
and of course we don't have a lot of
missingness so there's not a lot to see
but see this column here LDL cholesterol
and you see that that bar is not as high
as the others similarly the uh fasting
blood sugar so those are where they were
missing if they were more values missing
there would be more variation in terms
of the height of the bar you can also
use the Matrix the dot matrix method
from the missing though and it would
give you a matrix and it show you here
LDL is missing values in these four
places and so on and you know this looks
completely filled for the most part they
when you have a data set with a lot of
missing you're going to see a lot of
white spaces in there and then of course
you can do the heat map uh for missing
values and this will show you any type
of you know the correlation between the
missingness and and others you know and
so here we see 0.3 those are very low
correlation but total cholesterol of
course is correlated with
high cholesterol because total
cholesterol is a sum of low cholesterol
Plus
uh so LDL plus HDL so therefore
definitely correlated but just giving
you some ideas some tools with which you
can play when you're doing your
exploratory data analysis
so back to dealing with missing values
so we had uh two or three columns that
had more than others missing right so
most of them were below 0.3 percent but
there were some that were around one
point something and so what I'm doing
here is I'm using the fill any method
and what that does is it allows you to
fill in the missing values with a
defined value and here we are using the
average of that particular column to
fill in for all of those so we're taking
the mean LDL cholesterol and we're
filling that in into the places where
LDL cholesterol is missing and then
we're reassigning this new field in
column to our data set here and we're
doing the same thing with the fasting
blood sugar we're using fill n a and
here we're defining the mean of that
column and then we're filling that in
into the column and then that is going
to replace the column that is in the
data set so that's what's happening here
so if I ran that we've now filled that
in and if I do my missing check again so
now we see LDL cholesterol which was
missing
um values when we did it up here right
we saw it was missing here and then the
other was fasting blood sugar and so now
now that we have done that fill and a
method we now see that LDL cholesterol
as well as fasting blood sugar they no
longer are missing any values and so for
the rest of the columns because the
proportion of missingness is small we
usually use a threshold of less than
five percent I just I could have deleted
these um
with not much of a consequence but just
to give you an example of how you would
do that and then once I done that now I
can for as far as the other columns are
concerned I'm just going to drop the
rows that have missing values and that's
what drop n a that's what the drop n a
method does when you apply drop n a
method to your data set it's going to
look at each of the rows and if there is
any value missing on any of the The
Columns for that row it will simply
remove that row and so if we um we apply
that now
and we did a check of my data frame I
don't know why I have all these extra
um
let's see I don't oh that one I do need
if we apply that now then we see we have
dropped all the missing values and there
are no more missing values all of the
columns are completely filled and
accounted for and so now we've taken
care of our missing values now when it
comes to filtering we look for missing
values we look for duplicates and we
look for outliers
so there is a very simple code that we
use to look for duplicates it's called
dot duplicated
and so we use that to check if there's
just like we do the check for missing
numbers we use that duplicated to check
for our if there any of the rows are
duplicated now of course by default
that's what it checks for you can check
for duplication as far as the columns
are concerned but with this data set
only 34 columns
um very likely there will be
duplications you know when we when I
work with larger tables that have
hundreds of columns then for sure we
want to make sure and check that none of
those are repeated so if we did the
double dot duplicated it would return
false if that row is completely unique
if there were any duplicated rows then
it would say true and then when when we
applied the dot sum then we would get a
summation of the number of rows that did
have a uh that are are duplicates and if
they were duplicates then this would be
the code that we would use to remove
any duplication
right so we would use the dot drop on
the score duplicates
and if we don't put anything in the
bracket it'll just remove all of the
duplicated rows now if we only want to
um measure or to to take into account
duplication in a particular
column or a particular subset of the
data frame then we can Define it here so
let's say we only want to look for
duplication as far as well we got rid of
the the ID number but let's say we
wanted to see if there were any IDs that
were duplicated in the data set that's
how we do it so we'll use the dot
duplicate dot drop underscore duplicate
and then we would put the PID the name
of the column in there
but we do not have any duplicates in
this data set so we really don't need to
I just included this as an example for
you and so that's what we would do when
you want to
um find any duplication in the data set
so another important part of the data
filtering process is uh making sure that
the formatting is correct and the data
types are correct now if we remember
when we did the info method on our data
frame we saw that oh and I already ran
the code so it's showing up as a date
time but originally if it's still saved
here when we did do that the date time
this was showing up as a float
and so that is a time of value and so we
need to do some data conversion here and
I don't see it it's probably in the
middle here somewhere so this was
showing up as a float and it's not to be
supposed to be afloat and these are some
of the things that you do when you look
at your data set and so because it's not
supposed to be a float I ran this code
here the
pd.2 date time and I use the date column
to convert it to a pandas date time
object here and reassign it back to the
original column and so when I ran the
info now this feature is showing up as
it should as a date time object just
something to show now moving on to the
exploratory data analysis right this is
where all the fun happens this is where
you get to cut your hands dirty like I
say and really get to understand this
part of the the process why I love this
because it's statistics and I'm partial
to statistics so one of the first things
you want to do is to look at the
correlation
between the the features between the
columns and so because for some modeling
activity
um
you don't want there to be any High
correlation but even if that so you want
to know what kinds of patterns they are
and if there are variables that are
highly correlated they might be
measuring the same thing and then you
have to make a decision about which ones
you want to get rid of and which ones
you want to keep and so if we apply the
dot core method
um this is going to give us a Pearson
correlation uh between each pair of
variable
and this is what it looks like but very
rarely do we look at it in the Raw
numbers we generally always
um put it inside of a heat map because
you know the colors the pictures are
much more uh cognitively light and easy
to understand and so what this is
showing us here is that all of the light
colored ones maybe I move my head out of
the way
all of the light colors lighter the
color the stronger the positive
correlation so of course all of the
features are strongly correlated with
themselves hence these light colors
going across the diagonal right so these
are all ones so
um
fasting blood sugar is of course highly
correlated with testing blood sugar so
you want to look outside of the diagonal
to see what the relationships are now
the lighter the color the stronger the
positive relationship and then the
darker color is for negative
relationships so zero is around here and
then dark colors I mean the the the
lowest it gets I am thinking it's about
minus 0.3 gosh now I can't do that over
I'm startled by my husband uh let me I
feel like I should reduce the screen
somewhat just zoom out a little bit for
at least
for this uh correlation this heat map
here because we can't really see the
whole thing
in that view so here we go so now we can
get a better idea right so we have all
of the features
um both in the X and the y-axis and all
of the light colors are where the
stronghold and positive correlations are
and then the darker colors indicate the
negative correlations so
HDL cholesterol is negatively correlated
with almost everything it's negatively
correlated with fasting blood sugar it's
negatively correlated with BMI it's
negatively correlated with
um so the two rows for fasting blood
sugar here so what you know this is of
course what is this saying um the
features that are strongly correlated
with the outcome which is heart disease
now it's not included here in this
correlation Matrix because it is a
categorical variable and this is a
Pearson correlation which is measuring
linear correlation between continuous
variables
um so that's what happens sex gender is
included because it's just one and two
so that's okay
and so what we're seeing here are
correlations linear correlations between
the uh floats the you know those
numerical values that uh for the most
part continuous and so this is how we
would kind of get some idea of what's
happening and so HDL cholesterol is
associated this is known as the healthy
cholesterol and so that would explain
why to some extent it's negatively
correlated with all of the others
features that are Pointers towards
um
heart disease or or ill health or not as
healthy kind of thing so HDL is a marker
of good health and so
um we would see that oh we do have the
heart disease this is the target here
and yeah so heart disease now if we
notice here now this is a binary
uh feature we're gonna delve into that
whole feature very soon but this is
binary so
it is showing not too much Happening by
way of correlation with any of the other
features but again this is only
measuring linear correlation so you have
to be careful not because it's not
linear correlated doesn't mean that
there is no relationship it just means
that the relationship is not linear and
there are other types of relationships
um so we look at um
of course total cholesterol is highly
correlated with LDL cholesterol because
total cholesterol is the sum of HDL
cholesterol and LDL cholesterol so not
much information there
um fasting blood sugar is highly
correlated so I'm trying to see you know
what insights we can get from this chart
this heat map necessarily there's not
much but one thing I can point out is
that there are these interaction
features here which are created as a
combination of other features right so
we have glucose with blood pressure and
diabetes with HDL and diabetes with
systolic blood pressure and so on so
these are interactive interaction
variables and we notice that you know
the interaction variables are you know
there might be something going on here
is a pattern it's the lighter colors
um you know indicating a stronger
correlation between the variable
as opposed to
um you know the left hand side which
it's it's tending to be darker so we see
something happening here so total
cholesterol is
negatively correlated to some extent
with the diabetes HDL and the diabetes
SBC or systolic blood pressure
variable but yeah so just a kind of give
you some again an idea of how to
interpret some of these
findings nothing
stands out in particular so what I would
be interested in mostly here at this
time would be the target variable but
again not much by way of
linear correlation
so
um
that's that if we can generate a table
of the correlation
with each of the variable and just the
Target and so what I did here I used the
dot core method on the data frame and
then I use the square brackets to select
only the column with the the column for
the outcome variable and I then got the
absolute values because some of the
correlations were negative and then I
sorted so doing that now I can see and
maybe if I sorted and I said ascending
equals false in this case then we would
get the reverse order and so we see that
as far as the linear correlation is
concerned a just Z strongest correlation
with the outcome or
heart disease and then there is a
fasting blood sugar systolic blood
pressure
interaction variable then there's
glucose and the blood pressure
interaction variable so we get some idea
of the relationship and here I I
generated a graphical representation of
uh that actually I thought it was just
four but just just another code to look
at the heat map but we can do the
sns.heat map here and see how that shows
up for us it's for as just that column
is concerned
yes so I did a heat map of the
correlation Matrix and I selected only
the column for the Target and that's
what that looks like it did not sort it
so
Target is in the middle kind of thing
but again age is at the top with LDL a
cholesterol and smoking well smoking
these are negative so this is not uh
well that's
that code but it's way you can generate
you can look at just the the variable in
you know the
um outcome this is just another
correlation Matrix heat map we're using
a different scale for the coloring and
so it allows you maybe to see you know
and and it also includes the anode
equals true so that you can get the
actual values for the correlation
coefficient
all right and so that is how we would do
that so let's look at some uh see if we
can find some outliers
so we're looking now for looking for
outliers and because we know that our
data set is made up of
two or three types of data types now we
have numerical values as floats and we
have our categorical values as objects
and then we have the date time so we can
create a subset of the data frame that
contains only the floats or only the
numerical values using the dot select
underscore D types and so here I'm doing
that and then I put inside of that
function the data type that I want
included into my new subset and so if I
did an info on that I would see that I
now have this new subset 23 columns and
they are all floats
so now that we have that we can use that
to generate some plots to look for our
outliers and so that's what this code is
doing here we are going to generate a
histogram from that this new column this
new data subset that we have here and
I'm going to get them as subplots and
we'll see what that looks like
so
we have our histogram here and we
so in second four outliers I noticed
that there was an additional column
that was an ID column and at least it
was an ID type column which is
interesting but
I went ahead and I removed that column
from the data frame
and then I was able to get a better
um you know column numerical column so
you see now it's no longer containing
the PID or the ID type
and so here we have the the numerical
column FSH underscore num and now I also
and I was curious about these fasting
blood sugar and then fasting blood sugar
underscore one and then fasting blood
sugar underscore
BG yeah so I was wondering are these all
the same
so I did a correlation between the
fasting underscore bg1 and the fast
underscore BG underscore one and sure
enough it is uh 100 correlated so I did
remove one of those
and just to confirm the shape of the
data for in the subset and so now I'm
going to use this subset of the data
frame
to generate a series of histograms so
for each feature I'm going to generate a
histogram it's going to have bins of 30
there will be no grid and the entire
data the entire visualization will be a
30 by 20 but there will be three rows
and seven columns
so we can get an idea of how these
values are distributed
so here we have age and LDL and HDL and
you know these are the ones that are
normally distributed at least
um to some extent it's not perfectly
normal
but then if we look at smoking and
fasting blood sugar and heart disease of
course the target
significant imbalance here gender these
are all uh actual
categorical variables that are labeled
with numerical values and so that is
something that you want to pay attention
to right now the interesting thing is
for the exploratory data analysis we
need to treat it treat them as
categorical but then when we go to build
our model we need to treat them we need
to have do some ways some masking them
such that they are represented by
numbers because the algorithms the
machine learning algorithms do not
recognize categories
but so we generated this um series of
histograms so it gives us a very good
idea of how the values are distributed
in our data frame as far as the outliers
are concerns there's skewness here's
cuteness here definite skewness going on
here with this with this variable very
much cuteness going on here for total
cholesterol the mean is 200 somebody is
out here in the 650 plus category
so um yeah so
uh these are all skewed to some extent
but
um what where I think there's an outlier
it would more likely be with the this
total cholesterol if if their Val even
though the range is wide but their
values you know going all the way
between and then it's not an outlier
necessarily that's just the distribution
of the variable but with the uh with the
HDL cholesterol now that one is an
interesting thing to me and so I would
go ahead and either do a separate
histogram for that but here I have the
code for the um for generating
box plots for each of those
distributions so let's see what they
look like and we can then get a better
idea
for each variable
and again that's for these that those
that are labeled
floats I.E continuous
or or numerical I should say
so these are
interesting these are the box plots for
each of the variables so there's age and
LDL there's smoking of course that's a
binary uh fasting blood sugar definitely
categorical
and then we have H oh I think I see they
probably use the continuous continuous
variable and created another variable a
categorical variable from that
um so yeah so the one that we were
saying had a lot of skew that was a
total cholesterol so let me see what the
box plot for that looks like so that
yeah that's classic outlier going on
over there
this is the heart disease so it's it's a
binary
and
um
what else are we looking at anybody
looking out of the ordinary kind of
thing but that's how you would look at
the distribution to see if there are any
outliers and then you know subsequently
deal with that
so if you wanted to get a box plot where
all of the columns are the same plot
that's easy you don't need the for Loop
per se you would just do the data frame
dot box plot and that will give you a
box plot the challenge with that is
the variables are on different scales so
the ones that are on the larger scale
are going to dominate and you can't
really see anything if your variables
are on the same scale then that should
be enough but of course usually they
aren't
alternatively there's the facet grid
that's another way of generating these
subplots and so that's SNS you would use
your data frame and what that does it
gives you a nice
bivariate analysis so here I'm
generating Scatter Plots
of you know a pairs of variable right so
I want to see fasting blood sugar
against cholesterol but with the facet
grid what it's going to do it is you can
define a column so it's going to
separate
um each plot by the categories of
whatever column you specify here and
then you can include a hue so that when
you generate the Scatter Plots you have
the X and the Y as defined here and then
for each category in the age group so we
have 26 to 40 here 41 to 55 56 to 70
less than 25 more than 70. so you have
each chart representing one of these
and then you also have the Hue uh to
represent the distribution of heart
disease so you know and this can be very
interesting now mind you this is the
small data set so some of the categories
of sparse so less than 25 and over 70.
not a lot but if you can see even though
there's not a lot of people over 70
the proportion of them that did have
heart disease which is the orange
markers it's much higher for any than
any other category and I mean that makes
sense they're the older ones and is
similarly by contrast for the youngest
group there's a very low proportion of
them that would have had uh some type of
cardiovascular
um
you know illness and yeah so that can
give you and you can change these out
into whatever uh feature you want to
look at keeping the heart disease
because that's our Target and yeah and
just kind of change up the the how you
want this broken down so you need a
categorical variable here in the column
section and then you need two numerical
or continuous variables when you
actually
um you know generate the scatter plot
but that that's one of the nice things
about this fasted grid
um and here finally if you want to see
box plots of the variables with
um you know doing a bivariate type of
analysis again so you're looking at the
age groups like we did here and so
you're generating
a a box plot for the total cholesterol
measures for each of those age grouping
right and we see for the youngest
they're you know they have the widest
range oh wow and this is where our
outlier is so see why it's important to
really dig into your data because here
you have someone in the the youngest age
group having the highest total
cholesterol value now again now we have
to go further to that because it's that
total cholesterol
um High because
of the HDL or the LDL now that is
important to know because HDL is a
healthy cholesterol whereas LDL
is not so much so we want to see
what is the contribution
of his
total cholesterol so I'm going to switch
this out instead of the HDL I'm going to
have
instead of total cholesterol I'm going
to use HDL and see what happens
very interesting because now
instead of this person in that young age
group having the high cholesterol is
someone else is standing out here
so what that is telling me is that this
person who had that high cholesterol it
was not
HDL
so you know that is a very
um something that should be noted and
now this person here is an outlier but
it's good up like this is HDL it's a
healthy cholesterol but you know we
we're not um dealing with individuals
here we're looking at patterns in the
entire data set but these are just some
of the things you as the scientist the
data scientists you have to have that
understanding of your data set
so we looked at all of the numerical
values now we're going to look at our
categorical variables right we had
subset of the data set took out all of
the numericals we're going to do the
same thing and so now instead of
including the floats we are excluding
the floats and that is going to give us
all our categorical values including our
date and I think I may just drop that
from my categorical subset just because
the type of analysis
that we're doing is not
um you know suitable for date date type
of analysis it's a whole different type
so we're going to drop the date from
that category so we're using our drop uh
method and we'll identify the column and
I'm going to use that here
and then I'm going to stipulate the axis
as being that and then I want to make
sure that it did what I intended to do
and I'm going to check it again
very good so now our date is gone we
have nine columns and so now we can with
the categoricals we can't do histograms
we cannot do box plots those are for
continuous variables here we do bar
charts and
um you know we do single bar charts we
can do
stack bar charts column bar charts yeah
it's all about the bar charts in this
when it comes to the categorical
variables but mind you python has uh
you know a variety of
of categorical plots specifically for
categorical variables
but one of the things I like to do for
the categorical variables is the n
unique and so what that does it tells
you how many unique categories there are
in each of the variables now when I do
this I also sort it and I see something
very very interesting here so now
smoking to categories that's fine sex
two categories diabetic status two
categories
um heart disease two categories so all
of these
um
so these let me see missing so up until
here I think those are true categorical
variables
and the others might be mislabeled
because if you have fasting blood sugar
with 129 and then oh there's another
date column in here
right and BMI with 3 000 categories
yes those are definitely
giving
continuous or yeah not categorical
variables you know that those large
number of categories are reminiscent of
the continuous variables
so I went to transform that so here's
what I'm doing with this right I'm
looking for The Unique the number of
unique uh categories in the variables
and I'm creating a whole new data frame
with that and I'm going to now rename
the columns and then what I'm going to
do is I'm going to change those to the
integers that they should be as opposed
to
keeping them as floats
yeah so I went ahead and did a count
then I re um you know I did a conversion
a data type conversion because I wanted
to be able to sort the number of
categories and separate those with a
large number of categories from those
with less than 10 categories because I
believe those are the true categorical
variables so that's what I'm doing here
I'm creating a set of features that are
actual categorical variables and so this
is what my new subset of categorical
variables look like those were the ones
that were originally labeled as
numerical and then we had the original
categorical variables and so I wanted to
generate a plot of those original
categorical variables and I think I may
have not ingredients included the the
main variable in here so let's go ahead
and add that to my data frame so that DH
cat
underscore we're going to get
CHD HD back in there and that is going
to come from we can get it from the main
data frame
right because when we separated out our
categorical variables that got rid of
this the uh
ch2 so we're going to put that back as
that cat column
and
did we not Define the categorical
variable
originally here this was
my categorical variable here and this is
what I'm using
like the name of my data set
ah I have switched the
abbreviations
f h cat and this one should be what is
the original name of my data set and
then
here we go okay so now we've added the
CHD back there so now we can see these
plots these categorical plots with
respect to the Target variable so here
we have diabetes diabetic status for
example and it's showing you of the
non-diabetics
um of course majority of them they had
uh no heart disease right um now mind
you there is such significant
significant imbalance in the Target that
it doesn't even make sense to look at
these because if we were to look at the
distribution in the Target variable we
would see the the measure of imbalance
so the target variable is the CHD and if
we did look at that if we did our
original or any one of them because they
all now contain the
outcome variable so we could do a simple
value counts and that would give us an
idea of how you know what the
distribution is like but we already know
that it's significantly imbalanced
and yeah as we see it's 44
000 4400 no heart disease
127. if we wanted to get that as a
proportion then we would say normalize
equals true in this case and that would
give us so
um yeah
0.28 of the population of the sample
actually had the heart disease so
significant about so therefore uh if we
want to use if we want to model this
data we would have to treat that
imbalance in some way with smoking or
some other technique to kind of get that
big balance
um
to not be so severe so let's see how
we're going to do that
I do not
okay
all right so we have our cable cross
validation and we're going to do 10
splits and then we're going to see what
the mean and the standard deviation for
each of these versions are and it's 78.8
less than when we just did Regular
across uh logistic regression
now here I'm going to do some uh Shuffle
split I'm not sure why I have this code
for oh the uh
the dimensions of the
model is the data set is concerned so
now again we are going to do a shuffle
split instead of the K folds this time
and I did actually run that and did not
get anything different as far as these
models are concerned here I'm using
um the the scoring method as accuracy as
opposed to
the default and so when I ran that I got
an even smaller but we um we also did
the log loss negative not negative log
loss as the scoring method for the cross
validation and we looked at the mean of
negative laws as well of the long and
then the standard deviation of that so
we see all of our models here then
finally looking for the AUC of the
logistic regression and here we are
doing a right so we're doing the ROC AUC
as the scoring method and we'll get
an accuracy of
85 or so so yeah I really that that's an
improvement
um so we want to look at the confusion
Matrix and the classification report for
our model so I went ahead and did that
and
um I did not include I should include
the value
so I added this uh formula just to get
the instead of getting
the values
in the scientific notation I'm actually
getting
percentages so I'm dividing the values
in each of the quadrants a by the total
and then I'm representing that as a
percentages so we can see that for
example the true negatives
um here for 39 are true negatives and
then 40
0.63 are the true positives and then we
have
11.58 as the false positive and then
8.77 as the false negative so that's how
you would generate that right so we had
the model we got the prediction we used
the predictions to get the confusion
Matrix and then we put that confusion
Matrix into a heat map
alternatively we could do a
classification report so it would give
the Precision and the recall and the F1
score and support all of these can give
some way to estimate the performance of
the model and for example if we went in
and we optimized the model anyway and
then we generated a classification
report again we could see where
um you know what improvements were made
how did it form differently one last
thing I want to show is how we can get
an estimate of the feature importance so
we have a list of features as is found
in the column the original data frame
that we're using I do not need this
anymore and then all we can also get a
list of the coefficients from our model
so these coefficients actually match
these picture names and so we can
you know in order to bring that into one
object we first would unravel the
coefficients the name of the
coefficients and two are actually the
the values of the coefficients here have
problem this array into this list and
then we can combine that in a dictionary
that we then convert to a data frame and
this is what it looks like and if we
wanted we could actually do a sort of
the values
and we were sought by the coefficients
uppercase C here
and then we get another print
now I want to do
it will let me copy and paste that hello
all right and so there we have it did it
sort
if I don't think
so because I did not reassign it back to
the table so it sorted and then it went
back to whatever
it was so now I want that to be the new
data frame
and if we
generate that
I'm not sure why
my
notebook is no longer responding there
we go
but was
unraveling because I already made
changes to it as
far as the coefficients are concerned
and here we are
so we have a sorting of the data frame
the coefficients now I did not make the
values absolute that's something else
that I can do but before you know doing
that I'm going to generate a plot so I'm
going to take this data from that data
frame and I'm going to to plot a so we
can see what these features look like as
far as which ones are important so here
we see that age is the most important
with respect to
increasing the likelihood that you would
have heart disease because this is
negatively correlated with the outcome
and so that means you have a greater
chance of getting the disease which is
labeled as the one
um then someone younger and similarly
with the H the LDL smoking all of these
are negatively associated with the
outcome
whereas we see that this combination of
coefficients um
is the strongest predictor of an outcome
of heart disease followed by
fasting but
glucose and HDL so these are all the
interact interaction
variables and now they are very good to
include in a model but when it comes to
actually risk factors that you can you
know make some changes to modify
lifestyle modifications then these are
the ones that you would
um you know focus on this would be good
for modeling and and being able to
screen to find out someone's risk of
heart disease but then when you want to
give them some concrete things that they
can do as far as their lifestyle is
concerned then they would definitely you
know focus on these features here so we
we looked at the modeling Factor here
and we we did quite a lot so we did
smooth because of the imbalance here and
when we smote this was how we got that
imbalance then we use the standard
scalar to scale our our X variables and
then we generated a bunch of models we
did the logistic regression with k-fold
logistic regression with Shuffle split
um we did a logistic regression where we
looked at either using accuracy or
negative log loss or the ROC AUC and we
generated this lovely
um confusion Matrix here and then
finally we got a feature
importance chart in terms of how the
variables Stack Up
with respect to the outcome in another
video I'll do a cat post but I think
this video is
long enough now
hi there if you like this video
subscribe to the simply learning YouTube
channel and click here to watch similar
videos to nerd up and get certified
click here