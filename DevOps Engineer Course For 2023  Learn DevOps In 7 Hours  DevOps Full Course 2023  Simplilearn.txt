welcome aspiring devops Enthusiast to an
impressive Journey Through the world of
devops engineering in this old
encompassing devs engineer full course
we will equip you with the skills and
knowledge to master the art of modern
software development and deployment from
understanding the core principles of
devops to getting Hands-On with the
latest tools and practices give good and
exciting roadmap ahead we'll dive into
the devops ingenious roles and
responsibilities exploring how the
bridge gap between development and
operation
posting collaboration and efficiency
together they will encode the top 9
Cutting Edge devops tools of 2023
empowering you to streamline processes
and enhance productivity you will delve
into the heart of Version Control with
an in-depth tool at kit learn what it is
how it's used and how great and GitHub
come together to revolutionize
collaborative coding we will guide you
through its installation and fundamental
commands ensuring you're well versed in
this pivotal tool so if you're ready to
unlock the potential of devops and
Propel your career forward buckle up
because this devops engineer full course
is your ultimate ticket to success and
before we begin if you are someone who
is interested in building a career in
devops by graduating from the best
universities or a professional who
elicits to switch careers with devops
while learning from the experts then
dragging a show to Simply learn skeltec
postgraduate program in devops the
course link is mentioned in the
description box that will navigate you
to the course page where you can find a
complete audio of the program being
offered and now let's take a minute to
hear from our Learners who have
experienced message success in their
careers
I was really impressed by how many job
interviews I landed after I added the
certification to my portfolio even
though I was new to the world of devops
I finally decided to join aquarie group
as a Java devops engineer and now I am
earning 40 more than my previous job I
am Pro Hilario and I am an I.T
professional I was born and brought up
in Manila Philippines
I have spent my whole life here I
started my ID Journey with Accenture
three years ago I joined there as a
cloud architect there I worked with AWS
and Azure Technologies while working
there I became interested with devops I
was anyway looking for higher paying job
and devops seemed the right career
choice I applied to several companies
but didn't get any callbacks I already
knew about cloud computing so I decided
to upskill myself with a course in
devops to enhance my overall profile
simply learn offered a flexible payment
system
I was able to split the course fee so I
decided to go with the postgraduate
program in devops in collaboration with
Caltech ctme during my interview at
Macquarie I was asked about git commands
kubernetes and other Technologies I was
able to answer with full confidence
because I had learned these Technologies
during the course I was hired
immediately with a 40 salary height I
also learned Technologies like ansible
terraform and darker I am using this
Technologies on a daily basis and I have
been able to make some contributions at
work my overall experience with simpler
was positive the course was divided into
modules
and we had assignments for each module I
would really like the Capstone project I
am really happy that I took this course
it didn't only boost my career but also
my confidence
and if these are the types of videos you
would like to watch then hit the
Subscribe button like and press on the
Bell icon to never miss on future
content so stay tuned with us until the
end of this video and don't forget to
register your opinion in the comment
section below I'm going to go through a
number of key elements today the first
two will be reviewing models that you're
already probably using for delivering
Solutions into your company and the most
popular one is waterfall followed by
agile then we'll look at devops and how
devops differs from the two models and
how it also borrows and leverages the
best of those models we'll go through
each of the phases that are used in
typical devops delivery and then the
tools used within those phases to really
improve the efficiencies within devops
finally we'll summarize the advantages
that devops brings to you and your teams
so let's go through waterfall so
waterfall is a traditional delivery
model that's being used for many decades
for delivering Solutions not just IT
solutions and digital Solutions but even
way before that it has its history it
goes back to World War II so waterfall
is a model that is used to capture
requirements and then Cascade each key
deliverable through a series of
different stage Gates that is used for
building out the solution so let's take
you through each of those stage Gates
the first that you may have done is
requirements analysis and this is where
you sit down with the actual client and
you understand specifically what they
actually do and what they're looking for
in the software that you're going to
build and then from that requirements
analysis you'll build out a project plan
so you have an understanding of what the
level of work is needed to be able to be
successful in delivering the solution
after that you've got your plan then you
start doing the development and that
means that the programmers start coding
out their solution they build out their
applications to build out the websites
and this can take weeks or even months
to actually do all the work when you've
done your coding and development then
you send it to another group that does
testing and they'll do full regression
testing of your app application against
the systems and databases that integrate
with your application you'll test it
against the actual code you'll do manual
testing you do UI testing and then after
you've delivered the solution you go
into maintenance mode which is just kind
of making sure that the application
keeps working there's any security risks
that you address those security risks
now the problem you have though is that
there are some challenges however that
you have with the waterfall model the
cascading deliveries and those complete
and separated stage Gates means that
it's very difficult for any new
requirements from the client to be
integrated into the project so if a
client comes back and it's the project
has been running for six months and
they've gone hey we need to change
something that means that we have to
almost restart the whole project it's
very expensive and it's very time
consuming also if you spend weeks and
months away from your clients and you
deliver a solution that they are only
just going to see after you spend a lot
of time working on it they could be
pointing out things that are in the
actual final application that they don't
want or are not implemented correctly or
lead to just general unhappiness the
chance you then have is if you want to
add back in the client's feedback to
restart the whole waterfall cycle again
so the client will come back to you with
a list of changes and then you go back
and you have to start your programming
and you have to then start your testing
process again and just you're really
adding in lots of additional time into
the project so using waterfall model
companies have soon come to realize that
you know the clients just aren't able to
get their feedback and quickly
effectively it's very expensive to make
changes once the teams have started
working and the requirement in today's
digital world is that solution simply
must be delivered faster and this has
led for a specific change in agile and
we start implementing the agile model so
the agile model allows programmers to
create prototypes and get those
prototypes to the client with the
requirements faster and the client is
able to then send their requirements
back to the programmer with feedback
this allows us to create what we call a
feedback loop where we're able to get
information to the client and the client
can get back to the development team
much faster typically when we're
actually going through this process
we're looking at the engagement cycle
being about two weeks and so it's much
faster than the traditional waterfall
approach and so we can look at each
feedback loop as comprising of four key
elements we have the planning where we
actually sit down with the client and
understand what they're looking for we
then have coding and testing that is
building out the code and the solution
that is needed for the client and then
we review with the clients the changes
that have happened but we do all this in
a much tighter cycle that we call a
Sprint and that typically as Sprint will
last for about two weeks some companies
run sprints every week some run every
four weeks it's up to you as a team to
decide how long you want to actually run
a Sprint but typically it's two weeks
and so every two weeks the client is
able to provide feedback into that Loop
and so you were able to move quickly
through iterations and so if we get to
the end of Sprint 2 and the client says
hey you know what we need to make a
change you can make those changes
quickly and effectively for Sprint three
what we have here is a breakdown of the
ceremonies and the approach that you
bring to Agile so typically what will
happen is that a product leader will
build out a backlog of products and what
we call a product backlog and this will
be just a whole bunch of different
features and they may be small features
or bug fixes all the way up to large
features that may actually span over
multiple Sprints but when you go through
the Sprint planning you want to actually
break out the work that you're doing so
the team has a mixture of small medium
and large solutions that they can
actually Implement successfully into
their Sprint plan and then once you
actually start running your Sprint again
it's a two-week activity you meet every
single day the two with the actual
Sprint team to ensure that everybody is
staying on track and if there's any
blockers that those blockers are being
addressed effectively and immediately
the goal at the end of the two weeks is
to have a deliverable product that you
can put in front of the customer and the
customer can then do a review the key
advantages you have are running a Sprint
with agile is that the client
requirements are better understood
because the client is really integrated
into the scrum team they're there all
the time and the product is delivered
much faster than with a traditional
waterfall model you're delivering
features at the end of each Sprint
versus waiting weeks months or in some
cases years for a waterfall project to
be completed how however there are also
some distinct disadvantages the product
itself really doesn't get tested in a
production environment it's only been
tested on the developer computers and
it's really hard when you're actually
running agile for the Sprint team to
actually build out a solution easily and
effectively on their computers to mimic
the production environment and the
developers and the operations team are
running in separate silos so you have
your development team running their
Sprint and actually working to build out
the features but then when they're done
at the end of their Sprint and they want
to do a release they kind of fling it
over the wall at the operations team and
then it's the operations team job to
actually install the software and make
sure that the environment is running in
a stable fashion that is really
difficult to do when you have the two
teams really not working together so
here we have is a breakdown of that
process with the developers submitting
their work to the op operations team for
deployment and then the operations team
may submit their work to the production
service but what if there is an error
what if there was a setup configuration
error with the developers test
environment that doesn't match the
production environment there may be a
dependency that isn't there there may be
a link to an API that doesn't exist in
production and so you have these charges
that the operations team are constantly
faced with and their challenge is that
they don't know how the code works so
this is where devops really comes in and
let's dig into how devops which is
developers and operators working
together is the key for successful
continuous delivery so devops is as an
evolution of the agile model the agile
model really is great for Gathering
requirements and for developing and
testing out your Solutions and what we
want to be able to do is kind of address
that challenge and that gap between the
Ops Team and the Dev team and so with
devops what we're doing is bringing
together the operations team and the
development team into a single team and
they are able to then work more
seamlessly together because they are
integrated to be able to build out
solutions that are being tested in a
production-like environment so that when
we actually deploy we know that the code
itself will work the operations team is
then able to focus on what they're
really good at which is analyzing the
production environment and being able to
provide feedback to the developers on
what is being successful so we're able
to make adjustments in our code that is
based on data so let's step through the
different phases of a devops team so
typically you'll see that the devops
team will actually have eight phases now
this is somewhat similar to Agile and
what I'd like to point out at the time
is that again agile and devops are very
closely related that agile and devops
are closely related delivery model tools
that you can use with devops it's really
just extending that model with the key
phases that we have here so let's step
through each of these key phases so the
first phase is planning and this is
where we actually sit down with a
business team and we go through and
understand what their goals are the
second stage is as you can imagine and
this is where it's all very similar to
Agile is that the code is actually start
coding but they typically they'll start
using tools such as git which is a
distributed Version Control software it
makes it easier for developers to all be
working on the same code base rather
than bits of the code that is rather
than them working on bits of the code
that they are responsible for so the
goal with using tools such as git is
that each developer always has the
current and latest version of the code
you then use tools such as Maven and
Gradle as a way to consistently build
out your environment and then we also
use tools to actually automate testing
now what's interesting is when we use
tools like selenium and junit is that
we're moving into a world where our
testing is scripted the same as our
build environment and the same as using
our get environment we can start
scripting out these environments and so
we actually have scripted production
environments that we're moving towards
Jenkins is the integration phase that we
use for our tools and another Point here
is that the tools that we're listing
here these are all open source tools
these are tools that any team can start
using we want to have tools that control
and manage the deployment of code into
the production environments and then
finally tools such as ansible and Chef
will actually operate and manage those
production environments so that when
code comes to them that that code is
compliant with the production
environment so that when the code is
then deployed to the many different
production servers that the expected
results of those servers which is you
want them to continue running is
received and then finally you monitor
the entire environment so you can just
zero in on spikes and issues that are
relevant to either the code or changing
consumer habits on the site so let's
step through some of those tools that we
have in the devops environment so here
we have is a breakdown of the devops
tools that we have and again one of the
things I want to point out is that these
tools are open source tools there are
also many other tools this is just
really a selection of some of the more
popular tools that are being used but
it's quite likely that you're already
using some of these tools today you may
already be using Jenkins you may already
be using git but some of the other tools
really help you create a fully
scriptable environment so that you can
actually start scripting out your entire
devops tool set this really helps when
it comes to speeding up your delivery
because the more you can actually script
out of the work that you're doing the
more effective you can be at running
automation against those scripts and the
more effective you can be at having a
consistent experience so let's step
through this devops process so we go
through and we have continuous delivery
which is a plan code build and test
environment so what happens if you want
to make a release well the first thing
you want to do is send out your files to
the build environment and you want to be
able to test the code that you've been
created because we're scripting
everything in our code from the actual
unit testing being done to the all the
way through to the production
environment because we're testing all of
that we can very quickly identify
whether or not there are any defects
within the code if there are defects we
can send that code right back to the
developer with a message saying what the
defect is and the developer can then fix
that with information that is real on
the either the code or the production
environment if however your code passes
the the scripting tests it can then be
deployed and once it's out to deployment
you can then start monitoring that
environment what this provides you is
the opportunity to speed up your
delivery so you go from the waterfall
model which is weeks months or even
years between releases to Agile which is
two weeks or four weeks depending on
your Sprint Cadence to where you are
today with devops where you can actually
be doing multiple releases every single
day so there are some significant
advantages and there are companies out
there that are really zeroing in on
those advantages if we take any one of
these companies such as Google Google
any given day will actually process 50
to 100 new releases on their website
through their devops teams in fact they
have some great videos on YouTube that
you can find out on how their devops
teams work Netflix is also a similar
environment now what's interesting with
Netflix is that Netflix have really
fully embraced devops within their
development team and so they have a
devops team and Netflix is a completely
digital company so they have software on
phones on Smart TVs on computers and on
websites interestingly though the devops
team for Netflix is only 70 people and
when you consider that a third of all
internet traffic on any given day is
from Netflix it's really a reflection on
how effective devops can be when you can
actually manage that entire business
with just 70 people so there are some
key advantages that devops has it's the
actual time to create and deliver a
software is dramatically reduced
particularly compared to Waterfall
complexity of maintenance is also
reduced because you're automating and
scripting out your entire environment
now you're improving the communication
between all your teams so teams don't
feel like they're in separate silos but
that are actually working cohesively
together and that there is continuous
integration and continuous delivery so
that your consumer your customer is
constantly being delighted if you're
into the tech industry or just curious
about the role of development software
development you have come to the right
place so what exactly is devops in
simple terms it's a set of practices and
tools that help developers and
operational team work better together
releasing software faster with higher
quality at its core devops is about
breaking down barriers between
development and operations and creating
a culture of collaboration that focuses
on delivering value to customers as
quickly and efficiently as possible of
course this is a vast oversimplification
and there are many different aspects of
devops that we could spend hours diving
into but for now let's focus on some of
the key responsibilities of a devops
engineer who is the person responsible
for implementing and overseeing devops
practices and processes but before we
begin if you're into the channel and
haven't subscribed already consider
getting subscribed to Simply learn to
stay updated with all the latest
Technologies and with that Bell icon to
never miss an update from us having said
that the demand for devops professionals
has overgrown in recent years as more
and more companies adapt devops
practices to improve their software
development and delivery processes so
are you ready to advance your
professional career to the next level
taking our step-by-step simply learns
project program in devops in
collaboration with IBM will help you
start your devops journey that will
prepare you for the devops engineer role
in order to match your skill set market
demand this devops training program is
designed in collaboration with Caltech
ctme our Cutting Edge branded learning
combines live online devop certification
classes with interactive labs to give
you practical experience this
post-target program covers topics
including git GitHub Docker CI CD
practices using Jenkins kubernetes and
much more so what else can you expect
from this program well this devop
trading program will cover skills like
devops methodology continuous
integration devops and Cloud deployment
Automation and you'll also get hands-on
experience with the latest tools and
techniques including terraform Maven and
symbol Jenkins Docker junit and many
more this program will cover industry
projects like Docker Rising junkins
pipeline deploy angular application and
Docker container branching development
model and many more exotic projects so
if you are looking to pursue your career
as a devops engineer and acquire skills
that will prepare you for your job
consider enrolling in this intensive
training program we will leave the link
in the description box below make sure
you check that out so without any
further Ado let's get started with
today's topic
firstly let us understand what is devops
now devops is a software development
approach that emphasizes collaboration
Automation and communication between
development and operations team it aims
to streamline the entire software
development lifecycle by integrating and
optimizing processes tools and
methodologies it encourages the culture
of shared responsibility where
developers and operations team work
together closely throughout the entire
software development life cycle from
planning and coding to testing
deployment and monitoring
now the question is who is a devops
engineer well you got it right a devops
engineer is a professional who combines
software development expertise with
operations knowledge to facilitate
collaboration streamline processes and
improve software delivery and
infrastructure management within an
organization a devops engineering role
is to bridge the gap between development
and operations team enabling efficient
and reliable software development and
deployment practices
but the question is how to become a
devops engineer what are the skills that
you need to possess to become a good
devops engineer well a devops engineer
possess a wide range of skills including
proficients in scripting and programming
languages knowledge of various tools and
Technologies expertise in
systemadministration dot platforms and
containerization Technologies as well as
strong problem solving and communication
skills are necessary firstly having a
good coding knowledge well tools like
Confluence jira git these tools can
support and enhanced collaboration and
project management within a devops
environment next having a good knowledge
on deployment tools are also necessary
now tools like dcos provides
orchestrationization capabilities for
distributed applications Docker enables
containerization for consistent and
scalable deployments and AWS offers a
broad range of cloud services for
infrastructure provisioning scalability
and managed Services next you need to
have a good knowledge on operations
tools as well now chef and ansimil focus
on infrastructure automation and
configuration management while
kubernetes specializes in container
orchestration and management these tools
are utilized in devops to automate
various aspects of software like
development life cycles including
infrastructure provisioning
configuration management application
deployment and scaling moving ahead you
need to have a strong grip on monitoring
tools nagios Splunk and dated dog are
three commonly used tools in the field
of monitoring and observatability now
each rule serves a specific purpose in
monitoring and managing system and
applications nagya specializes
infrastructure and application
monitoring Splunk focuses on log
analysis and data visualization datadog
provides comprehensive monitoring and
analytic capabilities in Cloud
environments these tools play a crucial
role in maintaining the health and
performance of systems and applications
moving ahead you need to have a good
knowledge on Jenkins and courtship now
Jenkins and courtships are both
essential tools and demo practices
Jenkins is a flexible and extensible
automation server that supports
continuous integration testing and
deployment on the other hand core ship
is a cloud-based CI CD platform that
offers Simplicity and ease of use
particularly for cloud native
applications both these tools contribute
to improving development productivity
and code quality and finally having a
good knowledge on testing tools like
selenium junit are necessary for a day
offs engineer junit is primarily focused
on unit testing and automated testing of
java code while selenium is geared
towards functional testing and
automation of web applications both
these tools play critical roles in
devops workflow contributing to faster
feedback Cycles improve code quality and
Reliable Software releases
so these are some of the main and
important skills that you need to
possess as a devops engineer before
moving ahead let's take a minute and
listen to the experiences of our
Learners who have enrolled in the devops
pgp program which has proven to be
highly beneficial for many aspiring
engineers and professionals leading them
to achieve New Heights in the field of
devops
and started my It Journey with Accenture
three years ago I joined there as a
cloud architect there I worked with AWS
and Azure Technologies looking for
higher paying job and devops seem the
right career choice so I decided to go
with the postgraduate program in devops
in collaboration with Caltech ctme the
course was divided into modules and we
had assignments
I was really impressed by how many job
interviews I landed after I added the
certification to my portfolio and now I
am earning 40 more than my previous job
it didn't only boost my career but also
my confidence
well now comes the main part what
exactly are the day-to-day roles and
responsibilities of a devops engineer
now a devops engineer play a crucial
role in Bridging the Gap between
development and operations team as we
discussed earlier so here are some of
the top five roles and responsibilities
of a devops engineer in detail first on
the list we have collaboration and
communication now devops engineer of a
state effective communication and
collaboration with development and
operations team the actively participate
in meetings and discussions to align
goals and expectations now as a devops
engineer you need to engage in regular
meetings and discussions regular
engagement ensures that they are up to
date with ongoing projects challenges
and goals enabling them to better align
their efforts and contribute effectively
regular engagement ensures that they are
up to date with ongoing projects
challenges and goals enabling them to
better align their efforts and
contribute effectively actively listen
and understand the requirement concerns
and feedback now when engaging with
development and operations team devops
Engineers practice active listening they
close attention to the requirements
concerns and feedback expressed by team
members from both the sites by
understanding their perspectives pain
points and suggestions devops Engineers
can better assess the needs of their
teams and collaborate to find suitable
Solutions facilitate effective
communication channels now devops
Engineers take the initiative to
establish and maintain effective
communication within the organization
this often involves settling up
dedicated chat platforms like slack
Microsoft teams or collaboration tools
like jira to Foster better collaboration
and ensure that information flows
smoothly between the teams and finally
encourage cost functional collaboration
now day of Engineers recognize the value
of cross-function collaboration and
knowledge sharing among the team members
they actively encourage them from
development and operations team to
collaborate exchange ideas and share
their expertise second on the list we
have infrastructure Automation and
configuration management now devops
Engineers focus on automating
infrastructure provisioning and managing
configurations using certain tools they
Define infrastructure as code enabling
efficient deployment and scaling of
resources
now as a devops engineer you have to
identify the infrastructure requirements
effectively now day of Engineers work
closely with development teams to
understand the infrastructure
requirements of the application this
involves analyzing the needs of
applications in terms of Computer
Resources Storage security networking
and scalability by gathering all these
requirements devops engineer can ensure
that infrastructure is provisioned and
configured to meet the application
specific need and future growth write
infrastructure automation scripts and
templates now once the infrastructure
requirements are identified devops
Engineers use automation tools and
techniques to define the desired state
of infrastructure components the right
scripts and templates that specify how
the infrastructure should be provisioned
configured and managed
automate the provisioning configuration
and management of servers well devops
Engineers leverage infrastructure as
code or in short IAC principles to
automate the provisioning configuration
and management of servers networks and
other infrastructure resources they use
tools like ansemble Chef or puppet to
automate the deployment and
configuration of infrastructure
components and finally regularly update
infrastructure code now devops engineer
uses Version Control Systems like git to
track changes collaborate with team
members and manage different versions of
infrastructure code by regularly
updating and versioning infrastructure
code devops Engineers can easily track
and reverse changes whenever necessary
now third on the list we have continuous
integration and continuous deployment or
CI CD in short now devops Engineers are
responsible for establishing and
maintaining cicd pipelines which enable
developers to integrate code changes
seamlessly and deploy applications
rapidly
so for that they have to set up a
version control system now Version
Control System like git play a crucial
role in devops by providing a
centralized repository for managing code
and tracking changes setting up a
Version Control System involves creating
a repository initializing it with the
code and defining branching and merging
strategies
configure a build server now a build
server automates the process of
compiling testing and packaging
application code tools like Jenkins and
gitlab cicd allows you to Define build
pipelines that specify the steps to be
executed these pipelines typically
involve tasks such as pulling code from
repository compiling source code
generating artifacts and packaging the
application
next automate the deployment process now
automation of the deployment process is
crucial for achieving rapid and
consistent software releases
containerization tools like Docker
provide a lightweight and portable way
to package application and their
dependencies Docker containers can be
created and deployed consistently across
different environments ensuring
consistency between development testing
and production Define an inverse quality
Gates and monitor CI CD kpis quality
Gates ensure that the code meets
predefined quality standards before it
is promoted to the next stage of the
cicd pipeline automated testing
including unit test integrated test and
end-to-end test integration test and
end-to-end test help catch bugs and
validate the functionality of the
managing applications and finally
measuring K pairs of the cicd pipeline
provide insights into its performance
and help identify areas for improvement
well next we have monitoring and
performance optimization now day of
Engineers monitor system performance
identify and
infrastructure and application Stacks
whenever necessary they Implement
monitoring tools to collect and analyze
metrics logs and traces so for that
select and configure monitoring tools
now monitoring tools like Prometheus or
graphene can be used to collect and
visualize these metrics allowing tapes
to identify bottlenecks or optimized
processes and enhance the overall
efficiency of the CI CD pipeline
also we have to collaborate with
development and operations team to
fine-tune application performance
so continuously optimizing the
infrastructure which will ensure High
availability scalability and reliability
of that application and finally we have
security and compliance now deox
Engineers play a critical role in
implementing security measures and
ensuring compliance with industry
standards and regulations they work
closely with security teams to Define
and Implement security controls
throughout the software delivery
pipeline so they have to continuously
collaborate with the security teams to
identify and Define security
requirements and controls and Implement
security measures such as vulnerability
scanning access management and secure
configuration they have to continuously
integrate security testing and code
analysis into the CI CD pipeline
and monitor for any sort of potential
security risk or breaches and respond
promptly to mitigate any identified
vulnerabilities
so these were some of the main or top
five roles and responsibilities of a
devops engineer now in a world driven by
rapid technological advancements
businesses are constantly seeking ways
to deliver software faster and more
efficiently enter devops the dynamic
philosophy that has revolutionized the
software development and operations
landscape devops is not just a buzzword
it's a game engineering approach that
Bridges the gap between developers and
operations team Foster collaboration and
enhancing the entire software
development life cycle
now at its core devops embodies a
cultural shift towards seamless
integration and continuous delivery a
promotes a collaborative environment
where developers system administrators
quality assurance professionals and
other stakeholders work together
harmoniously and promoting
cross-functional communication the
devops impulse teams streamline
processes automate workflows and
ultimately deliver high quality software
at an accelerated Pace having said that
the demand for devops professionals has
overgrown in recent years as more and
more companies are update of practices
to improve their software development
and delivery processes so are you ready
to advance your professional career to
the next level taking our step-by-step
simple learn sports archet program in
devops in collaboration with IBM will
help you start your day option A that
will prepare you for the devops engineer
role so in order to match your skill set
market demand this day of training
program is designed in collaboration
with caltex ctme and a cutting-edge
blended learning combines live online
day of certification classes with
interactive labs to give you a practical
experience this day of training program
will cover skills like day off
methodology continuous integration Day
Option Cloud infrastructure deployment
Automation and many more and along with
that you'll also get hands-on experience
with latest tools and techniques
including terraform Maven ansible Docker
Jenkins and much more so if you're
looking to pursue your career as devops
engineer and acquire these skills that
will prepare you for your job consider
enrolling in this intensive training
program today we will leave the link in
the description box below so make sure
you check that out so let us move ahead
and proceed with our topic today so the
main question is what fuels this devops
revolution now it's a powerful
collection of tools specifically
designed to support and amplify its
principles so these tools acts as a
catalyst empowering teams to automate
tasks marriage infrastructure
efficiently and monitor application
effectively so without any further delay
let us directly jump into the top nine
noteworthy tools highlighting their
features benefits and significance that
have become synonymous with the devops
ecosystem in no particular order
so first on the list we have Jenkins now
Jenkins is an open source automation
server known for its extensive plug-in
ecosystem making it highly versatile and
customizable it facilitates continuous
integration and continuous delivery or
in short cacd pipelines automating the
build test and deployment processes
Jenkins enables team to internet code
frequently ensuring early detection of
issues and faster software releases
so let us now look at some of the key
features of their Jenkins firstly easy
installation and configuration Jenkins
has a user-friendly web interface that
simplifies installation configuration
and management of build jobs and
pipelines distributed bills for
scalability now it's supposed to have
vast ecosystem of plugins for seamless
integration and it can distribute build
tasks across multiple nodes which helps
in paralyzing builds and reducing
overall build time especially in large
projects and finally extensible through
scripting languages like Ruby now
Jenkins provides extensibility through
scripting languages like groovy which is
a powerful and versatile language that
runs on a Java virtual machines or in
short jvm so by leveraging groovy
scripting capabilities Jenkins becomes
highly adaptable and customizable to
meet the specific requirements of
different development teams and projects
so those are some of the key features of
Jenkins let us now move ahead and
discuss some of the benefits or
advantages of using Jenkins so firstly
we have enabled faster feedback and
quicker time to market now Jenkins
continuous integration capabilities
enable developers to merge their code
changes into a shared repository
regularly by doing so Jenkins
automatically triggers build and testing
processes to validate any kind of
changes this automates feedback loop
allows developers to receive quick
feedback on the quality of their code so
as a result issues and bugs can be
identified early in the development
cycle preventing the accumulation of
defects and reducing the time required
to fix them as discussed earlier it also
facilitates early issue detection and
reduce bug turnaround time now with
Jenkins running automated builds and
test every time new code is committed
potential issues and bugs are bought
early in the development process so by
detecting issues at an early stage
developers can address them promptly
before they propagate further into the
code base
and finally automates repetitive tasks
save time and effort now Jenkins
automates various tasks involved in the
software development life cycle using
through Stars such as building the code
running tests and deploying applications
by automating these repetitive and time
consuming tasks gen can saves developers
and operations significant amount of
time and efforts so that is what Jenkins
is all about it's a widely adopted in
the devops landscape due to its
flexibility and extensive plug-in
support and learning it will add a great
value to your array of skills so it's
ability to automate crcd processes and
integrate with various tools makes it a
crucial component of models of your
development and delivery Pipelines
second on the list we have Docker Docker
has revolutionized application
deployment with its containerization
approach and allows developers to
package applications and their
dependencies into lightweight isolated
containers Docker contains provide
consistency across different
environments ensuring that applications
and consistently regardless of the
underlying infrastructure this
portability along with the rapid startup
times and efficient resource utilization
has made Docker a foundational tool at
dayos practices so let us now discuss
some of the key features of Docker first
one is packaging applications and
dependencies into containers now Dockers
allows developer to package their
applications and all their dependencies
into a self-contained units called
containers and the process is known as
containerization now these containers
encapsulate the application code its
runtime various libraries and system
tools required to run the application by
doing so Docker ensures consistency
across different environments secondly
efficient resource utilization through
containerization now this lightweight
nature of containers allows for more
efficient resource utilization multiple
containers can run on a single fiscal
machine without the need for individual
OS instances it means that you can host
more applications and services on the
same Hardware reducing the number of
servers needed and finally easy scaling
and management of containers Now
containerization simplifies application
scaling and management so when you need
to handle increased application load you
can quickly scale by running more
instances on the containerized
application on additional servers or
within a container orchestration
platform like kubernets
so let us now talk about some of the
benefits of the docker tool now firstly
rapid deployment and scalability Docker
enables rapid deployment of application
Studios lightweight and containerized
approach when using Dockers developers
package their applications and
dependencies into containers which
encapsulate everything needed to run the
application so these containers are
portable and can be easily moved from
one environment to the other and next we
have the isolation of applications and
dependencies for improved security
Docker utilizes contourization to
isolate applications and the
dependencies from the ecosystem and
other containers each containers operate
in its own user space separate from
other containers providing a strong
level of isolation this isolation
prevents applications from affecting
each other and helps contain potential
security breaches within the confines of
the container and finally simplify
development to production workflow now
dock streamlined the relevant production
workflow by providing consistency
between different environments with
Docker developers can create content
that run the same way in development
testing and production environments
thereby reducing the chances of
unexpected issues arising during the
deployment
so Docker has become a Cornerstone of
modern day of practices it's upgraded to
streamline application deployment and
improved resource utilization has
transformed software development and
operations enabling faster and more
reliable application relevant
so that was all about Docker so let us
now move ahead and discuss the next tool
which is kubernets so next on the list
we have kubernetes kubernetes is an open
source container orchestration platform
that automates the deployment scaling
and management of containerized
applications it provides a robust
infrastructure for running and
coordinating containers across clusters
of machine making it easy to manage
large scale deployments devops teams can
easily deploy update and scale
applications fascinating faster and more
Reliable Software delivery while
promoting collaboration and consistency
across development and operations life
cycle through kubernets so let us look
at some of the key features of it
automatic antenna deployment and scaling
now container orchestration platforms
the kubernetes provide automatic
container deployment and scaling
capabilities when deploying applications
on kubernets you can Define the desired
state of your application using yaml
files on declarative configuration
kubernetes then take care of this
ensuring that specified number of
container replicas is running at all
times service Discovery and load
balancing
now kubernet enables faster feedback and
weaker time to Market in a country rise
environment multiple instances of an
application may be running across
different containers making it
challenging for clients or other
services to know the IP addresses of all
the running instances and detecting and
keeping tracking of the locations of
various services within the container
cluster and kubernetes for example
provides a built-in service Discovery
mechanism next we have self-healing and
ordinary set of containers now container
orchestration platform provides
self-healing capabilities to ensure that
applications are always available and
responsive if a container fails to do an
application crash or any other issues
the orchestration platform detects the
failure and automatically restarts the
failed container
so let us now move ahead and discuss
some of the benefits of using kubernetes
tool firstly it is scalable and highly
available application and it provides a
seeming repeat and provides a seamless
scaling of resources based on demand
next it has a simplified management of
containerization applications across
various clusters and finally automated
deployment and updates reducing manual
Intervention which further improves
resource utilization and optimization
overall kubernetes has emerged as the
industry standard for container
orchestration its ability to automate
application deployment scaling and
management simplifies the complexities
of running contenderized applications at
scale making it a crucial tool for
devops practitioners
moving and let us discuss our next tool
which is ansible ansible is a powerful
automation tool that simplifies
configuration management application
deployment and orchestration it employs
a declarative language to Define desired
State configuration making it easy to
manage and automate infrastructure tasks
ansible follows an agentless
architecture allowing it to work
efficiently across a wide range of
systems and environments so let us now
look at look at some of its features
firstly it's a declarative language for
defining infrastructure configuration
and it is also an agentless architecture
for easy deployment and management as I
discussed earlier now this is done
through a Playbook driven automation for
orchestration where extensive library of
modules for a wide range of tasks are
employed so let us now discuss some of
its benefits so firstly it's simplify
infrastructure management through
automation which increases the
operational efficiency with reduced
manual task and its identity nature
ensures that it has consistently and
predictability overall
and finally it supports a wide range of
infrastructure automation use cases and
with its agentless architecture it
allows easy integration with various
systems and environments so in a
nutshell ansible Simplicity flexibility
and ease of use have made it a popular
choice for automating infrastructure and
application deployment task Dev approach
an agentless architecture contribute to
efficient and streamline development
Workforce
so moving ahead the next tool on our
list is git is a distributed version
control system that has become a
fundamental tool for modern software
development practices it allows
developers to track changes collaborate
effectively and manage code base
efficiently gets decentralized
architecture ensures that developers can
work offline and merge changes
seamlessly across branches let us now
discuss with some of its key features
now its distributed version control
system has efficient collaboration with
various tools within the management of
the ecosystem of devops and it also has
an integration with various code hosting
platforms which is the branching and
merging capabilities for concurrent
development and finally in support for
code reviews and collaboration workflows
which also gives a commit based tracking
of changes so these are some of the key
features of git so let us now discuss
some of the advantages or like benefits
of using it so firstly it's easy
tracking and management of code changes
which ensures efficient collaboration
and concurrent development within the
resource files and its ability to work
offline and merge changes seamlessly
will benefit a lot of devops Engineers
and finally it's easy integration with
other devops tools makes it a wonderful
to use to manage code versions track
changes and enable efficient
collaboration which is Revolution as a
software development so it has become a
essential tool for Version Control
facilitating effective collaboration and
enabling streamline devops workflows
so moving ahead the next tool on our
list is terraform now terraform is an
infrastructure as code or in short IEC
tool that allows teams to Define and
provision infrastructure resources in a
declarative manner it supports multiple
Cloud providers and enables consistent
and reproducible infrastructure
deployment terraforms declarative syntax
and State Management capabilities
simplify infrastructure provisioning and
configuration
let us now look at some of its key
features firstly it provides multi-cloud
support for provisioning resources
Surplus different providers its
infrastructure as code approach for
consistent and reproducible deployments
makes it a declarative Syntax for
defining infrastructure configurations
and finally automated resource
provisioning and dependency management
so some of its benefits are the
advantages of using terraform as its
simplified resource provisioning and
dependency management now terraforms
infrastructure as code approach supports
for multiple Cloud providers makes it an
essential tool for managing
infrastructure codes and collaboration
and Version Control for infrastructure
configurations as well and finally in
State Management for tracking
infrastructure changes which makes it a
beneficial tool for devops engineers
is used to Monitor and manage the health
and performance of the infrastructure
application and network resources it's a
comprehensive monitoring solution to
identify and resolve issues proactively
ensuring High availability and
reliability of systems so let us look
some of its key features firstly its
monitoring capabilities
secondly it's centralized configure
management and finally its even handling
and escalation tool so learning nagios
is also a crucial and beneficial if you
start just starting one day off's
Journey
so moving ahead let us now discuss our
next tool on a list which is Elk stack
now elk stack offers a comprehensive
solution for centralizing logs from
various applications and systems making
it easier for devops teams to monitor
troubleshoot and gain valuable insights
from their log data the elk stack
comprising elasticsearch log stats and
kibana provides a comprehensive log
management and Analysis platform it acts
as a distribution search an analytics
engine while log stack collects
processes and transform logs and finally
kibana offers a user-friendly interface
for visualizing and exploring data
so some of its key features including
real-time monitoring and alerting
scalable and efficient log storage and
retrieval and distributed searching
analytics to data visualization and
Exploration with cabana
let us look at some of its benefits so
firstly it's real-time monitoring for
proactive detectives repeat its
real-time log monitoring for proactive
ratio detection with the help of
centralized log management and Analysis
Advanced log analytics was
troubleshooting and performance
optimization making it a scalable
architecture for handling large-scale
volumes of data and finally efficient
log storage and retrieval for compliance
and auditing purposes in a nutshell elk
stack has gained immense popularity for
its ability to handle log management and
Analysis its scale it empowers devops
teams with real-time insights into
application and infrastructure logs
facilitating effective troubleshooting
and performance optimization
well finally on the list we have jira
software jira is a widely used project
management tool that supports Agile
development methodologies it offers
robust features for planning tracking
and managing task issues and workflows
jiras customizable boats backlogs and
workflows impact teams to collaborate
effectively visualize and progress to
gain transparency into project statuses
with integration to various devops tools
jira facility seamless tracking of
development activities enabling
continuous Improvement and efficient
project management
some of its key features are
customizable both workflows for project
management agile planning and estimation
features issue tracking and management
capabilities
and some of its benefits include
efficient task management and tracking
enhance collaboration and visibility
across various teams and integration
with various devops tools for stream and
workflows it also helps in reporting an
analytics for project insights and
performance measurement finally jira has
become a go-to tool for agile project
management in the devops ecosystem it's
upgraded to support agile methodologies
track task and integrate with other
devops rules makes it a valuable assets
for teams seeking efficient project
management and continuous Improvement so
learning it will add a great value to
Aerial skills again so these were some
of the top nine day Ops rules that you
must know which will help you enhance
and accelerate your career in devops and
before we begin if you are someone who
is interested in building a career in
devops by graduating from the best
universities or a professional who
elicits to switch careers with devops
while learning from the experts then
dragging a show to Simply learn skeltec
postgraduate program in devops the
coastline is mentioned in the
description box that will navigate you
to the course page where you can find a
complete Oreo of the program being
offered so a little scenario before
using gets one of the challenges that a
lot of developers and development teams
would have had is developers would be
working on different types of code
whether it's database code whether it
was python or whether it's Java or net
and they would have a central server
that they would be pushing all of their
source code in but there was little or
no communication that was actually going
on in between all the developers you
know the challenge you had with that
scenario is that when people would be
checking in code you could be like you
know there'll be conflicts and you'd
have to roll back different code
versions and and this is really kind of
the challenge that git addresses git is
a tool that allows all of a developers
no matter what stack they're working in
to have access to all of the code and it
makes it much more effective at being
able to have development teams work on
small medium or even massive application
locations and some of the biggest
applications out there are managed
through a git distributed server
environment so let's jump into what
we'll be covering so you have a clear
understanding of the value that you're
going to get out of watching this
presentation so we're going to go
through devops and the tools I have
available for devops we'll talk about
what version control means within a
devops environment and cover the two
different types of Version Control
centralized and distributed and then
we're going to zero in on git which is a
really fantastic distributed version
control system and we're going to go
through and understand the features
workflow branches and commands in git
we're going to give you a demo and then
summarize all the activities at the end
of the the presentation so what is
devops well this is one of my favorite
questions I love the idea of what devops
is so devops really is a culture of
being able to deliver Solutions faster
where your development teams and your
operation teams work effectively
together the idea is to be able to
continuously build out Solutions and
have testing codes that's built into
your Solutions so that no matter where
they are in the stage of the integration
and deployment life cycle they're always
being tested and you can always have
code being ready to be released out the
idea is that instead of having big
releases that you'd have maybe once
every two weeks or once a month that you
would actually have a continuous stream
of releases because you always have code
that is being tested you have your
network that's been tested you have your
environment being tested and you'll be
able to provide feedback directly to the
appropriate person whether they're in
debts or whether they're in arts to be
able to be successful at being able to
deliver Solutions faster the bottom line
your Dev team things like operations and
your operations team begin to feel think
like developers it's really a fantastic
way of being able to speed up delivery
and have your team just think and feel
and act in a different cultural
environment so let's have a look at some
of the tools that are available to you
in devops environments every look at
devops it's really kind of split into
two areas the dev side you have building
code planning and testing and then on
your upside you have release deployment
operating and monitoring but the tools
really all interact with each other and
what's really great is that the tools
are either open source or very very low
in cost and in fact most of the tools
that you're seeing in front of you right
now are open source tools which means
there's no licensing to you and you can
actually effectively manage them and
Implement them within your team right
now so let's have a look at some of the
dev tools that you have for doing code
versioning and so some of the tools that
you may have used in the past include
subversion team Foundation serves and
git these are all different types of
Version Control software that you have
out there one of the oldest ones is
subversion it's a centralized Version
Control System it's one of the tools I
used years ago it really is good at
doing what it's supposed to to do which
is just a very simple version controlled
solution it's open source so it's free
there's no licensing there are
challenges with working with it because
it is a centralized tool rather than a
distributed tool we'll get into that in
a little bit but for a very basic
Version Control System you know it does
what you wanted to do team Foundation
Service Solutions something that many of
you in Microsoft world may have been
using for a long time and the thing
that's great about TFS is it's built
right into the Microsoft environment
it's the server side the services side
of building out your Solutions with
Microsoft just recently in the last
couple of weeks at the beginning of
September 2018. Microsoft actually just
rebranded TFS as Azure devops and so you
have Azure pipelines and there are five
now Azure tools that you replace TFS but
these are all tools that are very
similar in concept the old TFS tools are
very similar to the centralized SVN
tools whereas the new tools that are
part of azure are actually much more
more lines with get so what you're
seeing is Microsoft moving from the
centralized server to a distributed
server with their new Azure devops tools
which is fantastic news for every
developer listening to this and then we
have get and get is a distributed
Version Control System it is again open
source which means that you can start
using it right now without fear of
having to have any costs or any
penalties and the thing that's really
good about it is that it's really you
can use it for almost any kind of
digital project and what it's good at is
being able to create that historical
record and versioning of your source
code whether you're doing a web
application a mobile application or
you're actually building a python script
for a machine learning solution so let's
dig into what version control systems
are and how they can be of value to you
so the role of a version control system
is to allow developers to be able to
check in their files into a repository
auditory and here you get to see how we
can check in three files into a
repository and that repository then
becomes a snapshot or a historical
record of the files that we're working
on and you want to be able to have it so
that the repository is flexible enough
so that as you want to be able to scale
and add in new files or new versions of
a file that you can actually do that
easily within your VCS environment and
the goal is to be able to constantly
have each of these versions available so
that anybody wants to be able to check
out the file can actually have the
latest version of see a history of how
that file became what it is today let's
look at centralized Version Control
Systems and then we'll compare it
against distributed Version Control
Systems so a centralized version control
system has a central server where all
the files are stored and everybody has
to check in and check out from that
centralized server and all of the
versions are managed within that server
environment the problem is is that if
that Central server crashes which
doesn't happen but very often but it can
happen you end up losing all of your
files and I was actually on a project
where that actually happened to us and
we lost six months of history of how the
application was created fortunately for
us we actually had a backup but it
didn't have all the historical data so
we were able to at least continue
working but we just lost a lot of files
and it took a while to get back into
place so let's now look at and compare
this to a distributed version control
system with a distributed Version
Control System you still have a
centralized server that manages the
files but the difference is is that as a
developer you check out all of the files
for a project so you can actually manage
the whole project locally on your
development machine and make your
changes and then you can just check in
and out the changes that you've made to
the centralized server the opportunity
you have here is that the server itself
if it goes down you're not going to be
in a situation where you lose all of the
history because everybody that's working
on the actual application has all of the
versions of the code locally on their
machine now one of the more popular
distributed Version Control Systems that
out there is git I mean it really is
probably the most popular by a margin
compared to a system out there so you
know let's dig into you know what
actually is get so git is a version
control system for managing files and as
a developer you have a get client on
your machine and you're able to make
changes and create local repositories of
a program on your computer and then you
can sync up with a remote service such
as GitHub or gitlab or the new Azure
Labs environment where you can actually
store files remotely and then allow
remote teams to have access to those
changes in the actual application you
made so you can track your changes
easily you have it's the the tool is
inherently distributed so it makes it
very easy to manage the code for large
teams and you can bring people in very
quickly and you can have people come in
as Specialists and spin them up quickly
and have them work on a piece of the
code and then drop them out of the
project if you want to have a check on
how this works go and start contributing
to any of the the many thousands of Open
Source projects at GitHub where you can
go in with your get tools and just
contribute to those projects and then
finally one of the things that I really
like with Git is that it's not a linear
development approach it's not starting
from a and going into Z it's a
non-linear approach which allows you to
have branches that people are working on
in parallel to your master Branch where
people are actually doing the delivery
of the production code so let's step
through some of these features so git as
you can imagine the first thing it does
is track history that's probably the
most important part of having a Version
Control System it's a free open source
you can actually go and use it right now
there are no costs for actually using it
it is a non-linear environment that
allows people to be able to build out
new features in parallel to the master
Branch so that you can be constantly
having the master code out there without
and adding in new features without
breaking the code and you can
automatically create backups by because
each developer has a version of the code
remotely it's incredibly scalable some
of the biggest projects out there now
are being managed through git and
because of this scale the collaboration
becomes a byproduct of teams working
together and branching just is so much
easier I would then get so as you can
see this whole thing leads to a very
effective distributed environment so
let's step through some of the the
workflow that we have available for git
so the way that git works is that you as
an operator as a developer Alpha you
have the get client on your local
development machine whether it's a Linux
machine a Mac or a PC and you connect to
a remote server and you pull down the
latest working copy and then you're able
to make all of your changes uh locally
and you can modify the codes you can
review changes and you can commit those
codes to your git repository that you
have locally and then you can push those
changes back up to the remote server and
as soon as you push them into the remote
server those changes then become
available to everybody else working on
the project so they can be constantly
keeping everybody updated on all the
work that is happening so you know
typically the way that it would work is
you know you would start off with having
the working filed working directory that
you have locally and then you would
stage those files into a staging area
and then you get those files ready to be
committed to your git repository and
these are all actions that you would do
locally and then connect to a remote
server and then later when you go go
back into work on your project the first
thing you should always do is check out
that code so you always have the latest
version of the code and everybody's kept
in sync so let's talk about branching
and get so the way that branches work is
that if you imagine you're working on a
project and projects always have a
tendency of getting bigger than you
imagine and what you want to be able to
do is keep that main product working and
keeping it working effectively but if
you want to be able to add a feature to
that product you may have two or three
different groups that are working on
multiple features
simultaneously and what you want to be
able to do is give them the freedom to
work on those individual features and
the way you do it in git is that you can
create branches off the master and while
you're um of the the main branch which
is called the master and so each person
could be working on their own separate
branch and then when they want to they
can then later merge those branches back
into the main master and all this time
the main Master still works and still
able to produce the right code for the
customer but at the same time it allows
the developers the freedom to be able to
write in their new features and so we
can look at this in a little bit more
visually with the master branch and we
can put in small features and large
features so we can then merge them back
as we need to so this kind of covers
some of the commands that we have in git
so the First Command you'd want to use
is called get init and that's allow you
to go into a folder on your local PC and
or your local Mac and convert that file
that folder into a local git repository
and that creates that repository then
you want to be able to make changes to
that repository and the commands you'd
be using in this instance would be add
commit or status and you want to be able
to sync your repositories with a remote
server so be able to put the code that
you have on your computer on the remote
server you push you're going to get the
code from the remote server you'd use
pull and then add origin and then if you
want to do parallel development from the
master Branch the main branch you would
use Branch merge or rebase as ways to be
able to do parallel development and now
we're going to go through and do a demo
on git so we're going to do a demo on
get so you can feel comfortable running
the commands in the interface and you're
able to get get running correctly in
your environment so the first thing
we're going to do is we're going to set
up which version of git that you have
and then we're going to see if we can
establish some Global configurations
within your git environment and we're
going to do sandeep. as the name email
address is sandeep.simplylearn and then
we're going to do a list of the
different configuration settings we have
so to check the version of git that we
have we do git dash dash version in our
terminal or command line window you can
also use Powershell that'll actually get
you running in the same environment as
well all of these tools are going to be
command line tools and then the next
thing we're going to do is we're going
to assign some Global use usernames and
a global email address so that we can
actually access the get account itself
locally on the device and so we're going
to Dash and we're going to put in config
dash dash Global user.name Sandeep and
then a git
config.g
globaluser.email and
sandeep.d at simplylearn.net and now
we're going to check the list of
usernames and email IDs in our
configuration environment with Dash list
and you'll see that we have everything
in there correctly here is our username
right here and our name and our email ID
so if you need any help when you're
actually doing any of your work and get
all you have to do is to get help config
or git config dash dash help and this
will allow you to actually get access to
the help screens so let's just go and
type that in git help config return and
this takes us to the help page that's
running locally on your device and here
we have a breakdown of all the different
commands and what those commands
actually do within a git configuration
and we can go ahead and we can do git
config dash dash help and that will
actually take us to the same page so two
ways of doing the same thing so we're
going to go ahead and we're going to
create a test repository on our local
system and we're going to do that with
make directory called test and then
we're going to move our cursor within
command window terminal window to the
test folder and then we're going to
initialize that folder to become a git
repository so let's go ahead and do that
so we're going to make a new directory
and we're going to call it test and
we're going to move the cursor so it's
actually in the test folder we do CD
change directory to test and you can see
now we're actually in the test folder
and now all you have to do is initialize
this folder as a git repository and so
we're going to do this as git init and
it's now a new get instance that we can
actually now use for managing our git
environment so we're going to create a
new file and we're going to call it
info.txt and we're going to put some
content in it and we're actually going
to put it in the folder that we've just
created so you can actually see what
it's like to test out the get
environment with a file that hasn't been
checked into it yet so let's go ahead
and do that so we open up our photo
directory and here you can see we have
the test folder and we're just going to
go ahead and create a default test file
so right click Text new document
info.txt open that and we'll just you
can put really whatever you want in here
we're just going to put in this
information right now name equals Sam
registration number one two
479 and save that file close it out now
if we actually go ahead and run a get
status command you'll see that the
info.txt file is in red that's because
that file has not actually been checked
into the project it's a new file that
we've added but it hasn't been committed
yet into the git repository so let's go
ahead and see what it takes to actually
add the file to the git repository that
we just created so we're going to do
that by doing get add info.txt and then
we can commit that into the history of
that git repository so we can do so it
now has Version Control and we can start
doing uh forking and other kind of sim
activities when we connect later to our
GitHub git environment so we do get add
info Dot txt and remember we're actually
still in the test folder so it's going
to look for that file in that folder and
just add it in and now if we go ahead
and do get status we'll actually be able
to see that the info.txt file is in a
active color first of all you have to
commit it so we go get commit Dash M and
then we're going to commit and we're
adding some quick text you can write
whatever you want committing the text
file it's now actually committed into
the get repository and it's all running
locally on your PC so we're going to go
make some changes to the file and we're
going to save it and we want to be able
to see how we can use git to compare the
differences between the two files so
let's go over and we're going to make
some changes to the info.txt file here
we are here's info.txt which is going to
add in a new line we're going to add in
the line address and we'll save that
file and now when we go over to get
we're going to be able to review the
difference so we're going to do git div
and here you actually see that we've
added in a new address file so now you
can actually see this is the difference
and the difference between the two
documents now as you can imagine when
you have code this would become more
elaborate where actually Google should
take show you code that's been pulled
out if you've run reduced any of
eliminate amazing code or if you added a
new code really useful so we're going to
go ahead and add our get username to a
remote location so we can actually start
testing out the file that we've created
into a remote repository in this case
we're going to use GitHub and you'll be
able to see how we can actually then
save that file into a git repository
that somebody else could then access and
be able to make edits to so let's just
go ahead and make those changes so git
config Dash Global user dot username and
we'll do simply learn and that will
associate it with this git repository
that we've just created locally so
simply on dash GitHub so simply learn
GitHub is the username that we use on
GitHub so we're going to go over to
GitHub here we are in GitHub and then
correct new Repository
and then as you can see we're simp so
we're in simply learn Dash GitHub and
we're going to call the new repository
test we're going to make it public so
anybody can access it and you'll see
that it automatically adds a readme file
and we have our test environment set up
correctly so we'll go ahead and copy the
URL link which is github.com simplylearn
GitHub for test.get copy that and we'll
go back over to our command line window
and so now we're going to add the
username to the GitHub configuration so
what we're doing is we're connecting the
local repository to the remote
repository so we type get remote add
origin and then we'll paste in the
address and this then allows us to
connect the local repository that we
created in the test folder with the
remote repository and now we're
connected so now that we've connected we
can actually push the file that we have
in the local repository on your PC or
Mac to the remote repository in the
GitHub server environment so let's go
ahead and do that so we're going to do
get push and push is the command to push
the documents from the origin which is
the local file to master and Masters a
remote file okay and there we are
success so what we have now is the file
that we just created in our local get
repository is actually now in the remote
repository as well so anybody can access
it so let's go have a look at that so
we're going to refresh the GitHub page
and hey there we are there's info.txt
has been uploaded and we're ready to go
that's great that's uh that's what you
should be seeing so refreshing your web
page on the remote GitHub folder and it
could be any get service which is
happening to be using GitHub because
it's free to use you'll be able to see
that the local file has been installed
and is now part of the git repository in
the remote server so what we can do now
is we're going to create three more
files and we're just going to call them
info.txt info 2 and info three and then
we're going to push them out to the
remote server and we're going to merge
everything together into a single
environment so let's go ahead and create
create those three files and we're just
going to add those into the folder so
let's go ahead create new text document
info one and then we're going to create
info two and then info three get my
typing right here okay here we go there
we are okay I'm opening info three and
I'm just gonna enter some text in here
and what we're going to do is illustrate
how we can do branching and each of
these files I'm going to save and then
close out and now I can go over and so
let's create a new branch and we're
going to call this one first underscore
branch and we go get Branch first
underscore branch and this will be a new
branch of the code environment that
we're creating and so hit return and
that allows us to create a new branch
and so what we do is we're going to move
to the new Branch so we can do get
checkout first underscore branch and
that allows us to move into that branch
and you'll see that info.txt is already
there which is good that's what we want
to see now what we need to do is add in
the new documents so here we have the
different steps we've taken we created a
branch we moved to the new branch and
now we're going to go ahead and add the
info 3 txt to that Branch so you can
actually see everything coming together
and we're going to go get add info 3.txt
so we're just adding a file like we were
previous see with our initially our
first initial git folder and that file
has now been added to the first Branch
so we're going to go ahead and commit
the file to the first branch and then
we're going to merge the documents into
the master Branch so we have everything
together so let's go ahead and do those
steps we're going to do the commit first
and then we're going to merge everything
together right so we do get commit Dash
M and we'll just put in we're just going
to say uh made changes to First branch
and that's just a documentation so that
we know what we've done and there we are
we've committed the file so we had that
file info three txt committed and now
we're going to go ahead and merge the
files into the main master and so we can
actually have everything together as one
consistent environment and we do that by
typing in so this list out what we have
in our Master file so we just do LS and
that will show everything in the folder
and we have info.txt info 1 txt info 2
txt and info 3txt are now all in the
first Branch so let's go ahead and we're
going to merge okay so actually before
we merge we have to check out the master
Branch so we do get checkout master so
it gets us into the master branch and
you'll see that we have just one file
there info.txt so we're going to list
out the files in the master branch and
so so what you'll see is info dot text
info one text and info two texts are
there but info dot info 3 text isn't
there because that was created in a
separate Bunch so let's go ahead and
merge the first Branch into the master
Branch so we do git merge and we do
first underscore branch and there we are
we've merged it in and excellent that
looks good so let's go ahead and list
out the files that we have in the master
branch and we do LS and then you'll see
that we have four files including the
info three dot tags that we had in a
separate Branch now all in the master
Branch so what are we going to cover
today so we're going to introduce the
concept of Version Control that you will
use within your devops environment then
we'll talk about the different tools
that are available in a distributed
Version Control System we'll highlight a
product called get which is typically
used for Version Control today and
you'll also go through what are the
differences between get and GitHub you
may have used GitHub in the past or
other products like getlab and we'll
explain what are the differences between
get and get and services such as GitHub
and gitlab will break out the
architecture of what a get process looks
like and how do you go through and
create forks and clones how do you have
collaborators being added into your
projects how do you go through the
process of branching merging and
rebasing your project and what are the
list of commands that are available to
you in get finally I'll take you through
a demo on how you can actually run get
yourself often in this instance use the
software of git against a public service
such as GitHub all right let's talk a
little bit about Version Control Systems
so you may have already been using a
virtual control system within your
environment today you may have used
tools such as Microsoft's team
Foundation services but essentially the
use of a Version Control System allows
people to be able to have files that are
all stored in a single repository so if
you're working on developing a new
program such as a website or an
application you would store all of your
Version Control software in a single
repository now what happens is that if
somebody wants to make changes to the
code they would check out all of the
code in the repository to make the
changes and then there would be an
addendum added to that so um there will
be the version one changes that you had
then the person would then later on
check out that code and then be a
version 2 and added to that code and so
you keep adding on versions of that code
the bottom line is that eventually
you'll have people being able to use
your code and that your code will be
stored in a centralized location however
the charge you're running is that it's
very difficult for large groups to work
simultaneously within a project the
benefits of a VCS system a Version
Control System should demonstrate that
you're able to store multiple versions
of the solution any single repository
now let's take a step at some of the
challenges that you have with
traditional Version Control Systems and
see how they can be addressed with
distributed Version Control so in a
distributed Version Control environment
what we're looking at is being able to
have the code shared across a team of
developers so if there are two or more
people working on a software package
they need to be able to effectively
share that code amongst themselves so
that they constantly are work working on
the latest piece of code so a key part
of a distributed Version Control System
that's different to just a traditional
version control system is that all
developers have the entire code on their
local systems and they try and keep it
updated all the time it is the role of
the distributed VCS server to ensure
that each client and we have a developer
here and developer here and developer
here and each of those our clients have
the latest version of the software and
then that each person can then share the
software in a peer-to-peer like approach
so that as changes are being made into
the server of changes to the code then
those changes are then being
redistributed to all of the development
team the tool to be able to do an
effective distributed VCS environment is
get now you may remember that we
actually covered get in a previous video
and we'll reference that video video for
you so we start off with our remote git
repository and people are making updates
to the copy of their code into a local
environment that local environment can
be updated manually and then
periodically pushed out to the git
repository so you're always pushing out
the latest code that you've code changes
you've made into the repository and then
from the repository you're able to pull
back the latest updates and so your git
repository becomes the kind of the
center of the universe for you and then
updates are able to be pushed up and
pulled back from there what this allows
you to be able to accomplish is that
each person will always have the latest
version of the code so what is get get
is a distributed Version Control tool
used for source code management so
GitHub is the remote server for that
source code management and your
development team can connect their get
clients to that that certain remote Hub
server now git is used to track the
changes of the source code and allows
large teams to work simultaneously with
each other it supports a non-linear
development because of thousands of
parallel branches and has the ability to
handle large projects efficiently so
let's talk a little bit about git versus
GitHub so get is a software tool whereas
GitHub is a service and I'll show you
how those two look in a moment you
install the software tool for get
locally on your system whereas GitHub
because it is a service it's actually
hosted on a website git is actually the
software that used to manage different
versions of source code whereas GitHub
is used to have a copy of the local
repository stored on the service on the
website itself git provides command line
tools that allow you to interact with
your files whereas GitHub has a
graphical interface that allows you to
check in and check out files so let me
just show you the two tools here so here
I am at the git website and this is the
website you would go to to download the
latest version of git and again git is a
software package that you install on
your computer that allows you to be able
to do Version Control in a peer-to-peer
environment for that peer-to-peer
environment to be successful however you
need to be able to store your files in a
server somewhere and typically a lot of
companies will use a service such as git
Hub as a way to be able to store your
files so git can communicate effectively
with GitHub there are actually many
different companies that provide similar
service to GitHub gitlab is another
popular service but you also find that
development tools such as Microsoft
Visual Studio are also incorporating git
commands into their tools so the latest
version of Visual Studio team Services
also provides this same ability but
GitHub it has to be remembered is a
place where we actually store our files
and can very easily create public and
shareable is a place where you can store
our files and create public shareable
projects you can come to GitHub and you
can do a search on projects you can see
at the moment I'm doing a lot of work on
blockchain but you can actually search
on the many hundreds of projects here in
fact I think there's something like over
a hundred thousand projects being
managed on GitHub at the moment that
number is probably actually much larger
than that and so if you are working on a
project I would certainly encourage you
to start at GitHub to see if somebody's
already maybe done a prototype that
they're sharing or they have an open
source project that they want to share
that's already available
um in GitHub certainly if you're doing
anything with um Azure you'll find that
there are thousands 45
000 Azure projects currently being
worked on interestingly enough GitHub
was recently acquired by Microsoft and
Microsoft is fully embracing open source
Technologies so that's essentially the
difference between get and GitHub one is
a piece of software and that's get and
one is a service that supports the
ability of using the software and that's
GitHub so let's dig deeper into the
actual git architecture itself so the
working directory is the folder where
you are currently working on your get
project and we'll do a demo later on
where you can actually see how we can
actually simulate each of these steps so
you start off with your working
directory where you store your files and
then you add your files to a staging
area where you are getting ready to
commit your files back to the main
branch on your git project you want to
push out all your changes to a local
repository after you major changes and
these will commit those files and get
them ready for synchronization with the
service and will then push your services
out to the remote repository an example
of a remote repository would be GitHub
later when you want to update your code
before you write any more code you would
pull the latest changes from the remote
repository so that your copy of your
local software is always the latest
version of the software that the rest of
the team is working on one of the things
that you can do is as you're working on
new features within your project you can
create branches you can merge your
branches with the mainline code you can
do lots of really creative things that
ensure the that Aid the code remains at
very high quality and B that you're able
to seamlessly add in new features
without breaking the core code so let's
step through some of the concepts that
we have available and get so let's talk
about forking and cloning and kit so
both of these terms are quite old terms
when it comes to development but forking
is certainly a term that goes way way
way back long before we had distributed
CVS systems such as the ones that we're
using with get to Fork a piece of
software is a particularly open source
project you would take the project and
create a copy of that project and but
then you would then associate a new team
and new people around that project so it
becomes a separate project in entirety a
clone and this is important when it
comes to working with get a clone is
identical with the same teams and same
structuring as the main project itself
so when you download the code you're
downloading exact copy of that code with
all the same security and access rights
as the main code and then you can then
check that code back in and potentially
your code because it is identity could
potentially become the mainline code in
the future now that typically doesn't
happen your changes are the ones that
merge into the main branch but also but
you do have that potential where your
code could become the main code with Git
You can also add collaborators that can
work on the project which is essential
for projects where particularly where
you have large teams and this works
really well when you have product teams
where the teams themselves are
self-empowered you can do a concept
what's called branching in git and so
say for instance you are working on a
new feature that new feature and the
main version of the project have to
still work simultaneously so what you
can do is you can create a branch of
your code so you can actually work on
the new feature whereas the rest of the
team continue to work on the main branch
of the the project itself and then later
you can merge the two together pull from
remote is the concept of being able to
pull in Services software the team's
working on from a remote server and git
rebase is the concept of being able to
take a project and re-establish a new
start from the project so you may be
working a project where there have been
many branches and the team has been
working for quite some time on different
areas and maybe you kind of losing
control of what the true main branch is
you may choose to rebase your project
and what that means though is that
anybody that's working on a separate
Branch will not be able to Branch their
code back into the mainline Branch so
going through the process of a get
rebase essentially allows you to create
a new start for where you're working on
your project so let's go through forks
and clones so you want to go through the
process so you want to go ahead and Fork
the code that you're working on so this
is this scenario that one of your team
wants to go ahead and add a new change
to the project the team member may say
yeah go ahead and you know create a set
separate Fork of the actual project so
what does that look like so when you
actually go ahead and create a fork of
the repository you actually go and you
can take the version of the mainline
Branch but then you take it completely
offline into a local repository for you
to be able to work from and you can take
the mainline code and you can then work
on a local version of the code separate
from the mainline Branch it's now a
separate Fork collaborators is the
ability to have team members working on
a project together so if you know if
someone is working on a piece of code
and they see some errors in the code
that you've created none of us are
perfect at writing code I know I've
certainly made errors in my code it's
great to have other team members that
have your bag and can come in and check
and see what they can do to improve the
code so to do that you have to then add
them as a collaborator now you do that
in GitHub you can give them permission
within GitHub itself and it's really
easy to do super Visual and interface
that allows you to do the work quickly
and easily and depending on the type of
permissions you want to give them
sometimes it could be very limited
permissions it may be just to be able to
read the files sometimes it's being able
to go in and make all the changes you
can go through all the different
permission settings on GitHub to
actually see what you can do but you'll
be able to make changes so that people
can actually have access to your
repository and then you as a team can
then start working together on the same
code let's step through branching and
get so suppose you're working on an
application but you want to add in a new
feature and this is very typical within
a devops environment so to do that you
can create a new branch and build a new
feature on that Branch so here you have
your main application on what's known as
the master branch and then you can then
create a sub branch that runs in
parallel which has your feature you can
then develop your feature and then merge
it back into the master Branch at a
later point in time now know the benefit
you have here is that by default we're
all working on the master Branch so we
always have the latest code the circles
that we have here on the screen show
various different commits that have been
made so we can keep track of the master
branch and then the branches that have
come off which have the new features and
there can be many branches in git so git
keeps you the new features you're
working on in separate branches until
you're ready to merge them back in with
the main branch so let's talk a little
bit about that merge process so you're
starting with the master branch which is
the blue line here and then here we have
a separate parallel branch which has the
new features so if we're to look at this
process the base commit of feature B is
the branch f is what's going to merge
back into the master branch and it has
to be said there can be so many
Divergent branches but eventually you
want have everything merge back into the
master Branch let's step through git
rebase so again we have a similar
situation where we have a branch that's
being worked in parallel to the master
branch and we want to do a get rebase so
we're at stage C and what we've decided
is that we want to reset the project so
that everything from here on out with
along the master branch is the standard
product however this means that any work
that's been done in parallel as a
separate Branch we'll be adding in new
features along this new rebased
environment now the benefit you have by
going through the rebase process is that
you're reducing the amount of storage
space that's required for when you have
so many branches it's a great way to
just reduce your total footprint for
your entire project so get rebase is the
process of combining a sequence of
commits to form a new base commit and
the prime reason for rebasing is to
maintain a linear project history when
you rebase and you unplug a branch and
re-plug it in on the tip of another
branch and usually you do that on the
master branch and that will then become
the new Master Branch the goal of
rebasing is to take all the commits from
a feature branch and put it together in
a single Master Branch it makes it the
project itself much easier to manage
let's talk a little bit about pull from
remote suppose our two developers
working together on application the
concept of having a remote repository
allows the code to the two developers
will be actually then checking in their
code into a remote repository that
becomes a centralized location for them
to be able to store their code it
enables them to stay updated on the
recent changes to the repository because
they'll be able to pull the latest
changes from that remote repository so
that they are ensuring that as
developers are always working on the
latest code so you could pull any
changes that you have made to your fault
remote repository to your local
repository the command to be able to do
that is written here and we'll go
through a demo of how to actually do
that command in a little bit good news
is if there are no changes you'll get a
notification saying that you're already
up to date and if there is a change it
will merge those changes to your local
repository and you get a list of the
changes that have been made remotely so
let's step through some of the commands
that we have in get so get and it
initializes a local git repository on
your hard drive get ads one or more
files to your staging area get commit
Dash M commit message is a commit
changes to the git command commits
changes to head up to the git command
commits changes to your local staging
area get status checks the status of
your current repository and lists the
files you have changed get Block
provides a list of all the commits made
on your current brand launch get Tiff
the user changes that you've made to the
file so you can actually have files next
to each other you can actually see the
differences between the two files get
push origin Branch name so the name of
your branch command will push the branch
to the remote repository so that others
can use it and this is what you do at
the end of your project git config Dash
Global username or tailgate Who You Are
by configuring the author name we'll go
through that in a moment git config
Global user email will tell get the
author of by the email ID git clone
creates a get repository copy from a
remote Source get remote ad origin
server connects the local repository to
the remote server and adds the server to
be able to push to it git branch and
then the branch name will create a new
Branch for you to create a new feature
that you may be working on get checkout
and then the branch name will allow you
to switch from one branch to another
brand Branch git merge Branch name Will
merge a branch into the active Branch so
if you're working on a new feature
you're going to merge that into the main
branch a get rebate will reapply commits
on top of another base Tab and get
rebase will reapply commits on top of
another base tip and these are just some
of the popular git commands there are
some more but you can certainly dig into
those as you're working through using
get so let's go ahead and run a demo
using get so now we are going to do a
demo using get on our local machine and
GitHub as the remote repository for this
to work I'm going to be using a couple
of tools first I'll have the deck open
as we've been using up to this point the
second is I'm going to have my terminal
window also available and let me bring
that over so you can actually see this
and the terminal window is actually
running git bash as the software in the
background which you'll need to download
and install you can also run get batch
locally on your Windows computer as well
and in education I'll also have the
GitHub repository that we're using for
simply learn and already set up and
ready to go all right so let's get
started so the first thing we want to do
is create a local repository so let's go
ahead and do exactly that so the local
repository is going to reside in my
development folder that I have on my
local computer and for me to be able to
do that I need to create a drive in that
folder so I'm going to go ahead and
change the directory so I'm actually
going to be in that folder before I
actually create make the new folder so
I'm going to go ahead and change
directory
and now I'm in the development directory
I'm going to go ahead and create a new
folder
and let's go ahead and created a new
folder called hello world
I'm going to move my cursor so that I'm
actually in the hello world folder
and now that I'm in the hello world
folder I can now initialize this folder
as a get Repository
so I'm going to use the get command init
to initialize and let's go ahead and
initialize that folder so let's see
what's happened so here I have my Hello
wall folder that I've created and you'll
now see that we have a hidden folder in
there which is called dot gate now we
expand that we can actually see all of
the different subfolders that git
repository will create so let's just
move that over a little bit so that we
can see the rest of the work
and now if we check on our folder here
we'll actually see this is users Matthew
development hello world dot get and that
matches up with hidden folder here
so we're going to go ahead and create a
file called readme.txt in our folder so
here is our hello world folder and I'm
going to go ahead and using my text
editor which happens to be Sublime
I'm going to create a file and it's
going to have in there the text hello
world and I'm going to call this one
readme.txt
if I go to my Hello World folder you'll
see that we have the readme.txt file
actually in the folder what's
interesting is if I select the get
status command what it'll actually show
me is that this file has not yet been
added to the commits yet for this
project so even though the file is
actually in the folder it doesn't mean
that it's actually part of the project
for us to do that we actually have to go
and select
foreign
for us to actually commit the file we
have to go into our terminal window and
we can use the get status to actually
read the files that we have there so
let's go ahead and use the git status
command and it's going to tell us that
this file has not been committed you can
use this with any folder to see which
files and subfolders haven't been
committed and what we can now do is we
can go and actually add the readme file
so let's go ahead I'm just going to
select get add so the git command is ADD
[Music]
readme.txt so that then adds that file
into our main project and we want to
then commit those files into the main
repositories history and so it's that do
that we'll hit the the get command
commit and we'll do a message in that
commit and this one will be
first commit
and it has committed that project what's
interesting is we can now go back into
readme file and I can change this so we
can go hello get
is a very popular
Version Control solution
and we'll
we'll save that now what we can do is we
can actually go and see if we have made
differences to the readme text so to do
that we'll use the disk command forget
so we do get
diff
and it gives us two releases the first
is what the original text was which is
hello world and then what we have
afterwards is what is now the new text
in green which has replaced the original
text
so what we're going to do now is you
want to go ahead and create an account
on GitHub we already have one so what
we're going to do is we're going to
match the account from GitHub with our
local account so to do that we're going
to go ahead and say get config
and we're going to do Dash and it's
going to be a
globaluser.name and we'll put in our
username that we use for GitHub and this
instance we're using the simply learn
Dash
GitHub account name
and under the GitHub account you can go
ahead and create a new repository name
in this instance we called the
repository hello dash world
and what we want to do is connect the
local GitHub account with the remote
hello world.get account and we do that
by using this command from get which is
our remote connection and so let's go
ahead and type that in open this up so
we can see the whole thing so we can
type in git remote add origin https
GitHub
.com slash
simply learn
Dash GitHub and you have to get this
type in correctly when you're typing in
the location hello dash world dot get
that creates the connection to your
hello world account
and now we want to do is we want to push
the files to the remote location using
the get push command commit git push
origin
master
so we're going to go ahead and connect
to our local remote GitHub so I'm just
going to bring up my terminal window
again and so let's select get remote add
origin
and we'll connect to the remote location
github.com slash
simply learn
Dash GitHub
slash
hello dash world dot get
oh we actually have already connected so
we're connected to that successfully and
now we're going to push the master Gish
so get
push origin
master and everything is connected and
successful
and if we go out to GitHub now
we can actually see that our file was
updated just a few minutes ago
so what we can actually do now is we can
go and Fork a project from GitHub and
clone it locally so we're going to use
the fork tool that's actually available
on GitHub let me show you where that is
located and here is our branching tool
it's actually changed more recently with
a new UI interface
and once complete we'll be able to then
pull a copy of that to our account using
the fox new HTTP URL address
so let's go ahead and do that
so we're going to go ahead and create a
fork of our project now to do that you
would normally go in when you go into
your project you'll see that there are
Fork options in the top right hand
corner of the screen now right now I'm
actually logged in with the default
primary count for this project so I
can't actually Fork the project as I'm
working on the main branch however if I
come in with a separate ID and here I am
I have a different ID and so I'm
actually pretending I'm somebody else I
can actually come in and select the fork
option and create a fork of this project
and this will take just a few seconds to
actually create the fork
and there we are we have gone ahead and
created the fork
so you want to say clone or download
with this and so this is the I select
that actually give me the web address I
can actually show you what that looks
like I'll open up my text editor
just that's not correct
I guess that is correct so I'm going to
copy that
and
I can Fork the project locally and clone
it locally I can change the directory so
I can create a new directory that I'm
going to put my files in and then post
in that content into that file so I can
now actually have multiple versions of
the same code running on my computer
I can then go into default content and
use the patchwork command
20
so I can create a copy of that code that
we've just created and we call it that's
a clone and we can create a new folder
that we're actually putting the work in
and we could for whatever reason we
wanted to we could call this where
folder Patchwork and that would be maybe
a new feature and then we can then paste
in the URL of the new directory that has
the forked and work in it and now at
this point we've now pulled in and
created a clone of the original content
and so this allows us to go ahead and
Fork out all of the work for our project
onto our computer so we can then develop
our work separately
so now what we can actually do is we can
actually create a branch of the fork
that we've actually pulled in onto our
computer so we can actually then create
our own code that runs in that separate
branch
and so we want to check out um the the
branch and then push the origin Branch
down to our computer
this will give us the opportunity to
then add our collaborators so we can
actually then go over to GitHub and we
can actually come in and add in our
collaborators
and we'll do that under settings and
select collaborators and here we can
actually see we have different
collaborators that have been added into
the project and you can actually then
request people to be added via their
GitHub name or by email address
or by their full name
one of the things that you want to be
able to do is ensure that you're always
keeping the code that you're working on
fully up to date by pulling in all the
changes from your collaborators
you can create a new branch and then
make changes and merge it into the
master Branch now to do that you would
create a folder and then that folder in
this instance would be called test we
would then move our cursor into the
folder called test and then initialize
that folder so let's go ahead and do
that so let's call create a new folder
and we're going to first of all change
our root folder and we're going to go to
development
create a new folder
call it test and we're going to move
into the test folder and we will
initialize
that folder
and we're going to move some files into
that test folder
close one test one
and then we're going to do file save as
and this one's going to be test
two
and now we're going to commit those
files
and
kit add and then we'll use the
dot to pull in all files
and then git commit
m
m files
Limited
make sure I'm in the right folder here I
don't think I was
and now that I'm in the correct folder
let's go ahead and
and get commit
and it's gone ahead and added those
files and so we can see the two files
that were created have been added into
the master
and we can now go ahead and create a new
Branch we call this one get branch
test underscore
branch
and let's go ahead and create a third
file to go into that folder
this is
file three
into file save as we'll call this one
test three dot text
and we'll go ahead and add
that file I need to get add
test three Dot txt
and we're going to move from the master
Branch to the test run branch
kit
check
out test underscore
branch
I switched to the test branch
and we'll be able to list out all of the
files that are in the tech in that
Branch now
and we want to go through and merge the
files into one area so let's go ahead
and we'll do get merge test underscore
branch
and it's well we've already updated
everything so that's good so otherwise
it would tell us what we would be
merging
and now all the files are merged
successfully into the master branch
there we go all my merged together
fantastic
and so what we're going to do now is
move from Master Branch to test branch
so get
check out
test underscore branch
and we can modify the files the test3
file that we took out
and pull that file up
and we can
now what if I
right
and we can then
commit
that file
back
in now we've actually been able to then
commit the file with one changes and now
we've seen as the text free change that
was made
now we can now go through a process of
checking the file back in switching back
to the master branch and ensuring that
everything is in sync correctly
we may at one point want to rebase all
the workers kind of a hard thing you
want to do but it will allow you to
allow for managing for changes in the
future so let's switch to it by to our
test branch
which I think we're actually on we're
going to create two more files
let's go to our folder here and let's go
copy those
and that's created
we'll rename those tests
four
and
five
and so we now have additional files
and we're going to add those into our
branch that we're working on so we're
going to go and select get add Dash a
and we're going to commit those files
get
commit Dash a dash m
adding
two new files
and it's added in the two new files
so we have all of our files now we can
actually list them out and we have all
the files that are in the branch
and we'll switch them to our Master
Branch we want to rebase the master
so we could do git rebase
master
and that will then give us the command
that everything is now completely up to
date
and we can go
get
check out
Master to switch to the master account
this will allow us to then continue
through and rebase the test branch and
then list all the files that are all in
the same area
so let's go get rebase
test underscore
branch
and now we can list and there we have
all of our files and before we begin if
you are someone who is interested in
building a career in devops by
graduating from the best universities
with a professional who elicits to
switch careers with devops by learning
from the experts then dragging a show to
Simply learn skeltec postgraduate
program in devops the course link is
mentioned in the description box that
will navigate you to the course page
where you can find a complete audio of
the program being offered so we'll start
by downloading and installing it on our
system we'll then have a look at the git
bash interface we'll type in some basic
git commands next we'll create a local
repository that is we create a
repository and a local machine we'll
then connect to a remote repository and
finally we'll push the file onto GitHub
first things first we need to download
and install git so download git for
Windows and I'll select the second link
so 2.19.1 which is the most latest
version of git that's the one we want
for Windows system choose your version
so mine is a 64-bit system and it's
downloading so this will take a while so
git is finally downloaded now we need to
install the sonar system
click here run
so go to next we don't have to change
this path uh just so just click on in
quick launch and on desktop next next
again next nothing to change here either
and install
so now git is getting installed on our
system
so we don't need to view the release
notes we just want to launch the git
bash so let's just tick that and then
click on finish
and your git bash interface appears here
so we are on the master Branch the first
thing we do is we'll check the version
for our git so the command is git dash
dash version and as you can see version
2.19.1 on our Windows system which is
exactly what we just downloaded we'll
now explore the help command so let's
just type get help config so config is
another command and as I hit enter the
manual page for the second command
opened up which is config so what help
command does is that it provides the
manual or the help page for the command
just following it so in case you have
any doubts regarding how a command is
used what a command is used for or the
various syntax of the command you can
always use the help command now this
another Syntax for using the help
command itself which is git config dash
dash help enter this does the exact same
thing as you can see
now that we looked at the help command
let's begin by creating a local
directory so mkdir test now test is my
new directory I'll move into this
directory so CD test
great so now that we are inside our test
directory let's initialize this
directory so get init is the command for
initializing the directory and as you
can see as you can see the path here
this is the local path where a directory
is created so I'll just show you the
directory test and it's completely empty
what we do now is we'll create a text
file within this new directory that we
created so new text document and I'll
just name this demo I'll open this and
just put in some dummy content so hello
simply learn
save this file and go back to your bash
interface let's just check the status
now so get status and as you can see a
file has appeared it's visible but
nothing is committed yet so this means
that we have not made any change to a
file through the git tool itself so the
next thing that we are going to do is
we'll be adding demo to a current
directory the next command that we'll be
applying is the commit command and when
you add certain files to the current
directory the comment command is applied
on all the above directories so git
commit minus M and a message that will
appear once the file is committed
so as you can see one file is changed
and one insertion I'll just clear the
screen next thing we need to do is we
need to link our git to our GitHub
account so the command for doing that is
git config Global user dot username
and this will be followed by our
username so let me just show you my
GitHub account
so this is my GitHub profile and my
username is simply learn Dash GitHub so
guys before you begin this procedure
just make a GitHub account type in my
username here simply learn Dash GitHub
and enter and there you go a git is
successfully linked with GitHub next
thing we do is we'll just open our
GitHub and we'll create a new repository
give a repository name so I'll get name
it test underscore demo and create
Repository
great so our repository is created this
is our remote repository what we do next
is just copy the link and then go back
to your bash interface
now we need to link our remote and a
local Repository
so get remote origin and then paste the
HTTP link and now that our local
repository and a remote repository are
linked we can push a local file onto a
remote repository so the command for
doing that is git push origin Master as
we are on the master branch
and that's done so now let's move back
to GitHub I'll just click on test demo
and as you can see here our local file
has been pushed to our remote repository
with that we have successfully completed
our demo if you have any queries
regarding this please post them in the
comment section below and we'll get back
to you as soon as possible today I'm
going to cover a few essential git
commands and walk you through a
demonstration for most of it I begin by
installing a git bash or the git client
and configuring it for its first time
usage once I'm done with configuring the
git client I'm going to spin up few
repositories locally and work on these
repositories using git commands once I'm
happy with my local changes I'm going to
commit few of them and then push these
changes to a git server of my choice for
today's demonstration I plan to use
github.com which is a cloud hosted free
service and it provides free account
registrations for anyone eventually I
will cover few topics regarding git
branching and merging because these two
in my opinion are the killer features of
the get distributed Version Control tool
kit is one of the most popular
distributed Version Control tools of
recent times and like any other
distributed Version Control tool it
allows us to perform various Version
Control tasks without really needing a
network connection before I jump into my
demonstration let me spend some time
explaining to you the git client server
working model what does it take for a
couple of users to collaborate and start
working with Git the bare minimal thing
that any user would need in order to
start working with Git is something
called as git bash or the git rind this
comes as an installer for all popular
operating systems like Windows all
flavors of Windows Linux Mac OS and
other operating systems so once you
install git bash you get a command line
utility using which you can fire up your
git commands and ensure that you can
bring up repositories you can work with
these repositories by adding files to it
committing changes to it and all those
git commands would work perfectly well
so you can bring up a small repository
and you can work with your repository
using this git bash but what does it
take for you to share this repository
with another user who has also got git
bash installed on his system this is
where you would need something called as
a git server so for user one or one user
to share his repository with another
user he would need to collaborate using
something called as a git server so in
the present Market there are a bunch of
git servers which are popularly
available some of them are free some of
them come with a cost because they are
licensed a bunch of these servers that I
can think of are GitHub which is one of
the most popular kids servers that is
around in the market for a long time now
it comes with two variants one is the
cloud hosted one which is the github.com
and then the other one is an Enterprise
server which comes as a black box that
can be installed into your data centers
so typically organizations who don't
want their source code to be put up on
the cloud but go for this GitHub
Enterprise servers wherein they buy this
servers and these servers are hosted on
their data centers other popular variant
of the git server is bitbucket this is
from the famous atlasian products and it
integrates very well with all other
atlassian products like jira in recent
times there's one other variant of the
git server called gitlab which is
getting very very popular these days
because gitlab not just provides a git
server it also provides something called
as a runner this is a part of the
continuous integration strategy where as
soon as you have a source code check-in
that is going in you have this Runner
that kicks up and builds your particular
project so all these are bundled
together in a gitlab and gitlab also
provides you with a Community Edition
which is almost free of cost having said
that there are a bunch of these tools
available for my demonstration what I'm
going to do is I've already registered
myself on the github.com by providing my
username and my password and I'm going
to use a free account so whatever
reposit is that I'm gonna put up on this
server is all public by default so I
want to register Myself by using my
username or my email ID and my password
so this set of authentication is also
referred to as https authentication so
in order for me to share my repository
with say user 2. the first thing that I
got to do is have an access to a git
server where both user 1 and user 2 are
set up so once I have that I can push my
repository onto this server and provide
right kind of access so that user 2 or
the other developer whoever wishes to
collaborate with me on this repository
can use his credentials as long as is
registered properly on the git server he
can use his credentials and pull down
this rep repository and work with this
repository so this is the https way of
authenticating and sharing repositories
another popular way of sharing
repositories or working with each other
is called as SSH authentication as many
of you would know SSH means nothing but
creation of a private and a public key a
bunch of keys public and private are
created on the client machine while I
create a keys I will be asked for a
passphrase I need to type in a
passphrase and remember by passphrase
and then I take this public key and kind
of post it on to my account within my
git server so that whenever I'm going to
connect next time using my command
prompt or any of the tools it's going to
challenge me for my passphrase as long
as I remember my passwords correctly I
can authenticate myself and get into the
server so both https and SSH are popular
ways of communicating with the git
server having said this let me just get
started by installing the git bash on my
local system and then fire up some of
these git commands and build up some
repositories make some changes in my
repository commit these changes and
later use my GitHub credential to
connect to my GitHub server and push my
repositories out there
let me now go ahead and find my git bash
for my windows so that I can go ahead
and install it so I just type in git
client for windows open up a browser and
type in for this the first one that I
find is a GUI tool I don't need this GUI
client I need this one so this is a git
client or the git bash that I was
referring to it's about 40 Megs and let
me just download this pause this video
for some time so that the download
happens and come back to you once I have
the download the download seems to be
complete when I'm recording this video
the version that I have the latest
version that is there is 2.19.0 and this
is a 64-bit version for Windows so I've
got that downloaded let me go ahead and
find it and install this EXE
all right I go ahead choosing all the
default options I want it to be
installed in SQL and program files under
the git folder everything looks good for
me I'm not going to make any changes
here
get Bash from Windows yes I may want to
use git from Windows command prompt that
sounds good for me open SSH that's fine
next card next card no I don't want any
of these new features
so what I'm doing is installing the git
bash or the git client so that I can
start using my git commands
and the version that I'm using is
2.19.0 which as I record this video this
happens to be the latest version
all right so looks good I don't want to
see the release notes I'll say again
launch git Bash
so what I get is this window so this is
the git command prompt so let me check
if everything is looking good it's a Git
Version it sounds good it's a git help
all right a lot of git help commands
everything looks good so I ended up
installing the git bash I can go ahead
with all other commands
now that I have git bash installed on my
system let me open up a git bash prompt
and start using it but before I do
anything I just need to configure It For
the First Time stating what would be the
username and the email ID with which I
want the git bash to be configured with
the command to do that is get config
hyphen iPhone Global which means that
this setting that I'm gonna set would be
a global setting user dot name and this
the name of the user would need to be
specified within a double code in my
case I am using a simply learn account
which is the official account and this
is the particular username of this
account the other setting is nothing but
git config hyphen iPhone Global this is
also a global setting
and this would be the email ID
the email ID need not contain a double
quote
so in my case this is the email ID that
I would be setting up it to work with
all right let me check if the
configurations were set correctly or not
all right if you see this the username
and the email ID has been picked up the
way I wanted it to be picked up looks
good so far all right so what do we do
next by default whenever you open up the
git prompt you're placed into your home
directory let me create a folder here
which would be a repository folder from
where I'm going to create all my
multiple git repositories so I will
create a folder called let's say git
underscore demo
all right
there's already a folder called git demo
so let me just delete that in the first
place and let me create this git demo
again
all right let me navigate to this
particular folder
so this is the folder let me open up
that folder using a command prompt
this is my get demo so this would be a
folder so I would use this folder as a
base folder for creating all other
repositories as a part of this demo so
let me create my first repository a
repository is nothing but a folder so
let me create my first folder called
first repo
and let me
get into this repo all right as of now
this is just an empty folder
if at all I navigate to this you could
see that this folder is empty there's
nothing much in this folder at all at
this moment okay so let me create a
repository out of this how do I create a
repository of this is pretty simple I
just run a git init command
when I say get init it means it's going
to initialize a repository as of now
the folder contained nothing in it was
an empty repository so it initially is
an empty repository in this particular
folder but all I look at my content you
would see that there's a hidden folder
this would be a hidden folder since I've
configured my Explorer to view all
hidden folders I see it otherwise you
wouldn't see it so a folder by name dot
get is created within this first repo
and this was created because we ran the
command kit init
if I get into this folder I see a bunch
of directories and other configurations
if you see there's something called as a
hooks directory a couple of
configuration files that are here in
this directory all these related to one
or the other type of a client hook that
can be enabled and this info this
objects
this references a bunch of stuff that is
there
so what is this folder this is a magical
git folder and this is created whenever
a repository is initialized so by
default if at all you get into any git
folder or any git repository you would
see this dot get folder a word of
caution do not get into this folder and
try to modify anything because if you
fiddle around with this and corrupt this
particular folder you hold repository
will go for a toss so let me come out of
this
and let me show you another thing now if
you notice here when I was in the folder
the repository did not have anything
after this now when I say git in it
there is something called as a master
that shows up so typically what happens
is whenever a git repository is created
for the first time it creates a default
Branch the name of the branch is called
master and that is why you would see
Master within braces this doesn't mean
anything else other than we have a
repository out in this particular folder
and by default there is a branch called
master and we are currently on that
Branch if at all I create multiple
branches in this folder whenever I
navigate to different branches the
branch name within this brackets would
change all right so far so good so we
created a folder we made it a git
repository by initializing that with the
git in it and then I'm not put anything
yet in the repository so let me try to
do that let me create a few files within
this particular repository so I use a
touch command touch is a Linux command
it kind of creates this particular
folder if not you're not comfortable
with the file creation by this way you
can always use a file explorer and
create some files in this so I create a
file called touch master.txt I'll open
up in the notepad
okay so this is my first file that's
what I'm going to write inside this
I save this and I come out of this all
right now let me do a git status command
so what does this set tell you there's
no commit set you're in the master
Branch there is an untracked file and
this shows up in red it also gives you
all the command that is supposed to be
run so if at all you want to add this
file you would say git add and the name
of the file and then you're going to
commit it
so what happened is that when the
repository was initialized it was an
empty repository now that a git notices
that there is a new file in this
repository called master.txt if at all
you want you want us to let git know
that you want to track this file we need
to add this to the repository so command
to add that is called git add I can
specify either the name of the file or I
can give a wild character saying git add
dot now if I run the git status command
again
it says there's no commits yet added a
file and the file which was earlier in
red now shows up in green and it says
it's a new file master.txt all right it
also gives you something called git
remove command if at all you want to
unstage or undo your changes so what has
happened is the file which was earlier
untracked has been added into the git
index or it is ready for staging or it's
in a staged State now I can go ahead and
comment this file if at all that's what
I want to do so let me do that
I'll do a kit commit minus M and I give
a message that I want I would say this
is my first get commit
all right I run the get status command
again and see it says on the branch
Master nothing to commit it's all nice
and clean so typically what happened was
I created a empty directory made it into
a git folder by doing a git in it I put
a file into it I made some changes to
the file I added that file into the
repository and I committed to the
repository if it were I run the git log
command
it says this was the commit ID or the
commit number or the commit hashtag and
this is the author if you notice this is
the email ID and the username that we
set earlier and this was done on this
date and this is the
commit message that is there all of it
looks so good let me do one more commit
into this repository so I will touch
Master One Dot text I'll put in another
file
I'll open up this particular file
write something into this
all right I save this change
let me also open up the older
file make some more change into this
all right so what I did I modified the
older file that existed I also added a
new file to my repository now if I run
the get status command
it says modified this file which is
showing in red and it says you have one
more file which is an untracked file
all right now let me add both these
files because these are the changes that
I want to
incorporate I would say git add dot I
run the git status command again
and it says now this is a modified file
this is a new file looks good to me let
me go ahead and Commit This so git
commit with the message let's say is
this is my second commit
all right
if I do a git log now it shows both the
commits the first comment is the one
that that shows up at the bottom and the
topmost one is the latest commit that is
there so far so good if at all you
notice
I'm doing all these git commands without
really connecting it to any git server I
don't need any network connection for
doing any of these things I can keep on
doing more and more git commands and run
git commands the way I want to
without really having any connections to
the kit server now that I have my
changes the way I wanted and I've
checked in all the code changes into my
local repository I may want to share my
repository with somebody else and work
in a collaborative way so the way to do
that is as I mentioned earlier to host
this repository or to push this
repository onto a git server I already
have a GitHub account wherein I've
registered myself using my username and
password so I'll log into my GitHub
account using my username and
credentials and let me go ahead and
create a new Repository
my repository is already there on the
local system so what I essentially need
is just a placeholder for my repository
so that I can link them together and
push the content from my local
repository to the server
if you notice the name of my repository
is his first repo
so let me copy this I would create a
folder structure or a repository
structure on the server with the same
name so that when I pushed my repository
from my local box it will go ahead and
get created in the server so the
repository name should be the same as
what I created on my local box
description I would say this is my first
get Hub Repository
on the server
this is just an optional description
that will show up and if you notice
I can't have any private repositories so
whatever repositories that I'm going to
create here would be a public repository
so I choose public and I don't want to
initialize this with a readme file or
anything of that because I already have
a repository with some content with some
code checked in all that I need is a
placeholder for my repository on the
server so with this option with just a
name and with an optional description
and the repository being public I go
ahead and click on the create repository
button
so if you notice it gives me a bunch of
commands that I would need to run in
order to create a repository and push it
onto the server and also important point
it shows me two URLs one is the https
URL and the other one is the SSH URL as
of now we do not have any SSH keys that
is set up so let me use this https URL
to connect my local repository with the
repository on this server
so I've done all these things I already
have the repositories set up all that I
need is a placeholder for me to connect
my local repository with the server URL
so this would be the command that I need
to copy
with essentially links or ads and origin
with the URL of the git server I just
copy this command
and run it here
get remote ad the name of the remote is
called origin and it points to an https
URL which is nothing but the placeholder
that I've created on the server
all right so let me check if it's added
correctly
okay the URL looks good I'm all set I've
got two comments in my repository I'm
done with all the changes so let me try
to push my repository onto the server
the command for doing it for the first
time is git push hyphen U this is for
linking the Upstream
origin
master
so git push hyphen U Upstream linking
origin master link the master
branch of my local repository with the
master branch on the server repository
and where is it going to push it is
going to push by default to An Origin
and the origin points to this URL all
right let me try doing this
so it essentially opens up an URL asking
me the credentials with which I need to
log into my GitHub server so let me just
copy and paste
the username
and the password with which I have
registered
I would say login
okay success so I am able to create a
repository on the server and the content
seems to be pushed and it created a new
branch called Master it is linked the
master Branch from the local repository
with the master branch on the server so
let me go back to my server and refresh
this page
and see if all my commits have come in
master master one dot text fit all I see
the commits they seem to be two commits
this is my first comment this is my
second comment and if at all you notice
each of this commit comes with a hash
key or a hash ID which contains the
details of my commits in this commit the
files were added this was a line that
was added if you look at the second
commit
I added these lines
I made this you know one line chain that
was there I also added this new file
which was not existing these are the two
commits that existed
and if you see
the timestamp of the Commit This is the
commit which was committed for 15
minutes ago so what does this mean is
that the commit is the timestamp when we
actually commented the code in the
client the pushing is just after we have
committed the change we have pushed it
so there's no timestamp of the push that
happened
this completes the first part of the
tutorial wherein I create a repository
on the client and then I pushed it to
the server after creating a repository
Skeleton on the server now let me
demonstrate to you the SSH configuration
that is required for creating a pair of
SSH keys and ensuring that I connect to
the GitHub server using the SSH
mechanism so for doing that I need to
First create a pair of private and
public key the command to do that is SSH
Keygen
hyphen T I will use the RSA mechanism
for creating my keys Capital C and I
need to specify the email ID using which
I'm going to connect to all right I need
to create the keys for
all right so generating a private and
public key pair it says let me know what
folder should I create the keys in so by
default the keys are created in the user
home directory under a DOT SSH folder so
this is pretty good for me so let me go
ahead and say that that's fine enter a
passphrase so let me enter some
passphrase
keep in mind that I would need to
remember my passphrase because when I'm
going to authenticate myself using SSH
mechanism it will challenge me for this
particular passphrase I need to enter
the right passphrase so that I get
authenticated
so the key is generated and by default
it is created in this particular folder
so let me go over to that particular
folder
and this is the folder where the keys is
generated you would see a public key and
a private key that is created let me
open up the public key with a notepad
and then copy the contents of the whole
file
and now I would need to stick it in
on the server so that I can get myself
authenticated using SSH keys so I have
logged in to GitHub using my credentials
and go to my settings
and you would see an SSH and gpg keys
let me click on that as of now there are
no SSH key or gpg keys so I would click
on new SSH key and let me paste my
public key that I've copied and I would
say add SSH key
it prompts me for my password just to
make sure that
I've got the right kind of
authentication to be adding a new key so
this one looks good that means the keys
were added successfully let me check if
my SSH keys are working well
so I do SSH hyphen capital T I would say
get
at github.com
this is just to double check if my SSH
keys
are authenticated correctly are you sure
you want the connection yes you want to
continue yes
all right now
a check for my passphrase let me enter
my passphrase I hope I remember it
correctly
all right
triggered I've been successfully
authenticated so that means now my SSH
keys are good and just by using my
passphrase I can connect to my GitHub
server using SSH all right so for the
Second Use case what I would do is
create a repository on the server with
some content in it and then clone that
repository cloning is nothing but making
a copy of that repository linking the
origin so that I create an exact replica
of the repository which is there on the
server on my local drive so let me first
create a repository on the server I will
create a new Repository
let me give a name for my repository and
let me kind of initialize that with the
readme file I would call this my second
repo
give some description
this is my second repo
that is created straight
on the git server on the GitHub server
to be precise
it's a public repository this time I'm
going to choose this option the reason
being that I don't want to create just a
skeleton as of now I want to create a
repository with some readme file in it a
random readme file with no content in it
but then I want a repository to be
created on the server all right I say
create a repository
So Random readme file with some content
in it gets created and I can see there
are two URLs to this repository https
and SSH so this time let me copy the SSH
URL so that I will use this URL to clone
this repository on the server
rather from the server to my local box
all right so let me navigate to my root
folder the folder that I created for
making all my repositories git
repositories if you see my first deposit
is already here so let me clone my
repository the second repo that I
created on the server the command to do
that is git clone the URL of the
repository that I want to clone
oops looks like I didn't copy the URL
correctly let me copy that again
copy this URL
all right so if you notice there is no
https to it I'm just cloning the URL
that is SSH URL of the Repository
it asks for my passphrase again
okay
so I just had first repository earlier
now I have something called a second
repo this is an exact replica of
whatever was the repository content that
was created on the server so if I get
into
the repository there's just a simple
readme file that is there let me try to
add a new file out here I would say
Notepad
second dot text
some text file I'm going to put in here
yes definitely you will not find that
file there
this is my second
file that
I will push onto the server
I'm going to put in some content out
there
and save this
and if I do get status
it says this this is a new untracked
file do you want to add this file onto
the repository do you want to track this
file yes I definitely want to track this
file let me also commit my change to the
file
with a message
this is my first
commit for the second repo
I given some particular message
and this time around since I cloned my
repository from a server the origin is
already set for me I don't really need
to add the origin and all that stuff
because what happened in the first case
was that I was creating a repository
from my local box and then pushing
another server which is why I had to set
up the Upstream branches and stuff like
that in this case I have cloned an
existing repository from the server so
when I clone I get the replica along
with all references to my repository so
I don't need to do anything more rather
than push the content I'm just saying
get push to be in the safer side I would
say origin and master
ask me for the passphrase
okay it posts the content onto the
server let me see if the contents have
come in here okay wonderful I had only
one comment now I see my second comment
so what was added as a part of the
second comment it says a new file was
added all this line that never existed
got added this is the second file the
content exactly what I pushed onto the
server
now this completes the tutorial when I
create a repository on the server cloned
it made some changes to the repository
and pushed it to the server using SSH
a quick recap of all the git commands
that we ran till now get init is used to
initialize the repository so if at all
you are creating a local repository you
can get into any of the folder which is
not a git repository and run this
command whenever you run this command a
magical folder by name dot kits gets
created in the folder and from that time
onwards git will start tracking any
changes that happen to that particular
folder it also creates a new repository
and a branch by default the branch that
is created is called the master Branch
git add dot is a wildcard character for
adding any number of new files into your
git repository if you have a bunch of
files one file two file 10 files a
folder containing multiple files in it
all of them could be added into the
repository using this command so once
you add any number of files into the
repository you can keep on adding more
and more files into the repository once
you're done with that you want to commit
the changes that has happened in the
repository you use the git commit
command git commit hyphen M with a
meaningful message this will commit all
the work items that you have done in
terms of the files that got changed the
files that got added all this with one
particular message get status will give
you the status of all the files and
folders any file that got added any file
that got created any file that was
deleted all the status of the files will
be obtained using the git status command
git log will show you all the comment
history with the latest history or the
latest commit showing up on top git add
remote origin this command is used
whenever you want to link any local
repository to a server and this is when
you want to really push a repository
from the local to the server for the
first time git push hyphen U origin and
master this is the command that you
would use whenever you want to push
contents from the local repository to
the server server git clone and the URL
of the server this is the command that
you would run whenever you have an
existing repository on the server and
you want to make a fresh clone or a
fresh copy of that particular repository
onto your local box
more than often many developers or git
users find themselves in situations
where they would want to go back in
history and modify or change some of the
files that they recently committed or
checked into the repository now from a
perspective of get this is a very very
simple activity however my perspective
Version Control tool you will have to be
really cautious about the chain that
you're gonna do in case the changes are
pretty local none of the developers or
none of the other users who are
collaborating with you would be affected
by this change you're good to go ahead
and make this change however if your
repository has been pushed to the server
and you're trying to modify something
from history this would have a very bad
or adverse effect on all the other users
who will be using it you need to be
really cautious while running these
commands in case your repository
contents are local if you've not pushed
your changes to your repository and if
at all you made some changes to the
repository maybe to fix a defect or to
include a new feature and you missed few
things you can go ahead and modify this
with some of these commands let's say I
have this simple repository called undo
undo repo and if at all I look at the
logs of these I have the commit
histories which are like this
I have about five commits that is there
in my repository and in C1 I've added a
file in C2 I added the file in C3 I've
also added some file in C4 I made some
changes to M1 and M3 and in C5 I have
added another file called M4 now this is
my commit history now let's say I made
these two commits but possibly I want to
Club them together along with some other
changes because whatever changes I did
did not really fix things the way I
expected you to fix so there is a git
command called git reset this would
allow me to go back in history to go
back to any of the snapshot get rid of
all this commit history on the commit
messages but retain the changes that
were made as a part of these comments so
if at all I need to go back in history
and go back to this particular comment
and however I just want to get rid of
these commits that are there in terms of
the commit messages but I want the
changes that were there as a part of
these commits to still exist I would do
command called get reset hyphen iPhone
soft
now this would go back or unwind my
changes back to this particular snapshot
getting rid of all the commit messages
that were there as a part of these two
comments but a soft would ensure that
the file changes that were done as a
part of these two comments would still
exist a full remain so the command for
that is git reset hyphen iPhone soft
which is the snapshot that I want to get
back to if this is the comment that I
want to get back to I just copy this
and I paste this
all right it doesn't give me any message
but if at all I do a git log pretty one
line
what it says is 7 C2 now the head which
is nothing but the current pointer which
is earlier pointing to C5 it's gone back
to C3 now what happened to the changes
that were there as a part of C4 and C5
if at all you do a git status if you see
the changes that were part of C4 was
some changes to M1 and M2 and C5 a new
file got M4 got added if I don't get
status
you'll see those changes still exist
however from the git history or the git
commit history C3 is the top most
comment that exists as of now c4 and C5
have been current laid off but the
changes that were there still exist
all right now I can make some more
changes that I want if as a part of my
commit and possibly go ahead and commit
this whole thing as a new commit
altogether maybe I'll open up a notepad
and possibly create a new file called
m6.text
and say one new file that was missed out
I make this change I go to a get status
all these files are still there I will
send it add dot I also get
m
C4
rewriting history
and I do a git
blog
pretty
there you go so C1 C2 C3 still existed
and whatever was a part of C4 and C5 the
older ones I have gotten rid of this
along with those old changes I also
added a new file I made some changes and
it was able to comment it now if you
look at my gate history it's all nice
and clean however do this activity only
when you don't push your git repository
to the server one other most powerful
and useful command while undoing or
resetting the history is something
called as revert this is a safe way of
undoing some things remember what I
talked about when exactly you can do a
reset it's only when your changes are
local and is not being pushed to the
repository take this scenario wherein
let's say I have a git server
and I have the changes which are C1
C2
C3
and possibly C4
all right so this is a bunch of comments
that has happened this was the oldest
comment and this is the newest comment
and if at all developer 1 is the one
who's responsible for pushing these
contents
in the first place so here's got C1 to
C4 on his history
I'm going to jump all these C2 and C3
and carry to C4 and he's pushed all
these contents out here
and there are a bunch of people from the
server who has pulled this repository
and all of them are at the same level
now imagine a use case where developer 1
then figures out that by mistake he put
in the C3 which is a wrong thing to do
and possibly you know he could have done
a better job of fixing this in in a
different way so he wants to get rid of
the C3 or C4 or any one of these
comments
if he's going to do that and push to the
server first of all the git server would
not allow him to go back in history but
assuming that he forcibly pushed
something
what would happen to all these people
who are looking to work ahead from C4
all of them would be affected in an
adverse way so in short never go back in
history if at all you need to undo
something go ahead and put in a new
comment to undo something
let's take this example maybe if at all
a developer one wants to get rid of C3
better way to do that is by adding a new
commit which is ahead say let's say C5
and what does this commit to this comic
possibly gets rid of the C3 instead of
going back in history it actually is
taking your git history ahead but as a
part of this comment you're undoing some
part of the work that was done as a part
of C3
in short you're removing a commit but to
remove a commit you're adding a new
commit and this is why you're doing it
because there are a bunch of other
people who are your losing this
repository collaboratively and what plan
to get ahead in terms of History they
cannot afford to go back in history and
modify something in history all right
the command to do this is called the get
revert command
so let me take up this repository which
has got a bunch of commits in it and if
at all I do a git log
pretty one nine
I have these bunch of comments
and C1 I've added a file in C2 I have
another file in C3 I've added one more
file and in C4 I made some changes and
this is the latest commit and this is
the oldest commit so if at all I see the
contents of the git folder it contains
M1 M2 M3 and M4 and let's see looking at
the previous use case that I mentioned I
may want to get rid of this particular
comment I want to get rid of the C3
which is nothing but I've added a file
called M3 dot text but I added it by
mistake I need a better way of fixing it
possibly I look at it later on but for
now I don't want this particular commit
and the changes that were made as a part
of that commit to exist so the better
way to do that is using the get revert
command so git revert and the comment ID
whatever I want to revert so if I want
to revert this
I copy this particular commit ID and
bear in mind that considering a reset
revert works on one commit at a time so
I can do one commit revert at a time I
can Club a bunch of them together and do
one commit but otherwise the revert
always works on one commit at a time so
get revert
face this
essentially what I'm trying to do I'm
trying to undo this particular commit
while I'm undoing this commit what does
git do it safely adds a new commit and a
part of this new commit It undoes
Whatever changes that were done as a
part of this command so if I say git
revert this
all right I forgot to mention a message
which is why this window is showing up
but that message is good for me I say
write and quit
all right and if I do a git log
all right if you see this C1 still is
there in history and our history is
going ahead from C5 I see another commit
but what was there as a part of this
comment actually is getting rid of this
commit C3 so if at all I see my files
here there's only M1 M2 and M4
M3 was added as a part of C3 now I've
gotten rid of M3 by getting rid of the
revert command for C3 so this is a
better way of undoing things
now that we have acquainted ourselves
with few of those basic git commands let
me get into a very very interesting
topic regarding git which is about
branching the topic is interesting
because git deals with branches in a
completely different way compared to any
other version control tool in most of
the Version Control tools whenever you
create a new Branch a complete copy or a
subset of the whole repository is
created which in my opinion is very very
cost inefficient because it's a very
very expensive operation both in times
of time and the disk space however in
Git You just create a very very
lightweight movable pointer whenever you
create a branch there is no folder or no
copy of the repository that is created
in short the branches are very very
inexpensive and great so go ahead and
create any number of branches that you
need however once you create your
branches do all the work relating to
that branch in that specific branch and
once you've completed your task go ahead
and merge your branches back into your
base branch and then delete the branch
let me go ahead and create a repository
with some branches in this and give you
a demonstration of that
so let me go back to my base directory
which is git demo and then I'm going to
create a new folder here
I'll create a folder called Arc details
all right this is my folder here
all right by navigating to this folder
and let me create a repository here I
initial as a repository and Dot get
folder is created without any files in
it so let me create a few files in it
say notepad name dot text
organization name
I create a name.txt within this maybe
I'll create one more file in this
Notepad
employees dot text
has over 3K employees
I just put in some content in this save
this content
all right
so if I try to get status I just see two
files in it so let me go ahead and add
these two files and let me commit my
changes it's okay
commit hyphen am or iPhone m
first comment
so if I do a git log
I see only one commit and there are two
files here you can see those two files
that is here
all right so at this stage let me go
ahead and create a new branch
and the command for creating new branch
is get Branch name of the branch so let
me name my branches Bangalore Branch blr
branch
and this created a branch but I don't
see any changes in the folder structure
or in the file that exists here I've
just created this Branch so if at all I
do a git branch
I see these two branches and whatever
star means I am currently in this master
Branch so let me go ahead and get into
the blr branch at this moment the number
of files that exist in this repository
is same there's absolutely no change in
any of the branches both master and
Baylor Branch point to the same snapshot
so let me get into
the other branch and the command to do
that is git checkout
plr Branch I've actually switched into
Bangalore Branch if you see the content
of the files there's exactly two files
the way it was earlier because I've
created a branch from my base branch
which is the master branch and when I
created a branch there were two files
existing here if you see the content of
these files they would all be the same
all right so let me create a file that
is specific to panel branch so I will
just say Notepad
maybe I want to put down the address of
this organization specific to Bangalore
in this address dot text so I see
notepad address dot text
I'll say simply learn
Bangor office
say located in
some arteries
I would say Bangalore
that is the content of this file
and I'm creating all this new file in
this blr Branch so I would sync it add
Dot
I would say kit
commit hyphen m
angalore
branch
commit
all right if you see this now I have an
address dot text which is there only in
the Bangalore branch that I'm presently
on now let me switch back to my master
Branch the command to do that is git
checkout Master Branch if you remember
when I forged out or when I created a
new Branch from the master Branch I
didn't have this address dot text in it
so let me go back to git git checkout
Master branch
and keep noticing this particular
what happened to the address dot text
it kind of disappeared how did that
happen it's all because of this magical
git folder so what happened is now git
knows that when you're in the master
Branch the file that was there in the
Bangalore branch which was called the
address dot text didn't exist
so git is playing around with the file
system snapshot so if I come back let's
say I come back to git checkout
Bangalore branch
if you see this the address.txt file
appears so this particular file is only
present in the branch and git is playing
around with the file system snapshot by
moving the pointers across and ensuring
that whenever I'm switching to Bangalore
Branch I see this address dot text when
I go back to my master Branch I don't
say this Bangalore branch
contents which is the address dot text
all right now that I created this Branch
maybe I may need to push this onto the
server so let me go ahead and create a
particular repository structure on the
server similar to whatever we did in our
previous exercise so the name of the
folder needs to be exactly this one so
let me copy this
and let me create a repository very
quickly
go here and say new Repository
all right this
is the name of the Repository get
Brand's demo this is what I'm gonna put
in as a description I would say public I
don't want any readme files in it
because I already have a repository I
will just say create a repository
all right let me use
the SSH URL yeah this is the SSH URL
this is good for me so let me copy this
URL
let me add it here
I'm linking my local repository with the
server and I would let me come back to
my master branch
let me first push my master Branch so I
would say git push hyphen U origin
master
it asks for my passphrase
all right
push my master Branch it looks good it
says new Bunch Master is created so if I
go back to my server and it will refresh
if you see it is only one branch
and it says what is the name of the
branch master
if you see the contents there is
employees.txt and name dot text now
let's try to push the other branch that
is there the command to do that is a
little different compared to
the way we created it on the client
because on the client we created the
branch with a git Branch command now
this Branch doesn't exist on the server
and I need to push this new Branch onto
the server so the command would be
get push hyphen new origin and the name
of the brand that I want to specifically
push
all right now it created a branch out
here
let me do refresh of this
all right it says two branches
if I get into this Bangalore Branch I
will see the address dot text if at all
I go back into Master Branch I would not
see this under stock text
wonderful
now let me do one thing let me try to
get the changes from the brands that I
created back into the master branch that
is what I would mean by merging I would
want the changes that is there as a part
of the bangle Branch to come back into
my master Branch so if I go to click
checkout
Bangalore Branch I see this address DOT
type this is there only as a part of
Bangalore Branch now assuming that I am
done with the changes from the Bangalore
Branch I want to get those changes back
into my master so let me come back
to my master branch
and the way to merge in the changes is
get merge so what is that you want to
merge I want to merge Bangalore Branch
changes and where will it merge it will
merge it into the current brands that
I'm on presently I'm in the master
Branch so this command would merge the
changes from the Bangalore Branch into
your master Branch so with this if you
keep looking here the content that was
there that was added as a part of
Bangalore branch which is nothing but
the address dot text this file should
appear as soon as we do a merge
all right so it happened quickly and you
will see this address dot text that is
here so if you see let's check the
contents in terms of the git log
foreign
that we had earlier which had only one
commit right now we are at this
Bangalore Branch commit which was there
as a part of the Bangalore Branch commit
so with this change let me push this
change onto the server as well git push
origin master
okay those changes were pushed to the
server and now
if at all I do a refresh out here
okay I mean two branches the princess's
Master even if I'm in master plans I
should see this address dot text that is
there even if I'm in the banker Branch
the content is all same because we
immersed these two branches now that
I've merged this Branch I don't need
this Branch anymore so I would go ahead
and delete this locally as well as push
it onto the server so that it gets
deleted from the server as well so
delete Branch locally the command is get
Branch hyphen D and the name of the
branch only thing you need to ensure is
that you should not be currently on that
branch which you're trying to delete so
I would say get brands and I'm presently
in the master Branch so I can go ahead
and delete the other Branch so get
Branch hyphen D bangle branch
all right this got rid of our Bender
Branch locally
if at all I don't get Branch hyphen V it
shows there's only one branch as of now
however this Branch would still exist on
the server because Server doesn't know
anything about the changes that we have
done we need to specifically push our
changes onto the server
okay there are two branches still here
so let me go ahead and push these
changes so that the branch gets deleted
from the server as well the command to
do that is get
push
origin
hyphen iPhone delete and the name of the
branch
all right
so this went ahead and deleted the
branch from the server let's check that
okay if you see now there's only one
branch
these are the basic steps that you would
need to know before you work with any of
the branches within git and as I said
before the branches are very very
inexpensive in git go ahead and create
any number of branches that you want but
just ensure that once you create your
branch finish off your task and once you
finish off your task go ahead and delete
the branch and before you delete the
brands please ensure that you merge in
the branches so that you get in or take
in the work that was done as a part of
that particular branch
to summarize the git branching commands
are as follows to create a new Branch
you issue a command git Branch the name
of the branch and this would
automatically spin off or create a new
Branch from the existing Branch from
where you issue this command if you are
on the master branch and if you execute
this command get Branch new underscore
Branch it creates a new underscore
Branch from the master Branch once you
create a new branch in order to get into
this Branch you would type in git check
out the name of the branch
this would automatically switch the file
system snapshot between different
branches now the branches that are
created are by default created on the
local repository in case you want to
push any specific Branch onto the server
you would have to issue a command get
push hyphen U origin and the name of the
branch so this will ensure that the
contents of the new Branch are pushed
onto this server at any time if you want
to list out all existing branches on
your local repositories the command
would be git Branch hyphen AV assuming
that you did all your work in a new
branch and you're happy with the changes
you would want to bring back those
changes into the master Branch the
command to merge the contents of any new
Branch into a master Branch would be git
merge and the name of the branch this
would automatically merge the branch
contents that you specified into the
branch on which you currently reside if
at all you're on the master branch and
if you issue a command get more launch
new underscore Branch this would merge
the contents of the new Branch onto the
master Branch so once you merge the
contents of any branch you would
possibly want to go ahead and delete
this Branch so the command to delete
those branch locally is get Branch
hyphen capital D and the name of the
branch this again would delete the
branch locally in order to make this
change on the server which is in order
to remove this Branch from the server
the command to do that is git push
origin hyphen iPhone delete and the name
of the branch this would also delete the
brands that you created on the server
if we talk in the literal sense Maven
means accumulator of knowledge Maven is
a very powerful project management tool
or we can call it a build tool that
helps building documenting and managing
a project
but before we move forward and dive deep
into the basics of Maven let's
understand what is meant by the term
build tool
a build tool takes care of everything
for building a project
it generates a source code generates
documentation from a source code it even
compiles the source code and packages
the compiled codes into jar of zip files
along with that the build tool also
installs the packaged code in local
repository server repository or Central
Repository
coming back to Maven it is written in
Java or c-sharp and it is based on
Project object model or pom
again let's have a pause and understand
what is meant by this term project
object model
a project object model or pom is a
building block in maven
it is an XML file that contains
information about the project and
configuration details used by Maven to
build a project this file resides in the
base directory of the project as
pom.xml file
the pum contains information about the
project and various configuration
details
it also includes the goals and plugins
used by Maven in a project
Maven looks for the pom in the current
directory while executing a task or a
goal it reads the pom gets the needed
configuration information and then runs
the goal
coming back Mabel is used to building
and managing any Java based project
it simplifies the day-to-day work of
Java developers and helps them in their
projects
now when we know the basics of Maven
let's have a look at some reasons to
know why is Maven so popular and why are
we even talking about it so let's have a
look at the need for maven
Maven as by now we know is properly used
for Java based projects it helps in
downloading libraries or jar files used
in the project
to understand the part of why do we use
Maven or the need of Maven let's have a
look at some problems that may even
solve
the first problem is getting the jar
files in a project getting the right jar
files is a difficult task where there
could be conflicts in the versions of
the two separate packages however it
makes sure all the jar files are present
in its repositories and avoid any such
conflicting scenarios
the next problem it sorted was
downloading dependencies
we needed to visit the official website
of different software which could be a
tedious task now instead of visiting
individual websites we could visit
mvnrepository.com which is a central
repository of the jar files
then Maven plays a vital role in the
creation of the right project structure
and servlets struts Etc
otherwise it won't be executed
then Maven also helps to build and
deploy the project so that it may work
properly
so the next point is what exactly Maven
does
it makes the building of the project
easy the task of downloading the
dependencies and jar files that were to
be done manually can now be done
automatically all the information that
is required to build the project is
readily available now
finally mirin helps manage all the
processes such as building documenting
releasing and other methods that play an
integral part in managing a project
now when we know everything about Maven
let's look at some companies that use
maven
there are over 2000 companies that use
Maven today
the companies that use Maven are mostly
located in the United States and in the
computer science Industry
Maven is also used in Industries other
than computer science like information
technology and services financial
service banking hospital and care and
much more
some of the biggest corporations that
use Maven are as follows
first we have via varijo then comes
Accenture followed by JPMorgan Chase and
Company then comes craft base and
finally we have red hat
now griddle is in kind of a bell tool
which can be used for the build
automation performance and it can be
used for various programming languages
primarily it's being used for the Java
based applications it's some kind of
build tool which can help you to see
that how exactly automatically you can
prepare the builds you can perform the
automations earlier we used to do the
build activity from the eclipse and we
used to do it manually right but with
the help of this build tool we are going
to do it like automatically without any
manual efforts as such here there are
like a lot of activities which we will
be doing during the build process
primarily there are different activities
like compilations linkage packaging
these are the different tasks which we
perform during the build process so that
we can understand that how the build can
be done and we can perform the
automations uh this uh process also it's
kind of a standardized because again if
you want to automate something standards
or a standard process is something which
we require for that before we been going
ahead with that part so that's the
reason why we are getting this well tool
because this build tool helps us to do a
standardization process to see that how
the standards can be met and how we can
proceed further with that part
also it's something which can be used
for variety of languages programming
languages Java is the primary language
for which we use the Gradle but again
other languages like Scala Android CC
plus plus Ruby these are some of the
languages for which we can use the same
tool now it's actually using like it's
referring to as an trophy based domain
specific language rather than XML
because ant and Maven these are the XML
based build tools but this one is not
that uh dependent on XML it's using The
Groovy based domain specific language
DSL language is being used here right
now um again it's something which can be
used to do the build it can further on
be used to perform the test cases
automations also there and then further
on you can deploy to the artifactory
also that okay I want to push the
artificial artifactory so that also that
part also you can get it done over here
so uh primary this tool is known for
doing the build automations for the big
and large projects the projects in which
the source code the amount of source
code and the efforts is more so in that
case is this particular tool makes sense
now griddle includes both the pros of
Maven and ant but it removes the
drawbacks or whatever the issues which
we face during these two build tools so
it's helping us to remove all the cons
which we face during the implementation
of ant and Maven and again again all the
pros of ADD and Maven is implemented
with this cradle tool
now let's see that why exactly this
griddle is used because that's a very
valid question that what is the activity
like what is the reason why we use the
Gradle because
um the first one is that it resolves
issues faced on other build tools that's
a primary reason because we all already
having the tools like Maven and and
which is available there but primarily
this griddle tool is something which is
removing all the issues which we are
facing with the implementation of other
tools so these issues are getting uh
removed as such second one is that it
focuses on maintainability performance
and flexibility so it's giving the focus
on that how exactly we can manage the
big large projects and uh we can have
flexibility that what different kind of
approaches I want to build today I want
to build in different ways tomorrow the
source code modifies gets added up so I
have the flexibility that I can change
this build script so I can perform the
automations so a lot of flexibility is
available which is being supported by
this tool and then the last one is like
it provides a lot of features a lot of
plugins now this is one of the benefit
which we get in the case of Maven also
that we get a lot of features but again
when we talk about cradle then it
provides a lot of plugins like let's say
that normally in a build process we do
the compilation of the source code but
sometimes let's say that we want to
build an angular or a node.js
application now in that case we may be
involved in running some command line
executions some command line commands
just to make sure that yes we are
running the commands and we are getting
the output so there are a lot of
features which we can use like uh there
are a lot of plugins which is available
there and we will be using those plugins
in order to go ahead and in order to
execute those builds process and doing
the automations now let's talk about the
cradle and Maven because again when we
talk about Maven like it was like
something which was primary used for the
Java but again when we are talking about
cradle so again it's just uh being used
primary for the Java here but what is
the reason that we prefer Gradle over
the create me one so what are the
different reason for that let's talk
about that part because this is very
important we need to understand that
what is the reason that Gradle is
preferred as a better tool for the Java
as compared to Maven when we talk about
for the build automation here
now the first one is that the Gradle
using The Groovy DSL language domain
specific language whereas the maven is
considered as a project management tool
which is uh creating the palms or XML
file format files so it's being used for
the Java project but XML format is being
used here and on the other hand griddle
is something which is not using the XML
formats and whatever the build scripts
you are creating that is something which
is there in the groupie based DSL
language and on the other hand in the
Palm we have to create the xmls
dependencies whatever the attributes you
are putting up in the May one that's
something which is available there in
the format of XML the overall goal of
the griddle is to add functionality to a
project whereas the goal of the maven is
to you know to complete a project phase
like to work on different different
project phase like compilation test
executions uh then uh packaging so then
deploying to artifactory so these are
all different phases which is available
there into the maven but on the other
hand griddle is all about about adding
the functionality that how you want to
have some particular features added up
into the build scripts in griddle there
are like we usually specify that what
are the different tasks we want to
manage so different different tasks we
can add up into the case of Gradle and
we can override those tasks also in case
of Maven it's all about the different
phases which has been happening over
here and it's in sequence manner so
these phases happens in the sequence
order that how exactly you can build up
the sequence there but in case of
griddle you can have your own tasks
custom tasks also and you can disrupt
the sequence and you can see that how
the different steps can be executed in a
different order so Maven is something
which is a phase mechanism there but
Gradle is something which is according
to the features or the flexibilities now
griddle works on the tasks whatever the
task you want to perform you uh it works
directly on those tasks there on the
other hand uh Maven is something does
not have any kind of inbuilt cache so
every time you're running the build so
separate uh things or the the plugins
and all this information gets loaded up
which takes definitely a lot of time on
the other hand gradually something which
is uh using its own internal cache so
that it can make the bills a little bit
faster because it's not something which
is doing the things from the scratch
whatever the uh things is already being
available in the cache so it's just pick
that part and from there it will proceed
further on the build Automation and
that's the reason why cradle performance
is much faster as compared to Maven
because it uses some kind of a cache in
there and then helps to improve the
overall performance now let's talk about
the Gradle installation because this is
a very important aspect to be done
because when we are doing the
installation we have to download the
Cradle executables right so let's see
that what are the different steps is
involved in the process of the Gradle
installation
so when we talk about the Gradle
installation so there are primary four
steps which is available the very first
one is that you have to check if the
Java is installed now if the Java is not
installed so you can go to the open jdk
or you can go for the Oracle Java so you
can do the installation of the jdk on
your system so jdk8 is something you can
most commonly use nowadays so you can
install that once the Java is downloaded
and installed then you have to do the
Cradle uh download cradle there now once
the Gradle binaries are executable uh or
the user file gets downloaded so you can
add the environment variables and then
you can validate if the critical
installation is working fine as expected
not so we will be doing the Gradle
installation into our local systems and
uh into the windows platform and we'll
see that how exactly we can go for the
installation of cradle and we'll see
that what are the different version we
are going to install here so let's go
back to the system and see that how we
can go for the Gradle installation so
this is the website of the jdk of a Java
Oracle Java now here you have different
jdk so from there you can do whatever
the option you want to select you can go
with that so jdk8 is something which is
most commonly used nowadays like it's
most comfortable or compatible version
which is available so in case you want
to see that if the jdk is installed into
your system all you have to do is that
you have to just say like Java hyphen
version and that will give you the uh
put it whether the Java is installed
into your system or not so in case my
system the Java is installed but if you
really want to do the installation you
have to download the jdk installer from
this website from this Oracle website
and then you can proceed further on that
part now once the jdk is installed so
you have to go for the Cradle
installation because Gradle is something
the which will be performing at the
build automations and all that stuff so
you have to download the bindies like uh
the zip file probably in which we have
the executables and all and then we have
to have have some particular environment
variables configured so that we will be
able to have the System modified over
there so right now we have got like the
pre-requests as in Java version
installed now the next thing is that we
have to install or download the
executables so uh in order to download
the latest Gradle distribution so you
have to click on this one right now over
here there are different options like uh
you want to go for 6.7 now it's say
having like binary only is or complete
we'll go for the binary only is because
we don't want to have the source we just
want the binaries and the executables
now it's getting downloaded it's around
close to 100 MB of the installer which
is there
now we have to just extract into a
directory and then the same path we need
to configure into the environment
variable so that in that way we will be
able to see that how the uh Gradle
executables will be running and it will
give the complete output to us over here
in this case so it may take some time
and once the particular modifications
and the download is done then we have to
extract it and once the extraction is
done so we will be able to go back and
have some particular version or have the
configurations established over there so
that let's just wait for some time and
then we will be continuing with the
environment variables like this one so
once the installation and the extraction
is done now we just have to go to the
downloads where this one is downloaded
we have to extract it now extraction is
required so that we can have the setup
like we can set up this path into our
environment variables and once the path
is configured and established we will be
able to start further on that part on
the execution so meanwhile these the
files are getting started let's see so
we already the folder structure over
here and we will see like we will give
this path here there is two environment
variables we have to configure one is
the Gradle underscore home and one is
the
um in the path variable so we'll copy
this path here so meanwhile this is
getting a
started we can save our time and we can
go to the environment variable so we can
right click on this one properties
in there we have to go for the advanced
systems settings then environment
variables
now here we have to give it like cradle
underscore home now in this one we will
not be going giving it till the bin
directory so that only needs to be there
where the griddle is extracted so we'll
say okay
and then we have to go for the path
variable where we will be adding up a
new entry in this one we will be putting
up till the bin directory here because
the Cradle executable should be there
when I am running the Gradle command so
these two variables I have to configure
then okay okay
and okay
so this one is done so now you have to
just open the command prompt and see
that whether the execution or the
commands which you're running is is
completely successful or not so
meanwhile it's extracting all the
executables and all those things it will
help us to understand that how the whole
build process or how the build tools can
be integrated over there now once the
extraction is done so you have to run
like CMD
Java iPhone version to check the version
of the Java and then the Gradle
underscore version is what you're going
to see check the version of the Cradle
which is installed and now you can see
that it saves that 6.7 version is being
installed over here in this case so
that's the way that how we are going to
have the Cradle installation performed
into our particular system so let's go
back to the content let's talk about the
Cradle Core Concepts here now in this
one we are going to talk about what are
the different Core Concepts of cradle
are all about the very first one is the
projects here now a project represents a
item to be performed over here to be
done like deploying an application to a
staging environment performing some
build so Gradle is something which is
required uh the projects
um the Gradle project which you prepare
is not having multiple tasks which is
available there which is configured and
all these tasks all these different
tasks needs to be executed into a
sequence now sequences again is a very
important part because again if the
sequence is not meant properly then the
execution will not be done in a proper
order so that's the very important
aspect here
tasks is the one in which is a kind of
an identity in which we will be
performing a series of steps these tasks
may be like compilation of a source code
preparing a jar file preparing a web
application archive file or ER file also
we can have like in some tasks we can
even publish our artifacts to the
artifactory so that we can store those
artifacts into a shared location so
there are different ways in which we can
have this uh particular tasks executed
now build scripts is the one in which we
will be storing all this information
what are the dependencies what are the
different tasks we want to refer it's
all going to be present in the
build.gradle file there build.gradle
file will be having the information
related to what are the different
dependencies you want to download and
you want to store there so all these
things will be a part of the build
Scripts
now let's talk about the features of
cradle what are the different features
which we can use in case of cradle here
there are different like type of
features which is available there so
let's talk about them one by one so the
very first one over here is the high
performance then high performance is
something which we can see that we
already discussed that in case you are
using a large projects so Gradle is
something which is in better approach as
compared to Maven because of the high
performance which we are getting it uses
an internal cache which makes sure that
you are using like you are doing the
builds faster and that can give you a
higher performance over there second one
is the support it provides the support
so it yes definitely provides a lot of
support on how you can perform the belts
and it's being a latest tool which is
available there so the support is also
quite good in terms of how you want to
prepare the build how you want to
download the plugins different plugin
supports and the dependencies uh
information also there next one is
multi-project build software so using
this one you can have multiple projects
in case in your repository you have
multiple projects say so all of them can
be easily built up with the help of this
part particular tool so it supports
multiple project to be built up using
the same Gradle project and Gradle
scripts so that support is also
available with this cradle a build tool
incremental builds are also something
which you can do with the help of Gradle
so if you have done only the incremental
changes and you want to perform only the
incremental build so that can also be
possible with the help of a griddle here
the build scans so we can also perform
the build scans so we can use some
Integrations with sonar Cube and all
where we can have the scans down to the
source code on understand on how the
build happens or how the source code
really happens on there so that code
scan or the build scans can also be
performed with this one and then it's a
familiarity with Java so for Java it's
something which is consideration by
default not even Java in fact Android
which is also using the Java programming
language is using the particular Gradle
over here so that the build can be done
and it can gain uh benefits out of that
so in in all the manners in all the
different ways it's basically helping us
to see that how we can make sure that
this tool can help us in providing a lot
of features and that can help us to make
a reliable build tool for our Java based
projects or any other programming based
project here right now let's see that
how we can invert a Java project with a
griddle here and for that we have to go
back and readily something which is
already installed we just have to create
a directory where we can have like how
we can perform some executions we can
prepare some build scripts and we can
have a particular execution of a Gradle
build happened over there so let's go
back to the machine okay so we are going
to open the terminal here and we'll see
that how we can create it so first of
all I have to create a directory
structure let's say that we'll say like
Gradle
hyphen project now once the project is
created so we can go inside this
directory so to create some critical
related projects and preparing the files
now uh in this one we'll let's first
create a particular one so we will be
saying like VI
build dot Gradle so in this one we are
going to put like a two plugins we are
going to use so we are going to select
apply frogging
Java and then we are going to say like
apply
for login
application
so these two plugins we are going to use
and when we got this file over here in
this one so it shows like build.gradle
which is available there in this case so
two these files are available now if you
want to learn like you know what are the
different tasks so you can run like
Grill tasks command over there so
griddle task will help you know that
what are the different tasks which is
available over here by processing the
build scripts and all so um this will
definitely help you to understand on
giving you the output so here all the
different tasks are being given and it
will help you to understand that what
are the different tasks you can
configure and you can work over here
just like jar files clean and all that
stuff build compile then uh in it is
there then all these different uh
executions assemble then javadoc then
build then check test all these
different tasks are there and if you
really want to run the Gradle build so
you can run like Gradle clean to perform
the clean activity because right now you
are doing like if a build so before that
you can have a clean and then you can
run a specific command or you can run
The Griddle clean build which will
perform the cleanup also and it will at
the same time will have the build
process also performed over there so
build and clean up both will be executed
over here and what is the status whether
it's the success was a failure that will
be given back to you now in this case in
the previous one if you see that when
you run the clean the Cradle clean it
was only running one task but when you
go for the build process when you run
the Gradle clean build it's going to
give you a much more information in fact
you can also give me further information
like you can have the hyphen iPhone Info
flag also there so that if you want to
get the details about the uh different
uh tasks which we which is being
executed over here so that also you're
going to get over here in this one so
you just have to put like hyphen iPhone
Info and then all these steps will be
given back to you that how these tasks
will be executed and the response will
be there so that's the way that how you
can create a pretty much simple
straightforward project in form of
Gradle which can definitely help you to
run some couple of creative commands and
then you can understand that what are
the basic commands you can run and how
the configurations really works on there
right let's go back to the main content
right now let's move on to the next one
so in the next one we are going to see
that how we can prepare a griddle build
project in case of eclipse now we are
now using the local system we are not
directly creating the folders and the
files here we are actually using the
eclipse for performing the creating a
new Gradle project over here so let's
move on that part okay so now the
eclipse is open and I have opened in
this one the very first thing is that we
have to do the Gradle plugin
installation so that we can create new
projects on cradle and then we have to
configure the path that's how the Gradle
plugin can be configured on the previous
uh preferences and all that stuff and
then we will be doing the build process
so the very first thing is that we have
to go to the eclipse Marketplace
in there we have to search for griddle
so once the search is done
it will show us the plugins related to
cable so we have to go for build ship
Gradle integration so we'll click on the
install
it will proceed with installation it
will download it in some cases maybe
it's part of the eclipse as in uh in the
ID so you can go to the install Tab and
you can see that also that if this
plugin is already installed or not but
in this case we are installing it and uh
once the installation is done we just
have to restart the uh specific uh once
we have to restart this Eclipse so that
the changes can be reflected
so it's downloading
it's downloading the Cradle here and
once that is installed we will be able
to use it over here in this case in this
scenario so we have to just wait for
that part so still downloading the jar
files so once the jar file is done it's
now over the areas and download it so
after that we will be able to proceed
further on that download aspect so it's
going to take some time to download it
and once it's done we will be able to
proceed further now once the progress is
done so it's asking us for the restart
now so uh before that uh we just have to
click on restart now and then the
eclipse will be restarted all together
again here so you can do it manually or
you can go for that options you just
require a restart so that the new
changes can be reflected over here so
the plugins can be activated and can be
referenced here now we have to just uh
put up like the you know the
configuration where we can have the
system so we can go for the Gradle
configuration so we can go for Windows
and then preferences
now in this case we have to go for the
uh for the ones in which the Cradle
option is available there so cradle is
what we are going to select now user
home the Gradle user home is what we
need to use right so you want to go for
the Gradle you want to go for local
installation so so all these options you
can use you can if if you go for the
griddle wrapper then it will be
downloading the Cradle locally and it is
going to use the Cradle W or griddlew
dot bat file but if you already have an
installation locally so you can prefer
that also right now in the previous demo
we have already got the Gradle uh
extracted so we just have to go for the
downloads in the downloads already
Gradle is available so we are going to
select that part here so this is what we
are going to select
right so this represents that this is
the directory structure in which we are
having the mechanism so you can either
go for the build scan so you can select
the build scan also so once this uh is
enabled then all the projects will be
scanned and will be you know published
and uh it's in kind of an additional
option which is available if you really
want to disable it you can disable it
also and you can go with this
configuration so so uh this is where the
particular Gradle folder is being put
over here in this case and then we have
to just click on apply
and we just have to click on apply and
close so with this one the particular
execution is done now we will be going
for the project creation so you can
right click over here or you can go to
the file also so here we are going to go
for the job project and in this we are
going to have a Gradle project so Gradle
project is what we are going to create
here
and next
so we are going to say like cradle
project
and then next
so once that is done so finish
so uh with this one when you create the
project so what will happen that uh
automatically there will be a folder
structure will be available there right
and uh there are some uh Gradle scripts
which will also be created there so we
will be doing the modifications there
and we'll see that how the uh particular
Gradle build script looks like and how
we can we will be adding some couple of
uh selenium related dependencies and
we'll see that how we can have more and
more dependencies added and what will be
the impact of those dependencies on the
overall project so that also it's very
important aspect to be considered so let
this processing be happen over there
just creating and uh some plugins and
binaries are getting installed and
getting downloaded so we'll see that
once the project is uh imported
completely executed over here and got
created we can extract that now if you
see here the particular option is
available about the Gradle tasks so you
can extract it also and you will be able
to know that what are the different
tasks which is available there let's see
that in the build they are running like
build these are the different tasks
which is happening inside the build
process so Gradle executions will be
also available over here in this case
and greater tasks will be different it
will be represented over here in this
one so you just have to extract on the
Gradle project okay so this is the
library which is available now uh what
happens that you will be able to have
like settings.gradle in this one you
will be able to have like okay Gradle
hyphen project is something which is
available there in this one so that's
what you're being aren't referring then
we have over here as in these folder
structures which is created like Source
main Java this is the one source test
Java is the one which is available as an
on the folder structure and so test
resources are also available here so the
main source main resources are also
available now in this case what happens
that these are the dependencies project
and external these are the different
dependencies are available there so
let's see let's add a dependency over
here in this one in the Bell dot uh
griddle script and see that how we can
do that if we open build.gradle file so
you can see that these dependencies are
there like test implementation junit is
available there right and then we have
implementations of this one which is
available now these jar files which you
put up it it will automatically be added
up as in part of this one as in part of
the particular dependencies over here
and which means that you don't have to
store them as a now within the
repository and automatically they can be
happened over there so let's open a
dependency page so we will be going to
MN repository where we will be opening a
dependency link so this is the
dependency link here so selenium iPhone
Java is available and it can give you
the dependency for all the different
options now we have for Maven this is
the one and for Kettle this is the one
here so we have to just copy this one
and we have to use it as independency so
this is a group and this is the name and
the version which we are using here now
we have copied this one so we will go
back to the Eclipse so here we have to
just put that dependency
and we have to just save it so uh this
is something which is providing like
selenium defenses which is available so
now we have to just refresh the project
so right click over here then you will
be able to see the options in the grader
saying that refresh credible project now
once the moment you do that so you will
be able to do like for the first time
maybe it will take some time to download
all the dependencies which is related to
selenium but after that you will be able
to select the dependencies will be
simply added up over here in this case
so you can see that all the selenium
related dependencies are added up for
any reason if you comment these ones
and you say like
synchronize again
so you will see that all the
dependencies which you are adding up
from this selenium represent uh from the
selenium perspective will be gone back
again so this is the way that how you
can keep on adding the dependencies
which is required for preparing your
bill for your source code and from there
you will be able to proceed further on
the execution part so that's the best
part about this cradle here so that's
the way that how we are going to prepare
a Gradle project within the eclipse and
now you can keep on adding like the
source code in this one and that's the
way that how the code base will be added
up over here and before we begin if you
are someone who is interested in
building a career in devops by
graduating from the best universities
where a professional who elicits to
switch careers with devops while
learning from the experts then dragging
a show to Simply learn skal Tech
postgraduate program in devops the
course link is mentioned in the
description box that will navigate you
to the course page where you can find a
complete Oreo of the program being
offered so when we talk about the Gradle
installation so there are primary four
steps which is available the very first
one is that you have to check if the
Java is installed now if the Java is not
installed so you can go to the open jdk
uh or you can go for the Oracle Java so
you can do the installation of the jdk
on your system so jdk8 is something we
can uh most commonly use nowadays so you
can install that once the Java is
downloaded and installed then you have
to do the Cradle uh download cradle
there now once the Gradle boundaries are
executable uh or the zip file gets
downloaded so you can add the
environment variables and then you can
validate if the Gradle installation is
working fine as expected not so we will
be doing the Gradle installation into
our local systems and uh into the
windows platform and we'll see that how
exactly we can go for the installation
of cradle and we'll see that what are
the different version we are going to
install here so let's go back to the
system and see that how we can go for
the Gradle installation so this is the
website of the jdk of a Java Oracle Java
now here you have different jdk so from
there you can do whatever the option you
want to select you can go with that so
jdk8 is something which is most commonly
used nowadays like it's most comfortable
or compatible version which is available
so in case you want to see that if the
jdk is installed into your system all
you have to do is that you have to just
say like Java hyphen version and that
will give you the output at whether the
Java is installed into your system or
not so in case my system the Java is
installed but if you really want to do
the installation you have to download
the jdk installer from this website from
this Oracle website and then you can
proceed further on that part now once
the jdk is installed so you have to go
for the Cradle installation because
Gradle is something the which will be
performing at the build automations and
all that stuff so you have to download
the bindies like the zip file probably
in which we have the executables and all
and then we have to have have some
particular environment variables
configured so that we will be able to
have the System modified over there so
right now we have got like the
prerequests as in Java version installed
now the next thing is that we have to
install or download the executables so
uh in order to download the latest
Gradle distribution so you have to click
on this one right now over here there
are different options like you want to
go for 6.7 now it's they're having like
binary only is all complete we'll go for
the binary only is because we don't want
to have the source we just want the
binaries and the executables now it's
getting downloaded it's around close to
100 MB of the installer which is there
now we have to just extract into a
directory and then the same path we need
to configure into the environment
variable so that in that way we will be
able to see that how the uh Gradle
executables will be running and it will
give the uh complete output to us over
here in this case so it may take some
time and once the particular
modifications and the download is done
then we have to extract it and once the
extraction is done so we will be able to
go back and have some particular version
or have the configurations established
over there so then let's just wait for
some time and then we will be continuing
with the environment variables like this
one so once the installation and the
extraction is done now we just have to
go to the downloads where this one is
downloaded we have to extract it now
extraction is required so that we can
have the setup like we can set up this
path into our environment variables and
once the path is configured and
established we will be able to start
further on that part on the execution so
meanwhile these the files are getting
started let's see so we already got the
folder structure over here and we will
see like we will give this path here
there is two environment variables we
have to configure one is the Gradle
underscore home and one is the
um in the path variable so we'll copy
this path here so meanwhile this is
getting a
started we can save our time and we can
go to the environment variable so we can
right click on this one properties
in there we have to go for the advanced
systems settings then environment
variables
now here we have to give it like cradle
underscore home now in this one we will
not be going giving it till the bin
directory so that only needs to be there
where the griddle is extracted so we'll
say okay
and then we have to go for the path
variable where we will be adding up a
new entry in this one we will be putting
up till the pin directory here because
the Cradle executables should be there
when I am running the Gradle command so
these two variables I have to configure
then okay okay
and okay
so this one is done so now you have to
just open the command prompt and see
that whether the execution or the
commands which you're running is is
completely successful or not so
meanwhile it's extracting all the
executables and all those things it will
help us to understand that how the whole
build process or how the build tools can
be integrated over there now once the
extraction is done so you have to run
like CMD
Java iPhone version to check the version
of the Java and then the Gradle
underscore version is what you're going
to see check the version of the Cradle
which is installed and now you can see
that it shows that 6.7 version is being
installed over here in this case so
that's the way that how we are going to
have the Cradle installation performed
into our particular system Maven is a
popular open source build tool developed
by the Apache group to build publish and
deploy several projects at once
the tool is written in Java and is used
to build projects written in C sharp
Scala and Ruby
Maven is based on the project object
model and focuses on the simplification
and standardization of the building
process
in the process it takes care of the
bills dependencies reports distributions
releases and the mailing list Maven is
chiefly used for Java based projects the
tool helps build the code and
downloading dependencies and is used to
develop and manage any Java based
project
it simplifies the day-to-day work of
Java developers and helps them build
their projects now when we know about
Maven the next tool to be considered is
Gradle
so let's begin with what is Gradle
Gradle is a build automation tool known
for its flexibility to build software
here a build automation tool is
something that is used to automate the
creation of applications
the building process includes compiling
linking and packaging the code
the process becomes more consistent with
the help of build automation tools
Gradle is known to build Automation in
many languages like Java Scala Android C
C plus plus and groovy
the tool supports groovy based domain
specific language or XML
Gradle provides building testing and
deploying software on several platforms
the tool is known to build any software
and large projects before we move forth
we must know that Gradle includes all
the pros of ant and maven along with
that it also curves all the cons of both
of these belt tools now let's have a
look at the contrast between these two
tools
we will have a look at the contrast
based on six prominent features
the first point of comparison is based
on
this answers the question what Maven or
Gradle is based on Maven is based on
developing Java based software and the
goal is related to a project phase
whereas Gradle is based on developing
domain specific language projects the
goal with Gradle is to add functionality
to a project
then comes the second point
the second point is the focus
what do these two tools focus on
Maven simply focuses on developing
applications within the time deadlines
provided
whereas in the case of Gradle there may
be new features that might be added to
the application while it is being built
or developed
now the third pointer is configuration
as we know Maven uses extensible markup
language or XML for making the project
structure
Maven uses an XML file for declaring the
project and its dependencies whereas
griddle on the other hand doesn't use an
XML file for the Declaration of project
configuration
Gradle uses groovy based domain-specific
language for making the project
structure
moving forward the fourth pointer is
languages
when we talk about languages we see that
Maven supports software development in
languages like Scala C sharp and Ruby
whereas Gradle supports development in
languages like Java C C plus plus and
groovy now the next pointer in our list
is customization
if we consider customization then Maven
provides a limited number of parameters
and resources
it can serve a limited number of
Developers
and is not much customizable however
this also makes Maven easier to
understand and configure whereas Gradle
is a highly customizable tool and
provides a large range of IDE support
builds
Gradle can be used for Native
development with C or C plus class
the sixth and the last pointer we shall
consider is performance
Maven has a slower build time since it
does not use build cache which means it
does not create local temporary files
during software creation hence uses
extensive time
whereas Gradle on the other hand
performs better than Maven as it is
optimized for tracking only current
running tasks this means it only works
on the tasks that have been changed to
give a better performance now when we
know the differences between the two
build tools it sometimes become critical
to decide which tool would be a better
fit for a work so the next topic how do
you choose between the two tools
we know that both Gradle and Maven have
their own set of advantages and
disadvantages
Gradle is undoubtedly the more powerful
tool out of the two still it has many
functionalities that are not always
needed and dealing with so many
functionalities is slightly a difficult
task if we consider the tools together
we can conclude that Maven is great for
small projects it should be used when
dependency management
modularization consistency lots of
plugins and conventions are preferred
at the same time griddle on the other
hand is fantastic when it comes to Big
projects and should be used when
flexibility ease of use speed and
incremental builds are of importance and
also if you have started with Maven and
now your project has overgrown you can
switch or migrate from Maven to Gradle
at any point of time
I hope this will help you when it comes
to choosing between the two build tools
in the near future
let's get started with the first topic
that is the birth of selenium
selenium was primarily created by Jason
Huggins in 2004.
Json an engineer at thoughtwork was
working on a web application that needed
to be tested frequently he realized that
the repeated manual testing of the
application is becoming inefficient so
he created a JavaScript language that
automatically controlled the browser's
actions this program was named
JavaScript test Runner
after he realized that his idea of
automating the web applications has a
lot of potential he made the JavaScript
test Runner open source and it was later
renamed as selenium core
so we know that Json was a person who
initially created selenium but then we
must also know that selenium is a
collection of different tools and since
there are different tools there will be
several developers too
to be exact about the number of
different tools there are four different
tools that have their own creators
let's have a look at all of them
the first tool is the selenium remote
control or the selenium RC that was
created by Paul Hammond
then comes selenium grade that was
developed by Patrick lightbody
the third tool is the selenium IDE that
was created by shinia kasathani and the
fourth tool that is the selenium
Webdriver was created by Simon Stewart
we shall be learning about all these
tools in great detail As you move forth
in our video
first let's have a look at what is
selenium
selenium is a very popular open source
tool that is used for automating the
test carried on the web browsers there
may be various programming languages
like Java c-sharp python Etc that could
be used to create selenium test Scripts
this testing that is done using selenium
tool is referred to as selenium testing
we must understand that selenium allows
the testing of web applications only
we can neither test any computer
software nor any mobile application
using selenium
selenium is composed of several tools
with each having its own specific role
and serving its own testing needs
moving forward let's have a look at the
features of selenium which will help us
understand the reason behind its
widespread popularity
so here we have a set of features of
selenium
selenium is an open source and portable
framework that has a playback and record
feature it is one of the best
cloud-based testing platform and
supports various OS and languages
it can be integrated with several
testing Frameworks and supports parallel
test execution
we will be talking about all these
features in detail as we move forward
so here the first feature that we have
is open source and portable framework
this feature states that selenium is an
open source and portable framework for
testing web applications
in addition to that selenium commands
are categorized in terms of different
classes which make it easier to
understand and implement
the second feature is the playback and
record feature the feature states that
the tests can be authorized without
learning a test scripting language with
the help of playback and record features
the next feature says that selenium is a
cloud-based testing platform
selenium is a leading cloud-based
testing platform that allows testers to
record their actions and Export them as
a reusable script with a simple to
understand and easy to use interface
moving forth the next feature states
that selenium supports various operating
systems browsers and programming
languages
the tool supports programming languages
such as c-sharp Java python PHP Ruby
Pearl and JavaScript
if you talk about the operating systems
then selenium supports operating systems
like Android iOS Windows Linux Mac and
Solaris
and the tool also supports various
browsers like Google Chrome Mozilla
Firefox Internet Explorer Edge Opera
Safari Etc
then the next feature we have is the
integration with testing Frameworks
selenium can be well integrated with
testing Frameworks like test NG for
application testing and generating
reports and also selenium can be
integrated with Frameworks like ant and
Maven for source code compilation
the last feature in our list is the
parallel test execution selenium enables
parallel testing which reduces time and
increases the efficiency of the tests
selenium requires fewer resources as
compared to other automation testing
tools
now let's move further and get to know
different selenium tools by now we know
that there are four different tools that
come under the selenium suit
the four tools are selenium remote
control selenium grid selenium IDE and
selenium Webdriver
we shall have a look at these four Tools
in detail one after the another
beginning with selenium remote control
selenium remote control enables the
writing of automated web applications in
languages such as Java c-sharp Perl
Python and PHP to build complex tests
such as reading and writing files
querying a database and emailing the
test results
selenium RC was a sustainable project
for a very long time before selenium
Webdriver came into existence and hence
selenium RC is hardly in use today as
the Webdriver offers more powerful
functionalities
the second tool we shall see is the
selenium grid
selenium grid is a smart proxy server
that enables the running of tests in
parallel on several machines
this is made possible when the commands
are routed to the remote web browser
instances and one server acts as the Hub
The Hub here is responsible for
conducting several tests on multiple
machines
selenium grid makes cross browser
testing easy as a single test can be
carried on multiple machines and
browsers all together making it easy to
analyze and compare the results
here there are two main components of
selenium grade Hub and the node
Hub is a server that accepts the access
request from the Webdriver client
routing the Json test commands to the
remote drivers on nodes and here the
node refers to the Remote device that
consists of a native OS and a remote web
driver
it receives the request from the Hub in
the form of a Json test commands and
executes them using the Webdriver moving
forth the third tool is the selenium IDE
if you want to begin with this selenium
IDE it needs no additional setup except
installing the extension of your browser
it provides an easy to use tool that
gives instant feedbacks
selenium IDE records multiple locators
for each element it interacts with if
one locator fails during the playback
the others will be tried until one is
successful
the IDE makes the test debugging easier
with features like setting breakpoints
and pausing exceptions
through the use of the Run command you
can reuse one test case inside one
another
and also selenium IDE can be extended
through the use of plugins they can
introduce new commands to the IDE or
integrate with a third-party service
the last and the fourth tool in the
selenium suit is the selenium Webdriver
selenium Webdriver is the most critical
component of the selenium tools suit
that allows cross browser compatibility
testing
the Webdriver supports various operating
systems like Windows Mac Linux Unix Etc
it provides compatibility with a range
of languages including python Java and
Perl along with it it provides supports
different browsers like Chrome Firefox
Opera Safari and Internet Explorer
the selenium Webdriver completes the
execution of test scripts faster when
compared to other tools and also it
provides compatibility with iPhone
driver HTML unidriver and Android driver
now after you know about the tools the
question arises which tool to choose
so in the next topic we shall see
different factors on the basis of which
we may decide which tool would be more
suitable for us here we shall have a
look at the reasons why one should
choose that particular tool we shall
begin with selenium remote control
selenium remote control or selenium RC
should be chosen to design a test using
a more expressive language than Celanese
selenis is a set of selenium commands
that are used to test our web
applications
then the selenium RC might be chosen to
run tests against different browsers on
different operating systems the RC may
be chosen to deploy tests across
multiple environments using selenium
grid it helps in testing the
applications against a new browser that
supports JavaScript and web applications
with complex Ajax based scenarios
now the second tool for which we shall
see the reasons are selenium grid
selenium grid as we learned in the
reasons for selenium RC is used to run
selenium RC scripts in multiple browsers
and operating systems simultaneously
the grid is used to run a huge test suit
that needs to be completed at the
earliest possible time now comes the
third tool the third tool is selenium
IDE
selenium IDE is used to learn about
Concepts on automated testing and
selenium these Concepts include selenis
commands such as type Open click and
weight assert verify Etc
the concepts also include locators such
as ID name XPath CSS selector
Etc
selenium IDE enables the execution of
customized JavaScript code using run
script and exporting test cases in
several different formats the IDE is
used to create tests with a limited
amount of knowledge in programming and
these test cases and test suits can be
exported later to RC or Webdriver now
finally let's have a look at the reasons
to choose a last tool that is selenium
Webdriver
selenium Webdriver uses a certain
programming language in designing a test
case the Webdriver is used to test
applications that are rich in Ajax based
functionalities execute tests on HTML
unit browser and create customized test
results
so this is what we're going to be
covering in this session we're going to
cover what life is like before using
Jenkins and the issues that Jenkins
specifically addresses then we'll get
into what Jenkins is about and how it
applies to continuous integration and
the other continuous integration tools
that you need in your devops team then
specifically we'll Deep dive into
features of Jenkins and the Jenkins
architecture and will give you a case
study of a company that's using Jenkins
today to actually transform how their it
organization is operating so let's talk
a little bit about life before Jenkins
let's see this scenario I think it's
something that maybe all of you can
relate to as developers we all write
code and we all submit that code into a
code repository and we all keep working
away writing our unit tests and
hopefully we're running our unit tests
but the problem is that the actual
commission actually gets sent to the
code repository aren't consistent you as
developer may be based in India you may
have another developer that's based in
the Philippines and you may have another
team lead that's based in the UK and
another development team that's based in
North America so you're all working at
different times and you have different
amounts of code going into the code
repository There's issues with the
integration and you're kind of running
into a situation that we'd like to call
development hell where things just
aren't working out and there's just lots
of delays being added into the project
and the bugs just keep mounting up the
bottom line is the project is delayed
and in the past what we would have to do
is we'd have to wait until the entire
software code was built and tested and
before we could even begin checking for
errors and this just really kind of
increased the amount of problems that
you'd have in your project the actual
process of delivering software was slow
there was no way that you could actually
iterate on your software and you just
ended up with just a big headache with
teams pointing fingers at each other and
blaming each other so let's jump into
Jenkins and see what Jenkins is and how
it can address these problems so Jenkins
is a product that comes out of the
concept of continuous integration that
you may have heard of as power
developers where you'd have two
developers sitting next to each other
coding against the same piece of
information what they were able to do is
to continuously develop their code and
test their code and move on to new
sections of code Jenkins is a product
that allows you to expand on that
capacity to your entire team so you're
able to submit your codes consistently
into a source code environment so there
are two ways in which you can do
continuous delivery one is through 90
builds and one is through continuous so
the approach that you can look at
continuous delivery is modifying the
Legacy approach to building out
Solutions so what we used to do is we
would wait for nightly bill and the way
that our 90 builds would work and
operate is that as code developers we
would all run and have a cut-off time at
the end of the day and that was
consistent around the world that we
would put our codes into a single
repository and at night all of that code
would be run and operated and tested to
see if there were any changes and a new
build would be created that would be
referred to as the nightly build with
continuous integration we're able to go
one step further we're able to not only
commit our changes into our source code
but we can actually do this continuously
there's no need to race and have a team
get all of their code in at arbitrary
time you can actually do a continuous
release because what you're doing is
you're putting your tests and your
verification Services into the build
environment so you're always running
Cycles to test against your code this is
the power that Jenkins provides in
continuous integration so let's dig
deeper into continuous integration so
the content of continuous integration is
that as a developer you're able to pull
from a repository the code that you're
working on and then you'll be able to
then at any time submit the code that
you're working on into a continuous
integration server and the goal of that
continuous integration server is that it
actually goes ahead and validates and
passes any tests that a tester may have
created now if on the continuous
integration server a test is a pass then
that code gets sent back to the
developer and the developer can then
make their changes it allows the
developer to actually do a couple of
things it allows the developer not to
break the build and we all don't want to
break the builds that are being created
but it also allows the developer not to
actually have to run all the tests
locally on their computer write tests
particularly if you have a large number
of tests can take up a lot of time so if
you can push that service up to another
environment like a continuous
integration server it really improves
the productivity of of your developer
what's also good is that if there are
any code errors that have come up that
may be Beyond just a standard CI test so
maybe there's a Code the way that you
write your code isn't consistent those
errors can then be passed on easily from
the tester back to the developer too the
goal from doing all this testing is that
you're able to release and deploy and
your customer is able to get new code
faster and when they get that code it's
simply just works
so let's talk a little bit about some of
the tools that you may have in your
continuous integration environment so
the cool thing with working with
continuous integration tools is that
they are all open source at least the
ones that we have listed here are open
source there are some that are private
but typically you'll get started with
open source tools and it gives you the
opportunity to understand how you can
accelerate your environment quickly so
bamboo is a continuous integration tool
that specifically runs multiple builds
in parallel for faster compilation so if
you have multiple versions of your
software that runs on multiple platforms
this is a tool that really allows you to
get that up and running super fast so
that your teams can actually test how
those different builds would work for
different environments and this has
integration with and Maven and other
similar tools so one of the tools you're
going to need is a tool that allows you
to automate the software build test and
release process and buildbot is that
open source product for you again it's
an open source tool so there's no
license associated with this so you can
actually go in and you can actually get
the environment up and running and you
can then test for and build your
environment and create your releases
very quickly so buildbot's also written
in Python and it does support parallel
execution jobs across multiple platforms
if you're working specifically on Java
projects that need to be built and test
then Apache Gump is the tool for you it
makes all of those projects really easy
it makes all the Java projects easier
for you to be able to test with API
level and functionality level testing so
one of the popular places to actually
store code and create a versioning of
your code is GitHub and it's a service
that's available on the web just
recently acquired by Microsoft if you
are storing your projects in GitHub then
you'll be able to use Travis continuous
integration or Travis CI and it's a tool
design specifically for hosted GitHub
projects and so finally we're covering
Jenkins and Jenkins is a central tool
for automation for all of your projects
now when you're working with Jenkins
sometimes you'll find there's
documentation that refers to a product
called Hudson Hudson is actually the
original version of the product that
finally became Jenkins and it was
acquired by Oracle when that acquisition
happened the team behind
um Hudson was a little concerned about
the direction that Oracle May
potentially go with Hudson and so they
created a hard Fork of Hudson that they
renamed Jenkins and Jenkins has now
become that open source product it is
one of the most popular and continuously
contributed projects that's available as
open source so you're always getting new
features being added to it it's a tool
that really becomes the center for your
CI environment so let's jump into some
of those really great features that are
available available in Jenkins so
Jenkins itself is really comprised of
five key areas around easy installation
easy configuration plugins extensibility
and distribution so as I mentioned for
the easy installation Jenkins is a
self-contained Java program and that
allows it to run on most popular
operating systems including Windows Mac
OS and Unix you can even run it on Linux
it really isn't too bad to set up it
used to be much harder than it is today
the setup process has really improved
the web interface makes it really easy
for you to check for any errors in
addition you have great built-in help
one of the things that makes tools like
Jenkins really powerful for developers
and continuous integration teams in your
devops teams as a whole when you have
plugins that you can then add in to
extend the base functionality of the
product Jenkins has hundreds of plugins
and you can go and visit the update
Center and see which other plugins that
would be good for your devops
environment certainly checking out
there's just lots of stuff out there in
addition to the plugin architecture
Jenkins is also extremely extensible the
opportunity for you to be able to
configure Jenkins to fit in your
environment it's almost endless now it's
really important to remember that you
are extending Jenkins not creating a
custom version of Jenkins and that's a
great differentiation because the core
Foundation remains as the court Jenkins
product the extensibility can then be
continued with newer releases of Jenkins
so you're always having the latest
version of Jenkins and your extensions
mature with those core Foundation the
distribution and the nature of Jenkins
makes it really easy for you to be able
to have it available across your entire
network it really will become the center
of your CI environment and it's
certainly one of the easier tools and
more effective tools for devops so let's
jump into the standard Jenkins pipeline
so when you're doing development you
start off and you're coding away on your
computer the first thing you have to do
when you're working in the Jenkins
pipeline is to actually commit your code
now as a developer this is something
that you're already doing or at least
you should be doing you're committing
your code to a git server or to an SVN
server or a similar type of service so
in this instance you'll be using Jenkins
as the place for you to commit your code
Jenkins will then create a build of your
code and part of that build process is
actually going through and running
through tests and again as a developer
you're already comfortable with running
unit tests and writing those tests to
validate your code but there may be
additional tests that Jenkins is running
so for instance as a team you may have a
standard set of tests for how you
actually write out your code so that
each team member can understand the code
that's been written and those tests can
also be included in the testing process
within the Jenkins environment assuming
everything past the the tests you can
then get everything placed in a stage
and release ready environment within
Jenkins and finally getting ready to
deploy or deliver your code to a
production environment Jenkins is going
to be the tool that helps you with your
server environment to be able to deploy
your code to the production environment
and the result is that you're able to
move from a developer to production code
really quickly this whole process can be
automated rather than having to wait for
people to actually test your codes or
going through a nightly build you're
looking at being able to commit your
code and go through this testing process
and release process continuously as an
example companies Etsy will release up
to 50 different versions of their
website every single day
so let's talk about the architecture
within Jenkins it allows you to be so
effective at applying a continuous
delivery devops environment so the
server architecture really is broken up
into two sections so on the left hand
side of section you have the code the
developers are doing and submitting that
code to a source code repository and
then from then Jenkins is your
continuous integration server and it
will then pull any code that's been sent
to the source code repository and we'll
run tests against it it'll use a build
service such as MAV in to actually then
build the code and every single stage
that we have that Jenkins manages there
are constant tests so for instance if a
build fails that it feedback is sent
right back to the developers so that
they can then change their code so that
the build environment can run
effectively the final stage is to
actually execute specific test scripts
and these test scripts can be written in
selenium amp so it's probably good to
mention here that both mavin and
selenium are plugins that run in the
Jenkins environment so before we were
talking about how Jenkins can be
extended with plugins Maven and selenium
are just two very popular examples of
how you can extend the Jenkins
environment the goal to go through this
whole process again it's an automated
process is to get your code from the
developer to the production server as
quickly as possible have it fully tested
and have no errors so it's probably
important at this point to mention uh
one piece of information around the
Jenkins environment that if you have
different code builds that need to be
managed and distributed this will
require that you need to have multiple
builds being managed Jenkins itself
doesn't allow for multiple files and
builds to be executed on a single server
you need to have a multiple server
environment with running different
versions of Jenkins for that to be able
to happen so let's talk a little bit
about the Master Slave architecture
within Jenkins so what we have here is
an overview of the Master Slave
architecture within Jenkins on the left
hand side is the remote source code
repository and that remote source code
repository could be GitHub or it could
be a team Foundation services or the new
Azure devops code repository or it could
be your own git repository the Jenkins
server acts as the master environment on
the left hand side and that Master
environment can then push out to
multiple other Jenkins slave
environments to distribute the workload
so it allows you to run multiple builds
and tests and production environments
simultaneously across your entire
architecture so Jenkins slaves can be
running the different build versions of
the code for different operating systems
and the server Master is controlling how
each of those builds operate so let's
step into a quick story of a company
that has used Jenkins very successfully
so here's a use case scenario
um over the last 10 or 15 years there
has been a significant shift within the
automotive industry where manufacturers
have shifted from creating complex
Hardware to actually creating software
we've seen that with companies such as
Tesla where they are creating a software
to manage their cars we see the same
thing with companies such as General
Motors with their OnStar program and
Ford just recently have rebranded
themselves as a technology company
rather than just a automotive company
what this means though is that the
software within these cars is becoming
more complex and requires more testing
to allow more capabilities and
enhancements to be added to the core
software so Bosch is a company that
specifically ran into this problem and
their challenge was that they wanted to
be able to streamline the increase
increasingly complex Automotive software
by adopting continuous integration and
continuous delivery best practices with
the goal of being able to delight and
exceed the customer expectations of the
end user so Bosch has actually used
Cloud bees which is the Enterprise
Jenkins environment so to be able to
reduce the number of manual steps such
as building deploying and testing Bosch
has introduced the use of cloud bees
from Jenkins and this is part of the
Enterprise Jenkins platform it has
significantly helped improve the
efficiencies throughout the whole
software developments cycle from
automation stability and transparency
because Jenkins becomes a self-auditing
environment now the results have been
tangible previously it took three days
before a build process could be done and
now it's taken that same three-day
process and reduced it to less than
three hours that is significant
large-scale deployments are now kept on
track and have expert support and there
is clear visibility and transparency
across the whole operations through
using the Jenkins tools to just to
implement the ca processes over here now
what is the purpose of Jenkins here now
Jenkins is normally a kind of a CI tool
which we use for performing the build
automations and the test cases
automation there it's one of the open
source tool which is available there and
one of the most popular CI tool also
available into their Market
now this tool makes it easier for the
developers to integrate the changes to
the project here so we can easily
integrate the changes and whatever the
modifications we want to manage we will
be able to do that with the help of
Jenkins
now Jenkins also achieves The Continuous
integration with the help of couple of
plugins each and every tool which you
want to integrate have its own plugins
which is available there for example you
want to integrate Maven we have a maven
plugin in Jenkins which you can install
you can configure in that case you will
be able to use the maven there now you
can deploy the mavener to build tool
onto the Jenkins server and then you can
prepare or you can configure any number
of Maven jobs in case of Jenkins
so uh what exactly the maven or the
Jenkins really do is the mavenwen
integrates with Jenkins through the
particular plugin so you can able to
automate the pills because for
automation the build you require some
integration with the maven and that
integration is what we are getting from
the maven plugin so in Jenkins you have
to install the maven plugin and once the
plugin is installed so what you can do
is that you can proceed with the
configurations you can proceed with the
setup and this a particular plugin can
help you to build out some of the Java
based projects which is available there
in the git repositories and once that is
done you will be able to go ahead and
you will be able to process a complete
integration of Maven within Jenkins
right so let's see that how we can go
for the integration now I have already
installed the maven onto the Linux
virtual machine which we are using so
using the app utility or using the Yum
utility you can actually download the
Jenkins package and the maven package
onto the server onto the virtual machine
and now I'm going to proceed further
with the plugin installation and the
configuration of a maven project so I
have a GitHub repository which is having
a maven project Maven uh source code and
the mavenized test cases over there so
let's see let's log into the uh Jenkins
and see that how it works
so this is the Jenkins interface which
we have over here now in this one what
we can do is that we can create some
Maven jobs over here and once those jobs
are created we will be able to do a
custom build onto of these Jenkins so
first of all we have to install the uh
particular plugin here
for that we have to go to the manage
Jenkins in manage Jenkins you have the
manage plugins option there so you have
to click on that now here you will be
having different tabs like updates
available installed Advanced all these
different tabs are available there so
what you can do is that you can click on
the available one when you go to the
available tab so what will happen that
here you can actually put up that what
exactly uh plugin you want to fetch here
so I can put a plugin called maven
now you can see that the very first one
the maven integration tool is available
so I'm going to select that particular
plugin and click on download now and
install after restart
now once that is done so what will
happen that the plugin will be
downloaded but in order to reflect the
changes we have to do a couple of
restart now for that you don't have to
go to the virtual machine you have the
option here itself that will allow you
to do the restart over here when you
click on this button so you check this
option and say that restart Jenkins when
the installation is done so what will
happen that the installation will be
automatically attempted whenever the
particular plugin installation is
completed here so you just have to
refresh the page again and you will be
able to see that the particular Jenkins
is being processed as such here
right so you can see that the screen is
coming up that Jenkins is restarting so
it will take a couple of five to six
seconds to do the restart and the login
screen to come up again over there
you can do the refresh also if you feel
automatically it will be reloaded once
the Jenkins is ready but sometimes we
have to refresh it so that we can get
the screen over there so once the login
is done so my Maven integration is done
so next thing which I will be doing is
that I will be creating a maven related
project so I'm going to put the admin
user
and the password so whatever the user
and password you have created you are
going to put that so that you will be
able to log into the Jenkins portal now
this is the Jenkins which is available
here so all you have to do is that you
have to click on create a new job or new
item so both the option is pretty much
same only
so here you will be able to see a maven
project here so I'm going to select like
maven build that's the name which I'm
going to give here and the maven project
I'm going to select here
and then press ok
now here you will be providing the first
of all the repository from which you
will be checking out the source code now
I can have a discard old builds over
here so if I feel that I want to have
like a low rotation so all the previous
builds should be deleted so I'm just
saying that dates to keep a build should
be 10 over here and the number of bills
which I need to keep over here is 20.
you can adjust these settings according
to your requirement but uh over here we
are you know doing a kind of
configurations which we are trying to do
a lot of configurations and settings
same so these are the particular
settings which we are looking forward as
such over here so now we are going to
have the log rotation here so we can
have it like how many days we want to
keep and how many number of bills we
want to keep here so both the values we
are providing over here and then now I'm
going to put the get uh integration here
like the repo URL so I have this
repository here in which I have the Java
source code and some uh particular uh
genuine test cases and all I also have
the particular source code and it's kind
of a maven project so that's what I'm
trying to clone over here with the help
of this plugin so this plugin will
download this repository it will clone
it on to the Jenkins server and then
depending on our integration with mav1
the maven build will be triggered here
so now I'm going to process with the uh
Maven here so you can see here that it's
saying that uh Jenkins needs to know
that where the maven is installed
because that Maven version it needs to
configure it needs to process on that
part so I'll just do the save over here
and or I can click on this tool
configuration so I'll just save or do
the apply click on this tool
configuration here
now here you have the options like where
you can have the jdk installation but
what happens that thing Jenkins is
running there so jdk is automatically
installed so in the tools configuration
you don't have to put the jdk
configuration but at least for the maven
configurations you have to provide that
where exactly the maven is available
there so I'm just saying that maven
three I want to process and the latest
moving Apache web server I want to
configure here so I just want to have
like I just want to save this settings
so that it will be automatically
download the latest version Apache 3.6.3
version there and that same should be
utilized over here in this case now I'm
just going to the maven Builder
configuration here and click on the
configure part so these git repository
is available here and in the build step
it automatically builds up that uh what
a maven environment you want to select
so you see that previously since I did
not configure my Maven environment so it
was throwing an error but once I have
configured that I have to download it
during the build process or before the
build that utility should be downloaded
so instead of doing the physical
installation of Maven on the server what
I have chosen over here is that I have
selected the particular version like I
have selected that a particular
3.6.3 version should be installed for
the maven purposes over here now once
the that is done I'm going to put the
particular
steps over here you can have it like
clean install you can have clean compile
test clean test or test alone you can
give it's just a part of the setup or
the goals which you want to configure
here it by default says that pom.xml
file is the current one in the current
directory you need to refer you need to
pick on that one what it's up to you
only that how you want to configure and
how you want to process as such this
information so according to your
requirement you can say that okay I'm
just want to go for these particular
goals and you can say like save over
here the particular configuration will
be saved now you can just click on the
build now and you will be able to see
that the first of all the git clone will
happen and then the desired Mi 1
executable will be uh the build tool
will be configured and according to that
it will be processed here so you can see
here that uh the maven is uh getting
downloaded it's getting configured here
and once it's configured because I have
explained over there that 3.6.3 version
I have to select so that specific
version will be configured and will be
picked up over here now even if you
don't have the maven installed on the
physical machine on which the Jenkins is
running still you will be able to do the
processing using this particular
component here so you can see here that
we have some particular test cases
executed and in the end we are able to
get a particular artifices also there
since I did not call upon the package or
install goal that's the reason why the
particular artifacts was not generated
War file or jar file whatever the
packaging mode is available at Palm
level but still what happens that my
test cases do gets executed and that's
what I have got over here in this case
so this is a kind of a mechanism where
we feel that how we can configure a git
repository once the git repository is
configured you are going to integrate
the maven plugin in the maven plugin you
are going to configure in the tools
configuration that this and so and so
version I want to configure to run my
build and once that is done after that
you just have to trigger the build and
click on the bill now option and once
that is done you will be able to get a
particular full-fledged build or
compilation happened onto the Jenkins
and this log will give you the complete
details that what are the different
steps which has happened on this one so
let's take a slow scenario of a
developer and a tester before you had
the world of Docker a developer would
actually build the code and then they
send it to the tester but then the code
wouldn't work on their system encoders
will work on the other system due to the
differences in computer environments so
what could be the solution to this well
you could go ahead and create a virtual
machine to be the same of the solution
in both areas we think Docker is an even
better solution so let's kind of break
out what the main big differences are
between Docker and virtual machines as
you can see between the left and the
right hand side both look to be very
similar what you'll see however is that
and on the docker side what you'll see
as a big difference is that the guest OS
for each container has been eliminated
Docker is inherently more lightweight
but provides the same functionality as a
virtual machine so let's step through
some of the pros and cons of a virtual
machine versus Docker so first of all a
virtual machine occupies a lot more
memory space on the host machine in
contrast Docker occupies significantly
less memory space the boot up time
between both is very different Docker
just boots up faster the performance of
the docker environment is actually
better and more consistent than the
virtual machine Docker is also very easy
to set up and very easy to scale the
efficiencies therefore are much higher
with a Docker environment versus a
virtual machine environment and you'll
find it is easier to Port Docker across
multiple platforms than a virtual
machine finally the space allocation
between Docker and a virtual machine is
significant when you don't have to
include the guest OS you're eliminating
a significant amount of space and the
dock environment is just inherently
smaller so after Docker as a developer
you can build out your solution and send
it to a tester as long as we're all
running in the docker environment
everything will work just great so let's
step through what we're going to cover
in this presentation we're going to look
at the devops tools and where Docker
fits within that space we'll examine
what Docker actually is and how Docker
works and then finally we'll step
through the different components of the
docker environment so what is devops
devops is a collaboration between the
development team the operation team
allowing you to continuously deliver
Solutions and applications and services
that both delayed and improve the
efficiency of your customers if you look
at the Venn diagram that we have here on
the left hand side we have development
on the right hand side we have operation
and then there's a crossover in the
middle and that's where the devops team
sits if we look at the areas of
integration between both groups
developers are really interested in
planning code building and testing and
operations want to be able to
efficiently deploy operate a monitor
when you can have both groups
interacting with each other on these
seven key and elements then you can have
the efficiencies of an excellent devops
team so planning and code base we use
tools like jit and gearer for building
we use Gradle and Maven testing we use
selenium the integration between Dev and
Ops is through tools such as Jenkins and
then the deployment operation is done
with tools such as docker and Chef
finally nagis is used to monitor the
entire environment so let's step deeper
into what Docker actually is so Docker
is a tool which is used to automate the
deployment of applications in a
lightweight container so the application
can work efficiently in different
environments now it's important to note
that the container is actually a
software package that consists of all
the dependencies required to run the
application so multiple containers can
run on the same Hardware the containers
are maintained in isolated environments
they're highly productive and they're
quick and easy to configure so let's
take an example of what Docker is by
using a house that may be rented for
someone using Airbnb so in the house
there are three rooms and only one
cupboard and kitchen and the problem we
have is that none of the guests are
really ready to share the cupboard and
kitchen because every individual has a
different reference when it comes to how
the cupboard should be stocked and how
the kitchen should be used this is very
similar to how we run software
applications today each of the
applications could end up using
different Frameworks so you may have a
framework such as rails perfect and
flask and you may want to have them
running for different applications for
different situations this is where
Docker will help you run the
applications with the suitable
Frameworks so let's go back to our
Airbnb example so we have three rooms
and a kitchen and cupboard how do we
resolve this issue well we put a kitchen
and cupboard in each room we can do the
same thing for computers Docker provides
the suitable Frameworks for each
different application and since every
application has a framework with a
suitable version this space could also
then be utilized for putting in software
and applications that are long and since
every application has its own framework
and suitable version the area that we
had previously stored for a framework
can be used for something else now we
can create a new application and this
instance a fourth application that uses
its own resources you know what with
these kinds of abilities to be able to
free up space on the computer it's no
wonder Docker is the right choice so
let's take a closer look to how Docker
actually works so when we look at Docker
and we call something Docker we're
actually referring to the base engine
which actually is installed on the host
machine that has all the different
components that run your Docker
environment and if we look at the image
on the left hand side of the screen
you'll see that Docker has a client
server relationship there's a client
installed on the hardware there is a
client that contains the docker product
and then there is a server which
controls how that Docker client is
created the communication that goes back
and forth to be able to share the
knowledge on that Docker client
relationship is done through a rest API
this is fantastic news because that
means that you can actually interface
and program that API so we look here in
the animation we see that the docker
client is constantly communicating back
to the server information about the
infrastructure and it's using this rest
API as that Communication channel the
docker server then will check out the
requests and the interaction necessary
for it to be the docker Daemon which
runs on the server itself well then
check out the interaction and the
necessary operating system pieces needed
to be able to run the container okay so
that's just an overview of the docker
engine which is probably where you're
going to spend most of your time but
there are some other components that
form the infrastructure for Docker let's
dig into those a little bit deeper as
well so what we're going to do now is
break out the four main components that
comprise of the docker environment the
four components are as follows the
docker client and server which we've
already done a deeper dive on Docker
images Docker containers and the docker
registry so if we look at the structure
that we have here on the left hand side
you see the relationship between the
docker client and the docker server and
then we have the rest API in between now
if we start digging into that rest API
particularly the relationship with the
docker Daemon on the server we actually
have our other elements that form the
different components of the docker
ecosystem so the docker client is
accessed from your terminal window so if
you are using Windows this can be
Powershell on Mac it's going to be your
terminal window and it allows you to run
the docker Daemon and the registry
service when you have your terminal
window open so you can actually use your
terminal window to create instructions
on how to build and run your Docker
images and containers if we look at the
images part of our registry here we
actually see that the image is really
just a template with the instructions
used for creating the containers which
you use within Docker the docker image
is built using a file called the docker
file and then once you've created that
Docker file you'll store that image in
the docker Hub or registry and that
allows other people to be able to access
the same structure of a Docker
environment that you've created the
syntax of creating the image is fairly
simple it's something that you'll be
able to get your arms around very
quickly and essentially what you're
doing is you're creating the option of a
new container you're identifying what
the image will look like what are the
commands that are needed and the
arguments for within those commands and
once you've done that you have a
definition for what your image will look
like so if we look here at what the
container itself looks like is that the
container is a standalone executable
package which includes applications and
their depend agencies it's the
instructions for what your environment
will look like so you can be consistent
in how that environment is shared
between multiple developers testing
units and other people within your
devops team now the thing that's great
about working with Docker is that it's
so lightweight that you can actually run
multiple Docker containers in the same
infrastructure and share the same
operating system this is its strength it
allows you to be able to create those
multiple environments that you need for
multiple projects that you're working on
interestingly though within each
container that container creates an
isolated area for the applications to
run so while you can run multiple
containers in an infrastructure each of
those containers are completely isolated
they're protected so that you can
actually control how your Solutions work
there now as a team you may start off
with one or two developers on your team
but when a project starts becoming more
important and you start adding in more
people to your team you you may have 15
people that are offshore you may have 10
people that are local you may have 15
Consultants that are working on your
project you have a need for each of
those developers or each person on your
team to have access to that Docker image
and to get access to that image we use a
Docker registry which is an Open Source
server-side service for hosting and
distributing the images that you have to
find you can also use Docker itself as
its own default Retreat and Docker Hub
now something that has to be bear in
mind though is that for publicly shared
images you may want to have your own
private images in which case you would
do that through your own registry so
once again public repositories can be
used to host the docker images which can
be accessed by anyone and I really
encourage you to go out to Docker and
see the other Docker images that have
been created because there may be tools
there that you can use to speed up your
own development environments now you
will also get to a point where you you
start creating environments that are
very specific to the solutions that you
are building and when you get to that
point you'll likely want to create a
private repository so you're not sharing
that knowledge with the world in general
now the way in which you connect with
the docker registry is through simple
pull and push commands that you run
through terminal window to be able to
get the latest information so if you
want to be able to build your own
container what you'll start doing is
using the pull commands to actually pull
the image from the docker repository and
the command line for that is fairly
simple in terminal window you would
write Docker pull and then you put in
the image name and any tags associated
with that image and use the command
pause so in your terminal window you
would actually use a simple line of
command once you've actually connected
to your Docker environment and that
command will be Docker pull with the
image name and any Associated tags
around that image what that will then do
is pull the image from the docker
repository whether that's a public
repository or a private one now in
Reverse if you want to be able to update
the docker image with new information
you do a push command where you take the
script that you've written about the
docker container that you define and
push it to the repository and as you can
imagine the commands for that are also
fairly simple in terminal window you
would write Docker push the image name
any Associated tags and then that would
then push that image to the docker
repository again either a public or a
private repository so if we recap the
docker file creates a Docker image
that's using the build commands Docker
image then contains all the information
necessary for you to be able to execute
the project using the docker image any
user can run the code in order to create
a Docker container and once a Docker
image is built it's uploaded to a
registry or to a Docker Hub where it can
be shared across your entire team and
from the docker Hub users can get access
to the docker image and build their own
new containers so let's have a look at
what we have in our current environment
so today when you actually have your
standard machine you have the
infrastructure you have the host
operating system and you have your
applications and then when you create a
virtual environment what you're actually
doing is you're actually creating
virtual machines but those virtual
machines actually are now sitting within
a hypervisor solution that sits still on
top of your host operating system and
infrastructure and with a Docker engine
what we're able to do is we're able to
actually reduce significantly the
different elements that you would
normally have within a virtualized
environment so we're able to get rid of
the the bins and the so we're able to
get rid of the guest OS and we're able
to eliminate the hypervisor environment
and this is really important as we
actually start working and creating
environments that are consistent because
we want to be able to make it so it's
really easy and stable for the
environment that you have within your
Dev and Ops environment now critical is
getting rid of that hypervisor element
it's just a lot of overhead so let's
have a look at a container as an example
so here we actually have a couple of
examples on the right hand side we have
different containers we have one
container that's running Apache Tomcat
with Java a second container is running
SQL server and microsoft.net environment
the third container is running python
with mySQL these are all running just
fine within the docker engine and
sitting on top of a host OS which could
be Linux it really could be any host OS
within a consistent infrastructure and
you're able to have a solution that can
be shared easily amongst your teams so
let's have a look at an example that
you'd have today if a company is doing a
traditional Java application so you have
your developers working in JBoss on his
system and he's coding away and he has
to get that code over to a test now what
will happen is that tester will then
typically in your traditional
environment then have to install JBoss
on their machine and get everything
running and cool and hopefully set up
identically to the developer chances are
they probably won't have it exactly the
same but they're trying to get it as
close as possible and then at some point
you want to be able to test this within
your production environment so you send
it over to a system administrator who
would then also have to install JBoss on
their environment as well yeah this just
seems to be a whole lot of duplication
so why go through the problem of
installing JBoss three times and this is
where things get really interesting
because the challenge you have today is
that it's very difficult to almost
impossible to have identical
environments if you're just installing
software locally on devices the
developers probably got a whole bunch of
development software they could be
conflicting with the JBoss environment
the tester has similar testing software
but probably doesn't have all the
development software and certainly
system administrator won't have all the
tools that the developer and tester have
their own tools and so what you want to
be able to do is kind of get away from
The Challenge you have of having to do
local installations on three different
computers and in addition what you see
is that this uses up a lot of effort
because when you're having to install
software over and over again you just
keep repeating doing really basic
foundational challenges so this is where
Docker comes in and Docker is the tool
that allows you to be able to share
environments from one group to another
group without having to install software
locally on device you install all of the
code into your Docker container and
simply share the container so in this
presentation we're going to go through a
few things we're going to cover what
Docker actually is and then we're going
to dig into the actual architecture of
Docker and kind of go through what
Docker container is and how to create a
Docker container and then we'll go out
through the benefits of using Docker
containers and then the commands and
finalize everything out with a brief
demo so what is darker so darker is as
you'd expect because all the software
that we cover in this series is an open
source solution and it is a container
solution that allows you to be able to
containerize all of the necessary files
and applications needed to run the
solution that you're building so you can
share it from different people in your
team whether it's a developer a tester
or system administrator and this allows
you to have a consistent environment
from one group to the next so let's kind
of dig into the architecture so you
understand why Docker runs effectively
so the docker architecture itself is
built up of two key elements there is
the docker client and then there is a
rest API connection to a Docker Daemon
which actually hosts the entire
environment within the docker host and
the docker demon you have your different
containers and each one has a link to to
a Docker registry the docker client
itself is a rest service so as you'd
expect a rest API and that sends command
line to the docker Daemon through a
terminal window or command line
interface window and we'll go through
some of these demos later on so you can
actually see how you can actually
interact with Docker the docker Dem then
checks the request against
um the docket components and then
performs the service that you're
requesting now the docker image itself
all it really is a collection of
instructions used to create a container
and again this is consistent with all
the devops tools that we have the devops
tools that we're looking to use
throughout this series of videos are all
environments that can be scripted and
this is really important because it
allows you to be able to duplicate and
scale the environments that you want to
be able to build very quickly and
effectively the agile container itself
has all of the applications and the
dependencies of those applications in
one package you can trying to think of
it as a really effective and efficient
zip file it's a little bit more than
that but it's one file that actually has
everything you need to be able to run
all of your Solutions the actual Docker
registry itself is an environment for
being able to host and distribute
different Docker images among your team
so say for instance you had a team of
developers that were working on multiple
different solutions so say you have a
team of developers and you have 50
developers and they're working on five
different applications you can actually
have the applications themselves the
containers shared in the docker registry
so each of those teams at any time check
out and have the latest container of
that latest image of the code that
you're working on so let's dig into what
actually is in a container so the
important part of a docking container is
that it has everything you need to be
able to run the application it's like a
virtualized environment it has all your
Frameworks and your libraries and it
allows the teams to be able to build out
and run exactly the right environment
that the developer intended what's
interesting though is the actual
applications then will run in isolation
so they're not impacting other
applications that using dependencies on
other libraries or files outside of the
container because of the architecture it
really uses a lot less space and because
it's using less space it's a much more
lightweight architecture so the files
and the actual folder itself is much
smaller it's very secure highly portable
and the boot up time is incredibly fast
so let's actually get into how you
actually create a Docker container so
the docker container itself is actually
built through command line and it's
built of a file and Docker image so the
actual Docker file is a text file that
contains all the instructions that you
would need to be able to create that
docket image and then we'll actually
then create all of the project code with
inside of that image then the image
becomes the item that you would share
through the docker registry you would
then use the command line and we'll do
this later on select Docker run and then
the name of the image to be able to
easily and effectively run that image
locally and again once you've created
the document you can store that in the
docker registry making it available to
anybody within your network so something
to bear in mind is that Docker itself
has its own registry called Docker Hub
and that is a public registry so you can
actually go out and see other doc images
that have been created and access those
images as your own company you may want
to have your own private repository so
you want to be able to go ahead and
either do that locally through your own
repository or you can actually get a
licensed version of Docker Hardware you
can actually then share those files now
something that's also very interesting
to know is that you can can have
multiple versions of a Docker image so
if you have a different version control
different release versions and you want
to be able to test and write code for
those different release versions because
you may have different setups you can
certainly do that within your Docker
registry environment okay so let's go
ahead and we're going to create a Docker
image using some of our basic Docker
commands and so there are essentially
really you know kind of just two
commands that you're going to be looking
for one is the build command another one
is to actually put it into your registry
which is a push command so if you want
to get a image from a Docker registry
then you want to use the pull command
and a pull command simply pulls the
image from the registry and in this
example using ngi next as our registry
and we can actually then pull the image
down to our test environment on our
local machine so we're actually running
the container within our Docker
application on a local machine we're
able to then have the image run exactly
as it would production and then you can
actually use the Run command to actually
use the docker image on your local
machine so just a you know a few
interesting tidbits about the docking
container once the container is created
a new layer is formed on top of the
docker image layer is called the
container layer each container has a
separate read write container layer and
any changes made in that docking
container is then reflected upon that
particular container layout and if you
want to delete the container layer the
container layer also gets deleted as
well so you know why with using Docker
and containers be of benefit to you well
you know some of the things that are
useful is that containers have no
external dependencies for the
applications they run once you actually
have the container running locally it
has everything it needs to be able to
run the application so there's no having
to install additional pieces of software
such as the example we gave with JBoss
at the beginning of the presentation now
the containers are really lightweight so
it makes it very easy easy to share the
containers amongst your teams whether
it's a developer whether it's a tester
whether it's somebody on your operations
environment it's really easy to share
those containers amongst your entire
team different data volumes can be
easily reused and shared among multiple
containers and again this is another big
benefit and this is a reflection of the
lightweight nature of your containers
the container itself also runs in
isolation which means that it is not
impacted by any dependencies you may
have on your own local environment so
it's a completely sandboxed environment
so some of the questions you might ask
us you know can you run multiple
containers together without the need to
start each one individually and you know
what yes you can with Docker compose
docking compose allows you to run
multiple containers in a single service
and again this is a reflection on the
lightweight nature of containers within
the docker environment so we're going to
end our presentation by looking at some
of the basic commands that you'd have
within Docker so we have here on the
left hand side we have a Docker
container and then the command for each
item we're actually going to go ahead
and use some of these commands and the
demo that we're going to do after this
presentation you'll see that in a moment
but just you know some of the basic
commands we have are committing the
docker image into the Container kill is
a you know standard kill command to you
know terminate one or more of the
running containers so they stop working
then restart those containers but
certainly you can look at all the image
all of the commands here and try them
out for yourself so we're going to go
ahead and start a demo of how to use the
basic commands to run Docker so to do
this we're going to open up terminal
window or command line now depending
whether you're running Linux PC or Mac
and we're going to go ahead and the
first thing we want to do is see what
our Docker image lists are so we can go
sudo Docker images and this will give us
well first we'll enter in our password
so let's go enter that in and this will
now give us a list of our Docker images
and here are the docker images I have
already been created in the system and
we can actually go ahead and actually
see the processes that are actually
running so I'm going to go ahead and
open up this window a little bit more
but this will show you the actual
processes and the containers that we
actually have and so on the far left
hand side you'll see under names we have
learn simply learn be unscore cool these
are all just different ones that we've
been working on so let's go ahead and
create a Docker image so we're going to
do sudo
docker
run
Dash Dash p
0.0.0.0 go on 80 colon 80
Ubuntu and this will allow us to go
ahead and run an Ubuntu image and this
will run the latest image and what we
have here is a hash number and this hash
number is a unique name that defines the
container that we've just created and we
can go ahead and we can check to make
sure that the container actually is
present so we're going to do
pseudo.co.ps and this actually show us
down there so it's not in a running
state right now but that doesn't mean
that we don't have it so let's list out
all the containers that are both running
and in the exits see so let's do sucker
PS Dash a and this lists all the
containers that I have running on my
machine
and this shows all the ones that have
been in the running State and in the
exit State and here we see one that we
just created about a minute ago and it's
called learn
and these are all running Ubuntu and
this is the one that we had created just
a few seconds ago
let's open it up and
there we go so let's change that to that
new Doc container to a running state so
scroll down and we're going to type sudo
docker
run
Dash it Dash Dash
name my
um so this is going to be the new
container name it's going to be my
Docker so this is how we name our Docker
environment
and we'll put in the image name which is
Ubuntu
and dash bin Dash Bash
and it's now in our root and we'll exit
out of that
so now we're going to go ahead and start
the new my Docker container so sudo
docker
start
my
and we'll get the container image which
will be my docker
my docker
return and that started that Docker
image and let's go ahead and check
against the other running Docker images
to make sure it's running correctly so
sudo docker
PS
and there we are underneath name on the
right hand side you have to see my
Docker along with the other Docker
images that we created and it's been
running for 13 seconds
quite fast so we want to rename the
container let's use the command sudo
docker
rename we can take another Docker image
this says grab this one and we'll put it
in rename
and we'll rename opening the old
container name which is image and then
we'll put in the new container name and
let's call it
purple
so now the container image that had
previously been called image is now
called Purple
so do sudo Docker PS
to list all of our Docker images
and if we scroll up and there there we
go purple
how easy is that to rename an image
and we can go ahead and use this command
if we want to stop container so we're
going to write sudo docker
stop
and then we'll have to put in the
container name
and we'll put in my Docker the container
that we originally created
and that image has now stopped
and let's go ahead and prove that we're
going to list out all the docker images
and what you see is that it's not listed
in the active images it's uh not on the
list on the far right hand side
but if we go ahead and we can list out
all of the docker images so you actually
see it's still there as an image it's
just not in an active State just what's
known as in an exit state
so here we go
and there's my Docker it's in an exited
state so that happened 27 seconds ago
so if we want to to remove a container
we can use the following command
so sudo docker
RM
for remove
my docker and that will remove it from
the exited state
and we're going to go ahead and we're
going to double check that
and yep
yep that's not not listed there under
exit State anymore
it's gone
and there we go
there that's where it used to be all
right let's go back
so if we want to exit a container in the
running state so we do sudo kill and
then the name of the container
I think one of them is called yellow
let's just check and see if that's going
to kill it
oops no I guess we don't have one called
yellow so let's find out name of
container that we actually have
so sudo Docker kill oh we're going to
list out of the ones that are running oh
okay there we go now yellow isn't in
that list so let's take I know let's
take simply learn and so we can actually
go ahead and let's write sudo Docker
kill simply learn
and that will actually kill an active
Docker container
boom there we go
and we list out all the active
containers you can actually see now
that's they simply learn container is
not active anymore
and these are all the basic commands for
Docker container
and before we begin if you are someone
who is interested in building a career
in devops by graduating from the best
universities but a professional who
elicits to switch careers with devops by
learning from the experts then try
getting a show to Simply learn skeltec
postgraduate program in devops the
course link is mentioned in the
description box that will navigate you
to the course page where you can find a
complete Oreo of the program being
offered on it we're going to break up
this presentation into four key areas
we're going to talk about life before
kubernetes which some of you are
probably experiencing right now what is
kubernetes the benefits that kubernetes
brings to you particularly if you are
using containers in a devops environment
and then finally we're going to break
down the architecture and working
infrastructure for kubernetes so you
understand what's happening and why the
actions are happening the way that they
are so let's jump into our first section
of life before kubernetes so the way
that you have done work in the past
where you may be doing work right now is
really really building out and deploying
Solutions into two distinct areas one is
a traditional deployment where you're
pushing out code to physical servers in
a Data Center and you're managing the
operating system and the code that's
actually running on each of those
servers another environment that you may
potentially be using is to Blind code
out to Virtual machines so let's go
through and look at the two different
types of deployments that you may be
experiencing when you have applications
running on multiple machines you run
into the potential risk that the setup
and configuration of each of those
machines isn't going to be consistent
and your code isn't going to work
effectively and there may be issues with
uptime and errors within the
infrastructure of your entire
environment there's going to be problems
with resource allocation and you're
going to have error issues where
applications may be running effectively
and not not effectively and not load
balance effectively across the
environment the problem that you have
with this kind of infrastructure is that
it gets very expensive you can only
install one piece of software one
service on one piece of Hardware so your
Hardware is being massively
underutilized this is where virtual
machines have become really popular with
a virtual machine you're able to have
better resource utilization and
scalability a much less cost and this
allows you to be able to run multiple
virtual machines on a single piece of
Hardware the problem is is that VMS are
for virtual machines are not perfect
either some of the challenges you run
with VMS is that the actual hardware and
software need needed to manage the VM
environment can be expensive there are
security risks with virtual with VMS
there are security risks with VMS there
have been data breaches recorded about
solutions that run in virtualized
environments you also run into an issue
of availability and this is largely
because is you can only have a finite
number of virtual machines running on a
piece of hardware and this results in
limitations and restrictions in the
types of environment you want to be
running and then finally setting up and
managing a virtualized environment is
time consuming it can take a lot of time
and it can also get very expensive so
how about kubernetes well kubernetes is
a tool that allows you to manage
containerized deployment of solutions an
inherently kubernetes is a tour that is
really a Next Level maturity of
deployment so if you can think of your
maturity curve as deploying code in
directly to Hardware in a Data Center
and then deploying your solutions to
Virtual machines the next evolution of
that deployment is to use containers and
kubernetes so let's kind of go through
and look at the differences between a
virtual machine and kubernetes we've got
a few here that we want to highlight and
you'll get an understanding of what the
differences are between the two so first
of all with virtual machines there is
inherently security risks and what
you'll find as we get dig through the
architecture later in the presentation
is that kubernetes is inherently secure
and this is largely because of the
Legacy code the Legacy see of kubernetes
and where it came from we will talk
about that in just a moment but
kubernetes is inherently secure virtual
machines are not easily portable now
with that said they they are technically
portable they're just not very easily
portable whereas with kubernetes it's
working with Docker container Solutions
it is extremely portable and it means
that you can actually spin up and spin
down and manage your infrastructure
exactly the way that you want it to be
managed and scale it on the demands of
the customers as they're coming in to
use the solution from a time consuming
point of view kubernetes is much less
time consuming than with a virtual
machine a few other areas that we want
to kind of highlight from differences
virtual machines use much less isolation
when building out the encapsulated
environment then kubernetes does for
instance with a virtual machine you have
to run hypervisor on top of the OS and
hardware and then inside of the the
virtual machine you also have to have
the operating system as well whereas in
contrast on a kubernetes environment
because it's leveraging a Docker
container and or container-like
Technologies it only has to have the OS
and the hardware and then inside of each
container it doesn't need to have that
additional OS layer it's able to inherit
what it needs to be able to run the
application this makes the whole
solution much more flexible and allows
you to run many more containers on a
piece of Hardware than versus running
virtual machines on a single piece of
Hardware so as we highlighted here VMS
are not as portable as kubernetes and
kubernetes is portable directly related
to the use of containerization and
because kubernetes is built on top of
containers it is much less time
consuming because you can actually
script and automatically allocate
resource to nodes within your kubernetes
environment because it allows the
infrastructure to run much more
effectively and much more efficient so
this is why if we look at our evolution
of the land of time before kubernetes
while we are running into a solution
where kubernetes had to come about
because the demand for having more
highly scalable solutions that are more
efficient was just really a natural
evolution of this software deployment
model that started with pushing out code
to physical hardware and then pushing
code out to Virtual machines and then
needing to have a solution much more
sophisticated kubernetes would have come
about at some point in time I'm just
really glad it came back when it did so
what is kubernetes does this dig into
the history of kubernetes and how it
came about so in essence kubernetes is
an open source platform that allows you
to manage and deploy and maintain groups
of containers and the container is
something like Docker and if you're
developing code you're probably already
using Docker today consider kubernetes
as the tool that manages multiple Docker
environments together now we talk a lot
about DACA and as a container solution
with kubernetes the reality is is that
kubernetes can actually use other
container tools out there but Docker
just simply is the most popular
container out there both these tools are
open source that's why they're so
popular and they just allow you to be
able to have flexibility in being able
to scale up your Solutions and they were
designed for the post-digital world that
we live and exist in today so a little
bit of background a little bit of trivia
around kubernetes so kubernetes was
originally a successor to a project at
Google and the original project was
Google Google does exactly what
kubernetes done does today but
kubernetes was Rewritten from the ground
up and then released as an open source
project in 2014 so that people outside
of Google could take advantage of the
power of kubernetes containerization
management tools and today it is managed
by the cloud native Computing foundation
and there are many many companies that
support and manage kubernetes so for
instance if you're signing up for
Microsoft Azure AWS Google Cloud all of
them will leverage kubernetes and it's
just become the the de facto tool for
managing large groups of containers so
let's kind of Step through some of the
key benefits that you'd experience from
kubernetes and so we have nine key
benefits the first it is highly portable
and is 100 open source code and this
means that you can actually go ahead and
contribute to this code project if you
want to through GitHub the ability to
scale up the solution is incredible
um what's the the history of kubernetes
being part of a Google project for
managing the Google network and
infrastructure kind of really sets the
groundwork for having a surgeon that is
highly scalable the out of the high
scalability also comes the need for high
avail ability and this is the desire to
be able to have a highly efficient and
highly energized environment that also
you can really rely on so if you're
building outer kubernetes management
environment you know that it's going to
be available for the solutions that
you're maintaining and it's really
designed for deployment so you can
script out the environment and actually
have it as part of your devops model so
you can scale up and meet the demands of
your customer then what you'll find is
that the load balancing is extremely
efficient and it allows you to
distribute the load efficiently across
your entire network so your network
remains stable and then also the tool
allows you to manage the orchestration
of your storage so you can have local
storage such as an SSD on the hardware
that the kubernetes is maintaining or if
the kubernetes environment is pulling
storage from a public Cloud such as
Azure or AWS you can actually go ahead
and make that available to your our
system and you can inherit the security
that goes back and forth between the
cloud environments and one of the things
you'll find consistent with kubernetes
is that it is designed for a cloud-first
environment um kubernetes as well is
that it's it's really a self-healing
environment so if something happens or
something fails uh kubernetes will
detect that failure and then either
restart the process kill the process or
replace it and then because of that you
also have automated rollouts and
rollbacks in case you need to be able to
manage the state of the environment and
then finally you have automatic bin
packaging so you can actually specify
the compute power that's being used from
CPU and ramp for each container so let's
dig into the final area which is the
actual kubernetes architecture I'm going
to cover this at a high level there's
actually another video that you can that
simply learn has developed which digs
deeper into the kubernetes architecture
and so the kubernetes architecture is a
constant based architecture and it's
really about two key areas you have the
kubernetes master which actually
controls
um all of the activities within your
entire kubernetes infrastructure and
then you have nodes that actually are
running on Linux machines
um outs that are controlled by the
master so let's kind of go through some
of these areas so if we look at the
kubernetes master to begin with
um we'll start with uh Etc is this is a
tool that allows for the configuration
and the information and the management
of nodes within your cluster and one of
the key features that you'll find with
all of the tools that are managed within
either a the master environment or
within a node is that they are all
accessible via the API server and what's
interesting about the API server is that
it's a restful based infrastructure
which means that you can actually secure
each connection with SSL and other um
security models to ensure that your
entire infrastructure and the
communication going back and forth
across your infrastructure is tightly
secured scheduler goes ahead and
actually as you'd expect it actually
manages the schedule of activities
within the actual cluster and then you
have the control and the controller is a
Daemon server that actually manages and
pushes out the instructions to all of
your nodes so the other tools really are
the the infrastructure and you can
consider them the administration site um
of the master whereas controller is the
management it actually pushes out all of
the controls via the API server so let's
actually dig into one of the actual
nodes themselves and there are three key
areas of the nodes one is the docker
environment which actually helps and
manage and maintain the container that's
actually inside of the node and then you
have the Kubler which is responsible for
information that goes back and forth and
it's going to do most of the
conversation with the API server on the
actual health of that node and then you
have of the actual kubernetes proxy
which actually runs the services
actually inside of the node so as you
see all of these infrastructures are
extremely lightweight and designed to be
very efficient and very available for
your infrastructure and so here's a
quick recap of the different tools that
are available and it really breaks down
into two key areas you have your
kubernetes master and the kubernetes
node the kubernetes Mazda has the
instructions of what's going to happen
within your kubernetes infrastructure
and then it's going to push out those
instructors to an indefinite number of
nodes that will allow you to be able to
scale up and scale down your solution in
a dynamic way so let's have an overview
of the kubernetes architecture so
kubernetes is really broken up into
three key areas you have your
workstation where you develop your
commands and you push out those commands
to your master and the master is
comprised of four key areas which
essentially control all of your nodes
and the node contains multiple pods and
each pod has your Docker container built
into it so consider that you could have
a really almost an infinite number of
PODS I'm sorry infinite number of nodes
are being managed by the master
environment so you have your cluster
which is a collection of servers that
maintain the availability and the
compute power such as RAM CPU and disk
utilization and you have the master
which is really components that control
and schedule the activities of your
network and then you have the node which
actually hosts the actual Docker virtual
machine itself and be able to actually
control and communicate back to the
master the health of that pod and we'll
get into more detail on the architecture
later in the presentation so you know
you keep hearing me talk about
containers but they really are the
center of the work that you're doing
with kubernetes and can the concept
around kubernetes and containers is
really just a natural evolution of where
we being with internet and digital
Technologies over the last 10 15 years
so before kubernetes you had tools where
you're either running virtual machines
or you're running data centers that had
to maintain and manage and notify you of
any interruptions in your network
kubernetes is the tool that actually
comes in and helps address those
interruptions and manages them for you
so the solution to this is the use of
containers so you can think of
containers as that Natural Evolution
from you know uh 15 20 years ago you
would have written your code and posted
it to a Data Center and more recently
you probably post your code to a virtual
machine and then move the virtual
machine and now you actually just work
directly into a container and everything
is self-contained and can be pushed out
to your environment and the thing that's
great about containers they're they're
isolated environments very easy for
developers to work on them but it's also
really easy for Opera operations teams
to be able to move a container into
production so let's kind of step and
back and look at a competing product to
kubernetes which is Docker swarm now one
of the things we have to remember is
that Docker containers which are
extremely popular
um built by the company Docker and made
open source and Docker actually has
other products one of those other
products is Docker swarm and Docker
swarm is a tool that allows you to be
able to manage multiple containers so if
we look at some of the uh the benefits
of using Docker swarm versus kubernetes
now one thing that you'll find is that
both tools have strengths and weaknesses
but it's really good that they're both
out there because it helps keep they
really kind of justifies uh the
importance of having these kind of tools
so kubernetes was designed originally
from the ground up to be Auto scaling
whereas took a swarm isn't the load
balancing is automatic on Docker Swan
whereas with kubernetes you have to
manually configure load band dancing
across your nodes the installation for
Docker swarm is really fast and easy I
mean you can be up and running within
minutes kubernetes takes a little bit
more time is a little bit more
complicated eventually you'll get there
um I mean it's not like it's going to
take you days and weeks but it's it is a
tool that's a when you compare the two
Docker swarms much easier to get up and
running now what's interesting is that
kubernetes is incredibly scalable and
it's you know that's its real strength
is its ability to have strong clusters
whereas with Docker swarm it's cluster
stream isn't um as strong when compared
to kubernetes now you compare it to
anything else on the market it's really
good um so this is kind of a splitting
hairs kind of comparison but kubernetes
really does have the advantage here if
you're looking at the two compared to
each other for scalability kubernetes
was designed for by Google to scale up
and support Google Cloud Network
infrastructure they both allow you to be
able to share um storage volumes with
Docking you can actually do it with any
container with that is managed by the
docker swamp whereas with kubernetes it
manages the storage with the pods and a
product can have multiple containers
within it but you can't take it down to
the level of the container interestingly
uh kubernetes does have a graphical user
interface for being able to control and
manage the environment the reality
however is that you're likely to be
using terminal to actually make the
controls and Commands to control your
either Docker swarm or kubernetes
environment um it's great that it has a
GUI and to get you started but once
you're up and running you're going to be
using terminal window for those fast
quick administrative controls that you
need to make so let's look at the
hardware components for kubernetes so
and what's interesting is that
kubernetes is extremely light of all the
systems that we're looking at it's
extremely lightweight
um it's allows you to have if you
compare it to like a virtual machine
which is very heavy you know kubernetes
is extremely lightweight and other uses
any resources at all interesting enough
though is that if you are looking at the
usage of CPU it's better to actually
take it for
um the cluster as a whole rather than
individual nodes because the nodes will
actually combined together to give you
that whole compute power again this is
why kubernetes works really well in the
cloud where you can do that kind of
activity rather than if you're running
in your own data center
um so you can have persistent volumes um
such as a local SSD or you can actually
attach to a cloud data storage again
kubernetes is really designed for the
cloud I would encourage you to use cloud
storage wherever possible rather than
relying on physical storage and the
reason being is that if you connect to
cloud storage and you need to flex your
your storage the cloud will do that for
you I mean that's just an inherent part
of why you'd have cloud storage where as
if you're connecting to physical storage
you're always restricted to the
limitations of the physical Hardware so
that's um kind of pivot and look at the
software components as compared to the
hardware components so the main part of
the components is the actual container
and all of the software running in the
container runs on Linux so if you have
Docker installed as a developer on your
machine it's actually running inside of
Linux and that's what makes it so
lightweight and really one of the things
that you'll find is that most data
centers and Cloud providers now are
running predominantly on Linux inside of
the um the the container itself is then
managed inside of a pod and a pod is
really just a group of containers
bundles together and the kubernetes
scheduler and proxy server then actually
manage what how the pods are actually
pushed out into your kubernetes
environment the the parts themselves can
actually then share resources both
networking and storage so the pods
aren't
um pushed out manually they actually
managed through a layer of abstraction
and part of their deployment and and
this is the strength of kubernetes you
you define your infrastructure and then
kubernetes will then manage it for you
and there isn't that problem of
um manual management of PODS uh if you
have to manage the deployment of them
and you that's simply taken away and
it's completely automated and the the
final area of software Services is on
Ingress and this is really the secure
way of being able to have communication
from outside of the cluster and passing
of information into that cluster and
again this has done securely through SSL
layers and allows you to ensure that
security is at the center of the work
that you have within your kubernetes
environment so let's dive now into the
actual architecture before we start
looking at a use case of how kubernetes
is being employed so kubernetes again is
um we looked at this uh diagram at the
beginning of the presentation and there
were really three key areas there's the
workstation where you develop your
commands and then you have your master
environment which controls the
scheduling the communication and the
actual commands that you have created
and pushes those out and manages the
health of your entire node Network and
each node has various pods so if we like
break this down so the master node is
the most vital component with the master
you have four key controls you have Etc
the controller manager schedule an API
server the cluster store Etc this
actually manages the details and values
that you've developed on your local
workstation and then we'll work with the
outlet control schedule and API server
to communicate that out those
instructions of how your infrastructure
should look like to your entire network
the control manager is really an API
server and again this is all about
security so we use wrestle apis which
can be the package in SSL to communicate
back and forth across your pods and the
master and indeed the services within
each of them as well so at every single
layer of abstraction the communication
is secure the schedule as the name would
imply really schedules when tasks get
sent out to the actual nodes themselves
the nodes themselves are are dumb nodes
they just have the applications um
running on them the master and the scam
is really doing all of the work to make
sure that your entire network is running
efficiently and then you have the API
server which has your rescue minds and
the communication and back and forth
across your networks that is secure and
efficient so your node environment is
where all the work that you do with your
containers gets pushed out too so um a
work is really a it's a combination of
containers and each container will then
logically run together on that node so
you'd have a collection of containers on
a node that all make logical sense to
have together within each node you have
a Docker and this is your isolated
environment for running your container
you have your cubelet which is a service
for conveying information back and forth
to the service about the actual health
of the kubernetes node itself and then
finally you have the proxy server and
the proxy server is able to manage the
nodes the volumes the the creation of
new containers and actually helps pass
the community the the health of the
container back up to the master to see
whether or not the containers should be
either killed stop started or updated so
finally let's look at see where
kubernetes is being used by other
companies so you know kubernetes is
being used by a lot of companies and
they're really using it to help manage
complex existing systems so that they
can have greater performance and with
the end goal of being able to Delight
the customer increase value to the
customer and enhance increased value and
revenue into the organization so example
of this is a company called BlackRock
where they actually went through the
process of implementing kubernetes so
they could so BlackRock had a challenge
where they needed to be able to have
much more Dynamic access to their
resources uh they were running a complex
installations on people's desktops and
it was just really really difficult to
be able to manage their entire
infrastructure so they actually went and
pivoted to using kubernetes and this
allowed them to be able to be much more
scalable and expansive in the in the
management of their infrastructure and
as you can imagine kubernetes was then
hooked into their entire existing system
and has really become a key part of the
success that BlackRock is now
experiencing of a very stable
infrastructure um and the bottom line is
that BlackRock is now able to have
confidence in their infrastructure and
be able to give their confidence as back
to their customers through the
implementation and more rapid deployment
of additional features and services
ansible today as one of the key tools
that you would have within your devops
environment so the things that we're
going to go through today is we're going
to cover why you would want to use a
product like ansible what ansible really
is and how it's of value to you in your
organization the differences between
ansible and other products that are
similar to it on the market and what
makes ansible a compelling product and
I'm going to dig into the architecture
for ansible we're going to look at how
you would create a Playbook how you
would manage your inventory of your
server environments and then what is the
actual workings of ansible there's a
little extra we're going to also throw
an ansible Tower one of the secret
Source solutions that you can use for
improving the Speed and Performance of
how you create your ansible environments
and finally we're going to go through a
use Case by looking at HootSuite social
media management company and how they
use ansible to really improve the
efficiency within their organizations so
let's jump into this so so the big
question is why ansible so you have to
think of ansible as another tool that
you have within your devops environment
for helping manage the servers and this
definitely falls on the operations side
of the devops equation so if we look
here we have a picture of Sam and like
yourselves Sam is a system administrator
and he is responsible for maintaining
the infrastructure for all the different
servers within his company so some of
the servers that he may have that he has
to maintain could be web servers running
Apache it could be database servers
running my Sequel and if you only have a
few servers then that's fairly easy to
maintain I mean if you have three web
servers and two database servers and
let's face it will we all love just to
have one or two servers to manage it
would be really easy to maintain the
trick however is as we start increasing
the number of servers and this is a
reality of the environments that we live
and operate in it becomes increasingly
difficult code to create consistent
setup of different infrastructures such
as web servers and databases for the
simple reason that we're all human as if
we had to update and maintain all of
those servers by hand there's a good
chance that we would not set up each
server identically now this is where
ansible really comes to the rescue and
helps you become an efficient operations
team ansible like other system Solutions
such as chef and puppet uses code that
you can write and describe the
installation and setup of your servers
so you can actually repeat it and deploy
those servers consistently into multiple
areas so now you don't have to have one
person redoing and re-following setup
procedures you just write one script and
then each script can be executed and
have a consistent environment so we've
gone through why you'd want to use
ansible let's step through what ansible
really is so you know this is all great
but you know how do we actually use
these tools in our environment so
ansible is a tool that really allows you
to create and control three key areas
that you'd have within your operations
environment first of all there's it
automation so you can actually write
instructions that automate the it setup
that you would typically do manually in
the past the second is the configuration
and having consistent configuration
imagine setting up hundreds of Apache
servers and being able to guarantee with
Precision that each of those Apache
servers is set up identically and then
finally you want to be able to automate
the deployment so that as you scale up
your server environment you can just
push out instructions they can deploy
automatically different servers the
bottom line is you want to be able to
speed up and make your operations team
more efficient so let's talk a little
bit about pull configuration and how it
works with ansible so there are two
different ways of being able to set up
different environments for Server Farms
one is to have a key server that has all
the instructions on and then on each of
the servers that connect to that main
Master server you would have a piece of
software known as a client in installed
on each of those servers that would
communicate to the main Master server
and then would periodically either
update or change the configuration of
the slave server this is known as a pull
configuration an alternative is a push
configuration and the push configuration
is slightly different the main
difference is as with a pull
configuration you have a master server
where you actually put up the
instructions but unlike the pull
configuration where you have a client
installed on each of the services with a
push configuration you actually have no
client installed on the remote servers
you simply are pushing out the
configuration to those servers and
forcing a restructure or a fresh clean
installation in that environment so
ansible is one of those second
environments where it's a push
configuration server and this contrasts
with other popular products like chef
and puppet which have a master's slave
architecture with a master server acting
with a client on a remote slave
environment where you would then be
pushing out the updates with ansible
you're pushing out the service and the
structure of the server to remote
hardware and you are just putting it
onto the hardware irrelevant of the
structure that's out there and there are
some significant advantages that you
have in that in that you're not having
to have the extra overhead weight of a
client installed on those remote servers
having to constantly communicate back to
the master environment so let's step
through the architecture that you would
have for an ansible environment
so when you're setting up an ansible
environment the first thing you want to
do is have a local machine and the local
machine is where you're going to have
all of your instruction and really the
power of the control that you'd be
pushing out to the remote server so the
local machine is where you're going to
be starting and doing all of your work
connected from the local machine are all
the different nodes pushing out the
different configurations that you would
set up on the local machine the
configurations that you would write and
you would write those in code within a
module so you do this on your local
machine for creating these modules and
each of these modules is actually
consistent playbooks the local machine
also has a second job and that job is to
manage the inventory of the nodes that
you have in your environment the local
machine is able to connect to each of
the different nodes that you would have
in your Hardware Network through SSH
clients so a secure client let's dig
into some of the different elements
within that architecture we're going to
take a first look at playbooks that you
would write and create for the ansible
environments so the core of ansible is
the Playbook this is where you create
the instructions that you write to
define the architecture of your Hardware
so the Playbook is really just a set of
instructions that configure the
different nodes that you have and each
of those set of instructions is written
in a language called yamo and this is a
standard language used for configuration
server environments to you know that
yaml actually stands for yaml 8 markup
language that's just a little tidbit to
hide behind your ear so let's have a
look or one of these playbooks it looks
like and here we have a sample yaml
script that we've written so you start
off your yaml script with three dashes
and that integrates the start of a
script and then the script itself is
actually consistent of two distinct
plays at the top we have play one and
below that we have play two within each
of those plays we Define which nodes are
we targeting so here we have a web
server in the top play and in the second
play we have a database server that
we're targeting and then within each of
those server environments we have the
specific tasks that we're looking to
execute so let's step through some of
these tasks we have an install patchy
task we have a start Apache task and we
have it installed my SQL task and when
we do that we're going to execute a
specific set of instructions and those
instructions can include installing
Apache and then setting the state of the
Apache environment or starting the
Apache environment and setting up and
running the MySQL environment so this
really isn't too complicated and that's
the really good thing about working with
yaml is it's really designed to make it
easy for you as an operations lead to be
able to configure the environments that
you want to consistently create so let's
take a step back though we have two
hosts we have web server and database
server why do these names come from well
this takes us into our next stage and
the second part of working with ansible
which is the inventory management part
of ansible so the inventory part of
ansible is where we maintain the
structure of our Network environment so
what we do here is part of the structure
and creating different nodes is we've
had to create two two different nodes
here we have a web server node and a
database server note and under web
server node we actually have the names
that were actually pointed to specific
machines within that environment so now
when we actually write our script all we
have to do is refer to either web server
or database server and the different
servers will have the instructions from
the yamascript executed on them this
makes it really easy for you to be able
to just point to new services without
having to write out complex instructions
so let's have a look at how ansible
actually works in real world so the real
world environment is that you'd have the
ansible software installed on a local
machine and then it connects to
different nodes within your network on
the local machine you'll have your first
your playbook which is the set of
instructions for how to set up the
remote nodes and then to identify how
you're going to connect to those nodes
you'll have an inventory we use secure
SSH connections to each of the servers
so we are encrypting the communication
to those servers we're able to grab some
basic facts on each server so we
understand how we can then push out the
Playbook to each server and configure
that server remotely the end goal is to
have an environment that is consistent
so this asks you a simple question what
are the major opportunities that Ansel
has over chef and puppet really like to
hear your answers in the comments below
pop them in there and we'll get back to
you and really want to hear how you feel
that ansible is a stronger product or
maybe you think it's a weaker product as
it compares to other similar products in
the market here's the bonus we're going
to talk a little bit about ansible Tower
so ansible Tower is an extra product
that red hat created it that really kind
of puts the cherry on the top of the ice
cream or is the icing on your cake add
ible by itself is a command line tool
however anselor Tower is a framework
that was designed to access ansible and
through the ansible tower framework we
now have an easy to use GUI this really
makes it easy for non-developers to be
able to create the environment that they
want to be able to manage in their
devops plan without having to constantly
work with the command prompt window so
instead of opening up terminal window or
a command window and writing out complex
instructions only in text you can now
use drag and drop and mouse click
actions to be able to create your
appropriate playbooks inventories and
pushes for your nodes alright so we've
talked a lot about ansible let's take a
look at a specific company that's using
ansible today and in this example we're
going to look at HootSuite now HootSuite
if you've not already used their
products and they have a great product
HootSuite is a social media management
system they are able to help with you
managing your pushes of social media
content across all of the popular social
media platforms they're able to provide
the analytics they're able to provide
the tools that marketing and sales teams
can use to be able to assess a sentiment
analysis of the messages that are being
pushed out really great tool and very
popular but part of their popularity
drove a specific problem straight to
HootSuite the challenge they had at
HootSuite is that they had to constantly
go back and rebuild their server
environment and they couldn't do this
continuously and be consistent there was
no standard documentation and they had
to rely on your memory to be able to do
this consistently imagine how complex
this could get as you're scaling up with
a popular product that now has tens of
thousands to hundreds of thousands of
users this is where ansible came in and
really helped the folks over at
HootSuite today the devops team at
HootSuite write out playbooks that have
Specific Instructions that Define the
architecture and structure of their
Hardware nodes and environments and are
able to do that as a standard product
instead of it being a problem in scaling
up their environment they now are able
to rebuild and create new servers in a
matter of seconds the bottom line is
ansible has been able to provide
HootSuite with it automation consistent
configuration and free up time from the
operations team so that instead of
managing servers they're able to provide
additional new value to the company this
is going to be six things that we're
going to stamp through the first is
really looking at what is the world that
many of you already work in today saying
how do you administer your networks and
your systems today then we're going to
look at how Chef can be the tool that
really helps improve the efficiencies of
creating a consistent operations
environment we're going to look at the
tools and the components and the
architecture that takes to construct
chef and then finally we're going to
take a use case of one of the many
companies that is using Chef today to
improve the efficiency of their
operations environments so let's take a
step back at what life was like before
Chef this may well be the life that
you're experiencing right now so as a
system administrator typically the way
that you work is that you are
responsible for the uptime of all of the
systems within your network and you may
have many many systems that you are
responsible for now if one system goes
down that's not a problem because as a
system administrator you can get
notifications and you can jump right
right onto the problem and solve it when
things start getting really difficult
however is when you have multiple
systems and you are not able as a single
person to get to all those systems and
solve them as quickly as possible the
problem that you start having is that
your environments are not in sync if
only this could all be automated well
this is where Chef comes to the rescue
for you so let me take some time and
introduce you to the concepts behind
chef and why it's a tool that you would
probably want to be using in your
environment so what actually is Chef so
Chef is a way in which you can use code
to fix your physical Hardware
environment and so what does that look
like well the way that we write out
scripts in Chef is that you can actually
code your entire environment so you
don't have to actually be managing your
environment on a hardware by Hardware
basis but actually use scripts called
recipes that will actually manage the
environment for you so Chef will
actually ensure that every system is
automatically sent to the right states
that meet the requirements that you have
defined in your code so you don't have
any issues where Hardware starts to fail
because it references the code to reset
the state within that environment and
that makes you a happy system
administrator so let's go through the
mechanics of how this actually works
with Chef so Chef is ultimately an
automation tool that converts
infrastructure to code and you'll often
hear the term infrastructure as code or
IAC and this really starts as a output
of the work that Chef has done because
you take the policies that you as an
organization have created and you
convert that into a scripted language
that can then be implemented
automatically this enables Chef to be
able to manage multiple systems very
very easily and considering how how
broad and deep our systems become it's
very easy to see how this can help you
as a system administrator the code is
then tested continuously and deployed by
Chef so that you're able to guarantee
that your standards and appropriate
state for your hardware and operating
environment are always maintained so
let's step through the different
components that are available in Chef so
the first one is actually creating the
actual recipes for chefs so you actually
work on these on a workstation and
you'll write out your code which is
referred to as a recipe and that recipe
will be written using a language called
Ruby good thing for you is that there
are lots of examples of rubies recipes
that have been created by the open
source Community a lot of examples knife
is the command tool that you use that
communicates between the recipes and the
server so your recipe is destruction a
knife is the tool that makes that
instruction work and sets the
appropriate State on the server
environment when you create more than
one recipe you start creating a
collection those collections are
referred to as cookbooks and you can
make take an environment such as a large
Network where you have multiple nodes
potentially hundreds of servers that
have to all have a consistent and equal
State environment and you would put that
cookbook and use that cookbook to ensure
with Chef that the state of each of
those nodes is consistent and then Ojai
fetches the current state of your nodes
and then the chef client configures the
nodes as defined within your cookbook so
let's step through the architecture and
working environment of Chef so for an
administrator a systems administrator
you only have to allow work from your
workstation and from that workstation
you can then configure all the nodes of
your environments you can use different
recipes that you create and those
recipes can be compiled into a cookbook
that can be then applied to a node the
thing that's of value is that as you
change the environment As you move and
mature as an organization and your nodes
need to mature consistently and quickly
with your organization all you have to
do is roll out a different recipe to
your nodes and then the nodes will use
the tools within Chef to make the
updates so you can create a second
recipe or even a third recipe to be
deployed out to your server environment
knife is going to be the tool that does
the hard work of doing the updates on
the server with the server you have of
your collection of recipes in a single
cookbook however understanding the state
of the individual nodes is important to
ensure that the information is broadcast
from the server with the instructions on
how to set up the network within those
nodes in this instance we use Ojai and
oi will fetch the state of the nodes and
send that back to the server and get the
information from The cookbook using the
chef client in your Chef node Network
there is the potential at times that
maybe one of your node fails if there
are two failed nodes then a request is
sent back to the server and information
on the latest node structure that is
identified and defined in the cookbook
will be sent to the failed nodes if that
fails to reset the nodes then the
workstation is notified and you as
administrator will receive a
notification to manually reset those
nodes it should be noted that this is a
very rare occasion within the chef
network setup so let's step through a
use case in this instance we're going to
look at herpolium and how they used Chef
so her poem is a bank and it's the
largest bank in Israel and they have a
mixture of Linux and windows servers
that they have to maintain and as you
can imagine this requires a lot of
constant configuration and work and this
has led to issues in the past so the
challenge that the team Napoleon faced
was creating and hardening software that
ran the major tasks and was repeatable
on those servers that it didn't matter
who the people were forming the jobs
that there was consistency in the work
that was being done and address the
problem that the servers including the
hard run the servers was not consistent
throughout the organization Chef
addresses those specific problems and
became the go-to product for her polium
they were able to write the recipes and
the cookbooks that could be deployed out
to the network effectively and it didn't
matter what this system was the recipes
reflected the standards that the system
administration team put together and
they were able to ensure and test each
script and use standard testing tools
for those scripts it didn't matter what
kind of environment that Napoleon had if
certain ports need to be closed or if a
firewall needs to be installed or
modified or if custom strong passwords
had to be created all of this could be
done using the recipes that Chef offers
and once those recipes had been
finalized they could be packaged into a
common cookbook that could then be
deployed to the entire network ensuring
a consistency in results for the company
and also it didn't matter whether the
cookbooks were being deployed to Linux
or Windows machine because the scripts
were being put together in the recipes
and were written in Ruby you could
actually go through and update and
modify those scripts depending on the
environment so if you needed to make a
modification for Linux you could make
those modifications so they were
consistent across all the Linux servers
or all the windows servers and this
really drove in the ability to harden
and secure the environment in the entire
network within a matter of minutes using
Chef versus the days and weeks that it
previously took so if we take a before
and after scenario using a shaft before
Chef tasks were repeated multiple times
and it was just hard to keep track of
all the people doing the work and the
reality is because so many people were
touching all the different systems
specific standards the banks had
established was simply not being met
after Chef all of the tasks could be
scripted using Ruby and using the recipe
and cookbook model that Chef has created
the chef tool was able to deploy out to
the entire network specific cookbooks
that provided a consistent experience
and consistent setup and operation
environment for all the hardware the end
result is that the system administration
team could guarantee that the standards
of the bank were being met consistently
so in this session what we can do is
we're going to cover what and why you
would use puppets what are the different
elements and components of puppet and
how does it actually work and then we'll
look into the companies that are
adopting puppet and what are the
advantages that they have now received
by having puppet within their
organization and finally we'll wrap
things up by reviewing how you can
actually write and manifest in puppet so
let's get started so why puppet so here
is a scenario that as an administrator
you may already be familiar with you as
an administrator have multiple servers
that you have to work with and manage so
what happens when a server goes down
it's not a problem you can jump jump
onto that server and you can fix it but
what if the scenario changes and you
have multiple servers going down so here
is where public shows its strap with
puppets all you have to do is write a
simple script that can be written with
Ruby and write out and deploy to the
servers your settings for each of those
servers the code gets pushed out now to
the servers that are having problems and
then you can choose to either roll back
to those servers to their previous
working States or set them to a new
state and do all of this in a matter of
seconds and it doesn't matter how large
your server environment is you can reach
to all of these servers your environment
is secure you're able to deploy your
software and you're able to do this all
through infrastructure as code which is
the advanced devops model for building
out Solutions so let's dig deeper into
what puppet actually is so puppet is a
configuration management tool maybe
similar tools like Chef that you may
already be familiar with it ensures that
all your systems are configured to a
desired and predictable state public can
also be used as a deployment tool for
software automatically you can deploy
your software to all of your systems or
to specific systems and this is all done
with code this means you can test the
environment and you can have a guarantee
that the environment you want is written
and deployed accurately so let's go
through those components of Puppets so
here we have a breakdown of the puppet
environment environment and on the top
we have the main server environment and
then below that we have the client
environment that would be installed on
each of the servers that would be
running within your network so if we
look at the top part of the screen we
have here our puppet master store which
has and contains our main configuration
files and those are comprised of
manifests that are actual codes for
configuring the clients we have
templates that combine our codes
together to render a final document and
you have files that will be deployed as
content that could be potentially
downloaded by the clients wrapping this
all together is a module of manifest
templates and files you would apply a
certificate authority to sign the actual
documents so that the clients actually
know that they're receiving the
appropriate and authorized modules
outside of the master server where you'd
create your manifest templates and files
you would have public clients as a piece
of software that is used to configure a
specific machine there are two parts to
the client one is the agent that
constantly interacts with the master
server to ensure that the certificates
are being updated appropriately and then
you have the fact that the current state
of the client that is used and
communicated back to through the agent
so let's step through the workings of
puppet so the puppet environment is a
master sleeve architecture the clients
themselves are distributed across your
network and they are constantly
communicating back to a Master server
environment where you have your puppet
modules the client agent sends a
certificate with the ID of that server
back to the master and then the master
will then sign their certificate and
send it back to the client and this
authentication allows for a secure and
verifiable communication between claims
and master the factor then collects the
state of the client and sends that to
the master based on the facts sent back
the master then compiles manifests into
the catalogs and those catalog sent back
to the clients and an agent on the
client will then initiate the catalog a
report is generated by the client that
describes any changes that have been
made and sends that back to the master
with the goal here of ensuring that the
master has full understanding of the
hardware running software in your
network this process is repeated at
regular infos ensuring all client
systems are up to date so let's have a
look at companies that are using puppets
today there are a number of companies
that have adopted puppet as a way to
manage their infrastructure so companies
that are using property today include
Spotify Google ATT so why are these
companies choosing to use puppet as
their main configuration management talk
the answer can be seen if we look at a
specific company Staples so Staples
chose to take and use puppet for their
configuration management tool and use it
within their own private Cloud the
results were dramatic the amount of time
that the it organization was able to
save in deploying and managing their
infrastructure through using puppets
enabled them to open up time to allow
them to expand with other and new
projects and assignments a real tangible
benefit to a company so let's look at
how you write a manifest in puppets so
so manifests are designed for writing
out in code how you would configure a
specific node in your server environment
the manifests are compiled into catalogs
which are then executed on the client
each of the manifests are written in the
language of Ruby without dot PP
extension now if we step through the
five key steps for writing a manifest
they are one create your manifest and
that is written by the system
administrator two compile your manifest
it's compiled into a catalog three
deploy the catalog is then deployed onto
the clients for or execute the catalogs
are run on the client by the agent and
then five and clients are configured to
a specific and desired state if we
actually look into how manifest is
written it's written with a very common
syntax if you've done any work with Ruby
or really configuration of systems in
the past this may look very familiar to
you so we break out the work that we
have here you start off with a package
file or service as your resource type
and then you give it a name and then you
look at the features that need to be set
such as IP address then you're actually
looking to have a command written such
as present or start the Manifest can
contain multiple resource types if we
continue to write our manifest and
puppets the default keyword applies a
manifest to all clients so an example
would be to create a file path that
creates a folder called sample in a main
folder called Etc the specified content
is written into a file that is then
posted into that folder and then we're
going to say we want to be able to
trigger an Apache service and then
ensure that that Apache service is
installed on a node so we write the
Manifest and we deploy it to a client
machine on that client machine a new
folder will be created with a file in
that folder and an Apache server will be
installed you can do this to any machine
and you will have exactly the same
results on those machines and today
we're going to go through a real tough
battle we're going to decide which is
better for your operations environment
is it Chef puppet and support or salt
stack all four are going to go head to
head so let's go through the scenario of
why you'd want to use these tools so
let's meet Tim he's our assistant
administrator and Tim is a happy camper
putting and working on all of the
systems and his network but what happens
if a system fails if there's a fire a
server goes down well Tim knows exactly
what to do he can fix that fire really
easily the problems become really
difficult for Tim however if multiple
servers start failing particularly when
you have large and expanding networks so
this is why Tim really needs to have a
configuration management tool and we
need to now decide what would be the
best tool for him because configuration
management tools can help make Tim look
like a superstar all he has to do is
configure the right codes that allows
him to push out the instructions on how
to set up each of the servers quickly
effectively and at scale all right let's
go through the tools and see which ones
we can use the tools that we're going to
go through are Chef puppet and support
and salt stacks and we have videos on
most of these software and services that
you can go and view to get an overview
or a deep dive in how those products
work so let's go and get to know our
contestants so I our first contestant is
Chef and Chef is a tool that allows you
to configure very large environments it
allows you to scale very effectively
across the entire ecosystem and
infrastructure Chev is by default an
open source code and one of the things
that you find is a consistent metaphor
for the tools that we recommend on
simplylearn is to use open source code
the code itself is actually written in
language of Ruby and erlang and it's
really designed for heterogeneous
infrastructures that are looking for a
mature solution the way that Chef works
is that you write recipes that are
compiled into cookbooks and those
cookbooks are the definition of how you
would set up a node and a node is a
selection of servers that you have
configured in a specific way so for
instance you may have Apache Linux
servers running or you may have a MySQL
server running or you may have a python
server running and Chef is able to
communicate back and forth forth between
the nodes to understand what nodes are
being impacted and we need to have
instructions sent out to them to correct
that impact you can also send
instructions from the server to the
nodes to make a significant update or a
minor update so there's great
communication going back and forth if we
look at the pros and cons the pros for
Chef is that there is a significant
following for chef and that has resulted
in a very large collection of recipes
that allow you to be able to quickly
stand up environments there's no need
for you to have to learn complex recipes
the first thing you should do is go out
and find the recipes that are available
it integrates with Git really well and
provides for really good strong Version
Control some of the consoles are really
around the learning speed it takes to go
from beginner user with Chef to being an
expert there is a considerable amount of
learning that has to take place and it's
compounded by having to learn Ruby as
the programming language and the main
server itself doesn't really have a
whole lot of control it's really
dependent on the communication
throughout the whole network all right
let's look at our second Contender
puppet and puppet is actually in many
ways very similar to Chef there are some
differences but again puppet is designed
to be able to support very large
heterogeneous organization it is also
built with Ruby and uses DSL for writing
manifests so there are some strong
similarities here to Chef as with a chef
there is a Master Slave infrastructure
with puppet and you have a master server
that has the manifests that you put
together in a single catalog and those
catalogs are then pushed out to the
clients over an SSL connection some of
the pros with puppet is that as with
Chef there is a really strong Community
around puppies and there's just a great
amount of information and support that
you can get right out of the gate it is
a very well developed reporting
mechanism that makes it easier for you
as an administrator to be able to
understand your infrastructure one of
the cons is that you have to really be
good at learning Ruby again as with shav
you know the more advanced tasks really
need to have those Ruby skills and as
with Chef the server also doesn't have
much control so let's look at our third
Contender here ansible and so ansible is
slightly different it is the way the
ansible works is that it actually just
pushes out the instructions to the
server environment there isn't a client
server or Master Slave environment where
ansible would be communicating backwards
and forwards with its infrastructure it
is merely going to push that
instructions out the good news is that
the instructions are written in yaml and
yowel stands for yaml 8 markup language
yaml is actually pretty easy to learn if
you know X XML and XML is pretty easy if
you know XML you're going to get yamo
really well ansible does work very well
on environments where the focuses are
getting servers up and running really
fast it's very very responsive and can
allow you to move quickly to get your
infrastructure up quick very fast and
we're talking seconds and minutes here
really really quick so again the way
that ansible works is that you put
together a Playbook and an inventory or
you have a Playbook so the way that
ansible works is that you have a
Playbook and the Playbook it then goes
against the inventory of servers and
will push out the instructions for that
Playbook to those servers so some of the
pros that we have for ansible we don't
need to have an agent installed on the
remote nodes and servers it makes it
easier for the configuration yaml is
really easy to learn you can get up to
speed and get very proficient with yaml
quickly the actual performance once you
actually have your infrastructure up and
running is less than other tools that we
have on our list now now I do have to
add a Proviso this is a relative less
it's still very fast it's going to be a
lot faster than individuals manually
standing up servers but it's just not as
fast as some of the other tools that we
have on this list and Gamma itself as a
language while it's easy to learn it's
not as powerful as Ruby Ruby will allow
you to do things that at an advanced
level that you can't do easily with the
ammo so let's look at our final
Contender here salt stack so sort stack
is a CLI based tool it means that you
will have to get your command line tools
out or your terminal window out so you
can actually manage the entire
environment via salt sack the
instructions themselves are based on
python but you can actually write them
in yaml or DSL which is really
convenient and as a product it's really
designed for environments that want to
scale quickly and be very resilient now
the way that sort snap works is that you
have a master environment that pushes
out the instructions to what they call
greens which is your network and so
let's step through some of the pros and
cons that we have here with salt stag so
salt stack is very easy to use once it's
up and running it has a really good
reporting mechanism that makes your job
as an operator in your devops
environment much much easier the actual
setup though is a little bit tougher
than some of the other tools and and
it's getting easier with the newer
releases but it's just a little bit
tougher and related to that is that sort
stack is fairly late in the game when it
comes to actually having a graphical
user interface for being able to create
and manage your environment other tools
such as ansible have actually had a UI
environment for quite some time all
right so we've gone through all four
tools let's see how they all stack up
next with each other so let the race
begin let's start with the first day
page architecture so the architecture
for most of our environments is a server
client environment so for Chef puppet
and salt snack so very similar
architecture there the one exception is
ansible which is a client Only Solution
so you're pushing out the instructions
from a server and pushing them out into
your network and there isn't a client
environment there isn't a two-way
communication back to that main client
for what's actually happening in your
network so let's talk about the next
stage ease of setup so we look at the
four tools there is one tool that really
stands out for ease of setup and that is
ansible it is going to be the easiest
tool for you to set up and if you're new
to having these types of tools in your
environment you may want to start with
ansible just to try out and see how easy
it is to create automated configuration
before looking at other tools now and so
with that said Chef puppet and salt
stack aren't that hard to set up either
and you'll find there's actually some
great instructions on how to do that
setup in the online community let's talk
about the languages that you can use in
your configuration so we have two
different types of language with both
chef and ansible being procedural in
that they actually specify at how you're
actually supposed to do the task in your
instructions with puppet and salt stack
it's decorative where you specify only
what to do in the instructions let's
talk about scalability which tools scale
the most effectively and as you can
imagine all of these tools are designed
for scalability that is the driver for
these kind of tools you want them to be
able to scale to massive organizations
what do the management tools look like
for our four contenders so again we have
a two-way split with ansible and salt
stack the management tools are really
easy to use you're going to love using
them they're just fantastic to use with
puppet and Chef the management tools are
much harder to learn and they do require
that you learn some either the puppet
DSL or the Ruby DSL to be able to be a
true master in that environment but what
does interoperability look like again as
you'd imagine with the similar to
scalability interoperability with these
products is very high in all four cases
now let's talk about Cloud availability
this is increasingly becoming more
important for organizations as they move
rapidly onto cloud services well both
ansible and salt stack have a big fail
here neither of them are available in
the most popular Cloud environments and
puppet and Chef are actually available
in both Amazon and Azure now we've
actually just haven't had a chance to
update our Chef link here but Chef is
now available on Azure as well as Amazon
so what does communication look like
with all of our four tools so the
communication is slightly different with
them Chef has its own knife tool and
whereas puppet uses SSL to secure
sockets layer and ansible and salt sack
use secure socket hashing and SSH as
their communication tool bottom line all
four tools are very secure in their
communication so who wins well here's
the reality all four tools are very good
and it's really dependent on your
capabilities and the type of environment
that you're looking to manage that will
determine which of these four tools you
should use the tools themselves are open
source so go out and experiment with
them there's a lot of videos our team
has done a ton of videos on these tools
and so feel free to find out other tools
that we have then covered so you can
learn very quickly how to use them but
consider the requirements that you have
and consider the capabilities of your
team if you have Ruby developers or you
have someone on your team that knows
Ruby your ability to choose a broader
set of tools becomes much more
interesting if however you're new to
coding then you may want to consider
yaml based tools again the final answer
is going to be up to you and we'll be
really interested on what your decision
is please post in the comments below
which tool you have decided to go with
and what are the challenges that you're
coming up with because we'll help answer
those challenges for you now let's start
with some beginner level delves project
so our first project is improve Jenkins
remoting the improved Jenkins remoting
project is a crucial initiative in the
devsell aimed at enhancing the
efficiency and reliability of Jenkins
reporting functionality Jenkins is a
widely used automation server in the
devops ecosystem serving as the backbone
of continuous integration and continuous
delivery CI CD pipelines the project
centers around identifying addressing
and optimizing issues related to Jenkins
remoting thereby elevating the overall
performance and user experience now
coming to project objectives so what
this project needs to be done the first
is to enhance security
so to strengthen the security measures
of Jenkins remoting to safeguard against
potential vulnerabilities and
unauthorized access Implement secure
communication protocols and
authentication mechanisms for better
data protection the next objective is
improve performance identify performance
bottlenecks in the remoting system and
optimize the resource utilization to
minimize latency and speed up the build
process conduct load testing to assess
the system's capability and scalability
and the next is stability and
reliability address any known stability
issues in the Jenkins recording module
conduct photo testing and debugging to
ensure a robust and reliable remote
execution environment and the next is
compatibility upgrades ensure
compatibility with the latest checking
release and keep up the remoting module
up to date with the evolving Jenkins
ecosystem this involves adapting to new
plugins apis and other changes
introduced in Jenkins and the next
objective for this project is
streamlined communication Implement
efficient communication protocols
between Jenkins master and agents to
reduce overhead and enhance data
transfer speeds evaluate potential
improvements such as binary
serialization and optimized data
exchange then we have error handling and
Reporting enhance error handling
capabilities in the remote system to
provide more informative and action
element error messages this will assist
users in diagnosing and resolving issues
promptly and the next is feature
expansion explore opportunities to
extend the capabilities of Jenkins
remoting this may evolve adding new
features like agent Auto Discovery
Dynamic scaling of Agents also support
for additional build environments so if
we just conclude the project that
improves Jenkins remoting project aims
to elevate the performance security and
functionality of Jenkins remote
execution capabilities by addressing
existing limitations and introducing new
features the project will contribute to
a more robust and efficient cicd
ecosystem enhancing the overall
experience for Jenkin choosers and
hosting continuous Improvement within
the devs community
so this was our first project and it
should and it is a beginner project
which you can showcase on your resume
and now moving on we'll see the second
project that is creating a simple web
server
so first we'll have a overview of the
project over the project description
so this project aims to demonstrate the
process of setting up a basic web server
using modern devops practices the
project will utilize a variety of tools
and Technologies to automate the
deployment configuration and management
of the web server by following this
project participants will gain insights
into how Dev principles streamline the
software development life cycle enabling
faster more reliable and efficient
application delivery and now we will
talk about the project objectives
so the first is infrastructure
provisioning the project will ensure
provisioning the necessary
infrastructure to host the web server
this includes selecting a cloud platform
example AWS Azure and gcp and setting up
virtual machines or containers to run
the server the next is Version Control
Version Control will be used to manage
the project source code it will be the
Version Control System of choice
allowing for collaboration code
management and easy rollback to previous
versions if necessary and the next is
configuration management
configuration management tools example
ansible puppet Chef will be employed to
automate the installation and setup of
the web server software this ensures
consistency across environments and
simplifies the deployment process and
the next is continuous integration and
continuous deployment the project will
Implement cicd pipelines using tools
such as Jenkins gitlab CI CD or private
CI automated testing and deployment
pipelines will be established to speed
up the deployment process and ensure
code quality and the next objective is
monitoring and logging monitoring tools
like form thus own refiner will be
integrated to keep track of the web
server's performance and health logging
Solutions like elk stack elasticsearch
log stash Ibana will be used to gather
and analyze logs for troubleshooting and
debugging and the next objective is
security
security practices will be implemented
throughout the project to protect the
web server from potential threats this
includes managing user access enabling
firewalls and applying security patches
regularly the next is scalability a
project will consider the server's
scalability to handle increased traffic
and load strategies like load balancing
and auto scaling will be explored to
ensure Optimal Performance under varying
conditions and what this project
delivers is that at the end of the
project you guys will have successfully
created a simple web server and deployed
it using modern devops tools and
methodologies they will have gained
hands-on experience in provisioning
infrastructure Version Control
configuration management CI CD pipelines
monitoring locking security
implementation and scalability
considerations now moving to a third
project that is create default base
images with token so we will have an
overview of this project so the create
default base images of the docker
project is a DeVos initiative aimed at
streamlining the process of creating
default base images using Docker
containers it revolves around optimizing
the development and deployment workflow
by providing a consistent and efficient
foundation for various applications and
services
so Docker like now we'll talk about
something
and a brief about the docker so Docker
is a popular containerization technology
that allows developers to package
applications and their dependencies into
lightweight portable containers
now we'll see the project benefits or
their objectives
so the first objective is consistency
with standardized base images all
applications run on a uniform
environment reducing compatibility
issues and unexpected Behavior across
different deployment stages the next is
Time Savings developers no longer need
to configure the environment from sketch
for each application using the
pre-configured base images saves the
time and effort leading to faster
development Cycles the next is improved
security by adding to security best
practices and base image creation the
organization can minimize potential
vulnerabilities and enhance overall
application security and the next is
scalability
the project allows for scalability and
flexibility as more applications can
leverage the same base images reducing
resource consumption and simplifying
infrastructure management and the next
is Version Control and rollbacks
poisoning Docker files and base images
enable easy rollbacks to previous
working States in case of issues or bugs
and the next we have is automated builds
integrating the base image creation
process into the CI CD Pipeline and
shows automated and continuous building
testing and deployment enhancing the
development workflow and the next
objective for this project is reduce
deployment errors standardized face
images help identify and resolve issues
early in the development process
reducing the likelihood of Errors during
deployment and the next is portability
Docker containers built on the base
images can run consistently across
different environments from development
to production ensuring portability and
consistency so to the conclusion for
this project the create default base
images with broker projects offers a
practical and effective approach to
streamline the software delivery process
by providing standardized secure and
automated base images the project
improves application development
deployment consistency and overall
software quality however to overcome
challenges and maximize benefits
effective collaboration regular
maintenance and continuous Improvement
are essential the successful
implementation of this project
organizations can achieve a more
efficient and reliable software
development life cycle meeting the
demands of today's Dynamic technology
landscape so that's why it's have been
important to keep this project on your
resume
and now for those looking to level up
their skills let's explore some
intermediate level doves project ideas
so starting with creating a CI CD
pipeline using Azure devops so in this
project we'll explore the world of
continuous integration and continuous
deployment CI CD and how it can
streamline your software development and
deployment processes so first we will
understand CI CD so to begin let's have
a brief overview of CI so continuous
integration is a practice where code
changes are automatically built and
tested whenever they are pushed to
Version Control repository such as git
this ensures that the code is
continuously integrated and validated
caching potential issues early in the
development cycle and on the other hand
continuous development CD is the process
of automatically deploying the code to
production environments after passing
through the CI phase CD animates manual
interventions and ensure fast and
reliable software releases and this is
what we know about CI and CD Now we will
have an introduction to Azure devops so
Azure devops is a powerful cloud-based
platform that provides a set of tools
and services for software development
including Version Control build
Automation and release management Azure
devops offers a seamless and integrated
environment for implementing CI CD
Pipelines and the project goals are that
is to set up a CI CD pipeline using
Azure devops for a sample application we
will demonstrate the end-to-end process
of automatically building testing and
deploying the application to various
environments including staging and
production
now we'll talk about the project goals
or the project objectives so the first
is automate the build process that is
set up an automated build process to
compile the source code and create
Deployable artifacts this ensures that
developers code changes this ensures
that developers code changes are
consistently built and tested the next
is automate testing Implement automated
testing to validate the application's
functionality and ensure the new changes
do not introduce regressions or bugs and
the next is continuous deployment
establish a seamless deployment process
that automates the release of
applications to different environments
example staging and production as soon
as they pass the required test and the
next we have is Version Control
integration integrate Version Control
System example kit with the CI CD
pipeline to manage source code changes
effectively and the next we have is
monitoring and feedback enable
monitoring and logging mechanisms to
provide feedback on the application's
performance and help during and after
deployment so for the conclusion of this
project you can create a CI CD pipeline
using Azure devops that's an excellent
project to Showcase your devops skills
and expertise by mastering cicd
pipelines you will be well equipped to
deliver software with speed reliability
and efficiency and the next project we
have in our pocket for intermediate
levels are Implement devops life cycle
with Amazon web services that is AWS
so we will have a brief introduction
that is the project implementing devops
lifecycle with Amazon web services
focuses on integrating the principles
and practices of devops into the
software development and deployment
process using the AWS platform devops is
a collaborative approach that emphasizes
seamless communication and cooperation
between development and operations team
aimed at delivering high quality
software at a private Pace by leveraging
AWS services and tools this project aims
to automate streamline and optimize the
software development drive cycle so now
you'll see the project objectives why we
should consider this project
so the first objective is seamless
collaboration poster culture of
collaboration between development and
operation scheme breaking down
traditional silos to enable continuous
feedback and Improvement throughout the
software development process and the
next is Automation and efficiency
utilize AWS services and tools to
automate repetitive tasks such as code
deployment testing and infrastructure
provisioning to improve efficiency and
reduce manual errors and the next we
have is continuous integration and
continuous deployment cicd Implement CI
CD pipelines using AWS code pipeline
code build and code deployed to enable
frequent and Reliable Software releases
ensuring faster time to Market and
better customer satisfaction
the next is infrastructure as code
utilize AWS cloud formation or AWS cdk
to Define infrastructure as code
allowing for Version Control repeatable
and consistent infrastructure
deployments the next we have is
monitoring and logging Implement robust
monitoring and logging Solutions using
AWS cloudwatch and other services to
gain real-time insights into application
performance and operational metrics the
next we have is scalability and
resilience leverage AWS Auto scaling and
load balancing capabilities to ensure
applications can handle varying
workloads and maintain High availability
and the next we have is security and
compliance Implement security best
practices using AWS identity and access
management IAM encryption mechanisms and
other AWS security features to protect
the applications and data from potential
threats so these are the project
objectives we need to consider for
before building this project and now
we'll just have a conclusion about this
project that is the implementing devops
life cycle with Amazon web services
project enables organizations to embrace
a devops culture and take full advantage
of the AWS platform's capabilities so by
automating processes implementing CI CD
and enhancing collaboration the project
aims to accelerate software delivery
while maintaining high quality and
reliability additionally it establishes
a foundation for scalable secure and
resilient applications on the AWS cloud
and if you have all these skills you
will be easily employable with these
employers
so now moving on to next project that is
build a scale level application with
kubernetes and docomo so this project
combines two powerful Technologies
kubernetes and offer to create a highly
scalable and efficient application
deployment system let's dive in and take
a closer look the main goal of this
project is to containerize your
application using Docker making it
portable and easy to manage then we will
utilize kubernetes a powerful container
or castration tool to manage and scale
your application effortlessly so first
we'll understand Docker that is before
we get started with kubernetes let's
understand the importance of dog doka is
a containerization platform that allows
you to package your application and its
dependencies into a single unit known as
a container these containers are
lightweight portable and can run
consistently across different
environments now we'll see what is
containerizing your application now
comes the exciting part containerizing
your application we'll use Docker to
package your application ensuring it
runs smoothly and consistently across
various platforms this step simplifies
the deployment process making it easier
to manage and maintain your application
the next we have is Introduction to
kubernetes
whether application containerized it's
time to introduce kubernetes kubernetes
is an open source container
orchestration platform that automates a
deployment scaling and management of
containerized applications it provides a
robust ecosystem for managing
microservices and scalable applications
now we'll see how we can deploy our
application so deploying your
application with kubernetes is a game
changer kubernetes takes charge of
managing your containers and showing
there and efficiently and remain highly
available you can easily scale your
application up or down based on the
demand making it ideal for handling
varying workloads now we'll see the
managing application updates one of the
most significant advantage of kubernetes
is its ability to manage application
updates seamlessly with kubernetes you
can roll out updates without any
downtime ensuring smooth transition for
your users kubernets also enhances the
high availability and full tolerance of
your application by replicating your
application's containers across multiple
nodes kubernetes ensures that your
application stays accessible even if
some nodes fail and another powerful
feature of kubernetes is the horizontal
pod Auto scalar HP the HPA automatically
adjusts the number of replicas based on
the application CPU usage or other
custom metrics this Dynamic scaling
keeps your application responsive under
varying workloads and the next is
monitoring and login monitoring and
logging are crucial in a production
environment kubernetes integrates with
various monitoring and logging Solutions
like chromethius and Elk stack enabling
you to keep track of your application's
performance and troubleshoot any issues
effectively and now we'll see some of
the benefits of this project that is
building a scalable application with
kubernetes and Dockers over several
benefits that is efficient deployment
scalability High availability for
tolerance and I will conclude this
project that is the exciting build a
scalable application with kubernetes and
Docker project that is by containerizing
your application with both these you'll
have a highly efficient and scalable
application deployment system and now
we'll move to Advanced helps project
ideas and we'll start with creating a
monitoring dashboard for an application
so first we'll have an overview of this
project the goal of this dev's project
is to develop and Implement a monitoring
dashboard for an application the
monitoring dashboard will provide
real-time insights and visualizations
into the application's performance
health and key metrics by setting up a
robust monitoring system the development
and operations teams can proactively
identify and address issues ensure
Optimal Performance and make data driven
decisions to improve the application's
reliability and user experience now
we'll talk about the key objectives of
this project the first is monitoring
setup the first step involves setting up
the monitoring infrastructure this may
include selecting appropriate monitoring
tools and Technologies such as home this
graphene elk stack elasticsearch log
stats or other Solutions based on
specific requirements of the project the
next is data collection once the
monitoring tools are in place the next
objective is to configure them to
collect relevant data from the
application and its underlying
infrastructure data sources may include
server logs application logs performance
metrics error dates database queries and
other relevant data points the next is
and the next is dashboard design the
core of the project is designing a
visually informative and user-friendly
Dash mode a dashboard should present the
collected data in a meaningful and easy
to understand format it should include
charts graphs tables and visual elements
that provide real-time insights into the
application's health and performance the
next is altering mechanism setting up an
opening mechanism is crucial for
notifying the relevant stakeholders
whenever certain predefined thresholds
or anomalies are detected alerts can be
configured to be sent through email
instant messaging platforms or other
communication channels the next is
scalability considerations in a real
world scenario application May scale
horizontally or vertically based on user
demand the monitoring dashboard should
be designed to handle such scaling
scenarios without compromising on
performance and data equalizing the next
is security and access control and the
next we have is documentation and
training and the next is continuous
Improvement that is monitoring
requirements can evolve over time and in
conclusion by creating this monitoring
dashboard for the application the
project aims to provide actionable
insights timely alerts and a
comprehensive view of the application's
performance and health
this will empower the devops team to
proactively address issues optimized
resources and ensure the application
meets its performance and reliability
targets ultimately leading to improved
user satisfaction and a more efficient
development process now moving to the
next project that is deploy a
containerized application
so first we'll have an overview of this
project that is in the deploy a
containerized application devs project
we focus on the process of
containerizing and deploying a web
application using Docker and kubernetes
this project is ideal for intermediate
level devs Engineers or the advanced
level we are talking about the advanced
levels that are looking to enhance their
skills in containerization and container
orchestration so first we will have a
little understanding of the project that
is we aim to take a web application and
package it as a Docker container Docker
containers offer a lightweight portable
and consistent environment for
applications to run ensuring that they
work seamlessly across different
platforms the Project's primary goal is
to demonstrate how containerization
simplifies its application deployment
and scalability now we'll see the
setting of the project that is these are
the steps to setting up a project the
first is containerizing the web
application the next is building and
pushing the docker image and the third
is deploying the containerized
application that is setting up the
kubernetes cluster then we have
deploying the application to kubernetes
and then exposing the application with a
service so benefits and learning what
the outcomes of this project are
throughout this project participants all
the people who want to do this project
will gain valuable knowledge and
experience in various areas that is
containerization container orchestration
Dev principles scalability and high
availability
so in conclusion this project is an
excellent Dev project for those looking
to level up their containerization and
orchestration skills by completing this
project participants will have a solid
understanding of container-based
deployments and the benefits of using
kubernets for managing containerized
applications and remember Hands-On
experiences invaluable in the devs field
so moving on to the last project that is
automate kubernetes cluster deployment
with terraform
so first we'll have an overview that is
the automate kubernetes cluster
deployment with terraform project is a
devops initiative aimed at streamlining
the process of setting up and managing
kubernetes clusters using terraform
kubernetes has become a popular
container orchestration platform due to
its flexibility scalability and
robustness however setting up a
kubernetes cluster manually can be
complex and time consuming this project
addresses this challenge by leveraging
terraforms infrastructure as code
capabilities to automate the entire
clustered deployment process
so now we'll see the key objectives of
this project that is infrastructure as
code
that is emphasizing the use of terraform
a declarative language to define the
desired taste or state of the kubernetes
cluster infrastructure the next is
automated kubernetes cluster
provisioning the next is customizability
and flexibility then we have multi-cloud
support that is ensuring compatibility
with various Cloud providers then we
have integrating with best practices
that is incorporating kubernetes best
practices and security standards
then we have some key features that is
the project includes the following key
features terraform configuration files
then variable management provider
plugins networking and security
kubernetes deployment continuous
integration and continuous deployment so
these are the benefits and in conclusion
this project empowers organizations to
rapidly and reliably set up kubernetes
clusters in a repeatable and efficient
manner by leveraging terraforms
capabilities the project brings
automation consistency and flexibility
to the kubernetes deployment process
ultimately enhancing the overall
efficiency and reliability of the
application development's life cycle and
congratulations you have explored a
range of doves project ideas that will
enrich your resume and make you more
desirable to employers remember the key
is to select projects aligned with your
interest and career goals working on
these projects will not only boost your
technical skills but also demonstrate
your commitment to becoming a proficient
devops engineer and before we begin if
you are someone who is interested in
building a career in devops by
graduating from the best universities
for a professional who elicits to switch
careers with devops while learning from
the experts then try hearing a show to
Simply learn skeltec postgraduate
program in devops the course link is
mentioned in the description box that
will navigate you to the course page
where you can find a complete audio of
the program being offered
the field of devops has grown X
over the past
tools that aims to break down the silos
between development and operations team
and streamline the software delivery
process by doing so devops enables
organizations to deliver high quality
software faster and more efficiently
than ever before
pursuing a career in devops then this
video on how to choose devops as a
career is for you with the proper
roadmap on how to get started with it
also do not forget to subscribe to our
YouTube channel and hit the Bell icon to
never miss an update from Simply learn
so without any further Ado let's get
started first is to understand the role
of a devops professional before diving
into the specifics of how to become a
devops professional it's essential to
understand the roles and
responsibilities of a devops
professional a devops professional is
typically responsible for Designing
implementing and maintaining the
infrastructure automation tools and
processes required for the efficient
delivery of software this includes
collaborating with development teams to
integrate automation into the software
development process setting up and
maintaining the infrastructure required
for the software delivery pipeline such
as servers databases and network systems
developing and implementing automation
scripts to improve the efficiency of
software delivery process
continuously monitoring and analyzing
the software delivery process to
identify and resolve bottlenecks and
improve efficiency
ensuring that the software delivery
process adheres to Industry best
practices and regulatory requirements so
now that you understand the roles and
responsibilities of a devops
professional let's take a a look at the
road map to become a devops professional
the first is to learn the basics of
software development to Be an Effective
devops professional it's essential to
have a good understanding of software
development processes including
programming languages software
development methodologies and Version
Control tools you can start by learning
programming languages like python Java
or Ruby well a devops engineer course
will prepare you for a career in devops
Technologies through this devops
engineer course you will learn to review
deployment methodologies CI CD pipelines
observability and use devops tools like
gate Docker and Jenkins with this devops
engineer certification you can check out
the link for this course in the
description following that learn about
infrastructure and operations as a
devops professional you will be
responsible for managing infrastructure
including servers databases and network
system so it's essential to have a good
understanding of infrastructure and
operations this include learning about
servers operating systems Network
protocols and storage systems
then explore about automation tools and
cloud services
automation is a critical part of devops
learning automation tools such as
ansible puppet or shirt can help you
streamline the software delivery process
and improve efficiency
most organizations today use cloud
services to deliver software learning
cloud services such as AWS Azure or gcp
can help you design and Implement Cloud
architecture that are efficient and
scalable
well to help you with learning devops
and cloud services simply learn has to
offer Azure devops solution expert
master's program to help you become an
industry ready professional in this
course you will learn to plan smarter
collaborate better and ship faster with
a set of modern development services the
link for this course is mentioned in the
description do check them out
once done with Skilling you need to gain
experience and get certified as
discussed before the best way to gain
experience in devops is to work on real
world projects this can be achieved
through internships volunteering for
open source projects or even taking on
small projects on your own this will
allow you to put your theoretical
knowledge into practice and gain
hands-on experience along with it there
are various certification programs
available such as the devops Institute
certification program AWS certification
devops engineer and Microsoft Azure
devops Solutions certification once you
have gained experience and acquired the
necessary skills it's time to start
looking for job opportunities
we have various job titles in devops
such as devops engineer site reliability
engineer automation engineer and release
engineer in top hiring companies in the
field include Amazon Google Microsoft
IBM and many others well a postgraduate
program in devops in collaboration with
Caltech ctme is a professional
development option that will Square your
skills with industry standards in this
course you will learn how to formalize
and document development processes and
create a self-documenting system devops
certification course will also cover
Advanced tools like puppet soft stack
and ansible that will help
self-governance and automated management
at scale link is mentioned in the
description do check it out what is
devops devops is like a teamwork
approach for making computer programs
better and faster it combines the work
of software developers and operation
team the goal is to help them work
together and use tools that speed up the
process and make fewer mistakes they
also keep an eye on the programs and fix
problems early This Way businesses can
release programs faster with few errors
and everyone works better together if
you want to learn more about this then
check post graduate program in devops to
understand from the basics to advanced
concepts this postgraduate program in
devops is crafted in partnership with
Caltech ctme this comprehensive course
aligns your expertise with industry
benchmarks experience or Innovative
Blended learning merging live online
devops certification sessions with
immersive labs for practical Mastery
advancing career with Hands-On training
that meets industry demands alright now
let's move on to the first question of
devops interview question which is why
what do you know about devops so think
of a devops like teamwork in the IIT
world it's become really important
because it helps teams work together
smoothly to make computer programs
faster and with fewer mistakes imagine a
soccer team everyone works together to
win the game devops is similar it's when
computer developers and operations
people team up to make software better
they start working together from the
beginning when they plan what the
software will be like until they finish
and put it out for people to use this
Teamwork Makes sure things go well and
the software works great so this was
about devops now moving on to the second
question which is how is devops
different from agile methodology devops
is like a teamwork culture where the
people who create the software and the
people who make it work smoothly join
forces this helps in always improving
and updating the software without any
big bricks agile is a way of making
softwares that's like taking small steps
towards instead of big jumps it's about
releasing small parts of the software
quickly and getting feedback from the
users this helps in solving any issues
or differences between what users want
and what developers are making so you
can answer this question in this way
alright moving on to the third question
which is what are the ways in which a
build can be scheduled slash run in
Jenkins so as you can see there are four
ways by source code management commits
second is after the completion of other
builds third is scheduled to run at a
specified time and fourth one is manual
build requests so if the interior asks
then you can answer these four ways in
which a build can be scheduled in
Jenkins all right now the fourth
question is what are the different
phases in devops so the various phases
of devops lifecycle are as follows so as
you can see first is plan so initially
there should be a plan for the type of
application that needs to be developed
getting a rough feature of the
development process is always a good
idea then code the application is coded
as per the end user requirements then
there is build build application by
integrating various codes formed in
previous steps after that there is test
this is the most crucial step of the
application development test the
application and rebuild if necessary
then there is integrate so multiple
codes from different programmers are
integrated into one after integrate
there is deploy so code is deployed into
a cloud environment for further usage it
is ensured that any new changes do not
affect the functioning of a high traffic
website after that there is operate so
operations are performed on the code if
required then there is Monitor
applications performance is monitored
changes are made to meet the end user
requirements so these all were the
different phases of devops and here we
have explained each one of these you can
go through it and if the interviewer
have asked you this question you can
answer it in a similar way all right now
moving on to the next question which is
mention some of the core benefits of
devops so the core benefits of devops
are as follows first of all we'll say
technical benefits first technical
benefit of devops's continuous software
delivery then second is less complex
problems to manage third is early
detection and faster correction of
defects then here comes the business
benefits first benefit of devops that is
business benefit of devops is faster
delivery of features so it allows faster
delivery of features then there is
stable operation environment
then third one is improve communication
and collaboration between the teams so
these were the business benefits so here
we have discussed both technical
benefits and business benefits all right
now the next question is how will you
approach a project that needs to
implement devops so here are the simpler
terms here's how we can bring devops
into a specific project using these
Steps step one would be first we look at
how things are currently done and figure
out where we can make them better this
makes about two to three weeks then we
can plan for what changes to make then
step two would be we create a small test
to show that our plan works when
everyone agrees with it we can start
making the real changes and put the plan
into action then third step would be we
are all set to actually use devops we do
things like keeping track of different
versions of our work putting everything
together testing it and making it
available to the users we also watch how
it's working to make sure everything
goes smoothly by doing these steps right
the keeping track of changes putting
everything together testing and watching
how it's going we are all set to use
devops in our project
so this is how you can approach a
project that needs to implement in
devops and this is how you can answer
this question all right moving on to the
question number seven which is what is
the difference between continuous
delivery and continuous deployment all
right so first would be continuous
delivery so it ensures code can be
safely deployed on production whereas
continuous deployment here every change
that passes the automated test is
deployed to production automatically
then in continuous delivery it ensures
business applications and services
functions as expected whereas in
continuous deployment it makes software
development and the release process
faster and more robust in continuous
delivery delivers every change to a
production like environment through
rigorous and automatic testing whereas
in continuous deployment there is no
explicit approval from a developer and
requires a developed culture of
monitoring so these were the three
points that you you can highlight while
differentiating between continuous
delivery and continuous deployment all
right so this was the question number
seven now moving on to the question
number eight which is name three
security mechanisms of Jenkins uses to
authenticate users so here we have to
name three security mechanisms so first
one would be Jenkins uses an internal
database to store user data and
credentials second is Jenkins can use
the lightweight directory access
protocol that is ldap server to
authenticate users third one is Jenkins
can be configured to employ the
authentication mechanism that the
deployed application server uses so
these were the three mechanisms that
Jenkins uses to authenticate users all
right now moving on to the question
number nine which is how does continuous
monitoring help you maintain the entire
architecture of the system so continuous
monitoring within devops involves the
ongoing identification detection and
reporting of any anomalies or security
risk across the entire system
infrastructure it guarantees the proper
functioning of services applications and
resources on servers by overseeing
server statuses it accesses the accuracy
of application operations this practice
also facilitates uninterpreted audits
transactions to utility and regulated
surveillance
so this was about the question number
nine that was how does continuous
monitoring help you maintain the entire
architecture of the system now moving on
to the question number 10 which is what
is the role of AWS in devops so in the
realm of devops AWS assumes several
rules first one would be adaptable
services so it offers adaptable
pre-configured Services eliminating the
necessity for software installation or
configuration second one is design for
expansion whether managing one instance
or expanding two thousand AWS Services
accommodate seamless scalability then
there is automated operations AWS
empowers task and process automation
freeing up valuable time for inventing
Pursuits the next one is enhanced
security
AWS and entity and accesses management
that is IAM allows precise user
permissions and policy establishment
then the last one is extensive partner
Network so AWS Fosters a vast partner
Network that integrates with and
enhances its service offerings so these
were the role of awfs in the realm of
devops all right now moving on to the
question number 11 name three important
devops kpis the three important kpis are
as follows first one is mean time to
failure recovery this is the average
time taken to recover from a failure
second gapy is deployment frequency the
frequency in which the deployment occurs
is called deployment frequency third one
is percentage of field deployments the
number of times the deployment Fields is
called percentage of field deployments
so these were the three important devops
kpis this question can also be asked all
right moving on to question number 12
which is how is IAC implemented using
AWS
so comments by discussing traditional
methods involving scripting commands
into files followed by testing in
isolated environments prior to
deployment notice how this practice is
giving way to infrastructure as code IAC
comparable to code for various Services
IAC aided by AWS empowers developers to
craft accesses and manage infrastructure
components descriptively utilizing
formats like Json or yaml this Fosters
streamline development and expeditious
implementation of alterations in
infrastructure all right now moving on
to question number 13 which is describe
the branching strategies you have used
they'll ask you this question so to test
our knowledge the purpose of branching
and our experience of branching at a
past job this question is usually asked
so here we will discuss topics that can
help considering this devops interview
question so release branching we can
clone the develop Branch to create a
release branch once it has enough
functionality for a release this Branch
kicks off the next release cycle thus no
new features can be contributed Beyond
this point the things that can be
contributed are documentation generation
bug fixing and other release related
tasks the release is merged into master
and given a version number once it is
ready to ship it should also be merged
back into the development branch which
may have evolved since the initial
release so this was the first one then
there is future branching this branching
model maintains all modifications for a
specific feature contained within a
branch the branch gets merged into
Master once the feature has been
completely tested and approved by using
tests that are automated then the third
branching is Task branching in this
branching model every task is
implemented in its respective Branch the
task key is mentioned in the branch name
we need to Simply look at the task key
in the branch name to discover which
code which tasks so these were the
branching strategies that you can say
that you have used or if you have really
used it otherwise you can say something
else
all right moving on to question number
14 which is can you explain the shift
left to reduce failure Concept in devops
so shifting left within the devops
framework is a concept aimed to enhance
security for performance and related
aspects to illustrate consider the
entity of diverse processes currently
security valuations occur before the
deployment stage by employing the left
ship approach we can introduce security
measures during the earlier development
phase denoted as the left
this integration spans on multiple
phases encompassing pre-development and
testing not confined to development
alone this holistic integration is
likely to elevate security measures by
identifying vulnerabilities at initial
stage leading to a more fortified
overall process now moving on to the
question number 15 which is what is blue
teen deployment pattern so this approach
involves seamless deployment aimed at
minimizing downtime it entails shifting
traffic from one instance to another
requiring their placement of outdating
code and a new version to integrate
fresh code
the updated version resides in a green
environment while the older one remains
in a blue environment after modifying
the existing version a new instance is
generated from the old one to execute
the updating instance ensuring a
smoother transition so the main focus of
this approach is smooth deployment
all right so this was question number
15. now moving on to the question number
16 which is what is continuous testing
so continuous testing involves the
automated execution of tests within the
software delivery pipeline
offerings immediate insights into
potential business rates within the
latest release
by seamlessly integrating testing into
every stage of the software develop pill
isifit I repeat by seamlessly
integrating testing into the every stage
of software delivery life cycle
continuous testing minimizes issues
during transition phases and empowers
development teams and instant feedback
this approach accelerates developer
efficiency by obtaining the need to
re-run all tests after each update and
project rebuild culminating in notable
gains in speed and productivity so this
was about continuous testing
now moving on to question number 17 what
are the benefits of automation testing
so some of the benefits of automation
testing includes first it helps to save
money and time
second is unattended execution can be
easily done third one is huge test
matrices can be easily tested the next
one is parallel execution is enabled
then there is reduced human generated
errors which results in improved
accuracy last one is repeated test task
execution is supported
so these are the benefits of automation
testing
coming to question number 18 which is
what is a Docker file used for
so a Docker file is used for creating
Docker images using the build command
with a Docker image any user can run the
code to create Docker containers once
Docker image is built it's uploaded in
Docker registry
from the docker registry users can get
the docker image and build new
containers whenever they want so this is
what Docker file is used for
coming to question number 19 which is
what is the process for reverting a
commit that has already been pushed and
made public so there are two ways to do
that to revert a comment so first would
be remove or fix the bad file in a new
commit and push it to the remote
Repository
then commit it to the remote repository
using get commit minus M commit message
again git commit minus M commit message
so this is the first way then second is
create a new commit that undoes all the
changes that were made in the bad
comment
then use the command get reward
commit ID as you can see in the screen
First Command also on the second command
also the second command says get revert
commit ID commit ID will have to put the
commit ID all right so question number
19 this was now moving on to the last
question which is question number 20 how
do you find a list of files that have
been changed in a particular command
so the answer would be the command to
get a list of files that have been
changed in a particular comment is
git diff 3 minus r then there is commit
hash so this command as you can see on
the screen get div 3 minus r commit hash
you can put it then example is also
there like
commit hash
87e6735
f21b so this example as you can see on
the screen then there is minus r flag
instruct so the command to list
individual files and commit hash will
list all the files that were changed or
added in that command so there's telling
about the as you can see minus our
functionality minus r is a flag that
instructs the command to list individual
files and commit hash will list all the
files that were changed or added in that
comment
so these were the top 20 devops
interview questions that you must
understand if you are planning to give a
devops interview
so in this video we have explored key
Concepts methodologies and best
practices that are crucial in fostering
collaboration between development and
operation streams by understanding the
principles discussed here you are well
equipped to navigate the dynamic
landscape of devops and that's a wrap on
a DeVos engineer full course we hope you
have gained valuable insights into the
world of devops and are now equipped
with the knowledge to streamline
development processes enhance
collaboration and deliver software
faster and more reliably remember devops
are constantly evolving field so keep
exploring experimenting and adapting to
new tools and practices if you found
this course helpful don't forget to like
share and subscribe for more engaging
content until next time keep exploring
keep innovating and let devops take you
to the New Horizons thanks for watching
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos to nerd up and get certified
click here
foreign