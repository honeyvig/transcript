hello everyone and welcome to the cnn vs
rnn full course video by simply loan in
this video we are going to look at the
basics of neural network followed by
learning about convolutional neural
network use the tensorflow library in
python and solve a use case demo using
the cnn algorithm after that we are
going to understand about recurrent
neural network and learn about lstms
that is long short-term memory networks
again in rn also we are going to
solve two different use cases one using
tensorflow and the other one using keras
so let's get started
last summer my family and i visited
russia even though none of us could read
russian we did not have any trouble in
figuring our way out all thanks to
google's real-time translation of
russian boards into english
this is just one of the several
applications of neural networks
neural networks form the base of deep
learning a subfield of machine learning
where the algorithms are inspired by the
structure of the human brain
neural networks take in data train
themselves to recognize the patterns in
this data and then predict the outputs
for a new set of similar data
let's understand how this is done
let's construct a neural network that
differentiates between a square circle
and triangle
neural networks are made up of layers of
neurons these neurons are the core
processing units of the network
first we have the input layer which
receives the input the output layer
predicts our final output
in between exists the hidden layers
which perform most of the computations
required by our network
here's an image of a circle
this image is composed of 28 by 28
each of available channels is assigned a
numerical value known as weight
the inputs are multiplied to the
corresponding weights and their sum is
sent as input to the neurons in the
hidden layer
each of these neurons is associated with
a numerical value called the bias which
is then added to the input sum
this value is then passed through a
threshold function called the activation
function
the result of the activation function
determines if the particular neuron will
get activated or not
an activated neuron transmits data to
the neurons of the next layer over the
channels
in this manner the data is propagated
through the network
this is called forward propagation
in the output layer the neuron with the
highest value fires and determines the
output the values are basically a
probability
for example here our neuron associated
with square has the highest probability
hence that's the output predicted by the
neural network
of course just by a look at it we know
our neural network has made a wrong
prediction but how does the network
figure this out note that our network is
yet to be trained
during this training process along with
the input our network also has the
output fed to it
the predicted output is compared against
the actual output to realize the error
in prediction the magnitude of the error
indicates how wrong we are and the sign
suggests if our predicted values are
higher or lower than expected
the arrows here give an indication of
the direction and magnitude of change to
reduce the error
this information is then transferred
backward through our network
this is known as back propagation
now based on this information the
weights are adjusted
this cycle of forward propagation and
back propagation is iteratively
performed with multiple inputs
this process continues until our weights
are assigned such that the network can
predict the shapes correctly in most of
the cases
this brings our training process to an
end
you might wonder how long this training
process takes honestly neural networks
may take hours or even months to train
but time is a reasonable trade-off when
compared to its scope
let us look at some of the prime
applications of neural networks facial
recognition cameras on smartphones these
days can estimate the age of the person
based on their facial features this is
neural networks at play first
differentiating the face from the
background and then correlating the
lines and spots on your face to a
possible age forecasting neural networks
are trained to understand the patterns
and detect the possibility of rainfall
or rise in stock prices with high
accuracy music composition neural
networks can even learn patterns in
music and train itself enough to compose
a fresh tune so here's a question for
you which of the following statements
does not hold true a activation
functions are threshold functions b
error is calculated at each layer of the
neural network c both forward and back
propagation take place during the
training process of a neural network d
most of the data processing is carried
out in the hidden layers
leave your answers in the comments
section below with deep learning and
neural networks we are still taking baby
steps the growth in this field has been
foreseen by the big names companies such
as google amazon and nvidia have
invested in developing products such as
libraries predictive models and
intuitive gpus that support the
implementation of neural networks the
question dividing the visionaries is on
the reach of neural networks to what
extent can we replicate the human brain
we'd have to wait a few more years to
give a definite answer but if you
enjoyed this video it would only take a
few seconds to like and share it also if
you haven't yet do subscribe to our
channel and hit the bell icon as we have
a lot more exciting videos coming up fun
learning till then
neural network tutorial my name is
richard kirschner with the simply learn
team that's www.simplylearn.com
get certified get ahead today we're
going to be covering the convolutional
neural network tutorial
do you know how deep learning recognizes
the objects in an image and really this
particular neural network is how image
recognition works it's very central one
of the biggest building blocks for image
recognition it does it using convolution
neural network and we over here we have
the basic picture of a hummingbird
pixels of an image fed as input you have
your input layer coming in so it takes
that graphic and puts it into the input
layer you have all your hidden layers
and then you have your output layer and
your output layer one of those is going
to light up and say oh it's a bird we're
going to go into depth we're going to
actually go back and forth on this a
number of times today so if you're not
catching all the image
don't worry we're going to get into the
details so we have our input layer
accepts the pixels of the image as input
in the form of arrays and you can see up
here where they've actually labeled each
block of the bird in different arrays so
we'll dive into deep as to how that
looks like and how those matrixes are
set up here hidden layer carry out
feature extraction by performing certain
calculations and manipulation so this is
the part that kind of reorganizes that
picture multiple ways until we get some
data that's easy to read for the neural
network this layer uses a matrix filter
and performs convolution operation to
detect patterns in the image and if you
remember that convolution means to coil
or to twist so we're going to twist the
data around and alter it and use that
operation to detect a new pattern there
are multiple hidden layers like
convolution layer rel u is how that is
pronounced when that's the rectified
linear unit that has to do with the
activation function that's used
pooling layer also uses multiple filters
to detect edges corners eyes feathers
beak etc and just like the term says
pooling is pulling information together
and we'll look into that a lot closer
here so if you're if it's a little
confusing now we'll dig in deep and try
to get you squared away with that and
then finally there is a fully connected
layer that identifies the object in the
image so we have these different layers
coming through in the hidden layers and
they come into the final area and that's
where we have a one node or one neural
network entity that lights up that says
it's a bird
what's in it for you we're going to
cover an introduction to the cnn what is
convolution neural network how cnn
recognizes images we're going to dig
deeper into that and really look at the
individual layers in the convolutional
neural network and finally we do a use
case implementation using the cnn we'll
begin our introduction to the cnn by
introducing the pioneer of convolutional
neural network jan le he was the
director of facebook ai research group
built the first convolutional neural
network called lynette in 1988 so these
have been around for a while and have
had a chance to mature over the years it
was used for character recognition tasks
like reading zip code digits imagine
processing mail and automating that
process
cnn is a feed forward neural network
that is generally used to analyze visual
images by producing data with a
grid-like topology a cnn is also known
as a convent and very key to this is we
are looking at images that was what this
was designed for and you'll see the
different layers as we dig in near some
of the other some of them are actually
now used since we're using tensorflow
and keras in our code later on you'll
see that some of those layers appear in
a lot of your other neural network
frameworks but in this case this is very
central to processing images and doing
so in a variety that captures multiple
images and really drills down into their
different features in this example here
you see flowers are two varieties orchid
and a rose i think the orchid is much
more dainty and beautiful and the rose
smells quite beautiful i have a couple
rose bushes in my yard they go into the
input layer that data is in sent to all
the different nodes in the next layer
one of the hidden layers based on its
different weights and its setup it then
comes out and gives those a new value
those values then are multiplied by
their weights and go to the next hidden
layer and so on and then you have the
output layer and one of those nodes
comes out and says it's an orchid and
the other one comes out and says it's a
rose depending on how it was well it was
trained what separates the cnn or the
convolutional neural network from other
neural networks is a convolutional
operation forms the basis of any
convolutional neural network in a cnn
every image image is represented in the
form of arrays of pixel values so here
we have a real image of the digit 8
that then gets put onto
its pixel values represented in the form
of an array in this case you have a two
dimensional array and then you can see
in the final in form we transform the
digit eight into its representational
form of pixels of zeros and one where
the ones represent in this case the
black part of the eight and the zeros
represent the white background to
understand the convolution neural
network or how that convolutional
operation works we're going to take a
side step and look at matrixes in this
case we're going to simplify it and
we're going to take two matrices a and b
of one dimension now kind of separate
this from your thinking as we learned
that you want to focus just on the
matrix aspect of this and then we'll
bring that back together and see what
that looks like when we put the pieces
for the convolutional operation here
we've set up two arrays we have uh in
this case are a single dimension matrix
and we have a equals five three seven
five nine seven and we have b equals one
two three so in the convolution as it
comes in there's gonna look at these two
and we're gonna start by doing
multiplying them a times b and so we
multiply the arrays element wise and we
get five six six
where five is the five times one six is
three times two and then the other six
is two times three and since the two
arrays aren't the same size they're not
the same setup we're gonna just truncate
the first one and we're gonna look at
the second array multiplied just by the
first three elements of the first array
now that's going to be a little
confusing remember a computer gets to
repeat these processes hundreds of times
so we're not going to just forget those
other numbers later on we'll see we'll
bring those back in and then we have the
sum of the product in this case 5 plus 6
plus 6 equals 17. so in our a times b
our very first digit in that matrix of a
times b is 17. and if you remember i
said we're not going to forget the other
digits so we now have 3
five we move one set over and we take
three two five and we multiply that
times b and you'll see that three times
one is three two times two is four and
so on and so on we sum it up so now we
have the second digit of our a times b
product in the matrix and we continue on
with that same thing so on and so on so
then we would go from three seven five
to seven five nine to five nine seven
this short matrix that we have for a
we've now covered all the different
entities in a that match three different
levels of b now in a little bit we're
going to cover where we use this math at
this multiplying of matrixes and how
that works
but it's important to understand that
we're going through the matrix of
multiplying the different parts to it to
match the smaller matrix with the larger
matrix i know a lot of people get lost
at is you know what's going on here with
these matrixes oh scary math not really
that scary when you break it down we're
looking at a section of a and we're
comparing it to b so when you break that
down your mind like that you realize
okay so i'm just taking these two
matrixes and comparing them and i'm
bringing the value down into one matrix
a times b we're reducing that
information in a way that will help the
computer see different aspects let's go
ahead and flip over again back to our
images
here we are back to our images talking
about going to the most basic
two-dimensional image you can get to
consider the following two images the
image for the symbol backslash when you
press the backslash the above image is
processed you can see there for the
image for the forward slash is the
opposite so we click the forward slash
button that flips
very basic we have four pixels going in
can't get any more basic than that here
we have a little bit more complicated
picture we take a real image of a smiley
face
then we represent that in the form of
black and white pixels so if this was an
image in the computer it's black and
white and like we saw before we convert
this into the zeros and one so where the
other one would have just been a matrix
of just four dots now we have a
significantly larger image coming in so
don't worry we're going to bring this
all together here in just a little bit
layers in convolutional neural network
we're looking at this we have our
convolution layer and that really is the
central aspect of processing images in
the convolutional neural network that's
why we have it and then that's going to
be feeding in and you have your relu
layer which is you know as we talked
about the rectified linear unit we'll
talk about that a little bit later the
relu isn't how it act is how that layer
is activated is the math behind it what
makes the neurons fire you'll see that a
lot of other neural networks when you're
using it just by itself it's for
processing smaller amounts of data where
you use the atom activation feature for
large data coming in now because we're
processing small amounts of data in each
image the relu layer works great you
have your pooling layer that's where
you're pulling the data together pooling
is a neural network term it's very
commonly used i like to use the term
reduce so if you're coming from the map
and reduce side you'll see that we're
mapping all this data through all these
networks and then we're going to reduce
it we're going to pull it together and
then finally we have the fully connected
layer that's where our output is going
to come out so we have started to look
at matrixes we've started to look at the
convolutional layer where it fits in and
everything we've taken a look at images
so we're going to focus more on the
convolution layer since this is a
convolutional neural network a
convolution layer has a number of
filters and perform convolution
operation every image is considered as a
matrix of pixel values consider the
following five by five image whose pixel
values are only zero and one now
obviously when we're dealing with color
there's all kinds of things that come in
on color processing but we want to keep
it simple and just keep it black and
white and so we have our image pixels uh
so we're sliding the filter matrix over
the image and computing the dot product
to detect the patterns and right here
you're going to ask where does this
filter come from this is a bit confusing
because the filter is going to be
derived
later on we build the filters when we
program or train our model so you don't
need to worry what the filter actually
is but you do need to understand how a
convolution layer works is what is the
filter doing filter and you'll have mini
filters you don't have just one filter
you'll have lots of filters that are
going to look for different aspects and
so the filter might be looking for just
edges it might be looking for different
parts we'll cover that a little bit more
detail in a minute right now we're just
focusing on how the filter works as a
matrix remember earlier we talked about
multiplying matrixes together and here
we have our two dimensional matrix and
you can see we take the filter and we
multiply it in the upper left image and
you can see right here one times one one
times zero one times one we multiply
those all together then sum them and we
end up with the convolved feature of
four we're going to take that and
sliding the filter matrix over the image
and computing the dot product to detect
patterns so we're just going to slide
this over we're going to predict the
first one and slide it over one notch
predict the second one and so on and so
on all the way through until we have a
new matrix and this matrix which is the
same size as the filter has reduced the
image and whatever filter whatever
that's filtering out is going to be
looking at just those features reduced
down to a smaller matrix so once the
feature maps are extracted the next step
is to move them to the relu layer so the
relu layer the next step first is going
to perform an element-wise operation so
each of those maps coming in if there's
negative pixels so it says all the
negative pixels to zero um and you can
see this nice graph where it just zeroes
out the negatives and then you have a
value that goes from zero up to whatever
value is coming out of the matrix this
introduces non-linearity to the network
so up until now we have a we say
linearity we're talking about the fact
that the feature has a value so it's a
linear feature this feature
came up and has let's say the feature is
the edge of the beak you know it's like
or the backslash that we saw um you'll
look at that and say okay this feature
has a value from negative 10 to 10 in
this case um if it was one it'd say yeah
this might be a beak it might not might
be an edge right there a minus five
means no we're not even going to look at
it to zero and so we end up with an
output and the output takes all these
features all these filtered features
remember we're not just running one
filter on this we're running a number of
filters on this image and so we end up
with a rectified feature map that is
looking at just the features coming
through and how they weigh in from our
filters so here we have an input of a
looks like a toucan bird
very exotic looking real image is
scanned in multiple convolution and the
relu layers for locating features and
you can see up here is turn it into a
black and white image and in this case
we're looking in the upper right hand
corner for a feature and that box scans
over a lot of times it doesn't scan one
pixel at a time a lot of times it will
skip by two or three or four pixels uh
to speed up the process that's one of
the ways you can compensate if you don't
have enough resources on your
computation for large images and it's
not just one filter slowly goes across
the image you have multiple filters have
been programmed in there so you're
looking at a lot of different filters
going over the different aspects of the
image and just sliding across there and
forming a new matrix
one more aspect to note about the relu
layer is we're not just having one value
coming in uh so not only do we have
multiple features going through but
we're generating multiple relu layers
for locating the features that's very
important to note you know so we have a
quite a bundle we have multiple filters
multiple rail u which brings us to the
next step forward propagation now we're
going to look at the pooling layer the
rectified feature map now goes through a
pooling layer pooling is a down sampling
operation that reduces the
dimensionality of the feature map that's
all we're trying to do we're trying to
take a huge amount of information and
reduce it down to a single answer this
is a specific kind of bird this is an
iris this is a rose so you have a
rectified feature map and you see here
we have our rectified feature map coming
in
we set the max pooling with a two by two
filters and a stride of two and if you
remember correctly i talked about not
going one pixel at a time uh well that's
where the stride comes in we end up with
a two by two pooled feature map but
instead of moving one over each time and
looking at every possible combination we
skip a step we skip a few there we go by
two we skip every other pixel and we
just do every other one and this reduces
our rectified feature map which is you
can see over here 16 by 16 to a 4x4 so
we're continually trying to filter and
reduce our data so that we can get to
something we can manage and over here
you see that we have the max
three four one and two and in the max
pooling we're looking for the max value
a little bit different than what we were
looking at before so coming from the
rectified feature we're now finding the
max value and then we're pulling those
features together so instead of think of
this as image of the map think of this
as how valuable is a feature in that
area how much of a feature value do we
have we just want to find the best or
the maximum feature for that area they
might have that one piece of the filter
of the beak said oh i see a one in this
beak in this image and then it skips
over and says i see a three in this
image and says oh this one is rated as a
four we don't want to sum it together
because then you know you might have
like five ones and it'll say ah five but
you might have uh four zeros and one ten
and that ten says well this is
definitely a beak where the ones will
say probably not a beak a little strange
analogy since we're looking at a bird
but you can see how that pulled feature
map comes down and we're just looking
for the max value in each one of those
matrixes pooling layer uses different
filters to identify different parts of
the image like edges corners body
feathers eyes beak etc i know i focus
mainly on the beak but obviously each
feature could be each a different part
of the bird coming in so let's take a
look at what that looks like structure
of a convolution neural network so far
this is where we're at right now we have
our input image coming in and then we
use our filters and there's multiple
filters on there that are being
developed to kind of twist and change
that data and so we multiply the
matrixes we take that little filter
maybe it's a two by two we multiply it
by each piece of the image and if we
step two then it's every other piece of
the image that generates multiple
convolution layers so we have a number
of convolution layers we have
set up in there just looking at that
data we then take those convolution
layers we run them through the relu
setup and then once we've done through
the release setup and we have multiple
values going on multiple layers that are
relu then we're going to take those
multiple layers and we're going to be
pooling them so now we have the pooling
layers or multiple poolings going on up
until this point we're dealing with
sometimes multiple dimensions you can
have three dimensions some strange data
setups that aren't doing images but
looking at other things they can have
four five six seven dimensions uh so
right now we're looking at 2d image
dimensions coming in into the pooling
layer so the next step is we want to
reduce those dimensions or flatten them
so flattening flattening is a process of
converting all of the resultant
two-dimensional arrays from pooled
feature map into a single long
continuous linear vector so over here
you see where we have a pooled feature
map maybe that's the bird wing and it
has values 6847 and we want to just
flatten this out and turn it into 6847
or a single linear vector and we find
out that not only do we do each of the
pooled feature maps we do all of them
into one long linear vector so now we've
gone through our convolutional neural
network part and we have the input layer
into the next setup all we've done is
taken all those different pooling layers
and we flatten them out and combine them
into a single linear vector going in so
after we've done the flattening we have
just a quick recap because we've covered
so much so it's important to go back and
take a look at each of the steps we've
gone through the structure of the
network so far is we have our
convolution where we twist it and we
filter it and multiply the matrixes we
end up with our convolutional layer
which uses the rel u to figure out the
values going out into the pooling as you
have numerous convolution layers that
then create numerous pooling layers
pulling that data together which is the
max value which one we want to send
forward we want to send the best value
and then we're going to take all of that
from each of the pooling layers and
we're going to flatten it and we're
going to combine them into a single
input going into the final layer once
you get to that step you might be
looking at that going boy that looks
like the normal intuit to most neural
network and you're correct it is so once
we have the flattened matrix from the
pooling layer that becomes our input so
the pooling layer is fed as an input to
the fully connected layer to classify
the image and so you can see as our
flattened matrix comes in in this case
we have the pixels from the flattened
matrix fed as an input back to our
toucan or whatever that kind of bird
that is i need one of these to identify
what kind of bird that is it comes into
our ford propagation network
and that will then have the different
weights coming down across and then
finally it selects that that's a bird
and it's not a dog or a cat in this case
even though it's not labeled the final
layer there in red is our output layer
our final output layer that says bird
cat or dog so quick recap of everything
we've covered so far we have our input
image which is twisted and multiplied
the filters are multiplied times the
matrix the two matrixes multiplied all
the filters to create our convolution
layer our convolution layers there's
multiple layers in there because it's
all building multiple layers off the
different filters then goes through the
relu as this activation and that creates
our pooling and so once we get into the
pooling layer we then and the pooling
look for who's the best what's the max
value coming in from our convolution and
we take that layer and we flatten it and
then it goes into a fully connected
layer our fully connected neural network
and then to the output and here we can
see the entire process how the cnn
recognizes a bird this is kind of nice
because it's showing the little pixels
and where they're going you can see the
filter is generating this convolution
network and that filter shows up in the
bottom part of the convolution network
and then based on that it uses the relu
for the pooling the pulling then find
out which one's the best and so on all
the way to the fully connected layer at
the end or the classification in the
output layer so that'd be a
classification neural network at the end
so we covered a lot of theory up till
now and you can imagine each one of
these steps has to be broken down in
code so putting that together can be a
little complicated not that each step of
the process is overly complicated but
because we have so many steps we have
one two three four five different steps
going on here with sub steps in there
we're going to break that down and walk
through that in code so in our use case
implementation using the cnn we'll be
using the cfar10 dataset from canadian
institute for advanced research for
classifying images across 10 categories
unfortunately they don't let me know
whether it's going to be a toucan or
some other kind of bird but we do get to
find out whether it can categorize
between a ship a frog deer bird airplane
automobile cat dog horse truck so that's
a lot of fun and if you're looking
anything in the news at all of our
automated cars and everything else you
can see where this kind of processing is
so important in today's world and very
cutting edge as far as what's coming out
in the commercial deployment i mean this
is really cool stuff we're starting to
see this just about everywhere in
industry so great time to be playing
with this and figuring it all out let's
go ahead and dive into the code and see
what that looks like when we're actually
writing our script
before we go on let's do uh one more
quick look at what we have here let's
just take a look at data batch one keys
and remember jupiter notebook i can get
by with not doing the print statement if
i put a variable down there it'll just
display the variable and you can see
under data batch one for the keys since
this is a dictionary we have the batch
one label data and file names so you can
actually see how it's broken up in our
data set so for the next step or step
four as we're calling it uh we want to
display the images using matte plot
library there's many ways to display the
images you can even uh well there's
other ways to drill into it but matplot
library is really good for this and
we'll also look at our first reshape
setup or shaping the data so you can
have a little glimpse into what that
means uh so we're going to start by
importing our map plot and of course
since i am doing jupiter notebook i need
to do the map plot inline command so it
shows up on my page so here we go we're
going to import matplot library.pipelot
as plt and if you remember matplot
library the pie plot is like a canvas
that we paint stuff onto and there's my
percentage sign matplot library inline
so it's going to show up in my notebook
and then of course we're going to import
numpy as np for our numbers python array
setup and let's go ahead and set
x equals to data batch one so this will
pull in all the data going into the x
value and then because this is just a
long stream of binary data uh we need to
go a little bit of reshaping so in here
we have to go ahead and reshape the data
we have 10 000 images okay that looks
correct and this is kind of an
interesting thing it took me a little
bit to i had to go research this myself
to figure out what's going on with this
data and what it is is it's a 32 by 32
picture and let me do this let me go
ahead and do a drawing pad on here so we
have 32 bits by 32 bits and it's in
color so there's three bits of color now
i don't know why the data is
particularly like this it probably has
to do with how they originally encoded
it but most pictures put the three
afterward so what we're doing here is
we're gonna take uh the shape we're
gonna take the data which is just a long
stream of information and we're going to
break it up into 10 000 pieces and those
10 000 pieces then are broken into three
pieces each and those three pieces then
are 32 by 32. you can look at this like
an old-fashioned projector where they
have the red screen or the red projector
the blue projector in the green
projector and they add them all together
and each one of those is a 32 by 32 bit
so that's probably how this was
originally formatted was in that kind of
ideal things have changed so we're going
to transpose it and we're going to take
the 3 which was here and we're going to
put it at the end so the first part is
reshaping the data from a single line of
bit data or whatever format it is into
10 000 by 3 by 32 by 32 and then we're
going to transpose the color factor to
the last place so it's the image then
the 32 by 32 in the middle that's this
part right here and then finally we're
going to take this which is three bits
of data and put it at the end so it's
more like we do process images now and
then as type this is really important
that we're going to use an integer 8.
you can come in here and you'll see a
lot of these they'll try to do this with
a float or a float 64. what you got to
remember though is a float uses a lot of
memory so once you switch this into uh
something that's not integer eight which
goes up to 128 you are just gonna the
the amount of ram let me just put that
in here it's going to go way up the
amount of ram that it loads
so you want to go ahead and use this you
can try the other ones and see what
happens if you have a lot of ram on your
computer but for this exercise this will
work just fine and let's go ahead and
take that and run this so now our x
variable is all loaded and it has all
the images in it from the batch one data
batch one and just to show we were
talking about with the as type on there
if we go ahead and take x0 and just look
for its max value let me go ahead and
run that uh you'll see it doesn't oops i
said 128 it's 255. uh you'll see it
doesn't go over 255 because it's
basically an ascii character is what
we're keeping that down to we're keeping
those values down so they're only 255 0
to 255 versus float value which would
bring this up
exponentially in size and since we're
using the matplot library we can do
oops that's not what i wanted since
we're using the matplot library we can
take our canvas and just do a plt dot im
for image show and uh let's just take a
look at what x0 looks like and it comes
in i'm not sure what that is but you can
see it's a very low grade image broken
down to the minimal pixels on there and
if we did the same thing oh let's do uh
let's see what one looks like hopefully
it's a little easier to see run on there
not enter let's hit the run on that
and we can see this is probably a semi
truck that's a good guess on there and i
can just go back up here instead of
typing the same line in over and over
and we'll look at three uh that looks
like a dump truck unloading uh and so on
you can do any of the 10 000 images we
can just jump to 55
looks like some kind of animal looking
at us there probably a dog and just for
fun let's do just one more uh
run on there and we can see a nice car
for our image number four uh so you can
see we paste through all the different
images it's very easy to look at them
and they've been reshaped to fit our
view and what the
matplot library uses for its format so
the next step is we're going to start
creating some helper functions we'll
start by a one hot encoder to help us or
processing the data remember that your
labels they can't just be words they
have to switch it and we use the one hot
encoder to do that and then we'll also
create a class uh cfar helper so it's
going to have an init and a setup for
the images and then finally we'll go
ahead and run that code so you can see
what that looks like and then we get
into the fun part where we're actually
going to start creating our model our
actual neural network model so let's
start by creating our one hot encoder
we're going to create our own here ah
and it's going to return an out and
we'll have our vector coming in and our
values equal 10. what this means is that
we have the 10 values the 10 possible
labels and remember we don't look at the
labels as a number because a car isn't
one more than a horse that'd be just
kind of bizarre to have horse equals
zero car equals one plane equals two cat
equals three so a cat plus a car equals
what uh so instead we create a numpy
array of zeros and there's going to be
10 values so we have a 10 different
values in there so you have
0 or 1. 1 means it's a cat 0 means it's
not a cat
in the next line it might be that 1
means it's a car zero means it's not a
car so instead of having one output with
a value of zero to ten you have ten
outputs with the values of zero to one
that's what the one hot encoder is doing
here and we're gonna utilize this in
code in just a minute so let's go ahead
and take a look at the next helpers we
have a few of these helper functions
we're going to build and when you're
working with a very complicated python
project dividing it up into separate
definitions and classes is very
important otherwise it just becomes
really ungainly to work with so let's go
ahead and put in our next helper which
is a class and this is a lot in this
class so we'll we'll break it down here
and let's just start uh oops we put a
space right in there there we go that
was a little bit more readable at a
second space so we're going to create
our class the cipher helper and we'll
start by initializing it now there's a
lot going on in here so let's start with
the init part uh self dot i equals zero
that'll come in a little bit we'll come
back to that in the lower part we want
to initialize our training batches so
when we went through this there was like
a meta batch we don't need the meta
batch but we do need the data batch one
two three four five and we do not want
the testing batch in here this is just
the self all train batches so we're
gonna come make an array of of all those
different images and then of course we
left the test batch out so we have our
self.test batch
we're gonna initialize the training
images and the training labels and also
the test images and the test labels so
these are just this is just to
initialize these variables in here then
we create another definition down here
and this is going to set up the images
let's just take a look and see what's
going on in there now we could have all
just put this as part of the
init part
since this is all just helper stuff but
breaking it up again makes it easier to
read it also makes it easier when we
start executing the different pieces to
see what's going on so that way we have
a nice print statement to say hey we're
now running this and this is what's
going on in here we're going to set up
the self training images at this point
and that's going to go to a numpy array
v stack and in there we're going to load
up
in this case the data for d itself all
train batches again that points right up
to here so we're going to go through
each one of these uh files or each one
of these data sets because they're not a
file anymore we've brought them in data
batch one points to the actual data and
so our self training images is going to
stack them all into our into a numpy
array and then it's always nice to get
the training length and that's just the
total number of uh self training images
in there and then we're going to take
the self training images and let me
switch marker colors because i am
getting a little too too much on the
markers up here oops there we go bring
down our marker change
so we can see it a little better and at
this point this should look familiar
where did we see this well when we
wanted to uh
look at this above and we want to look
at the images in the matplot library we
had to reshape it so we're doing the
same thing here we're taking our self
training images and uh based on the
training length total number of images
because we stacked them all together so
now it's just one large file of images
we're going to take and look at it as
our
three video cameras that are each
displaying a 32 by 32 we're going to
switch that around so that now we have
each of our images that stays the same
place and then we have our 32 by 32 and
then by our three
our last are three different values for
the color and of course we want to go
ahead and they run this where we say
divide by 255 that was from earlier it
just brings all the data into zero to
one that's what this is doing so we're
turning this into a zero to one array
which is uh all the pictures 32 by 32 by
three and then we're going to take the
self training labels and we're going to
pump those through our one hot encoder
we just made and we're going to stack
them together and
again we're converting this into an
array that goes from uh instead of
having horse equals one dog equals two
and then horse plus dog would equal
three which would be cat
now it's going to be you know an array
of ten where each one is 0 to 1. then we
want to go ahead and set up our test
images and labels and when we're doing
this you're going to see it's the same
thing we just did with the rest of them
we just changed colors right here this
is no different than what we're doing up
here with our training set we're going
to stack the different images
we're going to get the length of them so
we know how many images are in there you
certainly could add them by hand but
it's nice to let the computer do it
especially if it ever changes on the
other end and you're using other data
and again we reshape them and transpose
them and we also do the one hot encoder
same thing we just did on our training
images so now our test images are in the
same format so now we have a definition
which sets up all our images in there
and then the next step is to go ahead
and batch them or next batch and let's
do another breakout here for batches
because this is really important to
understand tends to throw me for a
little loop when i'm working with
tensorflow or cross or a lot of these we
have our data coming in if you remember
we had like 10 000 photos let me just
put 10 000 down here we don't want to
run all 10 000 at once so we want to
break this up into batch sizes and you
also remember that we had the number of
photos in this case length of test or
whatever number is in there we also have
32 by 32
by 3. so when we're looking at the batch
size we want to change this from 10 000
to
a batch of in this case i think we're
going to do batches of a hundred so we
want to look at just 100 the first
hundred of the photos and if you
remember we set self i
equal to
zero uh so what we're looking at here is
we're going to create x we're going to
get the next batch from the very
initialize we've already initialized it
for 0. so we're going to look at x from
0 to batch size which we set to 100 so
just the first 100 images and then we're
going to reshape that into
and this is important to let the data
know that we're looking at 100 by 32 by
32 by 3. now we've already formatted it
to the 32 by 32 by 3. this just sets
everything up correctly so that x has
the data in there in the correct order
in the correct shape and then the y just
like the x uh is our labels so our
training labels again they go from zero
to batch size in this case they do sell
five plus batch size because the cell
phi is going to keep changing and then
finally we increment the self i because
we have zero so we so the next time we
call it we're going to get the next
batch size and so basically we have x
and y x being the photograph data coming
in and y being the label and that of
course is labeled through one hot
encoder so if you remember correctly if
it was say horse is equal to zero it
would be
one for the zero position since this is
the horse and then everything else would
be zero in here let me just put lines
through there there we go there's our
array
hard to see that array so let's go ahead
and take that and uh we're going to
finish loading it since this is our
class and now we're armed with all this
uh our setup over here let's go ahead
and load that up and so we're going to
create a variable ch with the c4 helper
in it and then we're going to do ch dot
setup images
now we could have just put all the setup
images under the init but by breaking
this up into two parts it makes it much
more readable and also if you're doing
other work there's reasons to do that as
far as the setup let's go ahead and run
that and you can see where it says
setting up training images and labels
setting up test images and that's one of
the reasons we broke it up is so that if
you're testing this out you can actually
have print statements in there telling
you what's going on which is really nice
uh they did a good job with this setup i
like the way that it was broken up in
the back and then one quick note you
want to remember that batch to set up
the next batch since we have to run uh
batch equals ch next batch of 100
because we're going to use the 100 size
uh but we'll come back to that we're
going to use that just remember that
that's part of our code we're going to
be using in a minute from the definition
we just made so now we're ready to
create our model first thing we want to
do is we want to import our tensorflow
as tf i'll just go ahead and run that so
it's loaded up and you can see we got a
warning here
that's because they're making some
changes it's always growing and they're
going to be depreciating one of the
values from float 64 to float type or is
treated as an np float64 uh nothing to
really worry about this doesn't even
affect what we're working on because
we've set all of our stuff to a 255
value or zero to one and do keep in mind
that zero to one value that we converted
the 255 is still a float value but it'll
easily work with either the numpy float
64 or the numpy d type float it doesn't
matter which one it goes through so the
depreciation would not affect our code
as we have it and in our tensorflow uh
we'll go ahead and just increase the
size in there just a moment so you can
get a better view of the um what we're
typing in uh we're gonna set a couple
placeholders here and so we have we're
going to set x equals tf placeholder tf
float 32 we just talked about the float
64 versus the numpy float we're actually
just going to keep this at float 32.
more than uh significant number of
decimals for what we're working with and
since it's a placeholder we're going to
set the shape equal to and we've set it
equal to none
because at this point we're just holding
the place on there we'll be setting up
as we run the batches that's what the
first value is and then 32 by 32 by 3
that's what we've reshaped our data to
fit in and then we have our y true
equals placeholder tf float 32 and the
shape equals none comma 10. 10 is the 10
different labels we have so it's an
array of 10. and then let's create one
more placeholder we'll call this a hold
prob or hold probability and we're going
to use this we don't have to have a
shape or anything for this this
placeholder is for what we call drop out
if you remember from our theory before
we drop out so many nodes is looking at
or the different values going through
which helps decrease bias so we need to
go ahead and put a placeholder for that
also and we'll run this so it's all
loaded up in there so we have our three
different placeholders and since we're
in tensorflow when you use keras it does
some of this automatically but we're in
tensorflow direct cross sits on
tensorflow we're going to go ahead and
create some more helper functions we're
going to create something to help us
initialize the weights initialize our
bias if you remember that each layer has
to have a bias going in we're going to
go ahead and work on our conversional 2d
our max pool so we have our pooling
layer our convolutional layer and then
our normal full layer so we're going to
go ahead and put those all into
definitions and let's see what that
looks like in code and you can also grab
some of these helper functions from the
mnist the uh nist setup let me just put
that in there if you're under the
tensorflow so a lot of these already in
there but we're going to go ahead and do
our own and we're going to create our
init weights and one of the reasons
we're doing this is so that you can
actually start thinking about what's
going on in the back end so even though
there's ways to do this with an
automation sometimes these have to be
tweaked and you have to put in your own
setup in here now we're not going to be
doing that we're just going to recreate
them for our code and let's take a look
at this we have our weights and so it
comes in is going to be the shape and
what comes out is going to be a random
number so we're going to go ahead and
just knit some random numbers based on
the shape with a standard deviation of
0.1 kind of a fun way to do that and
then the tf variable init random
distribution so we're just creating a
random distribution on there that's all
that is for the weights now you might
change that you might have a higher
standard deviation in some cases you
actually load preset weights that's
pretty rare usually you're testing that
against another model or something like
that and you want to see how those
weights configure with each other now
remember we have our bias so we need to
go ahead and initialize the bias with a
constant in this case we're using 0.1 a
lot of times the bias is just put in as
one and then you have your weights add
on to that but we're going to set this
as 0.1 so we want to return a
convolutional 2d in this case a neural
network this is uh would be a layer on
here what's going on with the con 2d is
we're taking our data coming in
we're going to filter it strides if you
remember correctly strides came from
here's our image and then we only look
at this picture here and then maybe we
have a stride of one so we look at this
picture here and we continue to look at
the different filters going on there the
other thing this does is that we have
our data coming in as 32
by
32
by
3 and we want to change this so that
it's just this is three dimensions and
it's going to reformat this as just two
dimensions so it's going to take this
number here and combine it with the 32
by 32. so this is a very important layer
here because it's reducing our data down
using different means and it connects
down i'm just going to jump down one
here it goes with the convolutional
layer so you have your your kind of your
pre-formatting and the setup and then
you have your actual convolution layer
that goes through on there and you can
see here we have a knit weights by the
shape a knit bias shape of three because
we have the three different uh here's
our three again and then we return the
tfnn relu with the convention 2d so this
convolutional
has this feeding into it right there
it's using that as part of it and of
course the input is the x y plus b the
bias so that's quite a mouthful but
these two are the are the keys here to
creating the convolutional layers there
the convolutional 2d coming in and then
the convolutional layer which then steps
through and creates all those filters we
saw then of course we have our pooling
so after each time we run it through the
convectional layer we want to pull the
data if you remember correctly on the on
the pool side and let me just get rid of
all my marks it's getting a little crazy
there and in fact let's go ahead and
jump back to that slide let's just take
a look at that slide over here uh so we
have our image coming in we create our
convolutional layer with all the filters
remember the filters go um you know the
filter is coming in here and it looks at
these four boxes and then if it's a step
let's say step two and then goes to
these four boxes and then the next step
and so on uh so we have our
convolutional layer that we generate or
convolutional layers they use the uh
relu function um there's other functions
out there for this though the value is
the most the one that works the best at
least so far i'm sure that will change
then we have our pooling now if you
remember correctly the pooling was max
uh so if we had the filter coming in and
they did the multiplication on there and
we have a one and maybe a two here and
another one here and a three here three
is the max and so out of all of these
you then create an array that would be
three and if the max is over here two or
whatever it is that's what goes into the
pooling of what's going on in our
pooling uh so again we're reducing that
data down reducing it down as small as
we can and then finally we're going to
flatten it out into a single array and
that goes into our fully connected layer
and you can see that here in the code
right here we're going to create our
normal full layer so at some point we're
going to take from our pooling layer
this will go into some kind of
flattening process and then that will be
fed into the full the different layers
going in down here
and so we have our input size you'll see
our input layer get shape which is just
going to get the shape for whatever's
coming in uh and then input size initial
weights is also based on the input layer
coming in and the input size down here
is based on the input layer shape so
we're just going to already use the
shape and already have our size coming
in and of course you have to make sure
you knit the bias always put your bias
on there and we'll do that based on the
size so this will return tf.matt mole
input layer w plus b this is just a
normal full layer that's what this means
right down here that's what we're going
to return so that was a lot of steps we
went through let's go ahead and run that
so those are all loaded in there and
let's go ahead and create the layers
let's see what that looks like
now that we've done all the heavy
lifting and everything uh we get to do
all the easy part let's go ahead and
create our layers we'll create a
convolution layer one and two two
different convolutional layers and then
we'll take that and we'll flatten that
out and create a reshape pooling in
there for our reshape and then we'll
have our full uh layer at the end so
let's start by creating our first
convolutional layer then we come in here
and let me just run that real quick and
i want you to notice on here the 3
and the 32 this is important because
coming into this convolutional layer we
have three different channels and 32
pixels each
so that has to be in there the four and
four you can play with this is your
filter size so if you remember you have
a filter and you have your image and the
filter slowly steps over and filters out
this image depending on what your step
is for this particular setup 4 4 is just
fine that should work pretty good for
what we're doing for the size of the
image and then of course at the end once
you have your convolutional layer set up
you also need to pull it and you'll see
that the pooling is automatically set up
so that it would see the different shape
based on what's coming in so here we
have max toolbar 2x2 and we put in the
convolutional one that we just created
the convolutional layer we just created
goes right back into it and that right
up here as you can see is the x that's
coming in from here so it knows to look
at the first model and set the the data
accordingly set that up
so it matches and we went ahead and ran
this already i think i read let me go
and run it again and if we're gonna do
one layer let's go ahead and do a second
layer down here and it's uh we'll call
it convo two
it's also a convolutional layer on this
and you'll see that we're feeding
convolutional one in the pooling so it
goes from convolutional one into
convolutional one pooling from
convolutional one pooling into
convolutional two and then from
convolutional two into convolutional to
pooling and we'll go ahead and take this
and run this so these variables are all
loaded into memory and for our flattened
layer uh let's go ahead and we'll do uh
since we have 64 coming out of here and
we have a four by four going in let's do
8 by 8 by 64. so let's do
4096. this is going to be the flat layer
so that's how many bits are coming
through on the flat layer and we'll
reshape this so we'll reshape our
convo 2 pooling and that will feed into
here the convo to pulling and then we're
going to set it up as a single layer
that's
4096 in size that's what that means
there we'll go ahead and run this so
we've now created this variable the
convo 2 flat and then we have our first
full layer this is the final uh neural
network where the flat layer going in
and we're going to again use the relu
for our setup on there on a neural
network for evaluation and you'll notice
that we're going to create our first
full layer our normal full layer that's
our definition so we created that that's
creating the normal full layer and our
input for the data comes right here from
the this goes right into it
the convo too flat so this tells it how
big the data is and we're going to have
it come out it's going to have 10 24
that's how big the layer is coming out
we'll go ahead and run this so now we
have our full layer one and with the
full layer one we want to also define
the full one dropout to go with that so
our full layer one comes in uh keep
probability equals whole probability
remember we created that earlier and the
full layer one is what's coming into it
and this is going backwards and training
the data we're not training every weight
we're only training a percentage of them
each time which helps get rid of the
bias so let me go ahead and run that and
finally we'll go ahead and create a y
predict which is going to equal the
normal full 1 drop out and 10 because we
have 10 labels in there now in this
neural network we could have added
additional layers that would be another
option to play with you can also play
with instead of 10 24 you can use other
numbers for the way that sets up and
what's coming out going into the next
one we're only going to do just the one
layer and the one layer drop out and you
can see if we did another layer it'd be
really easy just to feed in the full one
drop out into full layer two and then
full layer two dropout would have full
layer two feed into it and then you'd
switch that here for the y prediction
for right now this is great this
particular data set is tried and true
and we know that this will work on it
and if we just type in y predict and we
run that
we'll see that this is a tensor object
shape question mark 10 d type 32 a quick
way to double check what we're working
on so now we've got all of our we've
done a setup all the way to the y
predict which we just did we want to go
ahead and apply the loss function and
make sure that's set up in there
create the optimizer and then
trainer optimizer and create a variable
to initialize all the global tf
variables so before we dive in to the
loss function let me point out one quick
thing or just kind of a rehab over a
couple things and that is when we're
playing with this these setups um we
pointed out up here we can change the 4
4 and use different numbers there the
change your outcome so depending on what
numbers you use here will have a huge
impact on how well your model fits and
that's the same here with the 1024 also
this is also another number that if you
continue to raise that number you'll get
possibly a better fit you might overfit
and if you lower that number you'll use
less resources and generally you want to
use this in
the exponential growth an exponential
being 2 4 8 16 and in this case the next
one down would be 5 12. you can use any
number there but those would be the
ideal numbers when you look at this data
so the next step in all this is we need
to also create a way of tracking how
good our model is and we're going to
call this a loss function and so we're
going to create a cross entropy loss
function and so before we discuss
exactly what that is let's take a look
and see what we're feeding it
we're going to feed it our labels and we
have our true labels and our prediction
labels so coming in here is where the
two different variables we're sending in
or the two different probability
distributions is one that we know is
true and what we think it's going to be
now this function right here when they
talk about cross entropy
in information theory the cross entropy
between two probability distributions
over the same underlying set of events
measures the average number of bits
needed to identify an event drawn from
the set that's a mouthful uh really
we're just looking at the amount of
error in here how many of these are
correct and how many of these are
incorrect so how much of it matches and
we're going to look at that we're just
going to look at the average that's what
the mean the reduced to the mean means
here so we're looking at the average
error on this
and so the next step
is we're going to take the error we want
to know our cross entropy or our loss
function how much loss we have that's
going to be part of how we train the
model so when you know what the loss is
and we're training it we feed that back
into the back propagation setup and so
we want to go ahead and optimize that
here's our optimizer we're going to
create the optimizer using an atom
optimizer remember there's a lot of
different ways of optimizing the data
adam's the most popular used
so our optimizer is going to equal the
tf train atom optimizer if you don't
remember what the learning rate is let
me just pop this back into here here's
our learning rate when you have your
weights you have all your weights and
your different nodes that are coming out
here's our node coming out
it has all its weights and then the
error is being prop sent back through in
reverse on our neural network so we take
this error we adjust these weights based
on the different formulas in this case
the atom formula is what we're using we
don't want to just adjust them
completely we don't want to change this
weight so it exactly fits the data
coming through because if we made that
kind of adjustment it's going to be
biased to whatever the last data we sent
through is instead we're going to
multiply that by 0.001 and make a very
small shift in this weight so our delta
w is only 0.001 of the actual delta w of
the full change we're going to compute
from the atom and then we want to go
ahead and train it so our training or
set up a training
variable or function and this is going
to equal our optimizer minimize cross
entropy and we make sure we go ahead and
run this
so it's loaded in there and then we're
almost ready to train our model but
before we do that we need to create one
more
variable in here and we're going to
create a variable to initialize all the
global tf variables and when we look at
this
the tf global variable initializer this
is a tensorflow
object it goes through there and it
looks at all our different setup that we
have going under our tensorflow and then
initializes those variables
so it's kind of like a magic wand
because it's all hidden in the backend
of tensorflow all you need to know about
this is that you have to have the
initialization on there which is an
operation
and you have to run that once you have
your setup going so we'll go ahead and
run this piece of code and then we're
going to go ahead and train our data so
let me run this so it's loaded up there
and so now we're going to go ahead and
run the model by creating a graph
session graph session is a tensorflow
term so you'll see that coming up it's
one of the things that throws me because
i always think of graphics and spark and
graph as just general graphing but they
talk about a graph session so we're
going to go ahead and run the model and
let's go ahead and walk through this uh
what's going on here and let's paste
this data in here and here we go so
we're going to start off with it with a
tf session as s so that's our actual tf
session we've created uh so we're right
here with the tf uh
session our session we're creating we're
going to run tf global variable
initializer so right off the bat we're
initializing our variables here and then
we have for i in range 500 so what's
going on here remember 500 we're going
to break the date up and we're going to
batch it in at 500 points each we've
created our session run so we're going
to do with tf session as session
right here we've created our variable
session and then we're going to run
we're going to go ahead and initialize
it so we have our tf global variables
initializer that we created
that initializes our session in here the
next thing we're going to do is we're
going to go for i in range of 500 batch
equals ch.nextbatch
so if you remember correctly this is
loading up
100 pictures at a time and uh this is
going to loop through that 500 times so
we are literally doing uh what is that
uh 500 times 100 is uh 50 000. so that's
50 000 pictures we're going to process
right there and the first process is
we're going to do a session run we're
going to take our train we created our
train variable or optimizer in there
we're going to feed it the dictionary we
had our feed dictionary that we created
and we have x equals batch 0 coming in y
true
batch 1
hold the probability 0.5 and then just
so that we can keep track of what's
going on we're going to every 100 steps
we're going to run a print so currently
on step
accuracy is um and we're going to look
at matches equals tf dot equal tf
argument y prediction one tf dot arg max
y true comma one so we're going to look
at this as how many matches it has and
here our acc
all we're doing here is we're going to
take the matches how many matches they
have it creates it generates a chart
we're going to convert that to float
that's what the tfcast does and then we
just want to know the average we just
want to know the average of the accuracy
and then we'll go ahead and print that
out uh print session run accuracy feed
dictionary so it takes all this and it
prints out our accuracy on there so
let's go ahead and take this oops
screen's there let's go ahead and take
this and let's run it and this is going
to take a little bit to run
so let's see what happens on my old
laptop and we'll see here that we have
our current uh we're currently on step
zero it takes a little bit to get
through the accuracy and this will take
just a moment to run we can see that on
our step 0 it has an accuracy of 0.1 or
0.1028
and as it's running we'll go ahead you
don't need to watch it run all the way
but this accuracy is going to change a
little bit up and down so we've actually
lost some accuracy during our step two
but we'll see how that comes out let's
come back after we run it all the way
through and see how the different steps
come out it's actually reading that
backwards
the way this works is the closer we get
to one the more accuracy we have so you
can see here we've gone from a point one
to a point three nine
and we'll go ahead and pause this and
come back and see what happens when
we're done with the full run all right
now that we've
prepared the mill got it in the oven and
pulled out my finished dish here if
you've ever watched any of the old
cooking shows let's discuss a little bit
about this accuracy going on here and
how do you interpret that we've done a
couple things first we've defined
accuracy
the reason i got it backwards before is
you have
loss or accuracy and with loss you'll
get a graph that looks like this it goes
oops that's an s by the way there we go
you get a graph that curves down like
this and with accuracy you get a graph
that curves up this is how good it's
doing now in this case uh one is
supposed to be really good accuracy that
means it gets close to one but it never
crosses one so if you have an accuracy
of one that is phenomenal in fact that's
pretty much you know unheard of and the
same thing with loss if you have a loss
of zero that's also unheard of the
zero's actually on this this axis right
here as we go in there so how do we
interpret that because you know if i was
looking at this and i go oh 0.51 that's
uh 51 you're doing 50 50. no this is not
percentage let me just put that in there
it is not percentage this is log
rhythmic what that means is that 0.2 is
twice as good as 0.1 and when we see 0.4
that's twice as good as 0.2 real way to
convert this into a percentage you
really can't say this is is a direct
percentage conversion what you can do
though is in your head if we were to
give this a percentage we might look at
this as
fifty percent we're just guessing equals
point one and if fifty percent roughly
equals point one that's we started up
here at the top remember at the top here
here's our 0.1028 the accuracy of 50
percent then 75 percent is about 0.2 and
so on and so on don't quote those
numbers because it doesn't work that way
they say that if you have point nine
five that's pretty much saying a hundred
percent and if you have a anywhere
between you'd have to go look this up
let me go and remove all my drawings
there uh so the magic number is point we
really want to be over a 0.5 in this
whole thing and we have
both 0.504 remember this is accuracy if
we were looking at loss then we would be
looking the other way but 0.05 you know
instead of how high it is we want how
low it is uh but with accuracy being
over 0.5 is pretty valid that means this
is pretty solid and if you get to a 0.95
then it's a direct correlation that's
what we're looking for here in these
numbers you can see we finished with
this model at 0.5135
so still good
and if we look at when they ran this in
the other end remember there's a lot of
randomness that goes into it when we see
the weights they got
.5251 so a little better than ours but
that's fine you'll find your own comes
up a little bit better or worse
depending on just that randomness and so
we've gone through the whole model we've
created we've trained the model and
we've also gone through on every 100th
run to test the model to see how
accurate it is welcome to the rnn
tutorial that's the recurrent neural
network what's in it for you we will
start with the course of fundamentals
what is a neural network in popular
neural networks it's important to know
the framework we're in and what we're
going to be looking at specifically then
we'll touch on why a recurrent neural
network what is a recurrent neural
network and how does an rn in work
one of the big things about rnns is what
they call the vanishing and exploding
gradient problem so we'll look at that
and then we're going to be using a use
case uh study that's going to be in
cross on tensorflow cross is a python
module for doing neural networks in deep
learning and in there there's the what
they call long short term memory lstm
and then we'll use the use case to
implement our lstm on the cross so when
you see that lstm that is basically the
rnn network and we'll get into that the
use case is always my favorite part
before we dive into any of this we're
going to take a look at what is an rnn
or an introduction to the rnn do you
know how google's autocomplete feature
predicts the rest of the words a user is
typing i love that auto complete feature
as i'm typing away it saves me a lot of
time i can just kind of hit the enter
key and it auto fills everything and i
don't have to type as much well first
there's a collection of large volumes of
most frequently occurring consecutive
words
this is fed into a recurrent neural
network analysis the data by finding the
sequence of words occurring frequently
and builds a model to predict the next
word in the sentence and then google
what is the best food to eat in loss i'm
guessing you're going to say loss mexico
no it's going to be las vegas
so the google search will take a look at
that and say hey the most common
autocomplete is going to be vegas in
there it usually gives you three or four
different choices so it's a very
powerful tool it saves us a lot of time
especially when we're doing a google
search or even in microsoft words has a
some people get very mad at it auto
fills with the wrong stuff but you know
you're typing away and it helps you
autofill i have that in a lot of my
different packages is just a standard
feature that we're all used to now so
before we dive into the rnn and getting
into the depths let's go ahead and talk
about what is a neural network neural
networks used in deep learning consist
of different layers connected to each
other and work on the structure and
functions of a human brain you're going
to see that thread human in human brain
and human thinking throughout deep
learning the only way we can evaluate an
artificial intelligence or anything like
that is to compare it to human function
very important note on there and it
learns from a huge volumes of data and
it uses complex algorithm to train a
neural net so in here we have image
pixels of two different breeds of dog
one looks like a nice floppy eared lab
and one a german shepherd you know both
wonderful breeds of animals that image
then goes into an input layer that input
layer might be formatted at some point
because you have to let it know like you
know different pictures are going to be
different sizes and different color
content then it'll feed into hidden
layers so each of those pixels or each
point of data goes in and then splits
into the hidden layer which then goes
into another hidden layer which then
goes to an output layer r and n there's
some changes in there which we're going
to get into so it's not just a
straightforward propagation of data like
we've covered in many other tutorials
and finally you have an output layer and
the output layer has two outputs it has
one that lights up if it's a german
shepherd and another that lights up it's
if it's a labrador so identify as a
dog's breed set networks do not require
memorizing the past output so our
forward propagation is just that it goes
forward and doesn't have to be memorized
stuff and you can see there that's not
actually me in the picture dressed up in
my suit
i haven't worn a suit in years so as
we're looking at this we're going to
change it up a little bit before we
cover that let's talk about popular
neural networks first there's the feed
forward neural network used in general
regression and classification problems
and we have the convolution neural
network used for image recognition deep
neural network used for acoustic
modeling deep belief network used for
cancer detection and recurrent neural
network used for speech recognition now
taken a lot of these and mixed them
around a little bit so just because it's
used for one thing doesn't mean it can't
be used for other modeling but generally
this is where the field is and this is
how those models are generally being
used right now so we talk about a feed
forward neural network in a feed forward
neural network information flows only in
the forward direction from the input
nodes through the hidden layers if any
into the output nodes there are no
cycles or loops in the network and so
you can see here we have our input layer
i was talking about how it just goes
straight forward into the hidden layers
so each one of those connects and then
connects to the next hidden layer
connects to the output layer and of
course we have a nice simplified version
where it has a predicted output the
refer to the input is x a lot of times
and the output as y decisions are based
on current input no memory about the
past no future scope why recurrent
neural network issues in feed forward
neural network so one of the biggest
issues is because it doesn't have a
scope of memory or time a feed-forward
neural network doesn't know how to
handle sequential data it only considers
only the current input so if you have a
series of things and because three
points back affects what's happening now
and what your output affects what's
happening that's very important so
whatever i put as an output is going to
affect the next one um a feed forward
doesn't look at any of that it just
looks at this is what's coming in and it
cannot memorize previous inputs so it
doesn't have that list of inputs coming
in solution to feed forward neural
network you'll see here where it says
recurrent neural network and we have our
x on the bottom going to h going to y
that's your feed forward but right in
the middle it has a value c so there's a
whole another process was memorizing
what's going on in the hidden layers and
the hidden layers as they produce data
feed into the next one so your hidden
layer might have an output that goes off
to y
but that output goes back into the next
prediction coming in what this does is
this allows it to handle sequential data
it considers the current input and also
the previously received inputs and if
we're going to look at general drawings
and solutions we should also look at
applications of the rnn image captioning
rnn is used to caption an image by
analyzing the activities present in it a
dog catching a ball in midair that's
very tough i mean you know we have a lot
of stuff that analyzes images of a dog
in the image of a ball but it's able to
add one more feature in there that's
actually catching the ball in midair
time series prediction any time series
problem like predicting the prices of
stocks in a particular month can be
solved using rnn and we'll dive into
that in our use case and actually take a
look at some stock one of the things you
should know about analyzing stock today
is that it is very difficult and if
you're analyzing the whole stock the
stock market at the new york stock
exchange in the u.s produces somewhere
in the neighborhood if you count all the
individual trades and fluctuations by
the second
it's like three terabytes a day of data
so we're going to look at one stock just
analyzing one stock is really tricky in
here we'll give you a little jump on
that so that's exciting but don't expect
to get rich off of it immediately
another application of the rnn is
natural language processing text mining
and sentiment analysis can be carried
out using rnn for natural language
processing and you can see right here
the term natural language processing
when you stream those three words
together is very different than ice if i
said processing language natural leap so
the time series is very important when
we're analyzing sentiments it can change
the whole value of a sentence just by
switching the words around or if you're
just counting the words you may get one
sentiment where if you actually look at
the order they're in you get a
completely different sentiment when it
rains look for rainbows when it's dark
look for stars both of these are
positive sentiments and they're based
upon the order of which the sentence is
going in machine translation given an
input in one language rnn can be used to
translate the input into a different
languages as output i myself very
linguistically challenged but if you
study languages and you're good with
languages you know right away that if
you're speaking english you would say
big cat and if you're speaking spanish
you would say cat big so that
translation is really important to get
the right order to get there's all kinds
of parts of speech that are important to
know by the order of the words here this
person is speaking in english and
getting translated and you can see here
a person is speaking in english in this
little diagram i guess that's denoted by
the flags i have a flag i own it no um
but they're speaking in english and it's
getting translated into
chinese italian french german and
spanish languages some of the tools
coming out are just so cool so somebody
like myself who's very linguistically
challenged i can now travel into worlds
i would never think of because i can
have something translate my english back
and forth readily and i'm not stuck with
a communication gap so let's dive into
what is a recurrent neural network
recurrent neural network works on the
principle of saving the output of a
layer and feeding this back to the input
in order to predict the output of the
layer sounds a little confusing when we
start breaking it down i'll make more
sense and usually we have our
propagation forward neural network with
the input layers the hidden layers the
output layer with the recurrent neural
network we turn that on its side so here
it is and now our x comes up from the
bottom into the hidden layers into y and
they usually draw very simplified x to h
with c as a loop a to y where a b and c
are the perimeters a lot of times you'll
see this kind of drawing in here digging
closer and closer into the h and how it
works going from left to right you'll
see that the c goes in and then the x
goes in so the x is going upward bound
and c is going to the right a is going
out and c is also going out that's where
it gets a little confusing so here we
have xn
cn and then we have
y out and c out and c is based on h t
minus one so our value is based on the y
and the h value are connected to each
other they're not necessarily the same
value because h can be its own thing and
usually we draw this or we represent it
as a function h of t equals a function
of c where h of t minus 1 that's the
last h output and x of t going in so
it's the last output of h combined with
the new input of x where h t is the new
state fc is a function with the
parameters c that's a common way of
denoting it h t minus 1 is the old state
coming out and then x of t is an input
vector at time of step t
well we need to cover types of recurrent
neural networks and so the first one is
the most common one which is a
one-to-one single output
one-to-one neural network is usually
known as a vanilla neural network used
for regular machine learning problems
why because vanilla is usually
considered kind of a just a real basic
flavor but because it's very basic a lot
of times they'll call it the vanilla
neural network which is not the common
term but it is you know kind of a slang
term people will know what you're
talking about usually if you say that
then we run one to many so you have a
single input and you might have a
multiple outputs in this case uh image
captioning as we looked at earlier where
we have not just looking at it as a dog
but a dog catching a ball in the air and
then you have mini to one network takes
in a sequence of inputs examples
sentiment analysis where a given
sentence can be classified as expressing
positive or negative sentiments and we
looked at that as we were discussing if
it rains look for a rainbow so positive
sentiment where rain might be a negative
sentiment if you're just adding up the
words in there and then of course if
you're going to do a one-to-one mini to
one one to many there's many to many
networks takes in a sequence of inputs
and generates a sequence of outputs
example machine translation so we have a
lengthy sentence coming in in english
and then going out in all the different
languages you know just a wonderful tool
very complicated set of computations you
know if you're a translator you realize
just how difficult it is to translate
into different languages one of the
biggest things you need to understand
when we're working with this neural
network is what's called the vanishing
gradient problem while training an rnn
your slope can be either too small or
very large and this makes training
difficult when the slope is too small
the problem is known as vanishing
gradient and you'll see here they have a
nice
image loss of information through time
so if you're pushing not enough
information forward that information is
lost and then when you go to train it
you start losing the third word in the
sentence or something like that or it
doesn't quite follow the full logic of
what you're working on exploding
gradient problem oh this is one that
runs into everybody when you're working
with this particular neural network when
the slope tends to grow exponentially
instead of decaying this problem is
called exploding gradient issues in
gradient problem long training time poor
performance bad accuracy and i'll add
one more in there uh your computer if
you're on a lower end computer testing
out a model will lock up and give you
the memory error explaining gradient
problem consider the following two
examples to understand what should be
the next word in the sequence
the person who took my bike and blank a
thief the students who got into
engineering with blank from asia and you
can see in here we have our x value
going in we have the previous value
going forward and then you back
propagate the error like you do with any
neural network and as we're looking for
that missing word maybe we'll have the
person took my bike and blank was a
thief and the student who got into
engineering with a blank were from asia
consider the following example the
person who took the bike so we'll go
back to the person who took the bike was
blank a thief in order to understand
what would be the next word in the
sequence the rnn must memorize the
previous context whether the subject was
singular noun or a plural noun so was a
thief is singular the student who got
into engineering well in order to
understand what would be the next word
in the sequence the rnn must memorize
the previous context whether the subject
was singular noun or a plural noun and
so you can see here the students who got
into engineering with blank were from
asia it might be sometimes difficult for
the air to back propagate to the
beginning of the sequence to predict
what should be the output so when you
run into the gradient problem we need a
solution the solution to the gradient
problem first we're going to look at
exploding gradient where we have three
different solutions depending on what's
going on one is identity initialization
so the first thing we want to do is see
if we can find a way to minimize the
identities coming in instead of having
it identify everything just the
important information we're looking at
next is to truncate the back propagation
so instead of having
whatever information it's sending to the
next series we can truncate what it's
sending we can lower that particular uh
set of layers make those smaller and
finally is a gradient clipping so when
we're training it we can clip what that
gradient looks like and narrow the
training model that we're using when you
have a vanishing gradient the option
problem we can take a look at weight
initialization very similar to the
identity but we're going to add more
weights in there so it can identify
different aspects of what's coming in
better choosing the right activation
function that's huge so we might be
activating based on one thing and we
need to limit that we haven't talked too
much about activation function so we'll
look at that just minimally there's a
lot of choices out there and then
finally there's long short term memory
networks the lstms and we can make
adjustments to that so just like we can
clip the gradient as it comes out we can
also
expand on that we can increase the
memory network the size of it so it
handles more information and one of the
most common problems in today's
setup is what they call long-term
dependencies suppose we try to predict
the last word in the text the clouds are
in the and you probably said sky here we
do not need any further context it's
pretty clear that the last word is going
to be sky suppose we try to predict the
last word in the text i have been
staying in spain for the last 10 years i
can speak fluent maybe you said
portuguese or french no you probably
said spanish the word we predict will
depend on the previous few words in
context here we need the context of
spain to predict the last word in the
text it's possible that the gap between
the relevant information and the point
where it is needed to become very large
lstms help us solve this problem so the
lstms are a special kind of recurrent
neural network capable of learning long
term dependencies remembering
information for long periods of time is
their default behavior all recurrent
neural networks have the form of a chain
of repeating modules of neural network
connections in standard rnns this
repeating module will have a very simple
structure such as a single tangent h
layer lstms's
also have a chain like structure but the
repeating module has a different
structure instead of having a single
neural network layer there are four
interacting layers communicating in a
very special way lstms are a special
kind of recurrent neural network capable
of learning long-term dependencies
remembering information for long periods
of time is their default behavior ls
tmss also have a chain-like structure
but the repeating module has a different
structure instead of having a single
neural network layer there are four
interacting layers communicating in a
very special way as you can see the
deeper we dig into this the more
complicated the graphs get in here i
want you to note that you have x of t
minus 1 coming in you have x of t coming
in and you've got x at t plus 1. and you
have h of t minus 1 and h of t coming in
and h of t plus 1 going out and of
course on the other side is the output a
in the middle we have our tangent h but
it occurs in two different places so not
only when we're computing the x of t
plus one are we getting the tangent h
from x to t but we're also getting that
value coming in from the x at t minus
one so the short of it is as you look at
these layers not only does it does the
propagate through the first layer goes
into the second layer back into itself
but it's also going into the third layer
so now we're kind of stacking those up
and this can get very complicated as you
grow that in size it also grows in
memory too and in the amount of
resources it takes uh but it's a very
powerful tool to help us address the
problem of complicated long sequential
information coming in like we were just
looking at in the sentence and when
we're looking at our long short term
memory network there's three steps of
processing sensing in the lstms that we
look at the first one is we want to
forget irrelevant parts of the previous
state you know a lot of times like you
know is as in a unless we're trying to
look at whether it's a plural noun or
not they don't really play a huge part
in the language so we want to get rid of
them then selectively update cell state
values so we only want to update the
cell state values that reflect what
we're working on and finally we want to
put only output certain parts of the
cell state so whatever is coming out we
want to limit what's going out too and
let's dig a little deeper into this
let's just see what this really looks
like
so step one decides how much of the past
it should remember first step in the
lstm is to decide which information to
be omitted in from the cell in that
particular time step it is decided by
the sigmoid function it looks at the
previous state h to t minus 1 and the
current input x t and computes the
function so you can see over here we
have a function of t
equals the sigmoid function of the
weight of f the h at t minus 1 and then
x a t plus of course you have a bias in
there with any of our neural networks so
we have a bias function so f of t equals
forget gate decides which information to
delete that is not important from the
previous time step considering an stm is
fed with the following inputs from the
previous and present time step alice is
good in physics john on the other hand
is good in chemistry so previous output
john plays football well he told me
yesterday over the phone that he had
served as a captain of his college
football team that's our current input
so as we look at this the first step is
the forget gate realizes there might be
a change in context after encountering
the first full stop compares with the
current input sentence of x a t so we're
looking at that full stop and then
compares it with the input of the new
sentence the next sentence talks about
john so the information on alice is
deleted okay that's important to know so
we have this input coming in and if
we're going to continue on with john
then that's going to be the primary
information we're looking at the
position of the subject is vacated and
is assigned to john and so in this one
we've seen that we've weeded out a whole
bunch of information and we're only
passing information on john since that's
now the new topic so step two is then to
decide how much should this unit add to
the current state in the second layer
there are two parts one is a sigmoid
function and the other is the tangent h
in the sigmoid function it decides which
values to let through zero or one
tangent h function gives a weightage to
the values which are passed deciding
their level of importance minus one to
one and you can see the two formulas
that come up the i of t equals the
sigmoid of the weight of i h of t minus
1 x of t plus the bias of i and the c of
t equals the tangent of h of the weight
of c of h of t minus 1 x of t plus the
bias of c so our i of t equals the input
gate determines which information to let
through based on its significance in the
current time step if this seems a little
complicated don't worry because a lot of
the programming is already done when we
get to the case study understanding
though that this is part of the program
is important when you're trying to
figure out these what to set your
settings at you should also note when
you're looking at this it should have
some semblance to your forward
propagation neural networks where we
have a value assigned to a weight plus a
bias very important steps than any of
the neural network layers whether we're
propagating into them the information
from one to the next or we're just doing
a straightforward neural network
propagation let's take a quick look at
this what it looks like from the human
standpoint as i step out of my suit
again consider the current input at x of
t john plays football well he told me
yesterday over the phone that he had
served as a captain of his college
football team that's our input input
gate analysis the important information
john plays football and he was a captain
of his college team as important he told
me over the phone yesterday is less
important hence it is forgotten this
process of adding some new information
can be done via the input gate now this
example is as a human form and we'll
look at training this stuff in just a
minute
but as a human being if i wanted to get
this information from a conversation
maybe it's a google voice listening in
on you or something like that um how do
we weed out the information that he was
talking to me on the phone yesterday
well i don't want to memorize that he
talked to me on the phone yesterday or
maybe that is important but in this case
it's not i want to know that he was the
captain of the football team i want to
know that he served i want to know that
john plays football and he was the
captain of the college football team
those are the two things that i want to
take away as a human being again we
measure a lot of this from the human
viewpoint and that's also how we try to
train them so we can understand these
neural networks finally we get to step
three
decides what part of the current cell
state makes it to the output the third
step is to decide what will be our
output first we run a sigmoid layer
which decides what parts of the cell
state make it to the output then we put
the cell state through the tangent h to
push the values to be between minus one
and one and multiply it by the output of
the sigmoid gate so when we talk about
the output of t we set that equal to the
sigmoid of the weight of zero of the h
of t minus one and back one step in time
by the x of t plus of course the bias
the h of t equals the outer t times the
tangent of the tangent h of c of t so
our o t equals the output gate allows
the passed in information to impact the
output in the current time step let's
consider the example to predicting the
next word in the sentence john played
tremendously well against the opponent
and one for his team for his
contributions brave blank was awarded
player of the match there could be a lot
of choices for the empty space current
input brave is an adjective adjectives
describe a noun john could be the best
output after brave thumbs up for john
awarded player of the match and if you
were to pull just the nouns out of the
sentence team doesn't look right because
that's not really the subject we're
talking about contributions you know
brave contributions or brave team brave
player brave match
so you look at this and you can start to
train this these this neural network so
it starts looking at and goes oh no john
is what we're talking about so brave is
an adjective
john's going to be the best output and
we give john a big thumbs up and then of
course we jump into my favorite part the
case study use case implementation of
lstm let's predict the prices of stocks
using the lstm network based on the
stock price data between 2012 2016.
we're going to try to predict the stock
prices of 2017.
and this will be a narrow set of data
we're not going to do the whole stock
market it turns out that the new york
stock exchange generates roughly three
terabytes of data per day
that's all the different trades up and
down of all the different stocks going
on in each individual one
second to second or nanosecond to
nanosecond but we're going to limit that
to just some very basic fundamental
information so don't think you're going
to get rich off this today but you at
least you can give an eye you can give a
step forward in how to start processing
something like stock prices a very valid
use for machine learning in today's
markets
use case implementation of lstm let's
dive in we're going to import our
libraries we're going to import the
training set and get the scaling going
now if you watch any of our other
tutorials a lot of these pieces just
start to look very familiar because it's
very similar setup let's take a look at
that and just reminder we're going to be
using anaconda the jupiter notebook so
here i have my anaconda navigator when
we go under environments i've actually
set up a cross python
36 i'm in python36 and uh nice thing
about anaconda especially the newer
version i remember a year ago messing
with anaconda in different versions of
python in different environments
anaconda now has a nice interface
and i have this installed both on a
ubuntu linux machine and on windows so
it works fine on there you can go in
here and open a terminal window and then
in here once you're in the terminal
window this is where you're going to
start
installing using pip to install your
different modules and everything now
we've already pre-installed them so we
don't need to do that in here but if you
don't have them installed in your
particular environment you'll need to do
that and of course you don't need to use
the anaconda or the jupiter you can use
whatever favorite python id you like i'm
just a big fan of this because it keeps
all my stuff separate you can see on
this machine i have specifically
installed one for cross since we're
going to be working with cross under
tensorflow we go back to home i've gone
up here to application and that's the
environment i've loaded on here and then
we'll click on the launch jupiter
notebook now i've already in my jupiter
notebook
have set up a lot of stuff so that we're
ready to go kind of like martha
stewart's in the old cooking show so we
want to make sure we have all our tools
for you so you're not waiting for them
to load and if we go up here to where it
says new you can see where you can
create a new python 3. that's what we
did here underneath the setup so it
already has all the modules installed on
it and i'm actually renamed this if you
go under file you can rename it we've
i'm calling it rnn stock and let's just
take a look and start diving into the
code let's get into the exciting part
now we've looked at the tool and of
course you might be using a different
tool which is fine let's start putting
that code in there and seeing what those
imports and uploading everything looks
like now first half is kind of boring
when we hit the run button because we're
going to be importing numpy as np that's
uh the number python which is your numpy
array and the matplot library because
we're going to do some plotting at the
end
and our pandas for our data set our
pandas is pd and when i hit run it
really doesn't do anything except for
load those modules just a quick note let
me just do a quick draw here oops shift
alt there we go you'll notice when we're
doing this setup if i was to divide this
up oops i'm going to actually let's
overlap these here we go
this first part that we're going to do
is
our data
prep
a lot of prepping involved
in fact depending on what your system is
we're using cross i put an overlap here
uh but you'll find that almost
maybe even half of the code we do is all
about the data prep and the reason i
overlapped this with uh cross let me
just put that down because that's what
we're working in uh is because cross has
like their own preset stuff so it's
already pre-built in which is really
nice so there's a couple steps a lot of
times that are in the kara setup uh
we'll take a look at that to see what
comes up in our code as we go through
and look at stock and the last part is
to evaluate and if you're working with
shareholders or
classroom whatever it is you're working
with uh the evaluate is the next biggest
piece
so the actual code here crosses a little
bit more but when you're working with
some of the other packages you might
have like three lines that might be it
all your stuff is in your pre-processing
and your data since cross has is is
cutting edge and you load the individual
layers you'll see that there's a few
more lines here and crosses a little bit
more robust and then you spend a lot of
times like i said with the evaluate you
want to have something you present to
everybody else and say hey this is what
i did this is what it looks like so
let's go through those steps this is
like a kind of just general overview and
let's just take a look and see what the
next set of code looks like and in here
we have a data set train and it's going
to be read using the pd or pandas dot
read csv and it's a google stock price
train dot csv and so under this we have
training set equals data set train dot
ilocation and we've kind of sorted out
part of that so what's going on here
let's just take a look at let's look at
the actual file and see what's going on
there now if we look at this
ignore all the extra files on this i
already have a train and a test set
where it's sorted out this is important
to notice because a lot of times we do
that as part of the pre-processing of
the data we take
20 percent of the data out so we can
test it and then we train the rest of it
that's what we use to create our neural
network that way we can find out how
good it is but let's go ahead and just
take a look and see what that looks like
as far as the file itself and i went
ahead and just opened this up in a basic
word pad and text editor just so we can
take a look at it certainly you can open
up an excel or any other kind of
spreadsheet
and we note that this is a comma
separated variables we have a date
open high low close volume this is the
standard stuff that we import into our
stock or the most basic set of
information you can look at in stock
it's all free to download in this case
we downloaded it from google that's why
we call it the google stock price
and specifically is google this is the
google stock values from as you can see
here we started off at 1 3 2012.
so when we look at this first setup up
here
we have a data set train equals pd
underscore csv and if you noticed on the
original frame
let me just go back there
if getting your learning started is half
the battle what if you could do that for
free visit skill up by simply learn
click on the link in the description to
know more
they had it set to home ubuntu downloads
google stock price train i went ahead
and changed that because we're in the
same file where i'm running the code so
i've saved this particular python code
and i don't need to go through any
special paths or have the full path on
there and then of course we want to take
out
certain values in here and you're going
to notice that we're using
our data set
and we're now in pandas
so pandas basically it looks like
a spreadsheet
and in this case we're going to do i
location which is going to get specific
locations the first value is going to
show us that we're pulling all the rows
in the data and the second one is we're
only going to look at columns one and
two and if you remember here from our
data as we switch back on over columns
we so we start with zero which is the
date
and we're going to be looking at open
and high which would be one and two
we'll just label that right there so you
can see now
when you go back and do this you
certainly can extrapolate and do this on
all the columns
but for the example let's just limit a
little bit here so that we can focus on
just some
key aspects of stock
and then we'll go up here and run the
code and again i said the first half is
very boring whenever we hit the run
button it doesn't do anything because
we're still just loading the data and
setting it up
now that we've loaded our data we want
to go ahead and scale it we want to do
what they call feature scaling and in
here we're going to pull it up from the
sk learn or the sk kit pre-processing
import min max scalar and when you look
at this you got to remember that biases
in our data we want to get rid of that
so if you have something that's like a
really high value let's just draw a
quick graph
and i have something here like the maybe
the stock has a value one stock has a
value of a hundred and another stock has
a value of five
um
you start to get a bias between
different stocks and so when we do this
we go ahead and say okay 100 is going to
be the max
and 5 is going to be the min
and then everything else goes and then
we change this so we just squish it down
i like the word squish so it's between 1
and 0. so 100 equals one or one equals a
hundred and zero equals five and you can
just multiply it's usually just a simple
multiplication we're using uh
multiplication so it's going to be uh
minus five and then 100 divided or 95
divided by one so or whatever value is
is divided by 95.
and uh once we've actually created our
scale we've tolling is going to be from
zero to one we want to take our training
set and we're going to create a training
set scaled and we're going to use our
scalar sc we're going to fit we're going
to fit and transform the training set uh
so we can now use the sc this this
particular object will use it later on
our testing set because remember we have
to also scale that when we go to test
our model and see how it works and we'll
go ahead and click on the run again it's
not going to have any output yet because
we're just setting up all the variables
okay so we paste the data in here and
we're going to create the data structure
with the 60 time steps and output
first note we're running 60 time steps
and that is where this value here also
comes in so the first thing we do is we
create our x train and y train variables
we set them to an empty python array
very important to remember what kind of
array we're in what we're working with
and then we're going to come in here
we're going to go for i in range 60 to
1258 there's our 60 60 time steps and
the reason we want to do this is as
we're adding the data in there's nothing
below the 60 so if we're going to use 60
time steps we have to start at point 60
because it includes everything
underneath of it otherwise you'll get a
pointer error and then we're going to
take our x train and we're going to
append training set scaled this is a
scaled value between 0 and 1. and then
as i is equal to 60 this value is going
to be
60 minus 60 is 0. so this actually is
0 to i so it's going to be 0 60 1 to 61.
let me just circle this part right here
1 to 61
2 to 62 and so on and so on and if you
remember i said 0 to 60 that's incorrect
because it does not count remember it
starts at 0 so this is a count of 60. so
it's actually 59. important to remember
that as we're looking at this and then
the second part of this that we're
looking at so if you remember correctly
here we go we go from zero to 59 of i
and then we have a comma a zero right
here and so finally we're just going to
look at the open value now i know we did
put it in there for one to two
if you remember quickly it doesn't count
the second one so it's just the open
value we're looking at just open
um and then finally we have y train dot
append training set i to zero and if you
remember correctly i two or i comma zero
if you remember correctly this is 0 to
59 so there's 60 values in it uh so we
do i down here this is number 60. so
we're going to do this is we're creating
an array and we have 0
to 59
and over here we have number 60 which is
going into the y train it's being
appended on there and then this just
goes all the way up so this is down here
is a
0 to 59 and we'll call it 60 since
that's the value over here and it goes
all the way up to 12
58 that's where this value here comes in
that's the length of the data we're
loading
so we've loaded two arrays we've loaded
one array that has which is filled with
arrays from 0 to 59 and we loaded one
array which is just the value and what
we're looking at you want to think about
this as a time sequence uh here's my
open open open openopenopen what's the
next one in the series so we're looking
at the google stock and each time it
opens we want to know what the next one
0 through 59 what's 60 1 through 60
what's 61 2 through 62 what's 62 and so
on and so on going up and then once
we've loaded those in our for loop
we go ahead and take x-train and y-train
equals np.array x-train dot np array y
train we're just converting this back
into a numpy array that way we can use
all the cool tools that we get with
numpy array including reshaping
so if we take a look and see what's
going on here we're going to take our x
train
we're going to reshape it
wow what the heck does reshape mean
that means we have an array if you
remember correctly
so many numbers by 60.
that's how wide it is
and so we're when you when you do x
train dot shape that gets one of the
shapes and you get
x train dot shape of one gets the other
shape and we're just making sure the
data is formatted correctly and so you
use this to pull the fact that it's 60
by um
in this case where's that value
60 pi
1199 1258 minus 60 11.99 and we're
making sure that that is shaped
correctly so the data is grouped into
11 99 by 60 different arrays and then
the one on the end just means at the end
because this when you're dealing with
shapes and numpy they look at this as
layers and so the end layer needs to be
one value that's like the leaf of a tree
where this is the branch and then it
branches out some more
and then you get the leaf np dot reshape
comes from and using the existing shapes
to form it we'll go ahead and run this
piece of code again there's no real
output and then we'll import our
different cross modules that we need so
from cross models we're going to import
the sequential model dealing with
sequential data we have our dense layers
we have actually three layers we're
going to bring in our dents our lstm
which is what we're focusing on and our
dropout and we'll discuss these three
layers more in just a moment but you do
need the with the lstm you do need the
drop out and then the final layer will
be the dents but let's go ahead and run
this and i'll bring port our modules and
you'll see we get an error on here and
if you read it closer it's not actually
an error it's a warning what does this
warning mean these things come up all
the time when you're working with such
cutting edge modules or completely being
updated all the time we're not going to
worry too much about the warning all
it's saying is that the h5py
module which is part of cross is going
to be updated at some point and if
you're running new stuff on cross and
you start updating your cross system you
better make sure that your h5 pi is
updated too otherwise you're going to
have an error later on but you can
actually just run an update on the h5 pi
now if you wanted to not a big deal
we're not going to worry about that
today and i said we were going to jump
in and start looking at what those
layers mean i meant that and we're going
to start off with initializing the rnn
and then we'll start adding those layers
in and you'll see that we have the lstm
and then the dropout lstm then dropout
lstm then dropout what the heck is that
doing so let's explore that we'll start
by initializing the rnn regressor equals
sequential because we're using the
sequential model and we'll run that and
load that up and then we're going to
start adding our lstm layer and some
dropout regularization and right there
should be the q dropout regularization
and if we go back here and remember our
exploding gradient well that's what
we're talking about the dropout drops
out unnecessary data so we're not just
shifting huge amounts of data through
the network so and so we go in here
let's just go ahead and add this in i'll
go ahead and run this and we had three
of them so let me go ahead and put all
three of them in and then we can go back
over them there's the second one and
let's put one more in let's put that in
and we'll go ahead and put two more in i
mean i said one more in but it's
actually two more in and then let's add
one more after that and as you can see
each time i run these they don't
actually have an output so let's take a
closer look and see what's going on here
so we're going to add our first lstm
layer in here we're going to have units
50. the units is the positive integer
and it's the dimensionality of the
output space this is what's going out
into the next layer so we might have 60
coming in but we have 50 going out we
have a return sequence because it is a
sequence data so we want to keep that
true and then you have to tell it what
shape it's in well we already know the
shape by just going in here and looking
at x train shape so input shape equals
the x train shape of one comma one it
makes it really easy you don't have to
remember all the numbers that put in 60
or whatever else is in there you just
let it tell the regressor what model to
use and so we follow our stm with a
dropout layer now understanding the
dropout layer is kind of exciting
because one of the things that happens
is we can over train our network that
means that our neural network will
memorize such specific data that it has
trouble predicting anything that's not
in that specific realm to fix for that
each time we run through the training
mode we're going to take point two or
twenty percent of our neurons and just
turn them off so we're only gonna train
on the other ones and it's gonna be
random that way each time we pass
through this we don't over train these
nodes come back in in the next training
cycle we randomly pick a different 20.
and finally they see a big difference as
we go from the first to the second and
third and fourth the first thing is we
don't have to input the shape because
the shapes already the output units is
50 here this item the next step
automatically knows this layer is
putting out 50 and because it's the next
layer it automatically sets that and
says so 50 is coming out from our last
layer that's coming up you know goes
into the regressor and of course we have
our dropout and that's what's coming
into this one and so on and so on and so
the next three layers we don't have to
let it know what the shape is it
automatically understands that and we're
going to keep the units the same we're
still going to do 50 units it's still a
sequence coming through 50 units and a
sequence now the next piece of code is
what brings it all together let's go
ahead and take a look at that and we
come in here we put the output layer the
dense layer and if you remember up here
we had the three layers we had uh lstm
dropout and dents dents just says we're
going to bring this all down into one
output instead of putting out a sequence
we just know i want to know the answer
at this point and let's go ahead and run
that and so in here you notice all we're
doing is setting things up one step at a
time so far we've brought in our way up
here we brought in our data we brought
in our different modules we formatted
the data for training it we set it up
you know we have our y x train and our y
train we have our source of data and the
answers where we know so far that we're
going to put in there we've reshaped
that we've come in and built our cross
we've imported our different layers and
we have in here if you look we have what
uh five total layers now cross is a
little different than a lot of other
systems a lot of other systems put this
all in one line and do it automatic but
they don't give you the options of how
those layers interface and they don't
give you the options of how the data
comes in corros is cutting edge for this
reason so even though it's a lot of
extra steps in building the model this
has a huge impact on the output and what
we can do with this these new models
from cross so we brought in our dense we
have our full model put together a
regressor so we need to go ahead and
compile it and then we're going to go
ahead and fit the data we're going to
compile the pieces so they all come
together and then we're going to run our
training data on there and actually
recreate our regressor so it's ready to
be used so let's go ahead and compile
that and i'd go ahead and run that and
if you've been looking at any of our
other tutorials on neural networks
you'll see we're going to use the
optimizer atom atom is optimized for big
data there's a couple other optimizers
out there beyond the scope of this
tutorial but certainly atom will work
pretty good for this and loss equals
mean squared value so when we're
training it this is what we want to base
the loss on how bad is our error we're
going to use the mean squared value for
our error and the atom optimizer for its
differential equations you don't have to
know the math behind them but certainly
it helps to know what they're doing and
where they fit into the bigger models
and then finally we're going to do our
fit fitting the rn into the training set
we have the regressor.fit x train y
train epics and batch size so we know
where this is this is our data coming in
for the x train our y train is the
answer we're looking for of our data our
sequential input epics is how many times
we're going to go over the whole data
set we created a whole data set of x
train so this is each each of those rows
which includes a time sequence of 60.
and bad size another one of those things
where cross really shines is if you were
pulling the save from a large file
instead of trying to load it all into
ram it can now pick smaller batches up
and load those indirectly we're not
worried about pulling them off a file
today because this isn't big enough to
cause a computer too much of a problem
to run not too straining on the
resources but as we run this you can
imagine what happened if i was doing a
lot more than just one column in one
set of stock in this case google stock
imagine if i was doing this across all
the stocks and i had instead of just the
open i had open close high low and you
can actually find yourself with about 13
different variables times 60 because
there's a time sequence suddenly you
find yourself with a gig of memory
you're loading into your ram which will
just completely you know if it's just if
you're not on multiple computers or
cluster you can start running into
resource problems but for this we don't
have to worry about that so let's go
ahead and run this and this will
actually take a little bit on my
computer because it's an older laptop
and give it a second to kick in there
there we go all right so we have epic so
this is going to tell me it's running
the first run through all the data and
as it's going through it's batching them
in 32 pieces so 32 lines each time and
there's 1198 i think i said 11.99
earlier but it's 1198 is off by one but
each one of these is 13 seconds so you
can imagine this is roughly 20 to 30
minutes run time on this computer like i
said it's an older laptop running at 0.9
gigahertz on a dual processor and that's
fine what we'll do is i'll go ahead and
stop go get a drink of coffee and come
back and let's see what happens at the
end and where this takes us and like any
good cooking show
i've kind of gotten my latte i also have
some other stuff running in the
background so you'll see these numbers
jumped up to like 19 seconds 15 seconds
but you can scroll through and you can
see we've run it through 100 steps or
100 epics so the question is what does
all this mean one of the first things
you'll notice is that our loss can is
over here it kind of stopped at 0.0014
but you can see it kind of goes down
until we hit about 0.014 three times in
a row so we guessed our epic pretty
close since our losses remain the same
on there so to find out we're looking at
we're going to go ahead and load up our
test data the test data that we didn't
process yet and a real stock price data
set test eye location this is the same
thing we did when we prepped the data in
the first place so let's go ahead and go
through this code and we can see we've
labeled it part three making the
predictions and visualizing the results
so the first thing that we need to do is
go ahead and read the data in from our
test csv you see i've changed the path
on it for my computer and then we'll
call it the real stock price and again
we're doing just the one column here and
the values from ilocation so it's all
the rows and just the values from these
that one location that's the open stock
open let's go ahead and run that so
that's loaded in there and then let's go
ahead and create we have our inputs
we're going to create inputs here and
this should all look familiar this is
the same thing we did before we're going
to take our data set total we're going
to do a little panda concat from the
datastate train now remember the end of
the dataset train is part of the data
going in let's just visualize that just
a little bit here's our train data let
me just put tr for train and it went up
to this value here but each one of these
values generated a bunch of columns it
was 60 across and this value here equals
this one and this value here equals this
one and this value here equals this one
and so we need these top 60 to go into
our new data so to find out what we're
looking at we're going to go ahead and
load up our test data the test data that
we didn't process yet and a real stock
price data set test eye location this is
the same thing we did when we prepped
the data in the first place so let's go
ahead and go through this code and we
can see we've labeled it part three
making the predictions and visualizing
the results so the first thing we need
to do is go ahead and read the data in
from our test csv you see i've changed
the path on it for my computer and then
we'll call it the real stock price and
again we're doing just the one column
here and the values from i location so
it's all the rows and just the values
from these that one location that's the
open stock open let's go ahead and run
that so that's loaded in there and then
let's go ahead and create we have our
inputs we're going to create inputs here
and this should all look familiar
because this is the same thing we did
before we're going to take our data set
total we're going to do a little panda
concat from the datastate train now
remember the end of the data set train
is part of the data going in and let's
just visualize that just a little bit
here's our train data let me just put tr
for train and it went up to this value
here but each one of these values
generated a bunch of columns it was 60
across and this value here equals this
one and this value here equals this one
and this value here equals this one and
so we need these top 60 to go into our
new data because that's part of the next
data or it's actually the top 59. so
that's what this first setup is over
here is we're going in we're doing the
real stock price and we're going to just
take the data set test and we're going
to load that in and then the real stock
price is our data test test location so
we're just looking at that first column
the open price and then our data set
total we're going to take pandas and
we're going to concat and we're going to
take our data set train for the open and
our dataset test open and this is one
way you can reference these columns
we've referenced them a couple different
ways we've referenced them up here with
the one two but we know it's labeled as
a panda set as open so pandas is great
that way lots of versatility there and
we'll go ahead and go back up here and
run this there we go and you'll notice
this is the same as what we did before
we have our open data set where pended
our two different or concatenated our
two data sets together we have our
inputs equals data set total length data
set total minus length data set minus
test minus 60 values so we're going to
run this over all of them and you'll see
why this works because normally when
you're running your test set versus your
training set you run them completely
separate but when we graph this you'll
see that we're just going to be we'll be
looking at the part that we didn't train
it with to see how well it graphs and we
have our inputs equals inputs dot
reshapes or reshaping like we did before
we're transforming our inputs so if you
remember from the transform between zero
and one and uh finally we want to go
ahead and take our x test and we're
going to create that x test and for i in
range 60 to 80. so here's our x test and
we're appending our inputs i to 60 which
remember is 0 to 59 and i comma 0 on the
other side so it's just the first column
which is our open column and once again
we take our x test we convert it to a
numpy array we do the same reshape we
did before and then we get down to the
final two lines and here we have
something new right here on these last
two lines let me just highlight those or
or mark them predicted stock price
equals regressor dot predicts x test so
we're predicting all the stock including
both the training and the testing model
here and then we want to take this
prediction and we want to inverse the
transform so remember we put them
between zero and one well that's not
going to mean very much to me to look at
a float number between zero and one i
want the dollar amount so i want to know
what the cash value is and we'll go
ahead and run this and you'll see it
runs much quicker than the training
that's what's so wonderful about these
neural networks once you put them
together it takes just a second to run
the same neural network that took us
what a half hour to train add and plot
the data we're going to plot what we
think it's going to be and we're going
to plot it against the real data what
the google stock actually did so let's
go ahead and take a look at that in code
and let's uh pull this code up so we
have our plt that's our oh if you
remember from the very beginning let me
just go back up to the top we have our
matplot library dot pi plot as plt
that's where that comes in and we come
down here we're going to plot let me get
my drawing thing out again we're going
to go ahead and plt is basically kind of
like an object it's one of the things
that always threw me when i'm doing
graphs in python because i always think
you have to create an object and then it
loads that class in there well in this
case plt is like a canvas you're putting
stuff on so if you've done html5 you'll
have the canvas object this is the
canvas so we're going to plot the real
stock price that's what it actually is
and we're going to give that color red
so it's going to be in bright red we're
going to label it real google stock
price and then we're going to do our
predicted stock and we're going to do it
in blue and it's going to be labeled
predicted and we'll give it a title
because it's always nice to give a title
to your graph especially if you're going
to present this to somebody you know to
your shareholders in the office and the
x label is going to be time because it's
a time series and we didn't actually put
the actual date and times on here but
that's fine we just know they're
incremented by time and then of course
the y label is the actual stock price
plt.legend tells us to build the legend
on here so that the color red and real
google stock price show up on there and
then the plot shows us that actual graph
so let's go ahead and run this and see
what that looks like and you can see
here we have a nice graph and let's talk
just a little bit about this graph
before we wrap it up here's our legend i
was telling you about that's why we have
the legend to show the prices we have
our title and everything and you'll
notice on the bottom we have a time
sequence we didn't put the actual time
in here now we could have we could have
gone ahead and
plotted the x since we know what the
dates are and plotted this to dates but
we also know that it's only the last
piece of data that we're looking at so
last piece of data which in somewhere
probably around here on the graph i
think it's like about 20 of the data
probably less than that we have the
google price and the google price has
this little up jump and then down and
you'll see that the actual google
instead of a turn down here just didn't
go up as high and didn't load go down so
our prediction has the same pattern but
the overall value is pretty far off as
far as um stock but then again we're
only looking at one column we're only
looking at the open price we're not
looking at how many volumes were traded
like i was pointing out earlier we talk
about stock just right off the bat
there's six columns there's open high
low close volume then there's weather i
mean volume shares then there's the
adjusted open adjusted high adjusted low
adjusted close they have a special
formula to predict exactly what it would
really be worth based on the value of
the stock and then from there there's
all kinds of other stuff you can put in
here so we're only looking at one small
aspect the opening price of the stock
and as you can see here we did a pretty
good job this curve follows the curve
pretty well it has like a little jumps
on it bins they don't quite match up so
this bin here does not quite match up
with that bin there but it's pretty darn
close we have the basic shape of it and
the prediction isn't too far off and you
can imagine that as we add more data in
and look at different aspects in the
specific domain of stock we should be
able to get a better representation each
time we drill in deeper of course this
took a half hour for my program my
computer to train so you can imagine
that if i was running it across all
those different variables it might take
a little bit longer to train the data
not so good for doing a quick so how do
we implement rnn this is an example of
implementing an rnn for a particular use
case so in our particular example we
have the data about milk production over
a few months and using rnn we want to
predict because this is time series
analysis so rnn is good to do time
series analysis so using rnn we want to
predict what will be the milk production
in the future so let us see how we can
do that i will first take you through
quickly through the slides and then i
will actually run this in in a jupiter
notebook the code i will run it in the
jupyter notebook so this is how it looks
the code looks so while this is
tensorflow you still use the standard
python libraries like numpy and pandas
and even matplotlib to do some initial
processing getting the data processing
it cleaning it whatever all that can be
still done in within the same program so
that's what we're doing here we're
importing some libraries like numpy and
pandas and then we read the data file
and if we plot the data we see here it
is the data for for the years 1962 to 75
and we can see that there is a certain
trend right so this is how a typical
time series data would look again some
of you if you are not familiar with time
series and time series analysis and so
on i would recommend you to go through
some videos around that which will make
it easy to understand this so this is
our typical time series data would look
in this case it is nothing but there is
only one variable which is milk
production and it is spread across
several years so this is going from 1962
to 75 and that value has been plotted so
if you see there is a certain kind of a
trend here which is basically going
upwards and there will be some
seasonality so time series data has
these three components right so it will
have a trend it will have a seasonality
it will have seasonality and then it
will have some randomness so that's what
this graph is showing and
now if we want to perform analysis on
this time series analysis on this first
thing we need to do is split the data
into train and test and in this case we
will just use a straightforward method
which is taking the data for the first
13 years so the idea here is we need to
train our model right this is time
series data so what and we want to
predict for the future so the way we
need to use this is we have to take the
data for a certain period of time and
train our model and then we use a part
of the known data so that we can then
ask our model to predict for that
duration and compare it with the known
information so what do we mean by that
so here if you see this data i will show
you in the notebook as well jupiter
notebook as well this has 13 years of
data so what we are doing is we are
taking the first 12 years and using that
as our training set right so 12 years we
are doing and this has about i think 14
years of data so we are taking 13 years
of data for training purpose and then we
are using the last one year remaining
one year of data for testing purpose
because here what we can do is this one
year data we know the values right
because if we want to compare the
accuracy or anything like that we need
to know the value so in this case we
know the values of this one particular
year so we will use that but at the same
time we train the model for 13 years of
data and for the 14th year we will ask
our model to predict and then compare it
with this known value so that we know
how accurate our model is i hope that
makes sense okay so that's what we are
doing here 156 is nothing but 12 into 13
on a monthly basis we get the data the
next step is to scale the data this is
all regular data manipulation data
munching activity and then you split it
you're basically assigning the train and
test data to the to the scaled variables
and then you need to read the data in
batches so it is very important as i
mentioned earlier also that instead of
reading the entire data in one shot you
feed the data in batches so in order to
do that we are writing a function for
that that's all we are doing here so
that is still here what we have done is
regular python programming there is no
tensorflow as of now here okay so so far
what we have done is preparation data
preparation data munching now what we
are doing is actually training our model
so this is where tensorflow now comes
into picture so we are importing first
step is to import the tensorflow library
so this is how you do it import
tensorflow sdf and then you can define
some variables or constants whichever
way now here of course there are a
couple of ways of doing it you can
create them as tensorflow nodes but that
is a little bit of an overhead we will
just use the regular python
variables are constant so here i am
creating regular python variables and i
am saying number of inputs is one so
instead of hard coding i am just storing
them as variables so this is number of
inputs is 1 number of time steps is 12
number of neurons is 100 and so on right
and then learning rate is 0.03 and
number of iterations we are saying is 4
000 then we create placeholders now we
will be storing the independent
variables in x and the dependent
variables in y and this we will read
from an external file remember i told
you placeholders are used for getting
data from outside and then feeding it to
our model so that's the reason we have
two placeholders one for reading the x
values and another for reading the y
values in this step we are only just
creating the nodes right so this is just
creating the graph and similarly we are
mentioning what is the loss function and
what is optimizer and how to run the
optimizer and once that is done you are
initializing the variables and then
you're creating a saver variable or a
saver instance so saver is basically
nothing but in machine learning you can
train your model and you can save it for
later use so that's where this saving
comes into play we will see how to use
that as well and then the last step is
to create your session and run this
graph right so
we are initializing the variables
remember this we run init so which is
nothing but this one so we are
initializing the variables whatever are
there instead of hard coding remember in
earlier case we were hardcoding how many
iterations so here we are saying for the
given number of iterations which are
stored in this maybe how many iterations
we said is what is the value of
iterations this is training iterations
is 4000. so we specify that based on the
number of training iterations next batch
will basically fetch the data and then
we run the session we are basically
saying train is the node which we will
run in the session and
this will basically train our model and
this is more for printing every hundred
times you print it that's all this
there's nothing more to it so for
example the output would look somewhat
like this this is for 0 this 400 next
for 200 and so okay and then you save
the model you remember saver we created
so you save the model for later use so
this is our test data remember we are
doing this on training data right so
once the training data training is done
we then try to create the inference on
the test data so that we can compare how
accurate this is right so this is how
the test set would look and
then we will basically restore this
model okay because in the previous slide
we created the model and we stored it so
now we have to restore the model and
then run our test data against it and
see what are the values that are
predicted what are the y values and then
compare with x to see the accuracy so
train seed is what we are seeing here so
that is what will have the predicted
values and
these predicted values what we want to
do is we want to create an additional
column because remember we need to
compare because we want to find out the
accuracy of this model so we need to
compare the predicted values with the
actual value so what we are doing here
is adding another column called
generator and
assign a value to that all right so this
is the result of the prediction and then
if we need to reshape because we have to
show this in the form of a monthly
results so that's what we're doing here
and
once we generate the results and then
display it we can actually see it
month-wise here and
the actual and the generated values okay
so we create a data frame which is a
combination of the actual values and the
predicted values from the test set and
then we can plot to see how the trend is
as you can see pretty much the actual
value is in blue color and generated or
predicted value the curve is in yellow
color the trend is maintained so it will
probably it's not a hundred percent
accurate but the trend is maintained all
right so let's do one thing let's go
into the jupiter notebook and take a
look at how this works in tensorflow
environment actually the code i will
walk you through the code this is my
jupiter notebook and
the data is taken from this particular
link in case you're interested you can
download it from there and this is the
data for the years
1962-75 the first thing we do is import
these libraries because before we start
with the actual tensorflow activity we
need to prepare the data and so on so
for that you can use your regular python
libraries which are like numpy pandas
and so on so that's what we're doing
here and then we read the data using
pandas into a data frame so this is a
data frame milk is a data frame and we
will do some quick exploratory analysis
just to see how our data is looking
initials five records that we'll get one
two three four five that is ahead so it
goes from 1960 to january to 1960 to may
once we re change or basically we need
to split this into month specifically
separately so that's what we are doing
here date to time so we kind of do a
little bit of reindexing and then if we
plot this this is how the data look as
you can see there is a clear upward
trend and
there is some seasonality and so on but
anyway we will not break that up into
those components we will just use rnn to
predict and test okay so the next thing
is to check some if we can do run like
info it will tell us what is the
information about this data set so let
us just run that okay so it is just
telling us how many total columns and
what is the size of the file and so on
and so forth again in case you're
interested in doing some exploratory
analysis so what we'll do next is we
will take the 13 years of data for our
training data set so what are we going
to do now let me step back so what have
we seen here we have seen that there are
168 records so which is nothing but 14
years of data we now have to split this
into our training and test data set so
how do we do that i think in case of
time series we cannot do it like 80 20
like we do in normal splitting in normal
machine learning process here it is time
series data so what we are going to do
is we will take out of this 14 years we
will take the first 13 years of data and
we will use that for training and then
we will test it with the last which is
the 14th year data we will use it for
testing okay so that's what we are going
to do here so let's uh split that so
training set will consist of my 156
records which is nothing but 12 13 into
12 right my 13 years of data i'm taking
for training and then my test set will
consist of the bottom 12 observations
which is last one year of data which is
the 14th year okay so that's now done
training and test splitting is done the
next step is to do some normalizing
which is basically we'll use our scaling
rather we will use min max scala and we
will just scale the data and now we do
it for both test as well as train now we
are ready to create our rnn model but
before that one quick thing in order to
fetch the data we have to create a
function so that's what we are doing
here we create a function called next
batch and how you want to fetch the
steps they are all defined as our
constants if you remember and that's
what this function is all about so we
define that function we will be calling
that in our training method all right so
from here onwards up to there we are
done with the preparation of the data
and whatever functions or whatever are
required from here onwards is where the
tensorflow part starts so the first
thing we do is import the tensorflow
library typically this is a very
standard way of importing the library we
say import tensorflow as tf now tf is
nothing but a name so you can change it
to tf1 or abc or xyz whatever so this
part you can change but by and large
everybody uses this so we i would rather
recommend you also use the same syntax
so you say import tensorflow stf so it's
very easy for others to understand as
well and then you declare or define a
bunch of constants that's what we are
doing here like for example the number
of time steps in this case it is 12
number of neurons there are 100 number
of outputs it is only one and so on
right and then the learning rate and all
that we are declaring or defining those
variables in this particular block next
is to create placeholders you remember
the placeholders are used for feeding
the data so we have x and y x is for the
input which is the independent variable
and y is the output which is the
dependent variable and in our case these
are not different characteristics or
features but it is the same one feature
or one variable but it is over a period
of time so that's the only difference so
that's what we are doing here and now we
need to create our network right the
neural network so in our case we said we
will create a rnn layer now there are
different ways or different formats of
rnn which is probably not in scope of
this module so for now we will just
assume we are creating one rnn layer and
one type of rnn is gru
probably do in another video where we
talk in detail about rnn so for now we
are creating a gre cell with the wrapper
and the the syntax for creating the gru
cell is uh like this the number of units
which we we said there should be 100
neurons what will be the activation
function in this case it is relu and
then number of outputs in our case it is
only one okay so we create the cell here
okay then we got the gru cell and then
we say what is our output which is
nothing but we get the outputs and the
states and their states and which is uh
how the way we get that is tf.nnn
dot dynamic underscore rnn so if we call
this and we pass the cell and the data
which is basically in placeholder again
remember all we are doing here is we are
creating just the nodes for the graph so
nothing is actually getting executed
from a tensorflow perspective okay all
right so once that is done now we need
to pass this calculate the outputs and
the states using the dynamic underscore
rna method and we pass the cell as a
parameter and then the x values as a
parameter and if we
run that we will have the outputs and
the states and this is where we actually
run the training method
so
or create not really run the training
method but we create the nodes for the
training and optimizer then we have the
initialization of the variables and we
save this model we create a saver object
just to save the model because we will
then restore it and run it to do our
predictions so this is what we will do
here so that is another node and this is
where we actually create a session and
run the training okay so let's go ahead
and do that that will take a little
while because we said 4000 iterations so
we will allow it to finish we will
probably come back once the training is
completed all right so the training is
done now let's go and
run this on the test data so let's just
quickly take a look at the test data set
so this is our one year data for the
year of 1975 so
if we see this is from january february
and so on and we will pass this to our
model so what are we doing here we are
restoring the model here for example
right this is the path that we have
given when we were saving the model
let's go back once and show you let me
show you where we did the saving of the
model yes so we saved the model here so
that again we will restore that and we
will use that to run it on our test data
and see how well it predicts so let's go
ahead and run this and this is just 12
records so this will not take time
remember i told you training is what
takes a lot of time the regular
inference this is called inference
doesn't take much time right so it just
depends on how much data there were only
12 records here so it was very quick but
in training what we do we pass this
multiple times there are iterations 4
000 iterations needed for example so
that takes longer and in general in
machine learning deep learning training
is what takes the maximum amount of time
all right so let's go ahead and see the
results we will in order to plot it we
will have to kind of adjust the format
of the results otherwise we will not be
able to see it in a proper way and then
we will so this is our so what we have
done is we created a data frame which
consists of the predicted value and and
the actual value so this is the data
frame so this is the actual value this
834 782 and so on this is what our model
has predicted so it may not be so easy
to see in a tabler format so let's go
ahead and plot it so that we can compare
these two so when we plot it you will
see that the trend is more or less
maintained right so we go from the blue
color is the actual values and the
yellow color or the orange color
whatever is it is the one which is
generated by our model so it pretty much
is following the actual trend so not bad
for such a quick iteration and uh
training so that is basically it on rnn
and
let's go back to our and so in cross
we have sequential functional and
subclassing so remember those three
different setups in here we talked about
earlier and if you remember from here we
have a sequential where it's going
one tensorflow layer at a time you go
kind of look at think of it as going
from left to right or top to bottom or
whatever direction it's going in but it
goes in one direction all the time where
functional can have a very complicated
graph of directions you can have the
data split into two separate
tensors and then it comes back together
into another tensor
all those kinds of things and then
subclassing is really the really
complicated one where now you're adding
your own subclasses into the tensor to
do external computations right in the
middle of like a huge flow of data
but we're going to stick with sequential
it's not a big jump to go from
sequential to functional
but we're running a sequential
tensorflow and that's what this first
import is here we want to bring in our
sequential and then we have our layers
and let's talk a little bit about these
layers this is where cross and
tensorflow
really are happening this is what makes
them so nice to work with is all these
layers are pre-built
so from cross we have layers import
dense
from cross layers import lstm
when we talk about these layers
cross has so many built-in layers you
can do your own layers
the dense layer is your standard neural
network
by default it uses relu for its
activation
and then the lstm is a long short term
memory layer since we're going to be
looking probably at sequential data
we want to go ahead and do the lstm and
if we go into
karass and we look at their layers this
is across website you can see as we
scroll down for the cross layers that
are built in
we can get down here and we can look at
let's see here we have our layer
activation our base layers um
activation layer weight layer waste
there's a lot of stuff in here we have
the relu which is the basic activation
that was listed up here for layer
activations you can change those and
here we have our core layers
and our dense layers you have an input
layer a dense layer
and then we've added a more customized
one with the long term short term memory
layer and of course you can even do your
own custom layers in cross there's a
whole functionality in there for doing
your own thing
what's really nice about this is it's
all built in even the convolutional
layers this is for processing graphics
there's a lot of cool things in here you
can do
this is why cross is so popular it's
open source and you have all these tools
right at your fingertips so from cross
we're just going to import a couple
layers the dense layer
and the long short term memory layer
and then of course from
sk learn our scikit
we want to go ahead and do our min max
scalar standard scalar for pre editing
our
data and then metrics just so we can
take a look at the errors and compute
those let me go ahead and run this and
that just loads it up we're not
expecting anything from the output and
our file coming in is going to be air
quality.csv
and let's go ahead and take a quick look
at that this is in openoffice it's just
a standard you know like we do excel
whatever you're using for your
spreadsheet and you can see here we have
a number of columns a number of rows it
actually goes down to like 8 000.
the first thing we want to notice is
that the first row
is kind of just a random number put in
going down probably not something we're
going to work with
the second row
is bendung
i'm guessing that's a reference for the
profile
if we scroll to the bottom which i'm not
going to do because it takes forever to
get back up they're all the same
the same thing with the status the
status is the same
we have a date so we have a sequential
order here
here is the jam which i'm going to guess
is the time stamp on there so we have a
date and time
we have our o3 co no2 reading so2 no co2
voc
and then some other numbers here pm1 pm
2.5 pm 4 pm time 10
without actually
looking through the data i mean some of
this i can guess is like temperature
humidity i'm not sure what the pms are
but we have a whole slew of data here so
we're looking at air quality as far as
an area in a region and what's going on
with our date time stamps on there and
so code wise we're going to read this
into a pandas data frame so our data
frame df is a nice abbreviation commonly
used for data frames
equals
csv and then our the path to it just
happens to be on my d drive uh separated
by spaces and so if we go ahead and run
this
we'll print out the head of our data and
again this looks very similar to what we
were just looking at being in jupiter i
can take this and go the other way
make it real small so you can see all
the columns going across and we get a
full view of it
or we can bring it back up in size
that's pretty small on there overshot
but you can see it's the same data we
were just looking at we're looking at
the number we're looking at the profile
which is the ben dung
the
date we have a time stamp
our o3 count co and so forth on here
and this is just your basic
pandas printing out the top five rows we
could easily have done uh three rows
uh five rows ten whatever you want to
put in there by default that's five for
pandas now i talk about this all the
time so i know i've already said it at
least once or twice during this video
most of our work is in pre-formatting
data what are we looking at how do we
bring it together
so we want to go ahead and start with
our
date time it's come in in two columns
we have our date here and we have our
time
and we want to go ahead and combine that
and then we have this is just a simple
script in there that says combined date
time that's our formula we're building
our we're going to submit our
pandas data frame
and the tab name when we go ahead and do
this
that's all of our information that we
want to go ahead and create
and then goes for i in range df
shape 0.
so we're going to go through the whole
setup and we're going to list tab append
df location i
and here is our date going in there and
then return the numpy array list tab d
types date time 64. that's all we're
doing we're just switching this to a
date time stamp and if we go ahead and
do df date time equals combine date time
and then i always like to
print we'll do df head
just to see what that looks like
and so when we come out of this
we now have our setup on here and of
course it's edited on to the far right
here's our date time you can see the
format's changed
so there's our we've added in the date
time column and we've brought the date
over and we've taken this format here
and it's an actual variable with a 0 0 0
on here well that doesn't look good so
we need to also include the time part of
this and we want to convert it into
hourly data
so let's go ahead and do that
to do that to finish combining our date
time let's go ahead and create a
little
script here to combine the time in there
same thing we just did we're just
creating a numpy array returning a numpy
array and cr forcing this into a date
time format and we can actually spend
hours just going through
these conversions how do you
pull it from the panda's data frame
how do you set it up
so i'm kind of skipping through it a
little fast because i want to stay
focused on tensorflow and cross
keep in mind this is like 80 of your
coding when you're doing a lot of this
stuff is going to be reformatting these
things resetting them back up
so that it looks right on here and you
know it just takes time to get through
all that but that is usually what the
companies are paying you for that's what
the big
bucks are for
and we want to go ahead and a couple
things going on here is we're going to
go ahead and do our date time we're
going to reorganize some of our setup in
here convert into hourly data let me
just put a pause in there
um now remember we can select from df
are different columns we're going to be
working with and you're going to see
that we actually dropped
a couple of the columns those ones i
showed you earlier they're just
repetitive data so there's nothing in
there that exciting
and then we want to go ahead and we'll
create a
second
data frame here let me just get rid of
the df head
and df2 is we're going to group by date
time and we're looking at the mean value
and then we'll print that out so you can
see we're talking about
we have now reorganized this so we put
in date time 03 co
so now this is in the same order
as it was before and you'll see the date
time
now has our zero zero
uh same date one two three and so on so
let's group the data together
so there's a lot more manageable and in
the format we want and in the right
sequential order
and if we go back to there we go our air
quality
you can see right here we're looking at
these columns going across we really
don't need since we're going to create
our own
date time column we can get rid of those
these are the different columns of
information we want and that should
reflect right here in the columns we
picked coming across so this is all the
same columns on there that's all we've
done is reformatted our data
grouped it together by date and then you
can see the different data coming out
set up on there and then as a data
scientist
first thing i want to do is get a
description what am i looking at uh and
so we can go ahead and do the df2
describe and this gives us our you know
describe gives us our basic uh data
analytics information we might be
looking for like what is the mean
standard deviation
uh minimum amount maximum amount we have
our first quarter second quarter and
third quarter
numbers also in there
so you can get a quick look at a glance
describing the data or descriptive
analysis
and even though we have our quantile
information in here we're going to dig a
little deeper into that
we're going to calculate the quantile
for each variable
we're going to look at a number of
things for each variable and we'll see
right here q1 we can simply do the
quantile 0.25 percent
which should match
our 25 percent up here and we're going
to be looking at the min the max
and we're just going to do this is
basically we're breaking this down for
each uh different
variable in there
one of the things that's kind of fun to
do uh we're gonna look at that in just a
second let me get put the next piece of
code in here um we gotta clean out some
of our um we're gonna drop a couple
things our
last rows and first row because those
have
usually have a lot of null values in the
first row is just our titles so that's
important it's important to drop those
rows in here and so this right here as
we look at our different quantiles
again it's it's the same you know we're
still looking at the 25
quantile here
we're going to do a little bit more with
this
so now that we've cleared off our first
and last rows
we're going to go ahead and go through
all of our columns and this way we can
look at each
column individually and so we'll
just create a q1 q3 min max min iqr max
iqr
and calculate the quantile of i of df2
we're basically doing uh the math that
they did up here but we're splitting it
apart that's all this is
and
this happens a lot because you might
want to look at individual if this was
my own project i would probably spend
days and days going through
what these different values mean
one of the biggest
data science uh
things we can look at that's important
is uh use your use your common sense
you know if you're looking at this data
and it doesn't make sense and you go
back in there and you're like wait a
minute what the heck did i just do
at that point you probably should go
back and double check what you have
going on
now
we're looking at this and you can see
right here here's our attribute for our
o3 so we've broken it down
we have our q1 5.88 q3 10.37 if we go
back up here here's our five eight we've
rounded it off
10.37 is in there
so we've basically done the same math
just split it up we have our minimum and
our max iqr and that's computed let's
see where is it here we go uh q1 minus
1.5 times iqr and the iqr is your q3
minus q1 so that's the difference
between our two different quarters and
this is all
data science
as far as the hard math
we're really not we're actually trying
to focus on cross and tensorflow you
still got to go through all this stuff i
told you 80 percent of your programming
is going through and understanding what
the heck
uh happened here
what's going on what does this data mean
and so when we're looking in that we're
going to go ahead and say hey um
we've computed these numbers and the
reason we've computed these numbers is
if you take the minimum value and it's
less than your minimum iqr
uh that means something's going wrong
there and they usually in this case is
going to show us an outlier
so we want to go ahead and find the
minimum value if it's less than the min
minimum iqr it's an outlier and if the
max value is greater than the
max iqr we have an outlier and that's
all this is doing low outliers found
minimum value high outliers found
really important actually outliers are
almost everything in data sometimes
sometimes you do this project just to
find the outliers because you want to
know
crime detection what are we looking for
we're looking for the outliers what
doesn't fit a normal business deal and
then we'll go ahead and throw in
just threw in a lot of code oh my
goodness
so we have if your max is greater than
iqr print outlier is found what we want
to do is we want to start cleaning up
these outliers and so we want to convert
we'll do create a convert nand
x max iqr equals max
underscore iqr min iqr equals men iqr so
this is just saying this is the data
we're going to send that's all that is
in python and if x is greater than the
max iqr and x is less than the min iqr x
equals
null we're going to set it to null why
because we want to clear
these outliers out of the data now again
if you're doing fraud detection you
would do the opposite you would be
cleaning everything else that's not in
that series so that you can look at just
the outlier and then we're going to
convert the nand hum again we have x
max iqr is 100
min iqr is men iqr
if x is greater than max iqr and x is
less than min iqr again we're going to
return a null value otherwise it's going
to remain the same value x x equals x
and you can see as we go through the
code if i equals
our h2m
then we go ahead and do this
that's a column specific to humidity
that's your hum column
uh then we're going to go ahead and
convert do the
run a map on there and convert the non
hum
you can see here it's this is just
cleanup uh we run we found out that
humidity probably has some weird values
in it
we have our outliers
that's all this is
and so when we go ahead and finish this
and we take a look at our outliers and
we run this code here
we have a low outlier 2.04 we have a
high outlier
99.06
outliers have been interpolated
that means we've given them a new value
chances are these days when you're
looking at something like
these sensors coming in
they probably have a failed sensor in
there something went wrong
that's the kind of thing that you really
don't want to do your data analysis on
so that's what we're doing is we're
pulling that out and then
converting it over and setting it up
method linear
so we interpolate left linears it's
going to fill that data in
based on a linear regression model of
similar data
same thing with this up here with the
df2i interpolate that's what we're doing
again this is all data prep we're not
actually talking about tensorflow we're
just trying to get all our data
set up correctly so that when we run it
it's not going to cause problems or have
a huge bias
so we've dealt with outliers
specifically in humidity
and again this is one of these things
where when we start running
we run through this you can see down
here that we have our
outliers found
high low outliers
migrated them in we also know there's
other issues going on with this data
how do we know that
some of it's just looking at the data
playing with it until you start
understanding what's going on let's take
the temp value and we're going to go
ahead and use a logarithmic function on
the temp value
and
it's interesting because it's like how
do you how do you heck do you even know
to use logarithmic on the temp value
that's domain specific
we're talking about being an expert in
air care i'm not an expert in air care
um you know it's not
what i go look at i don't look at air
care data in fact this is probably the
first air care data setup i've looked at
but the experts come in there and they
come to you and say hey in data science
this is a exponentially variable
variable on here so we need to go ahead
and do
transform it
and use a logarithmic scale on that
so at that point that would be coming
from your
data here we go data science programmer
overview does a lot of stuff connecting
the database and connecting in with the
experts
data analytics a lot of times you're
talking about somebody who is a data
analysis might be all the way usually a
phd level
data science programming level
interfaces database manager that's going
to be the person who's your admin
working on it
so when we're looking at this we're
looking at something they've sent to me
and they said hey
domain air care
this needs to be this is a skew because
the data just goes up exponentially and
affects everything else and we'll go
ahead and take that data let me just go
ahead and run this
just for another quick look at it
we have our
we'll do a distribution df
we'll create another data frame from the
temp values and then from a data set
from the log temp so we can put them
side by side and we'll just go ahead and
do a quick histogram this is kind of
nice plot of figure figure size here's
our plt from matplot library
and then we'll just do a distribution
underscore df there's our data frames
this is nice because it just integrates
the histogram right into pandas love
pandas
and this is a chart you would send back
to your data analysis and say hey is
this what you wanted this is how the
data is converting on here as a data
science scientist the first thing i note
is we've gone from a 10
20 30 scales a 2.5 3.0 3.5 scale
and the data itself has kind of been uh
adjusted a little bit based on some kind
of a skew on there so let's jump into
we're getting a little closer to
actually doing our
cross on here
we'll go ahead and split our data up
and this of course is any good data
scientists
you want to have a training set and a
test set
and we'll go ahead and do the train size
we're going to use 0.75 percent of the
data make sure it's an integer you don't
want to take a slice as a float value
give you a nice error
uh and we'll have our train size of 75
percent and the test size is going to be
of course the train size minus the
length of the data set and then we can
simply do train comma test
here's our data set
which is going to be the train size the
test size
and then if we go and print this let me
just go ahead and run this we can see
how these values
split it's a nice split of 1298 and then
433 points of value
that are going to be for our
setup on here and if you remember we're
specifically looking at the data set
where did we create that data set from
um that was from up here that's what we
called the
logarithmic
value of the temp
that's where the data set came from so
we're looking at just that column with
this train size and the test with the
train and test data set here and let's
go ahead and do uh converted an array of
values into a data set matrix we're
going to create a little
setup in here we create our data set our
data set's going to come in we're going
to do a look back of one so we're going
to look back one piece of data going
backward
and we have our data x and our data y
for i and range length of data set look
back -1
this is creating let me just go ahead
and run this actually the best way to do
this
is to go ahead and create this data
and take a look at the shape of it let
me go ahead and just put that code in
here
so we're going to do a look back one
here's our train x our train y and it's
going to be adding the data on there and
then when we come up here
and we take a look at the shape
there we go
and we run this piece of code here
we look at the shape on this and we have
a new slightly different change on here
but we have a shape of x
1296 comma 1
shape of y train y
test x text y
and so what we're looking at is that
the x comes in
and we're only having a single value out
we want to predict what the next one is
that's what this little piece of code is
here for what are we looking for well we
want to look back one that's the um what
we're going to train the data with is
yesterday's data yesterday says hey the
humidity was at
97 percent what should today's humidity
be at if it's 97 yesterday is it going
to go up or is it going to go down today
of 97 does it go up to 100 what's going
on there uh and so our we're looking
forward to the next piece of data which
says hey tomorrow's is going to you know
today's humidity is this this is what
tomorrow's humidity is going to be
that's all that is all that is is
stacking our data so that
our y is basically
x plus 1 or x could be y minus 1.
and then a couple things to note is our
x data
we're only dealing with the one column
but you need to have it in a shape that
has it by the columns so you have the
two different numbers and since we're
doing just a single point of data
we have and you'll see with the train y
we don't need to have the extra shape on
here
now this is going to run into a problem
and the reason is is that we have what
they call a time step
and the time step is that long term
short term memory layer
so we're going to add another reshape on
here let me just go down here and put it
into the next cell and so we want to
reshape the input
array in the form of sample time step
features
we're only looking at one feature
and i mean this is one of those things
when you're playing with this you're
like why am i getting an error in the
numpy array why is this giving me
something weird going on
uh so we're going to do is we're going
to add one more
level on here instead of being 12.991
we want to go one more
and when they put the code together in
the back you can see we kept the same
shape the 1299
we added the one dimension and then we
have our train x shape one
and this could have depends again on how
far back in the long short term memory
you want to go
that is what that piece of code is for
and that reshape is and you can see the
new shape is now one uh
1299 one one
versus the 1299 one and then the other
part of the shape 432 one one
again this is our tr our x in and of
course our test x and then our y is just
a single column because we're just doing
one output that we're looking for
so now we've done our eighty percent um
you know that's all the the
writing all the code reformatting our
data
bringing it in now we want to go ahead
and do the fun part which is we're going
to go ahead and create and fit
the lstm neural network uh and if we're
going to do that the first thing we need
is we're going to need to go ahead and
create a model and we'll do this
sequential model and if you remember
sequential means it just goes in order
that means we have if you have two
layers the layers go from layer one to
layer two or layer zero to layer one
this is different than functional
functional allows you to split the data
and run two completely separate models
and then bring them back together
we're doing just sequential on here and
then we decided to do the long short
term memory
and we have our input shape uh which it
comes in again this is what all this
switching was we could have easily made
this uh one two three or four going back
as far as the end number on there we
just stuck to going back one
and it's always a good idea when you get
to this point where the heck
is this model coming from um what kind
of models do we have available
and
there's let me go and put the next model
in there
because we're going to do two models and
the next model is going to go ahead and
we're going to do dent so we have model
equal sequential
and then we're going to add the lstm
model and then we're going to add a
dense model and if you remember from the
very top of our code
when we did the imports oops there we go
our cross
this is it right here here's our
importing a dense model and here's our
importing an lstm now just about every
tensorflow model uses dents
your dense model is your basic
forward propagation reverse propagation
error
or it does reverse propagation to
program the model
so any of your neural networks you've
already looked at that
luxon says here's the error and sends
the error backwards that's what this is
the long short term memory is a little
different
the real question that we want to look
at right now is where do you find these
models what kind of models do you have
available and so for that let's go to
the cross website
which is the cross dot io
if you go under api slash layers and i
always have to do a search
just search for cross api layers it'll
open up and you can see we have
your base layers right here class
trainable weights all kinds of stuff
like there your activation
so a lot of your layers you can switch
how it activates
relu which is like your smaller arrays
or if you're doing convolutional neural
networks the convolution usually uses a
relu
your sigmoid all the way up to soft mac
soft plus all these different choices as
far as how those
are set up and what we want to do is we
want to go ahead and if you scroll down
here you'll see your core layers and
here is your dense layer
so you have an input object your dense
layer your activation layer embedding
layer
this is your your kind of your one set
up on there that's most common
uh convolutional neural networks or
convolutional layers these are like for
doing image categorizing so trying to
find objects in a picture that kind of
thing
we have pooling layers so as you have
the layers come together
usually you bring them down into
a single layer although you can still do
like global max pulling 3d and there is
just i mean this list just goes on and
on uh there's all kinds of different
things hidden in here as far as what you
can do and it changes you know you go in
here and you just have to do a search
for what you're looking for
and figure out what's going to work best
for you
as far as which project you're working
on
long short term memory is a big one
because this is when we start talking
about text
what if someone says the what comes
after the
uh the cat in the hat a little kid's
book there
it starts programming it and so you
really want to know not only
what's going on but it's going to be
something that has a history the history
behind it tells you what the next one
coming up is
now once we've built all our different
you know we built our model we've added
our different layers we went in there
play with it remember if you're in
functional you can actually link these
layers together and they branch out and
come back together if you do a uh
the sub
setup then you can create your own
different model you can embed a model in
there that might become the linear
regression you can embed a linear
regression model
as part of your functional split and
then have that come back together with
other things so we're going to go ahead
and compile
your model this brings everything
together we're going to put in what the
loss is which will use the mean squared
error
and we'll go ahead and use the atom
optimizer clearly there's a lot of
choices on here depending on what you're
doing and just like any of these
different prediction models if you've
been doing any
scikit from python
you'll recognize that we have to then
fit the model
uh so what are we doing in here we're
going to send in our train x our train y
um we're going to decide how many epics
we're going to run it through
500 is probably a lot for this i'm
guessing it'd probably be about two or
three hundred probably do just fine
our batch size
so how many different uh when you
process it this is the math behind it
if you're in data analytics
you might try know what this number is
as a data scientist where i haven't had
the phd level math that says this is why
you want to use this particular batch
size you kind of play with this number a
little bit
you can dig deeper into the math
see how it affects the results depending
on what you're doing
and there's a number of other settings
on here we did verbose 2. i'd have to
actually look that up to tell you what
verbose means i think that's actually
the default on there if i remember
correctly
there's a lot of different settings when
you go to fit it the big ones are your
epic and your batch size those are what
we're looking for
and so we're going to go ahead and run
this
and this is going to take a few minutes
to run because it's going through
500 times
through all the data so if you have a
huge data set this is the point where
you're kind of wondering oh my gosh is
this going to finish tomorrow
if i'm running this on a single machine
and i have a tera terabyte of data
going into it
if this is my personal computer and i'm
running a terabyte of data into this
um you know this is running rather
quickly through all 500 iterations uh
but yeah a terabyte of data we're
talking something closer to days week
you know even with a
3.5 gigahertz machine and in eight cores
it's still going to take a long time to
go through a full terabyte of data
and then we want to start looking at
putting it into some other framework
like spark or something that will print
the process on there more across
multiple um processors and multiple
computers
and if we scroll all the way down to the
bottom you're going to see here's our
square mean error of 0.0088
if we scroll way up you'll see it kind
of oscillates between 0.088 and 0.08089
it's right around 2 to 250 where you
start seeing that oscillation where it's
really not going anywhere so we really
didn't need to go through a full 500
epics
you know if you're retraining the stuff
over and over again it's kind of good to
know where that
error zone is so you don't have to do
all the extra processing of course if
you're going to build a model
we want to go ahead and run a prediction
on it
so let's go ahead and make our
prediction remember we have our training
test set
and our test set or the
we have the
train x and the train y for training it
or train predict and then we have our
test x and our test y going in there
so we can test to see how good it did
uh and when we come in here we have
you'll see right here we go ahead and do
our train predict equals
model predict train x
and test predict model predict test x
why would we want to run the prediction
on train x well it's not 100 on its
prediction we know it has a certain
amount of error and we want to compare
the error we have on what we programmed
it with with the error we get when we
run it on new data that's never seen the
model's never seen before and one of the
things we can do we go ahead and invert
the predictions
this helps us
level it off a little bit more
get rid of some of our bias we have
train predict equals and np
exponential m1 the train predict
and then train y equals the exponential
m1 for train y and then we do the again
that with train test predict and test y
um
again reformatting the data so that we
can it all matches and then we want to
go ahead and calculate the root mean
square error
so we have our train score uh which is
your math square root times the mean
square root error train
y and train predict and again we're just
um this is just feeding the data through
so we can compare it and the same thing
with the test
and let's take a look at that because
really
the code makes sense if you're going
through it line by line you can see
we're doing but the answer really helps
to zoom in
so we have a train score which is 2.40
of our root mean square error
and we have a test score of 3.16 of the
root mean square error
if these were reversed if our test score
is better than our training score
then we've over trained something's
really wrong at that point you got to go
back and figure out what you did wrong
because you should never have a better
result on your test data than you do
when you're training data and that's how
we put them both through that's why we
look at the error for both the training
and the testing
when you're going out and quoting you're
publishing this you're saying hey how
good is my model it's the test score
that you're showing people this is what
it did on my test data that the model
had never seen before this is how good
my model is
and a lot of times you actually want to
put together like a little formal code
where we actually want to print that out
and if we print that out you can see
down here
test prediction standard deviation of
data set 3.16 is less than 4.40
i'd have to go back
and we're
up here if you remember we did the
square means error this is standard
deviation that's why these numbers are
different
it's saying the same thing that we just
talked about
uh
3.16 is less than 4.40 model is good
enough we're saying hey this is this
model is valid we have a valid model
here so we can go ahead and go with that
and along with
putting a formal print out of there
we want to go ahead and plot what's
going on
uh and this we just want to pretty graft
here so that people can see what's going
on when i walk into a meeting and i'm
dealing with a number of people
they really don't want these numbers
they don't want to say hey what's i mean
standard deviation unless you know what
statistics are
you might be dealing with a number of
different departments head of cells
might not work with standard deviation
or have any idea what that really means
number wise and so at this point we
really want to put it in a graph so we
have something to display and with
displaying you remember that we're
looking at the data
today going into it and what's going to
happen tomorrow
so let's take a quick look at this we're
going to go ahead and shift the train
predictions for plotting we have our
train predict plot
np empty like data set train predict
plot
set it up with uh null values
yeah
you know it's just kind of it's kind of
a weird thing where we're creating the
um
the
data groups as we like them
and then putting the data in there is
what's going on here so we have our
train predict plot
which is going to be our look back our
length
plus look back
we're just is going to equal train
train predict
so we're creating this basically we're
taking this and we're dumping the train
predict into it so now we have our nice
train predict plot
and then we have the shift test
predictions for the plotting uh we're
going to continue more of that oops
looks like i put it in here double no
it's just a yeah they put it in here
double
didn't mean to do that
we really only need to do it once oh
here we go
this is where the problem was is because
this is the test predict
so we have our training prediction we're
doing the shift on here and then the
test predict we're going to look at that
same thing we're just creating those two
data sets
test
predict plot length prediction set up on
there
and then we're going to go through the
plotting the original data set and the
predictions so we have a time axis
always nice to have your time set on
there
set that to the time array
time axes lap
all this is setting up the time variable
for the bottom and then we have a lot of
stuff going on here as far as setting up
our figure
let's go ahead and run that and then
we'll break it down
we have on here
our main plot we have two different
plots going on here
the ispu going up and the data and the
ispu here with all these different
settings on it
and so we look at this we have our ax1
that's the main plot i mean our ax
that's the main plot and we have our ax1
which is the secondary plot over here so
we're doing a figure plt
or plt.figure
and we're going to dump those two graphs
on there
and so we take
and if you
go through the code piece by piece
which we're not going to do we're going
to do the
the
data set here
exponential reverse exponential so it
looks correctly we're going to label it
the original data set
we're going to plot the train predict
plot that's what we just created
we're going to make that orange and
we'll label it train prediction
uh test prediction plot we're going to
make that red and label it test
prediction and so forth
set our ticks up this actually just put
ticks time axes gets its ticks
the little little marks they're going
along the axes that kind of thing and
let's take a look and see what these
graphs look like
and these are just kind of fun you know
when you show up into a meeting and this
is the final output you say hey this is
what we're looking at
here's our original data in blue
here's our training prediction um you
can see that it trains pretty close to
what the data is up there
i would also probably put a um like a
little little time stamp and do just
right before and right after where we go
from train to test prediction
and you can see with the test prediction
the data comes in in red
and then you can also see what the
original data set look like behind it
and how it differs
and then we can just isolate that here
on the right that's all this is
is just the
test prediction on the right
and it's you know there's you'll see
with the original data set there's a lot
of peaks were missing and a lot of lows
were missing but as far as the actual
test prediction it's pretty does pretty
good it's pretty right on you can get a
good idea what to expect for your ispu
and so from this we would probably
publish it and say hey this is
what you expect and this is our area of
this is a range of error that's the kind
of thing i'd put out
on a daily basis maybe we predict the
cells are going to be this or maybe
weekly
so you kind of get a nice you kind of
flatten the
data coming out and you say hey this is
what we're looking at the big takeaway
from this is that we're working with
let me go back up here oops oh too far
there we go
is this model here this is what this is
all about we worked through all of those
pieces
all the tensorflows
and that is to build this sequential
model and we're only putting in the two
layers
this can get pretty complicated if you
get too complicated it never
it never verges into a usable model
so if you have like 30 different layers
in here there's a good chance you might
crash it kind of thing
so don't go too hate wire on that and
that you kind of learn as you go again
it's domain knowledge
um and also starting to understand all
these different layers and what they
mean
the data
analytics behind those layers
is something that your data analysis
professional would come in and say this
is what we want to try
but i tell you as a data scientist um a
lot of these basic setups are common and
i don't know how many times uh
working with somebody and they're like
oh my gosh if i only did a tangent h
instead of a relu activation
i worked for two weeks to figure that
out well as a data science i can run it
through the model in you know five
minutes instead of spending two weeks
doing the the math behind it
so that's one of the advantages of data
scientist is we do it from programming
side and data analytics is going to look
for it how does it work in math
and this is really the core right here
of tensorflow and cross is being able to
build your data model quickly and
efficiently and of course
with any data science
putting out a pretty graph so that your
shareholders again we want to take and
reduce the information down to something
people can look at and say oh that's
what's going on they can see stuff
what's going on as far as the dates and
the change in the ispu
that brings us to the end of this video
tutorial on cnn versus rn and full
course video by simplylon i hope it was
useful and informative if you have any
queries please feel free to put them in
the comments section of the video thank
you for watching
hi there if you like this video
subscribe to the simply learn youtube
channel and click here to watch similar
videos to nerd up and get certified
click here