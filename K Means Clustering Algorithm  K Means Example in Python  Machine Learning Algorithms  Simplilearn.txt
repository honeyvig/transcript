hello and welcome to the session on
kme's clustering I'm Douglas from Simply
learn
so what is k-means clustering K means
clustering is an unsupervised learning
algorithm
in this case you don't have labeled data
unlike in supervised learning
so you have a set of data and you want
to group them and as the name suggests
you want to put them into clusters which
means objects that are similar in nature
similar in characteristics they need to
be put together so that's what K means
clustering is all about
the term k is basically a number so you
need to tell the system how many
clusters you need to perform so if K is
equal to 2 there will be two clusters if
K is equal to three three clusters and
so on and so forth
that's what the k stands for
and of course there's a way of finding
out what is the best or Optimum value of
K for a given data and we will look at
that
so that is k-means clustering so let's
take an example K means clustering is
used in many many scenarios but let's
take an example of Cricket the game of
cricket let's say you received data of a
lot of players from maybe all over the
country or all over the world
and this data has information about the
runs scored by the people ordered by the
player and the wickets taken by the
player and based on this information we
need to Cluster this data into two
clusters batsman and bowlers
so this is an interesting example let's
see how we can perform this
so we have the data which consists of
primarily two characteristics which is
the runs and the wickets
so the bowlers basically take the chips
and the batsman score the runs and there
will be of course a few Bowlers who can
score some runs and similarly there will
be some batsmen who will uh who would
have taken a few wickets but with this
information we want to Cluster those
players into batsman and Bowlers so how
does this work let's say this is how the
data is so there are information there
is information on the y-axis about the
runs scored and on the x-axis about the
wickets taken by the players so if we do
a quick plot this is how it would look
and then we do the clustering we need to
have the Clusters like shown in the
third diagram out here we need to have a
cluster which consists of people who
have scored High runs which is basically
the batsman and then we need a cluster
with people who are taking a lot of
wickets which is typically the bowlers
now there may be a certain amount of
overlap but we're not going to talk
about that right now so with k-means
clustering we have we see here that K is
equal to 2 and we will have two clusters
which is the batsman and the bowlers so
how the way this works is the first step
in k-means clustering is the allocation
of two centroids randomly so two points
are assigned and as so-called centroids
so in this case we want two clusters
which means K is equal to two so two
points that have been randomly assigned
as centroids so keep in mind these
points can be uh anywhere they're random
points and they're not initially anyway
they're not really the centroid centroid
means it's a central point of a given
data set but in this case when it starts
off it's not really the centroid okay so
these points so in our presentation here
we have shown them at one point closer
to these data points and another closer
to these data points they can be
assigned randomly anywhere okay
so that's the first step the next step
is to determine the distance of each of
the data points from each of the
randomly assigned centroids
so for example we take this point and
find the distance from this centroid and
the distance from this centroid this
point is taken and the distance is found
from this centroid in this and so on and
so forth so every point the distance is
measured from both the centroids
and then whichever distance is less that
point is assigned to that centroid
so for example in this case visually it
is obvious that all these data points
are assigned to this centroid and all
these data points are assigned to this
centroid and that's what is represented
here in the blue color and in this
yellow color the next step is to
actually determine the central point
or the actual centroid for these two
clusters so we have this one initial
cluster this one initial cluster but as
you can see these points are not really
the centroid centroids means it should
be the central position of this data set
Central position of this data set so
that is what needs to be determined as
the next step so the central point on
the actual centroid is determined and
the original randomly allocated centroid
is repositioned to the actual centroid
of this new cluster
and this process is actually repeated
now what might happen is some of these
points may get reallocated in our
example that's not happening probably
but it may so happen that the distance
is found between each of these data
points once again with these centroids
and if it is required some points may be
reallocated
we'll see that in a later example but
for now we'll keep it simple so this
process is continued until the centroid
repositioning stops and that is our
final cluster
so this is our final iteration
we've come to this position in this
situation where the centroid doesn't
need any more repositioning and that
means our algorithm has converged
a convergence has occurred and we have
the cluster or two clusters we have the
Clusters with the centroid so this
process is repeated and the process of
calculating the distance and
repositioning the centroid is repeated
until the repositioning stops which
means that the algorithm has converged
we have the final cluster with the data
points and the centroids
so this is what you're going to learn
from this session we'll talk about the
types of clustering what is K means
clustering application of K means
clustering K means clustering is done
using distance measure so we will talk
about the common distance measures and
then we will talk about how K means
clustering works and go into the details
of the k-means clustering algorithm
and then we will end with the demo and a
use case for K means clustering so let's
begin
first of all what are the types of
clustering there are two primary
categories of clustering hierarchical
clustering and then partitioning
clustering and each of these categories
are further subdivided into
agglomerative and divisive clustering
and K means and fuzzy c means clustering
so let's take a quick look at each of
these types of clustering
in hierarchical clustering the Clusters
have a tree-like structure and
hierarchical clustering is further
divided into agglomerative and divisive
agglomerative clustering is a bottom-up
approach we begin with each element as a
separate cluster and merge them into
successfully larger clusters so for
example we have ABCD in F so we start by
combining B and C in one cluster d and e
form one more and then we combine d e
and f the larger cluster and then B and
C and then I finally add a
so compare that to divisive clustering
so divisive clustering is a top-down
approach where we begin with the whole
data set and proceed to divide it into
successfully smaller clusters so we have
a b c d e f we first take that as a
single cluster and then break it down
into a b c d e and f
then we have partitioning clustering
split into two subtypes k means
clustering and fuzzy c means and K means
clustering the objects are divided into
the number of clusters mentioned by the
number k
that's where the K comes from so if we
say k is equal to 2 the objects are
divided into two clusters C1 and C2
and the way it is done is the features
or characteristics are compared and all
objects having similar characteristics
are clubbed together so that's how K
means clustering is done and we'll see
that more in detail as we move forward
now fuzzy c means is very similar to
k-means in the sense that it clubs
objects that have similar
characteristics together but while in K
means clustering two objects cannot
belong to or any object a single object
cannot belong to two different clusters
in c means objects can belong to more
than one cluster
so that's the primary difference between
k-means and fuzzy c means so what are
some of the applications of k-means
clustering K means clustering has a
variety of uses or a variety of business
cases in real life
starting from academic performance
diagnostic systems search engines and
wireless sensor networks and and many
more
so let's take a deeper look at each of
these examples
academic performance So based on the
scores of students students are
categorized into ABC and so on now
clustering forms a backbone of search
engines when a search is performed the
search results need to be grouped
together then the search engines very
often use clustering to do this and
similarly in the case of wireless sensor
networks the clustering algorithm plays
the role of finding the cluster heads
which collects all the data in its
respective cluster
so clustering specifically K means
clustering uses distance measure so
let's take a look at what distance
measure is
so while these are different types of
clustering in this video we will focus
on k-means clustering
so distance measure tells how similar
some objects are so the similarity is
measured using what is known as distance
measure
and well so what are the various types
of distance measures there is euclidean
distance there is Manhattan distance
then we have squared euclidean distance
and cosine distance measure
these are some of the distance measures
supported by k-means clustering
let's take a look at each of these so
what is euclidean distance measure this
is nothing but the distance between two
points as we learned in high school how
to find the distance between two points
so this is a sophisticated formula for
that but we know a simpler one is the
square root of Y2 minus y1 whole square
plus X2 minus X1 whole Square
so this is just an extension of this
formula and that's the euclidean
distance between two points what is the
squared euclidean distance measure it's
nothing but the square of the euclidean
distance as the name suggests so instead
of taking the square root we leave the
square as it is
and then we have Manhattan distance
measure in the case of Manhattan
distance it is the sum of the distances
across the x-axis and the y-axis and
note that we are taking the absolute
value so that the negative values don't
come into play so that's the Manhattan
distance measure then we have cosine
distance measure in this case we take
the angle between the two vectors formed
by joining the points from the origin
so that's the cosine distance measure
okay so that was a quick overview about
the various distance measures that are
supported by K means
now let's check how exactly k-means
clustering works okay so this is how
k-means clustering works
this is like a flow chart of the whole
process there's a starting point then we
specify the number of clusters that we
want
now there are a couple of ways of doing
this that we can do by trial and error
so we specify a certain number maybe K
is equal to three or four or five to
start with and then as we progress we
keep changing it until we get the best
clusters or there's another technique
called the elbow technique whereby we
can determine the value of K or what
should be the best value of K and how
many customers should be formed so once
we have the value of K we specify that
and then the system will assign that
many centroids so it picks randomly to
start with randomly that many points
that are considered to be the centroids
of these clusters and then it measures
the distance of each of the data points
from these centroids and assigns those
points to the corresponding centroid
from which the distances are minimum so
each data point will be assigned to the
centroid Which is closest to it and
thereby we have a k number of initial
clusters
however this is not the final clusters
The Next Step it does is for the new
groups for the Clusters that have been
formed it calculates the main position
thereby calculates the new centroid
position the position of the centroid
moves compared to the randomly allocated
one so it's an iterative process once
again the distance of each point is
measured from this new centroid point
and if required the data points are re
located to the new centroids and the
mean position or the new centroid is
calculated once again if the centroid
moves then the iteration continues which
means the convergence has not happened
the clustering is not converged so as
long as there is a movement of the
centroid this iteration keeps happening
but once the centroid stops moving which
means that the cluster has converged or
the clustering process has converged
that will be the end result
so now we have the final position of the
centroid and the data points are
allocated accordingly to the closest
centroid
I know it's a little difficult to
understand from this simple flowchart
so let's do a little bit of
visualization and see if we can explain
it better
let's take the example of if we have a
data set for a grocery shop so let's say
we have a data set for a grocery shop
and now we want to find out how many
clusters this has to be spread across
so how do we find the optimum number of
clusters
there's a technique called the elbow
method so when these clusters are formed
there's a parameter called written sum
of squares and the lower this value is
the better the cluster is
that means all these points are very
close to each other
so we use this within sum of squares as
a measure to find the optimum number of
clusters that can be formed for a given
data set
so we create clusters or we let the
system create clusters of a variety of
numbers
so maybe above 10 10 clusters and for
each value of K the within SS is the
measure and the value of K which has the
least amount of within SS or WSS that's
taken as the optimum value of K
so this is the diagonal metric
representation so we have on the y axis
the written sum of squares or WSS and on
the x-axis we have the number of
clusters so as you can imagine if you
have K is equal to 1 which means all the
data points are in a single cluster the
within SS value would be very high
because they're probably scattered all
over the moment you split it into two
there will be a drastic fall in the
within SS value
that's what is represented here but then
as the value of K increases the decrease
the rate of decrease will not be so high
so it will continue to decrease but
probably the rate of decrease will not
be so high
so from here we get an idea for an
example the optimum value of K should be
either two or three or at the most four
but beyond that increasing the number of
clusters is not dramatically changing
the value of WSS because that pretty
much gets stabilized okay now that we've
got the value of K and let's assume that
these are our delivery points the next
step is basically to assign two
centroids randomly so let's say C1 and
C2 are the centroids assigned randomly
now the distance of each location from
the centroid is measured and each point
is assigned to the centroid which is the
closest to it
so for example these points are very
obvious that these are closest to C1
whereas this point is far away from C2
so these points will be assigned which
are close to C1 will be assigned to C1
and these points or locations which are
close to C2 will be assigned to C2
and then this is how the initial
grouping is done this is the part of C1
and this is part of C2 the next step is
to calculate the actual centroid of this
data because remember C1 and C2 are not
the centroids they're randomly assigned
points and the only thing that has been
done was that the data points which are
closest to them have been assigned but
now in this step the actual centroid
will be calculated which may be for each
of these data sets somewhere in the
middle so that's like the mean point
that will be calculated and the centroid
would be actually positioned or
repositioned
the same with C2 the new centroid for
this group is C2 this new position and
C1 is in this new position once again
the distance of each of the data points
is calculated from these centroids now
remember it's not necessary that the
distance Still Remains uh or each of
these data points still remain in the
same group by recalculating the distance
it may be possible that some points get
reallocated like so if you see this one
this point uh earlier was closer to C2
because C2 was here but after it was
recalculated and repositioned and you
can see that it is closer to C1 than C2
so this is the new grouping
so some points will be reassigned and
again the centroid will be calculated
and if the centroid doesn't change
during the iterative process
um if the centroid doesn't change once
it stops changing that means the
algorithm has converged and this is our
final cluster with this as the centroid
C1 and C2 as the centroids these data
points as a part of each cluster so I
hope this helps you understand the whole
process
the iterative process of k-means
clustering so let's take a look at the K
means clustering algorithm let's say we
have X1 X2 X3 n number of points as our
inputs and we want to split these into K
clusters or if we want to create K
clusters so the first step was to
randomly pick K points and call them
centroids now they're not real centroids
because uh the centroid is supposed to
be the center point but they are just
called centroids and we calculate the
distance of each and every input point
from each of the centroids
so the distance of X1 from C1 from C2 C3
each of the distances we calculate and
then we find out which distance is the
lowest and assigned X1 to that
particular random centroid then repeat
that process for X2 calculate its
distance from each of the centroids C1
C2 C3 to CK and find which is the lowest
distance unassigned X2 to that
particular centroid same with X3 and so
on so that is the first round of
assignments if you will
now we have K groups
because we have assigned the value of K
so there's K centroids and there are K
groups
and all these have been split into K
groups now remember we picked centroids
randomly so they're not real centroids
so now what we have to do is we have to
calculate the actual centroids for each
of these groups
which is like the main position which
means that the position of the randomly
selected centroids will now change and
they will be the main positions of this
newly formed k group and once again what
is done is we go through the process of
calculating the distance
so this is what we're doing as a part of
step four we repeat step two and three
so again we calculate the distance from
X1 from the centroid C1 C2 C3 and then
we see which is the lowest value and
assigned X1 to that calculate the
distance of X2 from C1 C2 C3 or whatever
all the way up to KCK and find whichever
is the lowest distance and assigned X2
to that Etc so in this process there may
be some reassignment X1 probably was
assigned to a cluster C2 and after doing
the calculation maybe now X1 is assigned
to C1 so that kind of reallocation may
happen so we'll repeat the steps two and
three until the position of the
centroids does not change or they stop
changing and that's when we have
convergence so let's take a detailed
look at it at each of these steps so
we'll randomly pick K cluster centers
and we'll call them centroids because
they're not initially they're not really
the centroids so we'll just name them C1
C2 all the way up to CK and then step
two we'll assign each data point to the
closest Center so what we can do is we
can calculate the distance of each x
value from each C value so the distance
between X1 C1 the distance between X1 C2
X1 C3 and then we find which is the
lowest value or the minimum value we
find and assigned X1 to that particular
centroid
then we go next to X2 and find the
distance of X2 from C1 X2 from C2 X2
from C3 and so on up to c k and then
assign it to the point or the centroid
which has the lowest value and so on so
that's step number two in Step number
three We Now find the actual centroid
for each group
so what has happened as a part of step
two we now have all the points all the
data points grouped into K groups
because we wanted to create K clusters
right so we have K groups and each one
may be having a certain number of input
values and they need not be equally
distributed by the way based on the
distance we will have K groups but
remember the initial values of the C1 C2
were not really the centroids of these
groups we had just randomly assigned
those now in step three we actually
calculate the centroid of each group
which means the original point which we
thought was the centroid will shift to
the new position which is the actual
centroid for each of these groups okay
and we again calculate the distance so
we'll go back to step two and then
calculate the distance of each of these
points from the newly positioned
centroids
and if required we reassign these points
to the new centroids
so as I said earlier there may be a
reallocation so now we have a new set or
a new group
we still have K groups but the number of
items and the actual assignment may be
different from what it was in step two
here
okay so that might change then we'll
perform step three once again to find
the new centroid of this new group so we
have again a new set of clusters new
centroids and new assignments we repeat
this step two once again and once again
we find that it is possible that after
running through four or five iterations
the centroid will stop moving
in the sense that when you calculate the
new value of the centroid it'll be the
same as the original value or there will
be very little or marginal change so
that's is when we say convergence has
occurred
and that is our final cluster
so that's the formation of the final
cluster
all right let's take a look at a couple
demos of k-means clustering some actual
live demos
first we have to determine what is the
problem that we're trying to solve
so let's say Walmart wants to open a
chain of stores across the state of
Florida
and it wants to find the optimal store
locations
now the issue here is if they open too
many stores close to each other
obviously they will not make profit but
if the stores are too far apart then
they don't have enough sales
so how do they optimize the locations
now in our organization like Walmart
which is an e-commerce joint they
already have the addresses of their
customers in their database
so they can actually use this
information or this data and use K means
clustering to find the optimal location
now before we go into the python
notebook
and show you the Live code I want to
take you through uh very quickly a
summary of the code in the slides and
then we will go into the python notebook
so in this block we are basically
importing all the required libraries
like numpy matlot lab and so on
and we are loading the data that is
available in the form of let's say the
addresses
for simplicity's sake we will just take
them as some data points and then the
next thing we do is we'll quickly do a
scatter plot to see how they are related
to each other with respect to each other
so in the scatter plot we see that there
are a few distinct groups
already being formed you can actually
get an idea about how the cluster would
look and how many clusters or or what
would be the optimal number of clusters
and then starts the actual k-means
clustering process so we will assign
each of these points to the centroids
and then check whether they're the
optimal distance which is the shortest
distance and assign each of the points
the data points to the centroids then go
through this iterative process until the
whole process converges
and finally we get an output like this
so we have four distinct clusters and
which is if we can say that this is how
the population is probably distributed
across Florida State and these centroids
are like the location where the store
should be the optimum location where the
store should be
so that's the way we determine the best
locations for the stores
and that's how it can help while I'll
find the best locations for stores in
Florida
so now let's dump this into python
notebook and see how it looks when we're
running the code live
all right so this is the code for
k-means clustering in jupyter Notebook
we have a few examples here which we
will demonstrate how k-means clustering
is used and even there is a small
implementation of k-means clustering as
well
so let's get started so this block is
basically importing the various little
libraries that are required like matlot
lib numpy and so on and so forth which
would be used as part of the code and
then we are going to be creating blobs
which are similar to clusters now this
is a very neat feature which is
available in scikit-learn make blobs is
a nice feature which creates clusters of
data sets
so it has wonderful functionality It's
relatively available for us to create
some test data kind of things so that's
exactly what we're going to do here and
we're using uh make blobs we can specify
how many clusters we want so we see
we're mentioning centers here and we're
just going to mention four centers so
it'll go ahead and create some test data
for us
and this is how it looks as you can see
visually also we can figure out that
there are four distinct classes or
clusters in this data set and that is
what make blobs provides
now
from here onwards we'll basically run
the standard k-means functionality
that is readily available so we really
don't have to implement K means itself
the k-means functionality or the
function is readily available you just
need to feed the data and create the
Clusters so this is the code for that we
import K means and then we create an
instance of k-means and we specify the
value of K so remember that K in K means
clusters is the value of the or the
number of clusters that you want to
create
and it's a integer value so this is
where we are specifying that so we have
K is equal to four so that instance is
created and we take that instance as
with any other machine learning
functionality fit is what we use uh or
the function or the method rather fit is
what we use to train the model
here there's no real training of any
kind but that's the call so we are
calling fit and what we are doing here
is we're just passing the data so X has
this these values the data has been
created
so that is what we're passing here
and then this will go ahead and create
clusters
and then we are using
um
after doing fit We Run The predict which
basically assigns for each of these
observations which cluster it belongs to
right so it will name the Clusters and
maybe this is cluster one two three and
so on or actually it might start from
zero cluster zero one two and three
maybe and then for each of the
observations it will assign based on
which cluster it belongs to it will
assign a value so that is stored in y
underscore K means when we call predict
that's what it does
now we can take a look at these
um
cluster numbers that have been assigned
for each observation so this is the
cluster number assigned for observation
one maybe this is for observation two
three and so on
so it looks like we have about 300
samples right so all 300 samples uh
there are 300 values here and each of
them uh cluster number is given
and the cluster number goes from zero to
three so there are four clusters so the
numbers go from zero to one two three
so this was just a quick example of
generating some dummy data and then
clustering that
and this can be applied if you have
proper data you can just load it up into
X for example here and then run it
okay so this was a central part of the K
means clustering program example so you
basically create an instance and you
mentioned how many clusters you want by
specifying this parameter and underscore
clusters and then there's also the value
of K and then pass the data and get the
values
now in the next section of this code the
implementation of K means this is kind
of a rough implementation of the k-means
algorithm
so we'll just take a walk through and
I'll walk you through the code at each
step and tell you what it's doing and
then we'll see a couple of more examples
of how k-means clustering can be used in
some real life situations real life use
cases
in this case here what we're doing is
basically implementing k-means
clustering
and there is a function uh where the
library calculates for a given two pairs
of points
it will calculate the distance between
them and see which one is the closest
and so on
so this is pretty much what k-means does
and so it calculates the distance of
each point or each data set from a
predefined centroid and then based on
whichever is the lowest that particular
data point is assigned to that centroid
so that's basically available as a
standard function and we will be using
that here so as we explained earlier the
first step that is done in the case of
k-means clustering is to randomly assign
some centroids
so as a first step we randomly allocate
a couple of centroids
which we see here we're calling them
centers and then we put this in a loop
and then we take it through an iterative
process for each of the data points we
first find out using this function
pairwise distances argument for each of
the points we find out which Center or
which randomly selected centroid is the
closest
and accordingly we assign that data or
the data point to that particular
centroid or cluster
and once that is done for all the data
points we calculate the new centroid by
finding out the mean position which is
the center position
so we calculate the new centroid and
then we check to see if the new centroid
or the coordinates or the position is
the same as the previous centroid
the positions we will compare and then
if it is the same that means the process
has converged
so remember we do this process until the
centroids are that centroid doesn't move
anymore so the centroid gets reallocated
each time and once the movement doesn't
change anymore uh or that position of
the centroid doesn't change anymore we
know that convergence has occurred until
then you see here this is like an
infinite Loop while true is an infinite
Loop and it only breaks when the centers
are the same the new center and the old
Center positions are the same and once
that is done you can see we return
centers and labels so this is not a very
uh sophisticated implementation
it's very basic because one of the flaws
in this is that sometimes what happens
is the centroid Position will keep
moving
but the change will be very minor
so in that case we could consider that
convergence for example say the change
please .0001 we can consider that to be
convergence otherwise what will happen
is this will either take forever or it
will be never ending so that's a small
flaw here for that reason additional
checks may have to be added but again
this example is a very simple
implementation of K means clustering
okay so if we execute this code this is
what we get as the output so this is the
definition of this particular function
and we call that find underscore
clusters and we pass out data X number
of clusters which is four and if we run
that and plot it this is the output that
we get so we see here that each cluster
is represented by a different color we
have a green cluster yellow cluster and
so on and so forth and these big points
here these are the centroids this is the
final position of the centroids and you
can see visually also that this appears
kind of uh to be a center of these
points right here and similarly this is
like the center of all these points and
so on so this is an example of an
implementation of k-means clustering so
next we will move on to see a couple of
examples of how k-means clustering is
used in maybe some real life scenarios
or use case examples
in the next example or demo we're going
to see how we can use k-means clustering
to perform color compression we'll take
a couple of images so there will be two
examples and we will try to use
K means clustering to compress the
colors
this is a common situation in image
processing when you have an image with
millions of colors but then you cannot
render it on some devices which may not
have enough memory so so that is a
scenario where something like this could
be used
so again before we begin we go into the
python notebook let's take a look
quickly at the code
as usual we import the libraries and
then we import the image and then we
will flatten it
so the reshaping is basically we have
the image formation is stored in the
form of pixels and if the images like
for example 427 by 640 and it has three
colors so that's the overall dimension
of the initial image we just reshape it
and then feed this to our algorithm
and this will then create clusters of
only 16 clusters so this is these are
the colors are that are the millions of
colors and now we need to bring it down
to 16 colors
so uh we'll use K is equal to 16 and
when we visualize it this is how it
looks there are about 16 million
possible colors right the input space
has 16 million possible colors and then
we just sub compress it to 16 colors so
this is how it would look when we
compress it to 16 colors so this is how
the original image looks and after
compression to 16 colors this is a look
at the new image
and you can see there's not a lot of
information that has been lost
though the image quality is definitely
reduced
so
this would be an example of which we're
going to now see in Python so let's go
into Python and once again as always we
will import some libraries and load the
image called flower dot jpeg
so let me load that and this is how it
looks this is the original image which I
think has 16 million colors and this is
the shape of the image which basically
it's the the pixels the 427 by 60 640
pixels
and then uh we see three layers which is
basically for RGB which is red grain and
blue so that is basically the shape of
this
so what we need to do now is take a look
at how the data looks
so let me just create a new cell and
show you what is in the data basically
we have captured this information so the
data
is uh let me just show you here
all right so let's take a look at China
what are the values in China
and if we see here this is how the data
is stored this is uh the pixel values
okay so this is like a matrix for each
one
uh which has about 427 by 640 pixels
right so this is how it looks and the
issue here is these values are large
the numbers are there are a lot of
numbers so we need to normalize them to
between zero and one so that's why it
will basically create one more variable
which is data which will contain the
values between 0 and 1. the way to do
that is to divide by 255 so we divide
China by 255 and we get new values in
data so let's just run this piece of
code
and this but this is the shapes that we
have now also all what we have done is
changed using reshape we've converted
these into three-dimensional uh or
two-dimensional data sets
and now let's take a look to uh let me
just insert probably
a cell here and take a look at how the
data is looking so this is how the data
is looking and now you see this is these
are the values between 0 and 1. so if
earlier you noticed in the case of china
the values were large numbers now
everything is between zero and one and
this is one of the things we need to do
so after that the next thing we need to
do is to visualize this and we can take
a random set of maybe 10 000 points and
plot up
and check to see how this looks so let's
just plot this so this is how the
original color with the pixel
distribution is and there are two plots
one is red against Green and the other
is red against Blue and this is the
original distribution of the color so
then what we'll do is we'll use k-means
clustering to create
just 16 clusters for the various colors
and then apply that to the image now
what will happen is since the data is
large because there are millions of
colors using regular K means maybe a
little time consuming so there's another
version of k-means which is called mini
batch
so we will use that which processes uh
the overall concept Remains the Same but
basically it processes it in smaller
batches that's the only difference so
the results will be pretty much the same
so let's go ahead and execute this piece
of code
and then we can visualize this so we
could see this is how the 16 colors
would look
so this is red against Green and this is
red against Blue
so there is quite a bit of similarity
between the two uh color schemes and we
could see that the new one doesn't look
too different or anything like that now
we apply this to the newly created
colors uh to the image and we can take a
look at how this image or now we can
compare both images so this is how our
original image looked and this is our
new image
so you can see there's not a lot of
information that has been lost pretty
much looks like the original image
though we can see uh for example here
there we've appeared to lose a little
bit of detail
compared to this one because we kind of
took out some of the finer details of
the color but overall the high level
information has been maintained at the
same time the main advantage is that now
this can be an image which can be
rendered on a device which may not be
that very sophisticated
now let's take one more example with a
different image in the second example we
will take an image of a palace in China
and we repeat the same process this is a
high definition color image with
millions of colors
and also three-dimensional now we will
reduce that to 16 colors using k-means
clustering and we use the same process
we did before
we reshape it and then we cluster the
colors to 16.
and then we render the image
once again and we will see that the
color the quality of the image slightly
deteriorates as you can see here this
has much finer details in uh which are
probably missing here but then that's
the compromise because there are some
devices which may not be able to handle
this kind of high density images
so let's run this code in Python
notebook all right so let's apply the
same technique for another picture which
is even more intricate
probably has a much more complicated
color schema so this is the image now
once again we can take a look at the
shape which is for 27 by 640 by 3 and
this uh is the new data would look
something like this compared to the
flower image so we have some new values
here and we will also
uh as you can see here the numbers are
much bigger
so we'll now have to scale them down to
values between 0 and 1 and that's done
by dividing by 255 again so let's go
ahead and do that
uh I'd reshape it
so we get a two-dimensional Matrix
and then as the next step we will go
ahead and visualize this
in 16 colors and this is basically how
it look
16 million colors and now we can create
uh the Clusters
out of this the 16k means clusters we
will create so this is how the
distribution of the pixels would look
with 16 colors and then we go ahead and
apply this
and uh visualize what it looks like uh
for the new uh just with 16 colors so
once again you can see this looks much
richer in color but at the same time
this probably doesn't have
um it doesn't look as rich as this one
but nevertheless this information is not
lost and the shape and all that stuff
and so this could also be rendered now
on a
um a device which is not as
sophisticated
so that's pretty much it we've seen two
examples of how color compression can be
done using k-means clustering
and we have also seen in previous
examples how to implement K means uh the
code roughly how to implement k-means
clustering
and we've used some data using blob to
just execute the k-means clustering all
right so let's move on so let's
summarize what we have learned in this
video
we started with an example of how we can
apply k-means clustering taking the
Cricut example
and then we talked about the types of
clustering two major categories
hierarchical clustering and partition
clustering which in turn had two sub
categories agglomerative and divisive
and then k-means and fuzzy C and then we
discussed the distance measures uh the
different types of distance measures
supported by k-means clustering and we
focused on k-means clustering we talked
about its applications and and how the
process flow works for k-means
clustering
and then finally we ended up with a demo
and a couple of use cases
all right so with that we've come to the
end of this video thank you very much
for watching
and with that if you have any more
questions please feel free to visit us
at www.simplylearn.com
you may also add your comments and
feedback at the bottom of this video
thanks once again I'm Douglas for simply
learning
[Music]
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here