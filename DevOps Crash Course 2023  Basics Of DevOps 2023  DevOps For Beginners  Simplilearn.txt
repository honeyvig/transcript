welcome to the devops crash course where
we will make understanding of devops
easy and fun devops short for
development and operations is like the
secret source of modern software
development it's all about making things
fun smooth and efficient in this course
we will cover the basics like what
devops actually is and why it's super
cool we'll also talk about how much
money devops Engineers can make which is
pretty impressive but the real fun
begins when we dive into the tools
you'll get to know the nine awesome
devop tools from git and GitHub for
Version Control to selenium for
automated testing Jenkins for continuous
integration Docker for containerization
and kubernetes for orchestralization
we'll cover it all join us as we unravel
the atrocities of this devops crash
course link the foundation for a
successful career in the world of
software development and operations get
ready to embark on exciting learning
Journey so if you are interested in
taking your career to the next level
look no further than a postgraduate
program in devops this comprehensive
course is designed to empower you with
the skills and knowledge needed to excel
in the dynamic world of devops this
program offers 50 hours of self-paced
learning master classes led by Caltech
ctme 20 plus real life projects in
integrated labs and opportunity to
acquire 40 plus in demand skills and
master 15 plus essential tools top it
all off with a Capstone project spanning
three domains and you will be well on
your way to a successful devops career
so don't wait and enroll now when it
goes through a number of key elements
today the first two will be reviewing
models that you're already probably
using for delivering Solutions into your
company and the most popular one is
waterfall followed by agile then we'll
look at devops and how devops differs
from the two models and how it also
borrows and leverages the best of those
models we'll go through each of the
phases that are used in typical devops
delivery and then the tools used within
those phases to really improve the
efficiencies within devops finally we'll
summarize the advantages that devops
brings to you and your teams so let's go
through waterfall so waterfall is a
traditional delivery model that's been
used for many decades for delivering
Solutions not just IT solutions and
digital Solutions but either way before
that it has its history it goes back to
World War II so waterfall is a model
that is used to capture requirements and
then Cascade each key deliverable
through a series of different stage
Gates that is used for building out the
solution so let's take you through each
of those stage Gates the first that you
may have done is requirements analysis
and this is where you sit down with the
actual client and you understand
specifically what they actually do and
what they're looking for in the software
that you're going to build and then from
that requirements analysis you'll build
out a project planning so you have an
understanding of what the level of work
is needed to be able to be successful in
delivering the solution after that
you've got your plan then you start
doing the development and that means
that the program is start coding out
their solution they build out their
applications to build out the websites
and this can take weeks or even months
to actually do all the work when you've
done your coding and development then
you send it to another group that does
testing and they'll do full regression
testing of your application against the
systems and databases that integrate
with your application you'll test it
against the actual code you'll do manual
testing you do UI testing and then after
you've delivered the solution you go
into maintenance mode which is just kind
of making sure that the application
keeps working there's any security risks
that you address those security risks
now the problem you have the is that
there are some challenges however that
you have with the waterfall model the
cascading deliveries and those complete
and separated stage Gates means that
it's very difficult for any new
requirements from the client to be
integrated into the project so if a
client comes back and it's the project
has been running for six months and
they've gone hey we need to change
something that means that we have to
almost restart the whole project it's
very expensive and it's very time
consuming also if you spend weeks and
months away from your client and you
deliver a solution that they are only
just going to see after you spend a lot
of time working on it they could be
pointing out things that are in the
actual final application that they don't
want or are not implemented correctly or
lead to just general unhappiness the
challenge you then have is if you want
to add back in the client's feedback to
restart the whole waterfall cycle again
so the client will come back two with a
list of changes and then you go back and
you have to start your programming and
you have to then start your testing
process again and just you're really
adding in lots of additional time into
the project so using waterfall model
companies have soon come to realize that
you know the clients just aren't able to
get their feedback in quickly
effectively it's very expensive to make
changes once the teams have started
working and the requirements in today's
digital world is that solution simply
must be delivered faster and this has
led for a specific change in agile and
we start implementing the agile model so
the agile model allows programmers to
create prototypes and get those
prototypes to the client with the
requirements faster and the client is
able to then send their requirements
back to the programmer with feedback
this allows us to create what we call a
feedback loop where we're able to get
information to the client and the client
can get back to the development team
game much faster typically when we're
actually going through this process
we're looking at the engagement cycle
being about two weeks and so it's much
faster than the traditional waterfall
approach and so we can look at each
feedback loop as comprising of four key
elements we have the planning where we
actually sit down with the client to
understand what they're looking for we
then have coding and testing that is
building out the code and the solution
that is needed for the client and then
we review with the clients the changes
that have happened but we do all this in
a much tighter cycle that we call a
Sprint and that typically a Sprint will
last for about two weeks some companies
run sprints every week some run every
four weeks it's up to you as a team to
decide how long you want to actually run
a Sprint but typically it's two weeks
and so every two weeks the client is
able to provide feedback into that Loop
and so you were able to move quickly
through iterations and so if we get to
the end of spring 2 and the client says
hey you know what we need to make a
change you can make those changes
quickly and effectively for Sprint three
what we have here is a breakdown of the
ceremonies and the approach that you
bring to Agile so typically what will
happen is that a product leader will
build out a backlog of products and what
we call a product backlog and this will
be just a whole bunch of different
features and maybe small features or bug
fixes all the way up to large features
that may actually span over multiple
Sprints but when you go through the
Sprint planning you want to actually
break out the work that you're doing so
the team has a mixture of small medium
and large solutions that they can
actually Implement successfully into
their Sprint plan and then once you
actually start running your Sprint again
it's a two-week activity you meet every
single day the two with the actual
Sprint team to ensure that everybody is
staying on track and if there's any
blockers that those blockers are being
addressed effectively and immediately
the goal at the end of the two weeks is
to have a deliverable product that you
can put in front of the customer and the
customer can then do a review the key
advantages you have are running a Sprint
with agile is that the client
requirements are better understood
because the client is really integrated
into the scrum team they're there all
the time and the product is delivered
much faster than with a traditional
waterfall model you're delivering
features at the end of each Sprint
versus waiting weeks months or in some
cases years for a waterfall project to
be completed however there are also some
distinct disadvantages the product
itself really doesn't get tested in a
production environment it's only been
tested on the developer computers and
it's really hard when you're actually
running agile for the Sprint team to
actually build out a solution easily and
effectively on their computers to mimic
the production environment and the
developers and the operations team are
running in separate silos so you have
your development team running their
Sprint and actually working to build out
the features but then when they're done
at the end of their Sprint and they want
to do a release they kind of fling it
over the wall at the operations team and
then it's the operations team job to
actually install the software and make
sure that the environment is running in
a stable fashion that is really
difficult to do when you have the two
teams really not working together so
here we have is a breakdown of that
process with the developers submitting
their work to the operations team for
deployment and then the operations team
may submit their work to the production
servers but what if there is an error
what if there was a setup configuration
error with the developers test
environment that doesn't match the
production environment there may be a
dependency that isn't there there may be
a link to an API that doesn't exist in
production and so you you have these
challenges that the operations team are
constantly faced with and their
challenge is that they don't know how
the code works so this is where devops
really comes in and let's dig into how
devops which is developers and operators
working together is the key for
successful continuous delivery so devops
is as an evolution of the agile model
the agile model really is great for
Gathering requirements and for
developing and testing out your
Solutions and what we want to be able to
do is kind of address that challenge and
that gap between the Ops Team and the
dev team and so with devops what we're
doing is bringing together the
operations team and the development team
into a single team and they are able to
then work more seamlessly together
because they are integrated to be able
to build out solutions that are being
tested in a production-like environment
so that when we actually deploy we know
that the code itself will work work the
operations team is then able to focus on
what they're really good at which is
analyzing the production environment and
being able to provide feedback to the
developers on what is being successful
so we're able to make adjustments in our
code that is based on data so let's step
through the different phases of a devops
team so typically you'll see that the
devops team will actually have eight
phases now this is somewhat similar to
Agile and what I'd like to point out at
the time is that again agile and devops
are very closely related that agile and
devops are closely related delivery
models that you can use with devops it's
really just extending that model with
the key phases that we have here so
let's step through each of these key
phases so the first phase is planning
and this is where we actually sit down
with a business team and we go through
and understand what their goals are the
second stage is as you can imagine and
this is where it's all very similar to
Agile is that the code is actually start
code but they typically they'll start
using tools such as git which is a
distributed Version Control software it
makes it easier for developers to all be
working on the same code base rather
than bits of the code that is rather
than them working on bits of the code
that they are responsible for so the
goal with using tools such as git is
that each developer always has the
current and latest version of the code
you then use tools such as Maven and
Gradle as a way to consistently build
out your environment and then we also
use tools to actually automate our
testing now what's interesting is when
we use tools like selenium and junit is
that we're moving into a world where our
testing is scripted the same as our
build environment and the same as using
our get environment we can start
scripting out these environments and so
we actually have scripted production
environments that we're moving towards
Jenkins is the the integration phase
that we use for our tools and another
Point here is that the tools that we're
listing here these are all open source
tools these are tools that any team can
start using we want to have tools that
control and manage the deployment of
code into the production environments
and then finally tools such as ansible
and Chef will actually operate and
manage those production environments so
that when code comes to them that that
code is compliant with the production
environment so that when the code is
then deployed to the many different
production servers that the expected
results of those servers which is you
want them to continue running is
received and then finally you monitor
the entire environment so you can
achieve zero in on spikes and issues
that are relevant to either the code or
changing consumer habits on the site so
let's step through some of those tools
that we have in the devops environment
so here we have is a breakdown of the
devops tools that we have and again one
of the things I want to point out is
that these tools are open source tools
there are also many other tools this is
just really a selection of some of the
more popular tools that are being used
but it's quite likely that you're
already using some of these tools today
you may already be using Jenkins you may
already be using git but some of the
other tools really help you create a
fully scriptable environment so that you
can actually start scripting out your
entire devops tool set this really helps
when it comes to speeding up your
delivery because the more you can
actually script out of the work that
you're doing the more effective you can
be at running automation against those
scripts and the more effective you can
be at having a consistent experience so
let's step through this devops process
so we go through and we have continuous
delivery which is a plan code build and
test environment so what happens if you
want to make a release well the first
thing you want to do is send out your
files to the build environment and you
want to be able to test the code that
you've been created because we're
scripting everything in our code from
the actual unit testing being done to
the all the way through to the
production environment because we're
testing all of that we can very quickly
identify whether or not there are any
defects within the code if there are
defects we can send that code right back
to the developer with a message saying
what the defect is and the developer can
then fix that with information that is
real on the either the code or the
production environment if however your
code passes the the scripting tests it
can then be deployed and once it's out
to deployment you can then start
monitoring that environment what this
provides you is the opportunity to speed
up your delivery so you go from the
waterfall model which is weeks months or
even years between releases to Agile
which is two weeks or four weeks
depending on your Sprint Cadence to
where you are today with devops where
you can actually be doing multiple
releases every single day so there are
some significant advantages and there
are companies out there that are really
zeroing in on those advantages if we
take any one of these companies such as
Google Google any given date will
actually process 50 to 100 new releases
on their website through their devops
teams in fact they have some great
videos on YouTube that you can find out
on how their devops teams work Netflix
is also a similar environment now what's
interesting with Netflix is that Netflix
have really fully embraced devops within
their development team and so they have
a devops team and Netflix is a
completely digital company so they have
software on phones on Smart TVs on
computers and on websites interestingly
though the devops team for Netflix is
only 70 people and when you consider
that a third of all internet traffic on
any given day is from Netflix it's
really a reflection on how effective
devops can be when you can actually
manage that entire business with just 70
people so there are some key advantages
that devops has it's the actual time to
create and deliver a software is
dramatically reduced particularly
compared to Waterfall complexity of
maintenance is also reduced because
you're automating and scripting out your
entire environment now you're improving
the communication between all your teams
so teams don't feel like they're in
separate silos but that are actually
working cohesively together and that
there is continuous integration and
continuous delivery so that your
consumer your customer is constantly
being delighted if you're into the tech
industry or just curious about the role
of devops in software development you
have come to the right place so what
exactly is devops in simple terms it's a
set of practices and tools that help
developers and operational team work
better together releasing software
faster with higher quality at its core
devops is about breaking down barriers
between development and operations and
creating a culture of collaboration that
focuses on delivering value to customers
as quickly and efficiently as possible
of course this is a vast
oversimplification and there are many
different aspects of devops that we
could spend hours diving into but for
now let's focus on some of the key
responsibilities of a devops engineer
who is the person responsible for
implementing and overseeing devops
practices and processes but before we
begin if you're into the channel and
haven't subscribed already consider
getting subscribed to Simply learn to
stay updated with all the latest
Technologies and with that Bell icon to
never miss an update from us having said
that the demand for devops professionals
has overgrown in recent years as more
and more companies adapt devops
practices to improve their software
development and delivery processes so
are you ready to advance your
professional career to the next level
taking our step-by-step simply learned
program in devops in collaboration with
IBM will help you start your devops
journey that will prepare you for the
devops engineer role in order to match
your skill set market demand this devops
training program is designed in
collaboration with Caltech ctme our
Cutting Edge branded learning combines
live online devop certification classes
with interactive labs to give you
practical experience this post-target
program covers topics including git
GitHub Docker CI CD practices using
Jenkins kubernetes and much more so what
else can you expect from this program
well this devops trading program will
cover skills like devops methodology
continuous integration devops and Cloud
deployment Automation and you'll also
get hands-on experience with the latest
tools and techniques including terraform
Maven and symbol Jenkins Docker junit
and many more this program will cover
industry projects like Docker Rising
junkins pipeline deploy angular
application and Docker container
branching development model and many
more exotic projects so if you are
looking to pursue your career as a
devops engineer and acquire skills that
will prepare you for your job consider
enrolling in this intensive training
program we will leave the link in the
description box below make sure you
check that out so without any further
Ado let's get started with today's topic
firstly let us understand what is devops
now devops is a software development
approach that emphasizes collaboration
Automation and communication between
development and operations team it aims
to streamline the entire software
development lifecycle by integrating and
optimizing processes tools and
methodologies it encourages the culture
of shared responsibility where
developers and operations team work
together closely throughout the entire
software development life cycle from
planning and coding to testing
deployment and monitoring
now the question is who is a devops
engineer well you got it right a devops
engineer is a professional who combines
software development expertise with
operations knowledge to facilitate
collaboration streamline processes and
improve software delivery and
infrastructure management within an
organization a devops engineering role
is to bridge the gap between development
and operations team enabling efficient
and reliable software development and
deployment practices
but the question is how to become a
devops engineer what are the skills that
you need to possess to become a good
devops engineer well a devops engineer
poses a wide range of skills including
proficients in scripting and programming
languages knowledge of various tools and
Technologies expertise in
systemadministration dot platforms and
containerization Technologies as well as
strong problem solving and communication
skills are necessary firstly having a
good coding knowledge well tools like
Confluence jira git these tools can
support and enhanced collaboration and
project management within a devops
environment next having a good knowledge
on deployment tools are also necessary
now tools like dcos provides
orchestrationization capabilities for
distributed applications Docker enables
containerization for consistent and
scalable deployments and AWS offers a
broad range of cloud services for
infrastructure provisioning scalability
and managed Services next you need to
have a good knowledge on operations
tools as well now chef and ansimil focus
on infrastructure automation and
configuration management while
kubernetes specializes in container
orchestration and management these tools
are utilized in devops to automate
various aspects of software like
development life cycles including
infrastructure provisioning
configuration management application
deployment and scaling moving ahead you
need to have a strong grip on monitoring
tools nagios Splunk and dated dog are
three commonly used tools in the field
of monitoring and observatability now
each rule serves a specific purpose in
monitoring and managing system and
applications nag your specializes
infrastructure and application
monitoring Splunk focuses on log
analysis and data visualization datadog
provides comprehensive monitoring and
analytic capabilities in Cloud
environments these tools play a crucial
role in maintaining the health and
performance of systems and applications
moving ahead you need to have a good
knowledge on Jenkins and courtship now
Jenkins and courtships are both
essential tools and demo practices
Jenkins is a flexible and extensible
automation server that supports
continuous integration testing and
deployment on the other hand core ship
is a cloud-based CI CD platform that
offers Simplicity and ease of use
particularly for cloud native
applications both these tools contribute
to improving development productivity
and code quality and finally having a
good knowledge on testing tools like
selenium junit are necessary for a day
offs engineer junit is primarily focused
on unit testing and automated testing or
Java code while selenium is geared
towards functional testing and
automation of web applications both
these tools play critical roles in
devops workflow contributing to faster
feedback Cycles improve code quality and
Reliable Software releases
so these are some of the main and
important skills that you need to
possess as a devops engineer before
moving ahead let's take a minute and
listen to the experiences of our
Learners who have enrolled in the devops
pgp program which has proven to be
highly beneficial for many aspiring
engineers and professionals leading them
to achieve New Heights in the field of
devops
I started my It Journey with Accenture
three years ago I joined there as a
cloud architect there I worked with AWS
and Azure Technologies looking for
higher paying job and devops seem the
right career choice so I decided to go
with the postgraduate program in devops
in collaboration with Caltech ctme the
course was divided into modules
and we had assignments
I was really impressed by how many job
interviews I landed after I added the
certification to my portfolio and now I
am earning 40 more than my previous job
it didn't only boost my career but also
my confidence
well now comes the main part what
exactly are the day-to-day roles and
responsibilities of a devops engineer
now a devops engineer play a crucial
role in Bridging the Gap between
development and operations team as we
discussed earlier so here are some of
the top five roles and responsibilities
of a devops engineer in detail first on
the list we have collaboration and
communication now devops engineer of a
state effective communication and
collaboration with development and
operations team they actively
participate in meetings and discussions
to align goals and expectations now as a
devops engineer you need to engage in
regular meetings and discussions regular
engagement ensures that they are up to
date with ongoing projects challenges
and goals enabling them to better align
their efforts and contribute effectively
regular engagement ensures that they are
up to date with ongoing projects
challenges and goals enabling them to
better align their efforts and
contribute effectively actively listen
and understand the requirement concerns
and feedback now when engaging with
development and operations team day offs
Engineers practice active listening they
close attention to the requirements
concerns and feedback expressed by team
members from both the sites by
understanding their perspectives pain
points and suggestions devops Engineers
can better assess the needs of their
teams and collaborate to find suitable
Solutions frustrated effective
communication channels now devops
Engineers take the initiative to
establish and maintain effective
communication within the organization
this often involves settling up
dedicated chat platforms like slack
Microsoft teams or collaboration tools
like jira to Foster better collaboration
and ensure that information flows
smoothly between the teams and finally
encourage cost functional collaboration
now day of Engineers recognize the value
of cross-function collaboration and
knowledge sharing among the team members
they actively encourage them from
development and operations team to
collaborate exchange ideas and share
their expertise second on the list we
have infrastructure Automation and
configuration management now demos
Engineers focus on automating
infrastructure provisioning and managing
configurations using certain tools they
Define infrastructure as code enabling
efficient deployment and scaling of
resources
now as a devops engineer you have to
identify the infrastructure requirements
effectively now day of Engineers work
closely with development teams to
understand the infrastructure
requirements of the application this
involves analyzing the needs of
applications in terms of Computer
Resources Storage security networking
and scalability by gathering all these
requirements devops engineer can ensure
that infrastructure is provisioned and
configured to meet the application
specific need and future growth write
infrastructure automation scripts and
templates now once the infrastructure
requirements are identified devops
Engineers use automation tools and
techniques to define the desired state
of infrastructure components the right
scripts and templates that specify how
the infrastructure should be provisioned
configured and managed
automate the provisioning configuration
and management of servers well devops
Engineers leverage infrastructure as
code or in short IAC principles to
automate the provisioning configuration
and management of servers networks and
other infrastructure resources they use
tools like ansemble Chef or puppet to
automate the deployment and
configuration of infrastructure
components and finally regularly update
infrastructure code now devops engineer
uses Version Control Systems like git to
track changes collaborate with team
members and manage different versions of
infrastructure code by regularly
updating and versioning infrastructure
code devops Engineers can easily track
and reverse changes whenever necessary
now third on the list we have continuous
integration and continuous deployment or
CI CD in short now devops Engineers are
responsible for establishing and
maintaining cicd pipelines which enable
developers to integrate code changes
seamlessly and deploy applications
rapidly
so for that they have to set up a
version control system now Version
Control System like git play a crucial
role in devops by providing a
centralized repository for managing code
and tracking changes setting up a
Version Control System involves creating
a repository initializing it with the
code and defining branching and merging
strategies
configure a build server now a build
server automates the process of
compiling testing and packaging
application code tools like Jenkins and
gitlab cicd allows you to Define build
pipelines that specify the steps to be
executed these pipelines typically
involve tasks such as pulling code from
repository compiling source code
generating artifacts and packaging the
application
next automate the deployment process now
automation of the deployment process is
crucial for achieving rapid and
consistent software releases
containerization tools like Docker
provide a lightweight and portable way
to package application and their
dependencies Docker containers can be
created and deployed consistently across
different environments ensuring
consistency between development testing
and production Define an inverse quality
Gates and monitor CI CD kpis quality
Gates ensure that the code meets
predefined quality standards before it
is promoted to the next stage of the
cicd pipeline automated testing
including unit test integrated test and
end-to-end test integration test and
end-to-end test help catch bugs and
validate the functionality of the
managing applications and finally
measuring K pairs of the cicd pipeline
provide insights into its performance
and help identify areas for improvement
well next we have monitoring and
performance optimization now day of
Engineers monitor system performance
identify and
infrastructure and application Stacks
whenever necessary they Implement
monitoring tools to collect and analyze
metrics logs and traces so for that
select and configure monitoring tools
now monitoring tools like Prometheus or
graphene can be used to collect and
visualize these metrics allowing tapes
to identify bottlenecks or optimized
processes and enhance the overall
efficiency of the CI CD pipeline
also we have to collaborate with
development and operations team to
fine-tune application performance
so continuously optimizing the
infrastructure which will ensure High
availability scalability and reliability
of that application and finally we have
security and compliance now deox
Engineers play a critical role in
implementing security measures and
ensuring compliance with industry
standards and regulations they work
closely with security teams to Define
and Implement security controls
throughout the software delivery
pipeline so they have to continuously
collaborate with the security teams to
identify and Define security
requirements and controls and Implement
security measures such as vulnerability
scanning access management and secure
configuration they have to continuously
integrate security testing and code
analysis into the CI CD pipeline
and monitor for any sort of potential
security risk or breaches and respond
promptly to mitigate any identified
vulnerabilities
so these were some of the main or top
five roles and responsibilities of a
devops engineer talking about the salary
figures of a senior devops engineer
according to glassdo a senior devops
engineer working in the United States
earns a whooping salary of 178
362 dollars the same senior devops
engineer in India earns 18 lakh rupees
annually
to sum it up as you progress from entry
level to mid level and eventually to
experience devop engineer your roles and
responsibilities evolved significantly
each level presents unique challenges
and opportunities for growth all
contributing to your journey as a
successful devops professional so
excited about the opportunities devops
offers great now let's talk about the
skills you will need to become a
successful devops engineer
coding and scripting strong knowledge of
programming languages like python Ruby
or JavaScript and scripting skills are
essential for Automation and Tool
development
system administration familiarity with
Linux Unix and Windows systems including
configuration and troubleshooting cloud
computing Proficiency in Cloud platforms
like AWS Azure or Google Cloud to deploy
and manage applications in the cloud
containerization and orchestration
understanding container Technologies
like Docker and container orchestration
tools like kubernetes is a must
continuous integration or deployment
experience with CI CD tools such as
Jenkins gitlab Ci or Circle CI to
automate the development workflow
infrastructure as code knowledge of IAC
tools like terraform or ansible to
manage infrastructure programmatically
monitoring and logging familiarity with
monitoring tools like promptials grafana
and logging Solutions like elk stack
acquiring these skills will not only
make you a valuable devops engineer but
will also open doors to exciting job
opportunities so to enroll in the
postgraduate program in devops today
click the link mentioned in the
description box below don't miss this
fantastic opportunity to invest in your
future let's take a minute to hear it
out from all Learners who have
experienced massive success in their
career through a postgraduate program in
devops devops a combination of
development and operations is a software
development approach that emphasizes
collaboration Automation and continuous
integration and delivery devops has
gained immense popularity in recent
years due to its ability to enhance
software development processes
streamline operations and improve
overall efficiency so if you're
interested in learning devops from
scratch this video will guide you
through a comprehensive roadmap
providing you with a clear path to
mastering the principles and practices
of devops but before that if you want to
learn about devops and its concept in
depth then simply learns Celtic post
graduate program in devops will be the
right choice for you this devops course
is designed in collaboration with caltex
ctme for a professional development
option that will Square your skills with
industry standards in this course you
will learn how to formalize and document
development processes and create a
self-documenting system devops
certification course will also cover
Advanced tools like puppet Soul stack
and ansible that helps self-governance
and automated management at scale so why
wait check out the course link mentioned
in the description now let's dive into
the video but let's start with
understanding the different software
development methodologies present out
there will software development models
also known as software development
methodologies or processes a systematic
approaches used to plan design develop
and deliver software applications there
are several different types of software
development models each with its own set
of principles practices and
characteristics so first among them is
waterfall model the waterfall model is a
sequential software development approach
that follows a linear progression
through various phases including
requirements Gathering design
implementation testing deployment and
maintenance a each phase is completed
before moving on to the next and it
relies heavily on upfront planning and
documentation it is often used in
projects with well-defined requirements
and stable environments second is agile
model now Asian methodologies such as
scrum and kanbin emerged as a response
to the limitations of the waterfall
method agile focuses on iterative and
incremental development with frequent
feedback and collaboration between
cross-functional teams the project is
divided into smaller time box iterations
and Sprints where features are developed
tested and delivered in a rapid and
flexible manner agile methodologies
prioritize adaptability customer
satisfaction and continuous Improvement
and third is devops now devops is not a
software development model per se but
rather a cultural and operational
philosophy that emphasizes collaboration
and integration between software
development and it operations teams it
aims to automate and streamline the
entire software delivery pipeline from
development and testing to deployment
and monitoring devops encourages closer
collaboration shared responsibilities
and the use of automation tools to
achieve faster and more Reliable
Software releases it promotes a
continuous delivery approach allowing
frequent and smaller updates to be
deployed in short devops is a philosophy
and set of practices that transcends
traditional software development models
it emphasizes collaboration automation
continuous delivery and a holistic view
of software development by an operations
by combining development and operations
into a unified approach devops strives
to achieve faster more reliable and
customer-centric software delivery now
that we have understood why devops is
different from other software
development models and why is it so
important to learn now let's see how you
can learn devops from scratch so the
first step is understand the Core
Concepts well to embark on your devops
journey it is crucial to grasp The Core
Concepts devops involves cultural
process and Technical aspects start by
understanding the key principles such as
continuous indicate creation continuous
delivery infrastructure as code and
automation continuous integration
involves integrating code changes
frequently into a shared repository and
running automated tests to catch issues
early continuous delivery focuses on
automating the deployment process
ensuring that software is always in a
releasable state infrastructure as code
emphasizes managing infrastructure
resources through code enabling
scalability consistency and
reproductibility automation plays a
pivotal role in reducing manual effort
and increasing efficiency throughout the
software development cycle second is
learn Version Control System well
version control system is a fundamental
aspect of devops get a widely used
Version Control System allows you to
track changes in your code base
collaborate with team members and manage
different versions of your code so begin
by learning the basic of git commands
such as initializing a repository
creating branches committing changes and
merging branches gain hands-on
experience by creating repositories
collaborating with others using git and
understanding branching strategies that
facilitate parallel development and
merging code changes third is Master
configuration management now
configuration management is essential
for efficient management and deployment
of software configurations across
various environments explore popular
configuration management tools like
ansible Chef or puppet these tools
enable you to define the desired state
of your infrastructure and automate
configuration management task learn how
to write infrastructure code known as
playbooks or recipes which specify the
desired configuration of servers and
services automating configuration
management ensures consistency reduces
manual errors and simplifies
infrastructure management at scale then
dive into continuous integration and
continuous delivery now continuous
integration and continuous delivery are
integral to devops practices enabling
teams to deliver software rapidly and
reliably CI involves regularly
integrating code changes into a shared
repository and running automated test to
identify issues early explore popular CI
CD tools like Jenkins Travis Ci or
gitlab CI CD setup cicd pipelines which
automate the build test and deployment
processes learn how to configure these
pipelines to compile and test code run
unit test execute integration test and
deploy applications to various
environments gain hands-on experience in
automating the release process and
delivering software frequently with
confidence next is Embrace
infrastructure as code infrastructure as
code is a vital component of modern
devops practices it involves managing
infrastructure resources through code
enabling you to provision and configure
infrastructure programmatically learn
tools like terraform or AWS cloud
formation to Define infrastructure
resources such as servers networking and
storage understand the declarative
nature of infrastructure code and how it
allows for reproductibility and
scalability gain knowledge of cloud
platforms like AWS Azure or Google cloud
and explore their respective devops
tooling to automate infrastructure
provisioning scaling and monitoring next
explore containerization and
orchestration now containerization
Technologies like Docker have
revolutionized software development by
providing lightweight isolated
environments for applications Begin by
understanding containers their benefits
and how they differ from traditional
virtual machines learn how to create and
manage Docker containers package your
applications into containers and ensure
consistency across different
environments dive given to container
orchestration platforms like kubernetes
which allows you to automate the
deployment scaling and management of
containers understand Concepts like
paths deployments services and Ingress
and gain practical experience by
deploying applications using
containerization and orchestration tools
next adopt monitoring and logging
monitoring and logging play a crucial
role in ensuring application stability
and performance learn about monitoring
tools like promethus grafana or nagios
which enables you to collect and
visualize metrics and gain insights into
the health of your systems exclude
logging Frameworks like the elk stack
like elasticsearch log stash kibana
which provides a centralized platform
for collecting analyzing and visualizing
logs understand how to set up monitoring
and blogging systems configure alerting
and troubleshoot issues in real time and
gain insights into the performance
availability T and reliability of your
applications next emphasize security and
compliance security is an essential
aspect of devops practices in corporate
security principles and best practices
India devops workflow learn about secure
coding practices vulnerability scanning
tools and penetration testing
methodologies understand compliance
standards and Frameworks like PCI DSS
Hita and gdpr and ensure that your
systems adhere to these regulations
explore security focused tools and
techniques to protect your
infrastructure applications and data
Implement secure authentication
authorization and encryption mechanisms
to ensure the integrity and
confidentiality of your systems Nexus
continuous learning and Improvement well
devops is a continuously evolving field
with new tools and Technologies emerging
regularly stay updated with the latest
industry Trends by following blogs
forums and and conferences engage with
the devops community participate in
discussions and contribute to open
source projects continuous learning and
Improvement are key to mastering devops
take up projects experiment with
different tools and techniques and
continually refine your skills
participate in hackathons attend
workshops and pursue relevant
certifications to enhance your knowledge
and stay competitive now in a world
driven by rapid technological
advancements businesses are constantly
seeking ways to deliver software faster
and more efficiently enter devops the
dynamic philosophy that has
revolutionized as a software development
and operations landscape devops is not
just a buzzword it's a game engineering
approach that Bridges the gap between
developers and operations team Foster
collaboration and enhancing the entire
software development life cycle now at
its core devops embodies a cultural
shift towards seamless integration and
continuous delivery a promotes a
collaborative environment where
developers system administrators quality
assurance professionals and other
stakeholders work together harmoniously
and promoting cross-functional
communication the devops impulse teams
streamline processes automate workflows
and ultimately deliver high quality
software at an accelerated Pace having
said that the demand for devops
professionals has overgrown in recent
years as more and more companies are
update of practices to improve their
software development and delivery
processes so are you ready to advance
your professional career to the next
level taking our step-by-step simple
learn sports archet program in devops in
collaboration with IBM will help you
start your day option A that will
prepare you for the devops engineer role
so in order to match your skill set
market demand this layoff training
program is designed in collaboration
with caltex ctme and a cutting-edge
blended learning combines live online
day of certification classes with
interactive labs to give you a practical
experience this day of training program
will cover skills like day off
methodology continuous integration Day
Option Cloud infrastructure deployment
Automation and many more and along with
that you'll also get hands-on experience
with latest tools and techniques
including terraform Maven ansible Docker
Jenkins and much more so if you're
looking to pursue your career as devops
engineer and acquire these skills that
will prepare you for your job consider
enrolling in this intensive training
program today we will leave the link in
the description box below so make sure
you check that out so let us move ahead
and proceed with our topic today so the
main question is what fuels is devops
Revolution now it's a powerful
collection of tools specifically
designed to support and amplify its
principles so these tools act as a
catalyst empowering teams to automate
tasks marriage infrastructure
efficiently and monitor application
effectively so without any further delay
let us directly jump into the top nine
noteworthy tools highlighting their
features benefits and significance that
have become synonymous with the devops
ecosystem in no particular order
so first on the list we have Jenkins now
Jenkins is an open source automation
server known for its extensive plug-in
ecosystem making it highly versatile and
customizable it facilitates continuous
integration and continuous delivery or
in short cacd pipelines automating the
build test and deployment processes
Jenkins enables team to internet code
frequently entering early detection of
issues and faster software releases
so let us now look at some of the key
features of their Jenkins firstly easy
installation and configuration Jenkins
has a user-friendly web interface that
simplifies installation configuration
and management of build jobs and
pipelines distributed bill for
scalability now it's a personal vast
ecosystem of plugins for CMS integration
and it can distribute build tasks across
multiple nodes which helps in paralyzing
builds and reducing overall build time
especially in large projects and finally
extensible through recipient languages
like Ruby now Jenkins provides
extensibility through scripting
languages like groovy which is a
powerful and versatile language that
runs on a Java virtual machines or in
short jvm so by leveraging groovy
scripting capabilities Jenkins becomes
highly adaptable and customizable to
meet the specific requirements of
different development teams and projects
so those are some of the key features of
Jenkins let us now move ahead and
discuss some of the benefits or
advantages of using Jenkins so firstly
we have enabled faster feedback and
quicker time to market now Jenkins
continuous integration capabilities
enable developers to merge their code
changes into a shared repository
regularly by doing so Jenkins
automatically triggers build and testing
processes to validate any kind of
changes this automates feedback loop
allows developers to receive quick
feedback on the quality of their code so
as a result issues and bugs can be
identified early in the development
cycle preventing the accumulation of
defects and reducing the time required
to fix them as discussed earlier it also
facilitates early issue detection and
reduce bug turnaround time now with
Jenkins running automated builds and
test every time new code is committed
potential issues and bugs are bought
early in the development process so by
detecting issues at an early stage
developers can address them promptly
before they propagate further into the
code base
and finally automates repetitive tasks
save time and effort now Jenkins
automates various tasks involved in the
software development life cycle using
include Stars such as building the code
running tests and deploying applications
by automating these repetitive and time
consuming tasks gen can saves developers
and operations significant amount of
time and efforts so that is what Jenkins
is all about it's a widely adopted in
the day of landscape due to its
flexibility and extensive plug-in
support and learning it will add a great
value to your array of skills so it's
ability to automate crcd processes and
integrate with various tools makes it a
crucial component of modern software
development and delivery Pipelines
second on the list we have Docker Docker
has revolutionized application
deployment with its containerization
approach it allows developers to package
applications and their dependencies into
lightweight isolated containers Docker
contains provide consistency across
different environments ensuring that
applications and consistently regardless
of the underlying infrastructure this
portability along with the rapid startup
times and efficient resource utilization
has made Docker a foundational tool and
DeVos practices so let us now discuss
some of the key features of Docker first
one is packaging applications and
dependencies into containers now Dockers
allows developer to package their
applications and all their dependencies
into a self-contained units called
containers and the process is known as
containerization now these containers
encapsulate the application code its
runtime various libraries and system
tools required to run the application by
doing so Docker ensures consistency
across different environments secondly
efficient resource utilization through
containerization now this lightweight
nature of containers allows for more
efficient resource utilization multiple
containers can run on a single fiscal
machine without the need for individual
OS instances it means that you can host
more applications and services on the
same Hardware reducing the number of
servers needed and finally easy scaling
and management of containers Now
containerization simplifies application
scaling and management so when you need
to handle increased application load you
can quickly scale by running more
instances on the containerized
application on additional servers or
within a container orchestration
platform like kubernets
so let us now talk about some of the
benefits of the docker tool now firstly
rapid deployment and scalability Docker
enables rapid deployment of application
Studios lightweight and containerized
approach when using Dockers developers
package their applications and
dependencies into containers which
encapsulate everything needed to run the
application so these containers are
portable and can be easily moved from
one environment to the other and next we
have the isolation of applications and
dependencies for improved security
Docker utilizes contourization to
isolate applications and the
dependencies from the ecosystem and
other containers each containers operate
in its own user space separate from
other containers providing a strong
level of isolation this isolation
prevents applications from affecting
each other and helps contain potential
security breaches within the confines of
the container and finally simplify
development to production workflow now
dock the streamlined the relevant
production workflow by providing
consistency between different
environments with Docker developers can
create containers that run the same way
in development testing and production
environments thereby reducing the
chances of unexpected issues arising
during the deployment
so Docker has become a Cornerstone of
modern day of practices it's upgraded to
streamline application deployment and
improved resource utilization has
transformed software development and
operations enabling faster and more
reliable application delivery
so that was all about Docker so let us
now move ahead and discuss the next tool
which is kubernetes so next on the list
we have kubernetes kubernetes is an open
source container orchestration platform
that automates the deployment scaling
and management of containerized
applications it provides a robust
infrastructure for running and
coordinating containers across clusters
of machine making it easier to manage
large scale deployments devop streams
can easily deploy update and scale
applications facilitating faster and
more Reliable Software delivery while
promoting collaboration and consistency
across development and operations
lifecycle through kubernetes so let us
look at some of the key features of it
automatic container deployment and
scaling now container orchestration
platform to the kubernetes provide
automatic container deployment and
scaling capabilities when deploying
applications on kubernets you can Define
the desired state of your application
using yaml files on declarative
configuration kubernetes then take care
of this ensuring that specified number
of container replicas is running at all
times service Discovery and load
balancing
now kubernet enables faster feedback and
weaker time to Market in a country rise
environment multiple instances of an
application may be running across
different containers making it
challenging for clients or other
services to know the IP addresses of all
the running instances and detecting and
keeping tracking of the locations of
various services within the container
cluster and kubernetes for example
provides a built-in service Discovery
mechanism next we have self-healing and
orderly set of containers now container
orchestration platform provides
self-healing capabilities to ensure that
applications are always available and
responsive if a container fails to do an
application crash or any other issues
the orchestration platform detects the
failure and automatically restarts the
failed container
so let us now move ahead and discuss
some of the benefits of using kubernetes
tool firstly it is scalable and highly
available application and it provides a
seeming repeat and provides a seamless
scaling of resources based on demand
next it has a simplified management of
containerization applications across
various clusters and finally automated
deployment and updates reducing manner
Intervention which further improves
resource utilization and optimization
overall kubernetes has emerged as the
industry standards of container
orchestration its ability to automate
application deployment scaling and
management simplifies the complexities
of running contenderized applications at
scale making it a crucial tool for
devops practitioners
moving and let us discuss our next tool
which is ansible ansible is a powerful
automation tool that simplifies
configuration management application
deployment and orchestration it employs
a declarative language to Define desired
shared configuration making it easy to
manage and automate infrastructure tasks
ansible follows an agentless
architecture allowing it to work
efficiently across a wide range of
systems and environments so let us now
look at look at some of its features
firstly it's a declarative language for
defining infrastructure configuration
and it is also an agentless architecture
for easy deployment and management as I
discussed earlier now this is done
through a Playbook driven automation for
orchestration where extensive library of
modules for a wide range of tasks are
employed so let us now discuss some of
its benefits so firstly it's simplify
infrastructure management through
automation which increases the
operational efficiency with reduced
manual task and its identity nature
ensures that it has consistently and
predictability overall
and finally it supports a wide range of
infrastructure automation use cases and
with its agentless architecture it
allows easy integration with various
systems and environments so in a
nutshell ansible Simplicity flexibility
and ease of use have made it a popular
choice for automating infrastructure and
application deployment task Dev approach
an agentless architecture contribute to
efficient and streamline development
Workforce
so moving guide the next tool on our
list is git is a distributed version
control system that has become a
fundamental tool for modern software
development practices it allows
developers to track changes collaborate
effectively and manage code base
efficiently gets decentralized
architecture ensures that developers can
work offline and more changes seamlessly
across branches let us now discuss with
some of its key features now its
distributed version control system has
efficient collaboration with various
tools within the management of
the ecosystem of devops and it also has
an integration with various code hosting
platforms which is the branching and
merging capabilities for concurrent
development and finally in support for
code reviews and collaboration workflows
which also gives a commit based tracking
of changes so these are some of the key
features of git so let us now discuss
some of the advantages or like benefits
of using it so firstly it's easy
tracking and management of code changes
which ensures efficient collaboration
and concurrent development within the
resource files and it's upgraded to work
offline and merge changes seamlessly
will benefit a lot of devops Engineers
and finally it's easy integration with
other devops tools makes it a wonderful
to use to manage code versions track
changes and enable efficient
collaboration which is Revolution as a
software development so it has become a
essential tool for Version Control
facilitating effective collaboration and
enabling streamline devops workflows
so moving ahead the next tool on our
list is terraform now terraform is an
infrastructure as code or in short IEC
tool that allows teams to Define and
provision infrastructure resources in a
declarative manner it supports multiple
Cloud providers and enables consistent
and reproducible infrastructure
deployment terraforms declarative syntax
and State Management capabilities
simplify infrastructure provisioning and
configuration
let us now look at some of its key
features firstly it provides multi-cloud
support for provisioning resources
across different providers its
infrastructure as code approach for
consistent and reproducible deployments
makes it a declarative Syntax for
defining infrastructure configurations
and finally automated resource
provisioning and dependency management
so some of its benefits are the
advantages of using terraform as its
simplified resource provisioning and
dependency management now terraforms
infrastructure as code approach supports
for multiple Cloud providers makes it an
essential tool for managing
infrastructure codes and collaboration
and Version Control for infrastructure
configurations as well and finally in
State Management for tracking
infrastructure changes which makes it a
beneficial tool for devops engineers
is used to Monitor and manage the health
and performance of the infrastructure
application and network resources it's a
comprehensive monitoring solution to
identify and resolve issues proactively
ensuring High availability and
reliability of systems so let us look
some of its key features firstly its
monitoring capabilities
secondly it's centralized configure
management and finally its even handling
and escalation tool so learning nagios
is also a crucial and beneficial if you
start just starting one day off's
Journey
so moving ahead let us now discuss our
next tool on a list which is Elk stack
now elk stack offers a comprehensive
solution for centralizing logs from
various applications and systems making
it easier for devops teams to monitor
troubleshoot and gain valuable insights
from their log data the elk stack
comprising elasticsearch log stats and
kibana provides a comprehensive log
management and Analysis platform it acts
as a distribution search an analytics
engine while log stack collects
processes and transform logs and finally
kibana offers a user-friendly interface
for visualizing and exploring data
so some of its key features including
real-time monitoring and alerting
scalable and efficient log storage and
retrieval and distributed social
analytics to data visualization and
Exploration with cabana
let us look at some of its benefits so
firstly it's real-time monitoring for
proactive detectives repeat its
real-time log monitoring for proactive
issue detection with the help of
centralized log management and Analysis
Advanced log analytics was
troubleshooting and performance
optimization making it a scalable
architecture for handling large-scale
volumes of data and finally efficient
log storage and retrieval for compliance
and auditing purposes in a nutshell elk
stack has gained immense popularity for
its ability to handle log management and
Analysis its scale it empowers devops
teams with real-time insights into
application and infrastructure logs
facilitating effective troubleshooting
and performance optimization
well finally on the list we have jira
software jira is a widely used project
management tool that supports Agile
development methodologies it offers
robust features for planning tracking
and managing task issues and workflows
jiras customizable boards backlogs and
workflows impact teams to collaborate
effectively visualize and progress to
gain transparency into project statuses
with integration to various devops tools
jiraf estate seamless tracking of
development activities enabling
continuous Improvement and efficient
project management
some of its key features are
customizable both workflows for project
management agile planning and estimation
features issue tracking and management
capabilities
and some of its benefits include
efficient task management and tracking
enhance collaboration and visibility
across various teams and integration
with various devops tools for stream and
workflows it also helps in reporting an
analytics for project insights and
performance measurement finally jira has
become a go-to tool for agile project
management in the devops ecosystem it's
upgraded to support agile methodologies
track task and integrate with other
devops rules makes it a valuable assets
for teams seeking efficient project
management and continuous Improvement so
learning it will add a great value to
Aerial skills again so these were some
of the top 9 Bay Ops rules that you must
know which will help you enhance and
accelerate your career in devops so if
you are interested in taking your career
to the next level look no further than a
postgraduate program in devops this
comprehensive course is designed to
empower you with the skills and
knowledge needed to excel in the dynamic
world of devops this program offers 50
hours of self-paced learning masterclass
is led by Caltech ctme 20 plus real life
projects in integrated labs and
opportunity to acquire 40 plus in-demand
skills and master 15 plus essential
tools top it all off with a Capstone
project spanning three domains and you
will be well on your way to a successful
devops career so don't wait and enroll
now so a little scenario before using
gets one of the challenges that a lot of
developers and development teams would
have had is developers will be working
on different types of code whether it's
database code whether it was a python or
whether it's Java or.net and they would
have a central server that they would be
pushing all of their source code in but
there was little or no communication
that was actually going on in between
all the developers you know the
challenge you had with that scenario is
that when people would be checking a
code you could be like you know there'll
be conflicts and you'd have to roll back
different code versions and and this is
really kind of the challenge that git
addresses git is a tool that allows all
of the developers no matter what stack
they're working in to have access to all
of the code and it makes it much more
effective at being able to have
development teams work on small medium
or even massive applications and some of
the biggest applications out there are
managed through a git distributed server
environment so let's jump into what
we'll be covering so you have a clear
understanding of the value that you're
going to get out of watching this
presentation so we're going to go
through devops and the tools have
available for devops we'll talk about
what version control means within a
devops environment and cover the two
different types of Version Control
centralized and distributed and then
we're going to zero in on git which is a
really fantastic distributed Version
Control System I'm going to go through
and understand the features workflow
branches and commands in git we're going
to give you a demo and then summarize
all the activities at the end of the the
presentation so what is devops well this
is one of my favorite questions I love
the idea of what devops is so devops
really is a culture of being able to
deliver Solutions faster where your
development teams and your operation
teams work effectively together the idea
is to be able to continuously build out
Solutions and have testing codes that's
built into your Solutions so that no
matter where they are in the stage of
the integration and deployment life
cycle they're always being tested and
you can always have code being ready to
be released out the idea is that instead
of having big releases that you'd have
maybe once every two weeks or once a
month that you would actually have a
continuous stream of releases because
you always have code that is being
tested you have your network that's been
tested you have your environment being
tested and you'll be able to provide
feedback directly to the appropriate
person whether they're in debts or
whether they're in arts to be able to be
successful at being able to deliver
Solutions faster the bottom line your
Dev team things like operations and your
operations team begin to feel and think
like developers it's really a fantastic
way of being able to speed up delivery
and have your team just think and feel
and act in a different cultural
environment so let's have a look at some
of the tools that are available to you
in devops environments if we look at
devops it's really kind of split into
two areas the dev side you have building
code planning and testing and then on
your upside you have release deployment
operating and monitoring but the tools
really all interact with each other and
what's really great is that the tools
are either open source or very very low
in cost in fact most of the tools that
you're seeing in front of you right now
are open source tools which means
there's no licensing to you and you can
actually effectively manage them and
Implement them within your team right
now so let's have a look at some of the
dev tools that you have for doing code
versioning and so some of the tools that
you may have used in the past include
subversion team Foundation service and
git these are all different types of
Version Control software that you have
out there one of the oldest ones is
subversion it's a centralized Version
Control System it's one of the tools I
used years ago it really is good at
doing what it's supposed to do which is
just a very simple version controlled
solution it's open source so it's free
there's no licensing there are
challenges with working with it because
it is a centralized tool rather than a
distributed tool we'll get into that in
a little bit but for a very basic
Version Control System you know it does
what you wanted to do team Foundation
Service Solutions something that many of
you in Microsoft world may be using for
a long time and the thing that's great
about TFS is it's built right into the
Microsoft environment it's the server
side the services side of building out
your Solutions with Microsoft just
recently in the last couple of weeks at
the beginning of September 2018.
Microsoft actually just rebranded TFS as
Azure devops and so you have Azure
pipelines and there are five now Azure
tools as you replace TFS but these are
all tools that are very similar in
concept the old TFS tools are very
similar to the centralized SVN tools
whereas the new tools that are part of
azure are actually much more aligned
with get so what you're seeing is
Microsoft moving from the centralized
server to a distributed server with
their new Azure devops tools which is
fantastic news for every developer
listening to this and then we have get
and get is a distributed Version Control
System it is again open source which
means that you can start using it right
now without fear of having to have any
costs or any penalties and the thing
that's really good about it is that it's
really you can use it for almost any
kind of digital project and what it's
good at is being able to create that
historical record and versioning of your
source code whether you're doing a web
application a mobile application or
you're actually building a python script
for a machine learning solution so let's
dig into what version control systems
are and how they can be of value to you
so the role of a version control system
is to allow developers to be able to
check in their files into a repository
and here you can actually have see how
we can check in three files into a
repository and that repository then
becomes a snapshot or a historical
record of the files that we're working
on and you want to be able to have it so
that the repository is flexible enough
so that as you want to be able to scale
and add in new files or new versions of
a file that you can actually do that
easily within your VCS environment and
the goal is to be able to constantly
have each of these versions available so
that anybody who wants to be able to
check out the file can actually have the
latest version or see a history of how
that file became what it is today let's
look at centralized Version Control
Systems and then we'll compare it
against distributed Version Control
Systems so a centralized version control
system has a central server where all
files are stored and everybody has to
check in and check out from that
centralized server and all of the
versions are managed within that server
environment the problem is is that if
that Central server crashes which
doesn't happen but very often but it can
happen you end up losing all of your
files and I was actually on a project
where that actually happened to us and
we lost six months of history of how the
application was created fortunately for
us we actually had a backup but it
didn't have all the historical data so
we were able to at least continue
working but we just lost a lot of files
and it took a while to get back into
place so let's now look at and compare
this to a distributed version control
system with a distributed Version
Control System you still have a
centralized server that manages the
files but the difference is is that as a
developer you check out all of the files
for a project so you can actually manage
the whole project locally on your
develop machine and make your changes
and then you can just check in and out
the changes that you've made to the
centralized server the opportunity you
have here is that the server itself if
it goes down you're not going to be in a
situation where you lose all of the
history because everybody that's working
on the actual application has all of the
versions of the code locally on their
machine now one of the more popular
distributed Version Control Systems that
out there is git I mean it really is
probably the most popular by a margin
compared to a system out there so you
know let's dig into you know what
actually is get so git is a version
control system for managing files and as
a developer you have a get client on
your machine and you're able to make
changes and create local repositories of
a program on your computer and then you
can sync up with a remote service such
as GitHub or gitlab or the new Azure
Labs environment where you can actually
store files remotely and then allow
remote teams to have access to those
changes in the actual application you
made so you can track your changes
easily you have it's the the tool is
inherently distributed so it makes it
very easy to manage the code for large
teams and you can bring people in very
quickly and you can have people come in
as Specialists and spin them up quickly
and have them work on a piece of the
code and then drop them out of the
project if you want to have a check on
how this works go and start contributing
to any of the the many thousands of Open
Source projects like GitHub where you
can go in with your get tools and just
contribute to those projects and then
finally one of the things that I really
like with Git is that it's not a linear
development approach it's not starting
from a and going into Z it's a
non-linear approach which allows you to
have branches that people are working on
in parallel to your master Branch where
people are actually doing the delivery
of the production code so let's step
through some of these features so git as
you can imagine the first thing it does
is track history that's probably the
most important part of having a Version
Control System it's a free open source
you can actually go and use it right now
there are no costs for actually using it
it is a non-linear environment that
allows people to be able to build out
new features in parallel to the master
Branch so that you can be constantly
having the master code out there without
and adding in new features without
breaking the code and you can
automatically create backups by because
each developer has a version of the code
remotely it's incredibly scalable some
of the biggest projects out there now
are being managed through git and
because of this scale the collaboration
becomes a byproduct of teams working
together and branching just is so much
easier within git so as you can see this
whole thing leads to a very effective
distributed environment so let's step
through some of the the workflow that we
have available for git so the way that
git works is that you as an operator as
a developer you have the get client on
your local development machine whether
it's a Linux machine a Mac or a PC and
you connect to a remote server and you
pull down the latest working copy and
then you're able to make all of your
changes uh locally and you can modify
the codes you can review changes and you
can commit those codes to your git
repository that you have locally and
then you can push those changes back up
to the remote server and as soon as you
push them into the remote server those
changes then become available to
everybody else working on the project so
they can be constantly keeping everybody
updated on all the work that is
happening so you know typically the way
that it would work is you know you would
start off with having the working file
the working directory that you have
locally then you would stage those files
into a state aging area and then you get
those files ready to be committed to
your git repository and these are all
actions that you would do locally and
then connect to a remote server and then
later when you go back into work on your
project the first thing you should
always do is check out that code so you
always have the latest version of the
code and everybody is kept in sync so
let's talk about branching and get so
the way that branches work is that if
you imagine you're working on a project
and projects always have a tendency of
getting bigger than you imagine and what
you want to be able to do is keep that
main product working and keeping it
working effectively but if you want to
be able to add a feature to that product
you may have two or three different
groups that are working on multiple
features
simultaneously and what you want to be
able to do is give them the freedom to
work on those individual features and
the way you do it in git is that you can
create branches off the master and while
you're of the the main branch which is
called the master and so each person
could be working on their own separate
branch and then when they want to they
can then later merge those branches back
into the main master and all this time
the main Master still works and still
able to produce the right code for the
customer but at the same time it allows
the developers the freedom to be able to
write in their new features and so we
can look at this in a little bit more
visually with the master branch and we
can put in small features and large
features so we can then merge them back
as we need to so this kind of covers
some of the commands that we have in git
so the First Command you'd want to use
is called get init and that's allow you
to go into a folder on your local PC and
or your local Mac and convert that file
that folder into a local git repository
and that creates that repository then
you want to be able to make changes to
that repository and you the commands
you'd be using in this instance would be
and commit or status and you want to be
able to sync your repositories through
the remote server so be able to put the
code that you have on your computer on
the remote server you push you're going
to get the code from the remote server
you'd use pull and then add origin and
then if you want to do parallel
development from the master Branch the
main branch you would use Branch merge
or rebase as ways to be able to do
parallel development and now we're going
to go through and do a demo on git so
we're going to do a demo on get so you
can feel comfortable running the
commands in the interface and you're
able to get get running correctly in
your environment so the first thing
we're going to do is we're going to set
up which version of git that you have
and then we're going to see if we can
establish some Global configurations
within your git environment and we're
going to do sandeep.s as the name email
address is sandeep.simplylearn and then
we're going to do a list of the
different configuration settings we have
so to check the version of git that we
have we do git dash dash version in our
terminal or command line window you can
also use Powershell that will actually
get you running in the same environment
as well all of these tools are going to
be command line tools and then the next
thing we're going to do is we're going
to assign some Global usernames and a
global email address and so that we can
actually access the get account itself
locally on the device and so we're going
to Dash and we're going to put in config
dash dash Global user.name Sandeep and
then a git
config.global user.email and
sandeep.d at simplylearn.net and now
we're going to check the list of
usernames and email IDs in our
configuration environment with Dash list
and you'll see that we have everything
in there correctly here is our username
right here and our name and our email ID
so if you need any help when you're
actually doing any of your work and get
all you have to do is to get help config
or git config dash dash help and this
will allow you to actually get access to
to the help screens so let's just go and
type that in git help config return and
this takes us to the help page that's
running locally on your device and here
we have a breakdown of all the different
commands and what those commands
actually do within a git configuration
and we can go ahead and we can do git
config dash dash help and that will
actually take us to the same page so two
ways of doing the same thing so we're
going to go ahead and we're going to
create a test repository on our local
system and we're going to do that with
make directory called test and then
we're going to move our cursor within
command window terminal window to the
test folder and then we're going to
initialize that folder to become a git
repository so let's go ahead and do that
so we're going to make a new directory
and we're going to call it test and
we're going to move the cursor so it's
actually in the test folder we do CD
change directory to test and you can see
now we're actually in the test folder
and now all you have to do is initialize
this folder as a git repository and so
we're going to do this as git init and
it's now a new get instance that we can
actually now use for managing our git
environment so we're going to create a
new file and we're going to call it
info.txt and we're going to put some
content in it and we're actually going
to put it in the folder that we've just
created so you can actually see what
it's like to test out the git
environment with a file that hasn't been
checked into it yet so let's go ahead
and do that so we open up our photo
directory and here you can see we have
the test folder and we're just going to
go ahead and create a default test file
so right click Text new document
info.txt open that and we'll just you
can put really whatever you want in here
we're just going to put in this
information right now name equals Sam
registration number one two
479 and save that file close it out now
if we actually go ahead and run a get
status command you'll see that the
info.txt file is in red that's because
that file has not actually been checked
into the project it's a new file that
we've added but it hasn't been committed
yet into the git repository so let's go
ahead and see what it takes to actually
add the file to the git repository that
we just created so we're going to do
that by doing get add info.txt and then
we can commit that into the history of
that git repository so we can do so it
now has a Version Control and we can
start doing uh forking and other kind of
sim activities when we connect later to
our GitHub git environment so we do git
add info Dot txt and remember we're
actually still in the test folder so
it's going to look for that file in that
folder and just add it in and now if we
go ahead and do get status we'll
actually be able to see that the
info.txt file is in an active color
first of all you have to commit it so we
go get commit Dash M and then we're
going to commit and we're adding some
quick text you can write whatever you
want committing the text file it's now
actually committed into the get
repository and it's all running locally
on your PC so we're going to go make
some changes to the file and we're going
to save it and we want to be able to see
how we can use git to compare the
differences between the two files so
let's go over and we're going to make
some changes to the info.txt file here
we are here's info.txt which is going to
add in a new line we're going to add in
the line address and we'll save that
file and now when we go over to get
we're going to be able to review the
difference so we're going to do get div
and here you actually see that we've
added in a new address file so now you
can actually see this is the difference
and the difference between the two
documents now as you can imagine when
you have code this would become more
elaborate where actually Google should
take show you code that's been pulled
out if you've run reduced any of
eliminate amazing code or if you added
in new code really useful so we're going
to go ahead and add our get username to
a remote location so we can actually
start testing out the file that we've
created into a remote repository in this
case we're going to use GitHub and
you'll be able to see how we can
actually then save that file into a git
repository that somebody else could then
access and be able to make edits to so
let's just go ahead and make those
changes so git config Dash Global user
dot username and we'll do simply learn
and that will associate it with this git
repository that we've just created
locally so simply on dash GitHub so
simply learn GitHub is the username that
we use on GitHub so we're going to go
over to GitHub here we are in GitHub and
then correct new Repository
and then as you can see we're simp so
we're in simply learn Dash GitHub and
we're going to call the new repository
test we're going to make it public so
anybody can access it and you'll see
that it automatically adds a readme file
and we have our test environment set up
correctly so we'll go ahead and copy the
URL link which is github.com simplylearn
GitHub for test.get copy that and we'll
go back over to our command line window
and so now we're going to add the
username to the GitHub configuration so
what we're doing is we're connecting the
local repository to the remote
repository so we type get remote add
origin and then we'll paste in the
address and this then allows us to
connect the local repository that we
created in the test folder with the
remote repository and now we're
connected so now that we've connected we
can actually push the file that we have
in the local repository on your PC or
Mac to the remote repository in the
GitHub server environment so let's go
ahead and do that so we're going to do
get push and push is the command to push
the documents from the origin which is
the local file to master and Master's a
remote file okay and there we are
success so what we have now is the file
that we just created in our local get
repository is actually now in the remote
repository as well so anybody can access
it so let's go have a look at that so
we're going to refresh the GitHub page
and hey there we are there's info.txt
has been uploaded and we're ready to go
that's great that's uh that's what you
should be seeing so refreshing your web
page on the remote GitHub folder and it
could be any get service but just having
to be using GitHub because it's free to
use you'll be able to see that the local
file has been installed and is now part
of the git repository in the remote
server so what we can do now is we're
going to create three more files and
we're just going to call them info.txt
info 2 and info three and then we're
going to push them out to the remote
server and we're going to merge
everything together into a single
environment so let's go ahead and create
create those three files and we're just
going to add those into the folder so
let's go ahead create new text document
info one and then we're going to create
info two and then info three get my
typing right here okay here we go there
we are okay I'm opening info three and
I'm just gonna enter some text in here
and what we're going to do is illustrate
how we can do branching and each of
these files I'm going to save and then
close out and now I can go over and so
let's create a new branch and we're
going to call this one first underscore
branch and we go get Branch first
underscore branch and this will be a new
branch of the code environment that
we're creating and so hit return and
that allows us to create a new branch
and so what we do is we're going to move
to the new Branch so we're going to do
get checkout first underscore branch and
that allows us to move into that branch
and you'll see that info.txt is already
there which is good that's what we want
to see now what we need to do is add in
the new documents so here we have the
different steps we've taken we created a
branch we moved to the new branch and
now we're going to go ahead and add the
info 3 txt to that Branch so you can
actually see everything coming together
and we're going to go ahead get add info
3.txt so we're just adding a file like
we were previous see with our initially
our first initial git folder and that
file has now been added to the first
Branch so we're going to go ahead and
commit the file to the first branch and
then we're going to merge the documents
into the master Branch so we have
everything together so let's go ahead
and do those steps we're going to do the
commit first and then we're going to
merge everything together right so we do
git commit Dash M and we'll just put in
we're just going to say uh made changes
to First branch and that's just a
documentation so that we know what we've
done and there we are we've committed
the file so we had that file info 3 txt
committed and now we're going to go
ahead and merge the files into the main
master and so we can actually have
everything together as one consistent
environment and we do that by typing in
so this list out what we have in our
Master file so we just do LS and that
will show everything in the folder and
we have info.txt info 1 txt info 2 txt
and info 3txt are now all in the first
Branch so let's go ahead and we're going
to merge okay so actually before we
merge we have to check out the master
Branch so we do get checkout master so
it gets us into the master branch and
you'll see that we have just one file
there info.txt so we're going to list
out the files in the master branch and
so so what you'll see is info dot text
info one text and info two texts are
there but info dot info three text isn't
there because that was created in a
separate Bunch so let's go ahead and
merge the first Branch into the master
Branch so we do git merge and we do
first underscore branch and there we are
we've merged it in and excellent that
looks good so let's go ahead and list
out the files that we have in the master
branch and we do LS and then you'll see
that we have four files including the
info three dot text that we had in a
separate Branch now all in the master
Branch so what are we going to cover
today so we're going to introduce the
concept of Version Control that you will
use within your devops environment then
we'll talk about the different tools
that are available in a distributed
Version Control System we'll highlight a
product called get which is typically
used for Version Control today and
you'll also go through what are the
differences between get and GitHub you
may have used GitHub in the past or
other products like get lab and we'll
explain what are the differences between
get and get and services such as GitHub
and gitlab will break out the
architecture of what a get process looks
like and how do you go through and
create forks and clones how do you have
collaborators being added into your
projects how do you go through the
process of branching merging and
rebasing your project and what are the
list of commands that are available to
you in get finally I'll take you through
a demo on how you can actually run get
yourself and in this instance use the
software of git against a public service
such as GitHub all right let's talk a
little bit about Version Control Systems
so you may have already been using a
virtual control system within your
environment today you may have used
tools such as Microsoft's team
Foundation services but essentially the
use of a Version Control System allows
people to be able to have files that are
all stored in a single repository so if
you're working on developing a new
program such as a website or an
application you would store all of your
Version Control software in a single
repository now what happens is that if
somebody wants to make changes to the
code they would check out all of the
code in the repository to make the
changes and then there will be an
addendum added to that so um there will
be the version one changes that you had
then the person would then later on
check out that code and then be a
version 2 and added to that code and so
you keep adding on versions of that code
the bottom line is that eventually
you'll have people being able to use
your code and that your code will be
stored in a centralized location however
the charge you're running is that it's
very difficult for large groups to work
simultaneously within a project the
benefits of a VCS system a Version
Control System should demonstrate that
you're able to store multiple versions
of a solution in a single repository now
let's take a step at some of the
challenges that you have with
traditional Version Control Systems and
see how they can be addressed with
distributed Version Control so in a
distributed Version Control environment
what we're looking at is being able to
have the code shared across a team of
developers so if there are two or more
people working on a software package
they need to be able to effectively
share that code amongst themselves so
that they constantly are working taking
on the latest piece of code so a key
part of a distributed Version Control
System that's different to just a
traditional version control system is
that all developers have the entire code
on their local systems and they try and
keep it updated all the time it is the
role of the distributed VCS server to
ensure that each client and we have a
developer here and developer here and
developer here and each of those our
clients have the latest version of the
software and then that each person can
then share the software in a
peer-to-peer like approach so that as
changes are being made into the server
of changes to the code then those
changes are then being redistributed to
all of the development team the tool to
be able to do an effective distributed
VCS environment is get now you may
remember that we actually covered get in
a previous video and we'll reference
that video for you so we start off with
our remote git repository and people are
making updates to the copy of their code
into a local environment that local
environment can be updated manually and
then periodically pushed out to the git
repository so you're always pushing out
the latest code that you've code changes
you've made into the repository and then
from the repository you're able to pull
back the latest updates and so your git
repository becomes the kind of the
center of the universe for you and then
updates are able to be pushed up and
pulled back from there what this allows
you to be able to accomplish is that
each person will always have the latest
version of the code so what is get get
is a distributed Version Control tool
used for source code management so
GitHub is the remote server for that
source code management and your
development team can connect their get
clients to that at some remote Hub
server now git is used to track the
changes of the source code and allows
large teams to work simultaneously with
each other it supports a non-linear
development because of thousands of
parallel branches and has the ability to
handle large projects efficiently so
let's talk a little bit about get versus
GitHub so get is a software tool whereas
GitHub is a service and I'll show you
how those two look in a moment you
install the software tool for get
locally on your system whereas GitHub
because it is a service it's actually
hosted on a website git is actually the
software that used to manage different
versions of source code whereas GitHub
is used to have a copy of the local
repository stored on the service on the
website itself git provides command line
tools that allow you to interact with
your files whereas GitHub has a
graphical interface that allows you to
check in and check out files so let me
just show you the two tools here so here
I am at the git website and this is the
website you would go to to download the
latest version of git and again git is a
software package that you install on
your computer that allows you to be able
to do Version Control in a peer-to-peer
environment for that peer-to-peer
environment to be successful however you
need to be able to store your files in a
server somewhere and typically a lot of
companies will use a service such as git
Hub as a way to be able to store your
files so git can communicate effectively
with GitHub there are actually many
different companies that provide similar
service to GitHub gitlab is another
popular service but you also find that
development tools such as Microsoft
Visual Studio are also incorporating git
commands into their tools so the latest
version of Visual Studio team Services
also provides this same ability but
GitHub it has to be remembered is a
place where we actually store our files
and can very easily create public and
shareable is a place where we can store
our files and create public shareable
projects you can come to GitHub and you
can do a search on projects you can see
at the moment I'm doing a lot of work on
blockchain but you can actually search
on the many hundreds of projects here in
fact I think there's something like over
a hundred thousand projects being
managed on GitHub at the moment that
number is probably actually much larger
than that and so if you are working on a
project I would certainly encourage you
to start at GitHub to see if somebody's
already maybe done a prototype that
they're sharing or they have an open
source project that they want to share
that's already available
um in GitHub certainly if you're doing
anything with um Azure you'll find that
there are thousands 45
000 Azure projects currently being
worked on interestingly enough GitHub
was recently acquired by Microsoft and
Microsoft is fully embracing open source
Technologies so that's essentially the
difference between get and GitHub one is
a piece of software and that's get and
one is a service that supports the
ability of using the software and that's
GitHub so let's dig deeper into the
actual git architecture itself so the
working directory is the folder where
you are currently working on your get
project and we'll do a demo later on
where you can actually see how we can
actually simulate each of these steps so
you start off with your working
directory where you store your files and
then you add your files to a staging
area where you are getting ready to
commit your files back to the main
branch on your git project you want to
push out all your changes to a local
repository after you major changes and
these will commit those files and get
them ready for synchronization with the
service and will then push your services
out to the remote repository an example
of a remote repository would be GitHub
later when you want to update your code
before you write any more code you would
pull the latest changes from the remote
repository so that your copy of your
local software is always the latest
version of the software that the rest of
the team is working on one of the things
that you can do is as you're working on
new features within your project you can
create branches you can merge your
branches with the mainline code you can
do lots of really creative things that
ensure the that Aid the code remains at
very high quality and B that you're able
to seamlessly add in new features
without breaking the core code so let's
step through some of the concepts that
we have available and get so let's talk
about forking and cloning and kit so
both of these terms are quite old terms
when it comes to development but forking
is certainly a term that goes way way
back long before we had distributed CVS
systems such as the ones that we're
using with get to Fork a piece of
software is a particularly open source
project you would take the project and
create a copy of that project and but
then you would then associate a new team
and new people around that project so it
becomes a separate project in entirety a
clone and this is important when it
comes to working with get a clone is
identical with the same teams and same
structuring as the main project itself
so when you download the code you're
downloading exact copy of that code with
all the same security and access rights
as the main code and then you can then
check that code back in and potentially
your code because it is identity could
potentially become the mainline code in
the future now that typically doesn't
happen your changes are the ones that
merge into the main branch but also but
you do have that potential where your
code could become the main code with Git
You can also add collaborators that can
work on the project which is essential
for projects where particularly where
you have large teams and this works
really well when you have product teams
where the teams themselves are
self-empowered you can do a concept
what's called branching in git and so
say for instance you are working on a
new feature that new feature and the
main version of the project have to
still work simultaneously so what you
can do is you can create a branch of
your code so you can actually work on
the new feature whereas the rest of the
team continue to work on the main branch
of the the project itself and then later
you can merge the two together pull from
remote is the concept of being able to
pull in Services software the team's
working on from a remote server and git
rebase is the concept of being able to
take a project and re-establish a new
start from the project so you may be
working a project where there have been
many branches and the team has been
working for quite some time on different
areas and maybe you're kind of losing
control of what the true main branch is
you may choose to rebase your project
and what that means though is that
anybody that's working on a separate
Branch will not be able to Branch their
code back into the mainline Branch so
going through the process of a get
rebase essentially allows you to create
a new start for where you're working on
your project so let's go through forks
and clones so you want to go through the
process so you want to go ahead and Fork
the code that you're working on so this
is a scenario that one of your team
wants to go ahead and add a new change
to the project the team member may say
yeah go ahead and you know create a set
separate Fork of the actual project so
what does that look like so when you
actually go ahead and create a fork of
the repository you actually go and you
can take the version of the mainline
Branch but then you take it completely
offline into a local repository for you
to be able to work from and you can take
the mainline code and you can then work
on a local version of the code separate
from the mainland Branch it's now a
separate Fork collaborators is the
ability to have team members working on
a project together so if you know if
someone is working on a piece of code
and they see some errors in the code
that you've created none of us are
perfect at writing code I know I've
certainly made errors in my code it's
great to have other team members that
have your bag and can come in and check
and see what they can do to improve the
code so to do that you have to then add
them as a collaborator now you do that
in GitHub you can give them permission
within GitHub itself that's really easy
to do super Visual and interface that
allows you to do the work quickly and
easily and depending on the type of
permissions you want to give them
sometimes it can be very limited
permissions it may be just to be able to
read the files sometimes it's being able
to go in and make all the changes you
can go through all the different
permission settings on GitHub to
actually see what you can do but you'll
be able to make changes so that people
can actually have access to your
repository and then you as a team can
then start working together on the same
code let's step through branching and
get so suppose you're working on an
application but you want to add in a new
feature and this is very typical within
a devops environment so to do that you
can create a new branch and build a new
feature on that Branch so here you have
your main application on what's known as
the master branch and then you can then
create a sub branch that runs in
parallel which has your feature you can
then develop your feature and then merge
it back into the master Branch at a
later point in time now now the benefit
you have here is that by default we're
all working on the master Branch so we
always have the latest code the circles
that we have here on the screen show
various different commits that have been
made so we can keep track of the master
branch and then the branches that have
come off which have the new features and
they can be many branches in git so git
keeps you the new features you're
working on in separate branches until
you're ready to merge them back in with
the main branch so let's talk a little
bit about that merge process so you're
starting with the master branch which is
the blue line here and then here we have
a separate parallel branch which has the
new features so if we're to look at this
process the base commit of feature B is
the branch f is what's going to merge
back into the master branch and it has
to be said there can be so many
Divergent branches but eventually you
want have everything merge back into the
master Branch let's step through git
rebase so again we have a similar
situation where we have a branch that's
being worked in parallel to the master
branch and we want to do a get rebase so
we're at stage C and what we've decided
is that we want to reset the project so
that everything from here on out with
along the master branch is the standard
product however this means that any work
that's been done in parallel as a
separate Branch we'll be adding in new
features along this new rebased
environment now the benefit you have by
going through the rebase process is that
you're reducing the amount of storage
space that's required for when you have
so many branches it's a great way to
just reduce your total footprint for
your entire project so get rebase is the
process of combining a sequence of
commits to form a new base commit and
the prime reason for rebasing is to
maintain a linear project history when
you rebase and you unplug a branch and
re-plug it in on the tip of another
branch and usually you do that on the
master branch and that will then become
the new Master Branch the goal of
rebasing is to take all the commits from
a feature branch and put it together in
a single Master Branch it makes it the
project itself much easier to manage
let's talk a little bit about pull from
remote so Suppose there are two
developers working together on
application the concept of having a
remote repository allows the code to the
two developers will be actually then
checking in their code into a remote
repository that becomes a centralized
location for them to be able to store
their code it enables them to stay
updated on the recent changes to the
repository because they'll be able to
pull the latest changes from that remote
repository so that they are ensuring
that as developers are always working on
the latest code so you could pull any
changes that you have made to your fault
remote repository to your local
repository the command to be able to do
that is written here and we'll go
through a demo of how to actually do
that command in a little bit good news
is if there are no changes you'll get a
notification saying that you're already
up to date and if there is a change it
will merge those changes to your local
repository and you get a list of the
changes that have been made remotely so
let's step through some of the commands
that we have in get so get init
initializes a local git repository on
your hard drive get ads one or more
files to your staging area get commit
Dash M commit message is a commit
changes via git command commits changes
to head up to the git command commits
changes to your local staging area get
status checks the status of your current
repository and lists the files you have
changed get Block provides a list of all
the commits made on your current brand
launch get Tiff the user changes that
you've made to the file so you can
actually have files next to each other
you can actually see the differences
between the two files get push origin
Branch name so the name of your branch
command will push the branch to the
remote repository so that others can use
it and this is what you would do at the
end of your project git config Dash
Global username or tailgate Who You Are
by configuring the author name we'll go
through that in a moment git config
Global user email will tell get the
author of by the email ID get clone
creates a get repository copy from a
remote Source get remote ad origin
server connects the local repository to
the remote server and adds the server to
be able to push to it git branch and
then the branch name will create a new
Branch for you to create a new feature
that you may be working on git checkout
and then the branch name will allow you
to switch from one branch to another
brand Branch git merge Branch name Will
merge a branch into the active Branch so
if you're working on a new feature
you're going to merge that into the main
branch a get rebate will reapply commits
on top of another base Tab and get
rebase or reapply commits on top of
another base tip and these are just some
of the popular git commands there are
some more but you can certainly dig into
those as you're working through using
get so let's go ahead and run a demo
using get so now we are going to do a
demo using get on our local machine and
GitHub as the remote repository for this
to work I'm going to be using a couple
of tools first I'll have the deck open
as we've been using up to this point the
second is I'm going to have my terminal
window also available and let me bring
that over so you can actually see this
and the terminal window is actually
running git bash as the software in the
background which you'll need to download
and install you can also run get batch
locally on your Windows computer as well
and in education I'll also have the
GitHub repository that we're using for
simply learn I already set up and ready
to go all right so let's get started so
the first thing we want to do is create
a local repository so let's go ahead and
do exactly that so the local repository
is going to reside in my development
folder that I have on my local computer
and for me to be able to do that I need
to create a drive in that folder so I'm
going to go ahead and change the
directory so I'm actually going to be in
that folder before I actually create
make the new folder so I'm going to go
ahead and change directory
and now I'm in the development directory
I'm going to go ahead create a new
folder
and let's go ahead and created a new
folder called hello world
I'm going to move my cursor so that I'm
actually in the hello world folder
and now that I'm in the hello world
folder I can now initialize this folder
as a get Repository
so I'm going to use the get command init
to initialize and let's go ahead and
initialize that folder so let's see
what's happened so here I have my Hello
wall folder that I've created and you'll
now see that we have a hidden folder in
there which is called dot gate now we
expand that we can actually see all of
the different subfolders that git
repository will create so let's just
move that over a little bit so that we
can see the rest of the work
and now if we check on our folder here
we actually see this is users Matthew
development hello world dot get and that
matches up with hidden folder here
so we're going to go ahead and create a
file called readme.txt in our folder so
here is our hello world folder and I'm
going to go ahead and using my text
editor which happens to be Sublime
I'm going to create a file and it's
going to have in there the text hello
world and I'm going to call this one
readme.txt
if I go to my Hello World folder you'll
see that we have the readme.txt file
actually in the folder what's
interesting is if I select the get
status command what it'll actually show
me is that this file has not yet been
added to the commits yet for this
project so even though the file is
actually in the folder it doesn't mean
that it's actually part of the project
for us to do that we actually have to go
and select
foreign
for us to actually commit the file we
have to go into our terminal window and
we can use the get status to actually
read the files that we have there so
let's go ahead and use the git status
command and it's going to tell us that
this file has not been committed you can
use this with any folder to see which
files and subholders haven't been
committed and what we can now do is we
can go and actually add the readme file
so let's go ahead I'm just going to
select at git add so the git command is
ADD
[Music]
readme.txt so that then adds that file
into our main project and we want to
then commit those files into the main
repositories history and so it's that do
that we'll have the the get command
commit and we'll do a message in that
commit and this one will be
first commit
and it has committed that project what's
interesting is we can now go back into
readme file and I can change this so we
can go hello get
get is a very popular
Version Control solution
and we'll
we'll save that now what we can do is we
can actually go and see if we have made
differences to the readme text so to do
that we'll use the diff command forget
so we do get
div
and it gives us two releases the first
is what the original text was which is
hello world and then what we have
afterwards is what is now the new text
in green which has replaced the original
text
so what we're going to do now is you
want to go ahead and create an account
on GitHub we already have one so what
we're going to do is we're going to
match the account from GitHub with our
local account so to do that we're going
to go ahead and say get config
and we're going to do Dash and it's
going to be a
globaluser.name and we'll put in our
username that we use for GitHub and this
instance we're using the simply learn
Dash
GitHub account name
and under the GitHub account you can go
ahead and create a new repository name
in this instance we called the
repository hello dash world
and what we want to do is connect the
local GitHub account with the remote
hello world.get account and we do that
by using this command from get which is
our remote connection and so let's go
ahead and type that in open this up so
we can see the whole thing so we can
type in git remote add origin https
GitHub
.com slash
simply learn
Dash GitHub and you have to get this
typed in correctly when you're typing in
the location hello dash world dot get
that creates the connection to your
hello world account
and now we want to do is we want to push
the files to the remote location using
the get push command commit git push
origin
master
so we're going to go ahead and connect
to our local remote GitHub so I'm just
going to bring up my terminal window
again and so let's select get remote add
origin
and we'll connect to the remote location
github.com slash
simply learn
Dash GitHub
slash
hello dash world dot get
oh we actually have already connected so
we're connected to that successfully and
now we're going to push the master Gish
so get
push origin
master and everything is connected and
successful
and if we go out to GitHub now
we can actually see that our file was
updated just a few minutes ago
so what we can actually do now is we can
go and Fork a project from GitHub and
clone it locally so we're going to use
the fork tool that's actually available
on GitHub let me show you where that is
located and here is our branching tool
it's actually changed more recently with
a new UI interface
and once complete we'll be able to then
pull a copy of that to our account using
the fox new HTTP URL address
so let's go ahead and do that
so we're going to go ahead and create a
fork of our project now to do that you
would normally go in when you go into
your project you'll see that there are
Fork options in the top right hand
corner of the screen now right now I'm
actually logged in with the default
primary count for this project so I
can't actually Fork a project because
I'm working on the main branch however
if I come in with a separate ID and here
I am I have a different ID and so I'm
actually pretending I'm somebody else I
can actually come in and select the fork
option and create a fork of this project
and this will take just a few seconds to
actually create the fork
and there we are we have gone ahead and
created the fork
so you want to say clone or download
with this and so this is the
I select that actually give me the web
address I can actually show you what
that looks like I'll open up my text
editor
just that's the correct
I guess that is correct so I'm going to
copy that
and I can Fork the project locally and
clone it locally I can change the
directory so I can create a new
directory that I'm going to put my files
in and then post in that content into
that file so I can now actually have
multiple versions of the same code
running on my computer
I can then go into default content and
use the patchwork command
20
so I can create a copy of that code that
we've just created and we call it that's
a clone and we can create a new folder
that we're actually putting the work in
and we could for whatever reason we
wanted to we could call this where
folder Patchwork and that would be maybe
a new feature and then we can then paste
in the URL of the new directory that has
the forked work in it and now at this
point we've now pulled in and created a
clone of the original content
and so this allows us to go ahead and
Fork out all of the work for our project
onto our computer so we can then develop
our work separately
so now what we can actually do is we can
actually create a branch of the fork
that we've actually pulled in onto our
computer so we can actually then create
our own code that runs in that separate
branch
and so we want to check out um the the
branch and then push the origin Branch
down to our computer
this will give us the opportunity to
then add our collaborators so we can
actually then go over to GitHub and we
can actually come in and add in our
collaborators
and we'll do that under settings and
select collaborators and here we can
actually see we have different
collaborators that have been added into
the project and you can actually then
request people to be added via their
GitHub name or by email address
or by their full name
one of the things that you want to be
able to do is ensure that you're always
keeping the code that you're working on
fully up to date by pulling in all the
changes from your collaborators
you can create a new branch and then
make changes and merge it into the
master Branch now to do that you would
create a folder and then that folder in
this instance would be called test we
would then move our cursor into the
folder called test and then initialize
that folder so let's go ahead and do
that so let's call create a new folder
and we're going to first of all change
our root folder and we're going to go to
development
to create a new folder
call it test and we're going to move
into the test folder and we will
initialize
that folder
and we're going to move some files into
that test folder
close one test one
and then we're going to do file save as
and this one's gonna be test
two
and now we're going to commit those
files
Okay add and then we'll use the dot to
pull in all files
and then git commit
m
files
it is
make sure I'm in the right folder here I
don't think I was
and now that I'm in the correct folder
let's go ahead and
and get commit
and it's gone ahead and added those
files and so we can see the two files
that were created have been added into
the master
and we can now go ahead and create a new
Branch we call this one get branch
test underscore
branch
and let's go ahead and create a third
file to go into that folder
this is
file three
into file save as we'll call this one
test three dot text
and we'll go ahead and add
that file I need to get add
test three Dot txt
and we're going to move from the master
Branch to the test run branch
kit
check
out test underscore
branch
I switched to the test branch
and we'll be able to list out all of the
files that are in the branch now
and we want to go through and merge the
files into one area so let's go ahead
and we'll do git merge test underscore
branch
as well we've already updated everything
so that's good so otherwise it would
tell us what we would be merging
and now all the files are merged
successfully into the master branch
there we go all merged together
fantastic
and so what we're going to do now is
move from Master Branch to test branch
so get
check out
test underscore branch
and we can modify the files the test3
file that we took out
and pull that file up
and we can
now what if I
right
and we can then
commit
that file
back
in and we've actually been able to then
commit the file with one changes and now
we've seen as the text free change that
was made
now we can now go through a process of
checking the file back in switching back
to the master branch and ensuring that
everything is in sync correctly
we may at one point want to rebase all
of the workers kind of a hard thing you
want to do but it will allow you to
allow for managing for changes in the
future so let's switch to it back to our
test branch which I think we're actually
on we're going to create two more files
let's go to our folder here and let's go
copy those
and that's created
we'll rename those tests
four
and
five
and so we now have additional files
and we're going to add those into our
branch that we're working on so we're
going to go and select get add Dash a
and we're going to commit those files
get
commit Dash a dash m
adding
two new files
and it's added in the two new files
so we have all of our files now we can
actually list them out and we have all
the files that are in the branch
and we'll switch them to our Master
Branch we want to rebase the master
so we could do git rebase
master
and that will then give us the command
that everything is now completely up to
date
and we can go
get
check out
Master to switch to the master account
this will allow us to then continue
through and rebase the test branch and
then list all the files that are all in
the same area
so let's go get rebase
test underscore
branch
and now we can list and there we have
all of our files listed incorrectly so
if you are interested in taking your
career to the next level look no further
than a postgraduate program in devops
this comprehensive course is designed to
empower you with the skills and
knowledge needed to excel in the dynamic
world of devops this program offers 50
hours of self-paced learning master
classes led by Caltech ctme 20 plus real
life projects in integrated labs and
opportunity to acquire 40 plus in demand
skills and master 15 plus essential
tools top it all off with a Capstone
project spanning three domains and you
will be well on your way to a successful
devops career so don't wait and enroll
now what is maven
if we talk in the literal sense Maven
means accumulator of knowledge Maven is
a very powerful project management tool
or we can call it a build tool that
helps building documenting and managing
a project
but before we move forward and dive deep
into the basics of Maven let's
understand what is meant by the term
build tool
a build tool takes care of everything
for building a project
it generates a source code generates
documentation from a source code it even
compiles the source code and packages
the compiled codes into jar of zip files
along with that the build tool also
installs the packaged code in local
repository server repository or Central
Repository
coming back to Maven it is written in
Java or C sharp and it is based on
Project object model or pom
again let's have a pause and understand
what is meant by this term project
object model
a project object model or pom is a
building block in maven
it is an XML file that contains
information about the project and
configuration details used by Maven to
build a project this file resides in the
base directory of the project as
pom.xml file
the poem contains information about the
project and various configuration
details
it also includes the goals and plugins
used by Maven in a project
Maven looks for the pom in the current
directory while executing a task or a
goal it reads the pom gets the needed
configuration information and then runs
the goal
coming back Mabel is used to building
and managing any Java based project
it simplifies the day-to-day work of
Java developers and helps them in their
projects
now when we know the basics of Maven
let's have a look at some reasons to
know why is Maven so popular and why are
we even talking about it so let's have a
look at the need for maven
Maven as by now we know is properly used
for Java based projects it helps in
downloading libraries or jar files used
in the project
to understand the part of why do we use
Maven or the need of Maven let's have a
look at some problems that may even
solved
the first problem is getting the jar
files in a project getting the right jar
files is a difficult task where there
could be conflicts in the versions of
the two separate packages
however it makes sure all the jar files
are present in its repositories and
avoid any such conflicting scenarios
the next problem it sorted was
downloading dependencies
we needed to visit the official website
of different software which could be a
tedious task
now instead of visiting individual
websites we could visit
mvnrepository.com which is a central
repository of the jar files
then Maven plays a vital role in the
creation of the right project structure
in servlets struts Etc
otherwise it won't be executed
then Maven also helps to build and
deploy the project so that it may work
properly
so the next point is what exactly Maven
does
it makes the building of the project
easy the task of downloading the
dependencies in jar files that were to
be done manually can now be done
automatically all the information that
is required to build the project is
readily available now
finally Maven helps manage all the
processes such as building documenting
releasing and other methods that play an
integral part in managing a project
now when we know everything about Maven
let's look at some companies that use
maven
there are over 2000 companies that use
Maven today
the companies that use Maven are mostly
located in the United States and in the
computer science Industry
Maven is also used in Industries other
than computer science like information
technology and services financial
service banking hospital and care and
much more
some of the biggest corporations that
use Maven are as follows
first we have via varejo then comes
Accenture followed by JPMorgan Chase and
Company then comes craft base and
finally we have red hat
now griddle is in kind of a bell tool
which can be used for the build
automation performance and it can be
used for various programming languages
primarily it's being used for the Java
based applications it's some kind of
build tool which can help you to see
that how exactly automatically you can
prepare the builds you can perform the
automations earlier we used to do the
build activity from the eclipse and we
used to do it manually right but with
the help of this build tool we are going
to do it like automatically without any
manual efforts as such here there are
like a lot of activities which we will
be doing during the build process
primarily there are different activities
like compilations linkage packaging
these are the different tasks which we
perform during the build process so that
we can understand that how the build can
be done and we can perform the
automations uh this uh process also it's
kind of standardized because again if
you want to automate something standards
or a standard process is something which
we require for that before we been going
ahead with that part so that's the
reason why we are getting this well tool
because this build tool helps us to do a
standardization process to see that how
the standards can be met and how we can
proceed further with that part
also it's something which can be used
for variety of languages programming
languages Java is the primary language
for which we use the Gradle but again
other languages like Scala Android CC
plus plus Ruby these are some of the
languages for which we can use the same
tool now it's actually using like it's
referring to as an trophy based domain
specific language rather than XML
because ant and Maven these are the XML
based build tools but this one is not
that uh dependent on XML it's using The
Groovy based domain specific language
DSL language is being used here right
now um again it's something which can be
used to do the build it can further on
be used to perform the test cases
automations also there and then further
on you can deploy to the artifactory
also that okay I want to push the
artificial artifactory so that also that
part also you can get it done over here
so uh primary this tool is known for
doing the build automations for the big
and large projects the projects in which
the source code the amount of source
code and the efforts is more so in that
case is this particular tool makes sense
now griddle includes both the pros of
Maven and ant but it removes the
drawbacks or whatever the issues which
we face during these two build tools so
it's helping us to remove all the cons
which we face during the implementation
of ant and Maven and again again all the
pros of ADD and Maven is implemented
with this cradle tool
now let's see that why exactly this
griddle is used because that's a very
valid question that what is the activity
like what is the reason why we use the
Gradle because
um the first one is that it resolves
issues based on other build tools that's
a primary reason because we all already
having the tools like Maven and and
which is available there but primary
this griddle rule is something which is
removing all the issues which we are
facing with the implementation of other
tools so these issues are getting uh
removed as such second one is that it
focuses on maintainability performance
and flexibility so it's giving the focus
on that how exactly we can manage the
big large projects and uh we can have
flexibility that what different kind of
approaches I want to build today I want
to build in different ways tomorrow the
source code modifies gets added up so I
have the flexibility that I can change
this build script so I can perform the
automations so a lot of flexibility is
available which is being supported by
this tool and then the last one is like
it provides a lot of features a lot of
plugins now this is one of the benefit
which we get in the case of Maven also
that we get a lot of features but again
when we talk about cradle then it
provides a lot of plugins like let's say
that normally in a build process we do
the compilation of the source code but
sometimes let's say that we want to
build an angular or a node.js
application now in that case we may be
involved in running some command line
executions some command line commands
just to make sure that yes we are
running the commands and we are getting
the output so there are a lot of
features which we can use like uh there
are a lot of plugins which is available
there and we will be using those plugins
in order to go ahead and in order to
execute those builds process and doing
the automations now let's talk about the
cradle and Maven because again when we
talk about mav1 like it was like
something which was primary used for the
Java but again when we are talking about
cradle so again it's just uh being used
primarily for the Java here but what is
the reason that we prefer Gradle over
the create me one so what are the
different reason for that let's talk
about that part because this is very
important we need to understand that
what is the reason that Gradle is
preferred as a better tool for the Java
as compared to Maven when we talk about
for the build automation here
now the first one is that the Gradle
using The Groovy DSL language domain
specific language whereas the maven is
considered as in project management tool
which is uh creating the palms or XML
format files so it's being used for the
Java project but XML format is being
used here and on the other hand griddle
is something which is not using the XML
formats and whatever the build scripts
you are creating that is something which
is there in the groupie based DSL
language and on the other hand in the
Palm we have to create the xmls
dependencies whatever the attributes you
are putting up in the May one that's
something which is available there in
the format of XML the overall goal of
the griddle is to add functionality to a
project whereas the goal of the maven is
to you know to complete a project phase
like to work on different different
project phase like compilation test
executions uh then uh packaging so then
deploying to artifactory so these are
all different phases which is available
there into the maven but on the other
hand griddle is all about about adding
the functionality that how you want to
have some particular features added up
into the build scripts in griddle there
are like we usually specify that what
are the different tasks we want to
manage so different different tasks we
can add up into the case of Gradle and
we can override those tasks also in case
of Maven it's all about the different
phases which has been happening over
here and it's in sequence manner so
these phases happens in the sequence
order that how exactly you can build up
the sequence there but in case of
griddle you can have your own tasks
custom tasks also and you can disrupt
the sequence and you can see that how
the different steps can be executed in a
different order so Maven is something
which is a phase mechanism there but
Gradle is something which is according
to the features or the flexibilities now
griddle works on the tasks whatever the
task you want to perform you uh it works
directly on those tasks there on the
other hand uh Maven is something does
not have any kind of inbuilt cache so
every time you're running the build so
separate uh things or the the plugins
and all this information gets loaded up
which takes definitely a lot of time on
the other hand gradually something which
is uh using its own internal cache so
that it can make the bills a little bit
faster because it's not something which
is doing the things from the scratch
whatever the uh things is already being
available in the cache so it's just pick
that part and from there it will proceed
further on the build Automation and
that's the reason why cradle performance
is much faster as compared to Maven
because it uses some kind of a cache in
there and then helps to improve the
overall performance now let's talk about
the Gradle installation because this is
a very important aspect to be done
because when we are doing the
installation we have to download the
Cradle executables right so let's see
that what are the different steps is
involved in the process of the Gradle
installation
so when we talk about the Gradle
installation so there are primary four
steps which is available the very first
one is that you have to check if the
Java is installed now if the Java is not
installed so you can go to the open jdk
or you can go for the Oracle Java so you
can do the installation of the jdk on
your system so jdk8 is something you can
most commonly use nowadays so you can
install that once the Java is downloaded
and installed then you have to do the
Cradle uh download cradle there now once
the Gradle binaries are executable uh or
the user file gets downloaded so you can
add the environment variables and then
you can validate if the critical
installation is working fine as expected
not so we will be doing the Gradle
installation into our local systems and
uh into the windows platform and we'll
see that how exactly we can go for the
installation of cradle and we'll see
that what are the different version we
are going to install here so let's go
back to the system and see that how we
can go for the Gradle installation so
this is the website of the jdk of a Java
Oracle Java now here you have different
jdk so from there you can do whatever
the option you want to select you can go
with that so jdk8 is something which is
most commonly used nowadays like it's
most comfortable or compatible version
which is available so in case you want
to see that if the jdk is installed into
your system all you have to do is that
you have to just say like Java hyphen
version and that will give you the uh
put it whether the Java is installed
into your system or not so in case my
system the Java is installed but if you
really want to do the installation you
have to download the jdk installer from
this website from this Oracle website
and then you can proceed further on that
part now once the jdk is installed so
you have to go for the Cradle
installation because Gradle is something
the which will be performing at the
build automations and all that stuff so
you have to download the bindies like uh
the zip file probably in which we have
the executables and all and then we have
to have have some particular environment
variables configured so that we will be
able to have the System modified over
there so right now we have got like the
pre-requests as in Java version
installed now the next thing is that we
have to install or download the
executables so uh in order to download
the latest Gradle distribution so you
have to click on this one right now over
here there are different options like uh
you want to go for 6.7 now it's there
having like binary only or complete
we'll go for the binary only is because
we don't want to have the source we just
want the binaries and the executables
now it's getting downloaded it's around
close to 100 MB of the installer which
is there
now we have to just extract into a
directory and then the same path we need
to configure into the environment
variable so that in that way we will be
able to see that how the uh Gradle
executables will be running and it will
give the complete output to us over here
in this case so it may take some time
and once the particular modifications
and the download is done then we have to
extract it and once the extraction is
done so we will be able to go back and
have some particular version or have the
configurations established over there so
that let's just wait for some time and
then we will be continuing with the
environment variables like this one so
once the installation and the extraction
is done now we just have to go to the
downloads where this one is downloaded
we have to extract it now extraction is
required so that we can have the setup
like we can set up this path into our
environment variables and once the path
is configured and established we will be
able to start further on that part on
the execution so meanwhile these the
files are getting started let's see so
we already the folder structure over
here and we will see like we will give
this path here there is two environment
variables we have to configure one is
the Gradle underscore home and one is
the
um in the path variable so we'll copy
this path here so meanwhile this is
getting a
started we can save our time and we can
go to the environment variable so we can
right click on this one properties
in there we have to go for the advanced
systems settings
then environment variables
now here we have to give it like cradle
underscore home now in this one we will
not be going giving it till the bin
directory so that only needs to be there
where the griddle is extracted so we'll
say okay
and then we have to go for the path
variable where we will be adding up a
new entry in this one we will be putting
up till the pin directory here because
the Cradle executables should be there
when I'm running the Gradle command so
these two variables I have to configure
then okay okay
and okay
so this one is done so now you have to
just open the command prompt and see
that whether the execution or the
commands which you're running is is
completely successful or not so
meanwhile it's extracting all the
executables and all those things it will
help us to understand that how the whole
build process or how the build tools can
be integrated over there now once the
extraction is done so you have to run
like CMD
Java iPhone version to check the version
of the Java and then the Gradle
underscore version is what you're going
to see check the version of the Cradle
which is installed and now you can see
that it saves that 6.7 version is being
installed over here in this case so
that's the way that how we are going to
have the Cradle installation performed
into our particular system so let's go
back to the content let's talk about the
Cradle Core Concepts here now in this
one we are going to talk about what are
the different Core Concepts of cradle
are all about the very first one is the
projects here now a project represents a
item to be performed over here to be
done like deploying an application to a
staging environment performing some
build so Gradle is something which is
required uh the projects
um the Gradle project which you prepare
is not having multiple tasks which is
available there which is configured and
all these tasks all these different
tasks needs to be executed into a
sequence now sequences again is a very
important part because again if the
sequence is not meant properly then the
execution will not be done in a proper
order so that's the very important
aspect here
tasks is the one in which is a kind of
an identity in which we will be
performing a series of steps these tasks
may be like compilation of a source code
preparing a jar file preparing a web
application archive file or ER file also
we can have like in some tasks we can
even publish our artifacts to the
artifact we so that we can store those
artifacts into a shared location so
there are different ways in which we can
have this uh particular tasks executed
now build scripts is the one in which we
will be storing all this information
what are the dependencies what are the
different tasks we want to refer it's
all going to be present in the
build.gradle file there build.gradle
file will be having the information
related to what are the different
dependencies you want to download and
you want to store there so all these
things will be a part of the build
Scripts
now let's talk about the features of
cradle what are the different features
which we can use in case of cradle here
there are different like type of
features which is available there so
let's talk about them one by one so the
very first one over here is the high
performance then high performance is
something which we can see that we
already discussed that in case you are
using a large projects so Gradle is
something which is in better approach as
compared to Maven because of the high
performance which we are getting it uses
an internal cache which makes sure that
you are using like you are doing the
builds faster and that can give you a
higher performance over there second one
is the support it provides the support
so it yes definitely provides a lot of
support on how you can perform the belts
and it's being the latest tool which is
available there so the support is also
quite good in terms of how you want to
prepare the build how you want to
download the plugins different plugin
supports and the dependencies uh
information also there next one is
multi-project build software so using
this one you can have multiple projects
in case in your repository you have
multiple projects say so all of them can
be easily built up with the help of this
part particular tool so it supports
multiple project to be built up using
the same Gradle project and Gradle
scripts so that support is also
available with this cradle build tool
incremental builds are also something
which you can do with the help of Gradle
so if you have done only the incremental
changes and you want to perform only the
incremental build so that can also be
possible with the help of a griddle here
the build scans so we can also perform
the build scans so we can use some
Integrations with sonar Cube and all
where we can have the scans down to the
source code on understand on how the
build happens or how the source code
really happens on there so that code
scan or the build scans can also be
performed with this one and then it's a
familiarity with Java so for Java it's
something which is consideration by
default not even Java in fact Android
which is also using the Java programming
language is using the particular Gradle
over here so that the build can be done
and it can gain uh benefits out of that
so in in all the manners in all the
different ways it's basically helping us
to see that how we can make sure that
this tool can help us in providing a lot
of features and that can help us to make
a reliable build tool for our Java based
projects or any other programming based
project here right now let's see that
how we can invert a Java project with a
griddle here and for that we have to go
back and readily something which is
already installed we just have to create
a directory where we can have like how
we can perform some executions we can
prepare some build scripts and we can
have a particular execution of a Gradle
build happened over there so let's go
back to the machine okay so we are going
to open the terminal here and we'll see
that how we can create it so first of
all I have to create a directory
structure let's say that we'll say like
Gradle
hyphen project now once the project is
created so we can go inside this
directory so to create some critical
related projects and preparing the files
now uh in this one we'll let's first
create a particular one so we will be
saying like VI
build dot Gradle so in this one we are
going to put like a two plugins we are
going to use so we are going to select
apply
frogging
Java and then we are going to say like
apply
for login
application
so these two plugins we are going to use
and when we got this file over here in
this one so it shows like build.gradle
which is available there in this case so
two these files are available now if you
want to learn like you know what are the
different tasks so you can run like
Grill tasks command over there so
griddle task will help you know that
what are the different tasks which is
available over here by processing the
build scripts and all so um this will
definitely help you to understand on
giving you the output so here all the
different tasks are being given and it
will help you to understand that what
are the different tasks you can
configure and you can work over here
just like jar files clean and all that
stuff build compile then uh in it is
there then all these different uh
executions assemble then Java doc then
build then check test all these
different tasks are there and if you
really want to run the Gradle build so
you can run like Gradle clean to perform
the clean activity because right now you
are doing like alpha build so before
that you can have a clean and then you
can run a specific command or you can
run The Griddle clean build which will
perform the cleanup also and it will at
the same time will have the build
process also performed over there so
build and clean up both will be executed
over here and what is the status whether
it's the success or the failure that
will be given back to you now in this
case in the previous one if you see that
when you run the clean the Cradle clean
it was only running one task but when
you go for the build process when you
run the Gradle clean build it's going to
give you a much more information in fact
you can also give me further information
like you can have the hyphen iPhone Info
flag also there so that if you want to
get the details about the uh different
uh tasks which we which is being
executed over here so that also you're
going to get over here in this one so
you just have to put like hyphen iPhone
Info and then all these steps will be
given back to you that how these tasks
will be executed and the response will
be there so that's the way that how you
can create a pretty much simple
straightforward project in form of
Gradle which can definitely help you to
run some couple of creative commands and
then you can understand that what are
the basic commands you can run and how
the configurations really works on there
right let's go back to the main content
right now let's move on to the next one
so in the next one we are going to see
that how we can prepare a griddle build
project in case of eclipse now we are
now using the local system we are not
directly creating the folders and the
files here we are actually using the
eclipse for performing the creating a
new Gradle project over here so let's
move on that part okay so now the
eclipse is open and I have opened in
this one the very first thing is that we
have to do the Gradle plugin
installation so that we can create new
projects on cradle and then we have to
configure the path that's how the Gradle
plugin can be configured on the previous
uh preferences and all that stuff and
then we will be doing the build process
so the very first thing is that we have
to go to the eclipse Marketplace
in there we have to search for griddle
so once the search is done
it will show us the plugins related to
cable so we have to go for build ship
Gradle integration so we'll click on the
install
it will proceed with installation it
will download it in some cases maybe
it's part of the eclipse as in uh in the
ID so you can go to the install Tab and
you can see that also that if this
plugin is already installed or not but
in this case we are installing it and uh
once the installation is done we just
have to restart the uh specific uh once
we have to restart this Eclipse so that
the changes can be reflected
so it's downloading
it's downloading the Cradle here and
once that is installed we will be able
to use it over here in this case in this
scenario so we have to just wait for
that part so still downloading the jar
files so once the jar file is done it's
now over the areas and download it so
after that we will be able to proceed
further on that download aspect so it's
going to take some time to download it
and once it's done we will be able to
proceed further now once the progress is
done so it's asking us for the restart
now so uh before that uh we just have to
click on restart now and then the
eclipse will be restarted all together
again here so you can do it manually or
you can go for that options you just
require a restart so that the new
changes can be reflected over here so
the plugins can be activated and can be
referenced here now we have to just uh
put up like the you know the
configuration where we can have the
system so we can go for the Gradle
configuration so we can go for Windows
and then preferences
now in this case we have to go for the
uh for the ones in which the Cradle
option is available there so cradle is
what we are going to select now user
home the Gradle user home is what we
need to use right so you want to go for
the Gradle you want to go for local
installation so so all these options you
can use you can if if you go for the
griddle wrapper then it will be
downloading the crater locally and it is
going to use the Cradle W or griddlew
dot bat file but if you already have an
installation locally so you can prefer
that also right now in the previous demo
we have already got the Gradle uh
extracted so we just have to go for the
downloads in the downloads already
Gradle is available so we are going to
select that part here so this is what we
are going to select
right so this represents that this is
the directory structure in which we are
having the mechanism so you can either
go for the build scan so you can select
the build scan also so once this is
enabled then all the projects will be
scanned and will be you know published
and uh it's in kind of an additional
option which is available if you really
want to disable it you can disable it
also and you can go with this
configuration also so uh this is where
the particular Gradle folder is being
put over here in this case and then we
have to just click on apply
and we just have to click on apply and
close so with this one the particular
execution is done now we will be going
for the project creation so you can
right click over here or you can go to
the file also so here we are going to go
for the job project and in this we are
going to have a Gradle project so Gradle
project is what we are going to create
here
and next
so we are going to say like cradle
project
and then next
so once that is done so finish
so uh with this one when you create the
project so what will happen that uh
automatically there will be a folder
structure will be available there right
and uh there are some uh Gradle scripts
which will also be created there so we
will be doing the modifications there
and we'll see that how the uh particular
Gradle build script looks like and how
we can we will be adding some couple of
uh selenium related dependencies and
we'll see that how we can have more and
more dependencies added and what will be
the impact of those dependencies on the
overall project so that also it's very
important aspect to be considered so let
this processing be happen over there
just creating and uh some plugins and
binaries are getting installed and
getting downloaded so we'll see that
once the project is uh imported
completely executed over here and got
created we can extract that now if you
see here the particular option is
available about the Gradle tasks so you
can extract it also and you will be able
to know that what are the different
tasks which is available there let's see
that in the build they are running like
build these are the different tasks
which is happening inside the build
process so Gradle executions will be
also available over here in this case
and greater tasks will be different it
will be represented over here in this
one so you just have to extract on the
Gradle project okay so this is the
library which is available now uh what
happens that you will be able to have
like settings.gradle in this one you
will be able to have like okay Gradle
hyphen project is something which is
available there in this one so that's
what you're being aren't referring then
we have over here as in these folder
structures which is created like Source
main Java this is the one source test
Java is the one which is available as an
on the folder structure and so test
resources are also available here so the
main source main resources are also
available now in this case what happens
that these are the dependencies project
and external these are the different
dependencies are available there so
let's see let's add a dependency over
here in this one in the Bell dot uh
griddle script and see that how we can
do that if we open build.gradle file so
you can see that these dependencies are
there like test implementation junit is
available there right and then we have
implementations of this one which is
available now these jar files which you
put up it it will automatically be added
up as in part of this one as in part of
the particular dependencies over here
and which means that you don't have to
store them as a now within the
repository and automatically they can be
happened over there so let's open a
dependency page so we will be going to
MN repository where we will be opening a
dependency link so this is the
dependency link here so selenium iPhone
Java is available and it can give you
the dependency for all the different
options now we have for Maven this is
the one and for Kettle this is the one
here so we have to just copy this one
and we have to use it as independency so
this is a group and this is the name and
the version which we are using here now
we have copied this one so we will go
back to the Eclipse so here we have to
just put that dependency
and we have to just save it so uh this
is something which is providing like
selenium defenses which is available so
now we have to just refresh the project
so right click over here then you will
be able to see the options in the Cradle
saying that refresh gradual project now
once the moment you do that so you will
be able to do like for the first time
maybe it will take some time to download
all the dependencies which is related to
selenium but after that you will be able
to select the dependencies will be
simply added up over here in this case
so you can see that all the selenium
related dependencies are added up for
any reason if you comment these ones
and you say like
synchronize again
so you will see that all the
dependencies which you are adding up
from this selenium represent uh from the
selenium perspective will be gone back
again so this is the way that how you
can keep on adding the dependencies
which is required for preparing your
bill for your source code and from there
you will be able to proceed further on
the execution part so that's the best
part about this cradle here so that's
the way that how we are going to prepare
a Gradle project within the eclipse and
now you can keep on adding like the
source code in this one and that's the
way that how the code base will be added
up over here
so when we talk about the Gradle
installation so there are primary four
steps which is available the very first
one is that you have to check if the
Java is installed now if the Java is not
installed so you can go to the open jdk
uh or you can go for the Oracle Java so
you can do the installation of the jdk
on your system so jdk8 is something we
can most commonly use nowadays so you
can install that once the Java is
downloaded and installed then you have
to do the Cradle uh download cradle
there now once the Gradle boundaries are
executable uh or the zip file gets
downloaded so you can add the
environment variables and then you can
validate if the Gradle installation is
working fine as expected not so we will
be doing the Gradle installation into
our local systems and uh into the
windows platform and we'll see that how
exactly we can go for the installation
of cradle and we'll see that what are
the different version we are going to
install here so let's go back to the
system and see that how we can go for
the Gradle installation so this is the
website of the jdk of a Java Oracle Java
now here you have different jdk so from
there you can do whatever the option you
want to select you can go with that so
jdk8 is something which is most commonly
used nowadays like it's most comfortable
or compatible version which is available
so in case you want to see that if the
jdk is installed into your system all
you have to do is that you have to just
say like Java hyphen version and that
will give you the output at whether the
Java is installed into your system or
not so in case my system the Java is
installed but if you really want to do
the installation you have to download
the jdk installer from this website from
this Oracle website and then you can
proceed further on that part now once
the jdk is installed so you have to go
for the Cradle installation because
Gradle is something the which will be
performing at the build automations and
all that stuff so you have to download
the bindies like the zip file probably
in which we have the executables and all
and then we have to have have some
particular environment variables
configured so that we will be able to
have the System modified over there so
right now we have got like the
prerequests as in Java version installed
now the next thing is that we have to
install or download the executables so
uh in order to download the latest
Gradle distribution so you have to click
on this one right now over here there
are different options like you want to
go for 6.7 now it's they're having like
binary only is all complete we'll go for
the binary only is because we don't want
to have the source we just want the
binaries and the executables now it's
getting downloaded it's around close to
100 MB of the installer which is there
now we have to just extract into a
directory and then the same path we need
to configure into the environment
variable so that in that way we will be
able to see that how the uh Gradle
executables will be running and it will
give the uh complete output to us over
here in this case so it may take some
time and once the particular
modifications and the download is done
then we have to extract it and once the
extraction is done so we will be able to
go back and have some particular version
or have the configurations established
over there so then let's just wait for
some time and then we will be continuing
with the environment variables like this
one so once the installation and the
extraction is done now we just have to
go to the downloads where this one is
downloaded we have to extract it now
extraction is required so that we can
have the setup like we can set up this
path into our environment variables and
once the path is configured and
established we will be able to start
further on that part on the execution so
meanwhile these the files are getting
started let's see so we already what the
folder structure over here and we will
see like we will give this path here
there is two environment variables we
have to configure one is the Gradle
underscore home and one is the
um in the path variable so we'll copy
this path here so meanwhile this is
getting a
started we can save our time and we can
go to the environment variable so we can
right click on this one properties
in there we have to go for the advanced
systems settings then environment
variables
now here we have to give it like cradle
underscore home now in this one we will
not be going giving it till the bin
directory so that only needs to be there
where the griddle is extracted so we'll
say okay
and then we have to go for the path
variable where we will be adding up a
new entry in this one we will be putting
up till the pin directory here because
the Cradle executable should be there
when I'm running the Gradle command so
these two variables I have to configure
then okay okay
and okay
so this one is done so now you have to
just open the command prompt and see
that whether the execution or the
commands which you're running is is
completely successful or not so
meanwhile it's extracting all the
executables and all those things it will
help us to understand that how the whole
build process or how the build tools can
be integrated over there now once the
extraction is done so you have to run
like CMD
Java iPhone version to check the version
of the Java and then the Gradle
underscore version is what you're going
to see that you see check the version of
the Gradle which is installed and now
you can see that it shows that 6.7
version is being installed over here in
this case so that's the way that how we
are going to have the Cradle
installation performed into our
particular system the first topic that
is the birth of selenium
selenium was primarily created by Jason
Huggins in 2004. Jason an engineer at
partwork was working on a web
application that needed to be tested
frequently he realized that the repeated
manual testing of the application is
becoming inefficient so he created a
JavaScript language that automatically
controlled the browser's actions this
program was named JavaScript test Runner
after he realized that his idea of
automating the web applications has a
lot of potential he made the JavaScript
test Runner open source and it was later
renamed as selenium core
so we know that Jason was a person who
initially created selenium but then we
must also know that selenium is a
collection of different tools and since
there are different tools there will be
several developers too
to be exact about the number of
different tools there are four different
tools that have their own creators
let's have a look at all of them
the first tool is the selenium remote
control or the selenium RC that was
created by Paul Hammond
then comes selenium grid that was
developed by Patrick lightbody
the third tool is this selenium IDE that
was created by shinia kasathani and the
fourth tool that is the selenium
Webdriver was created by Simon Stewart
we shall be learning about all these
tools in great detail As you move forth
in our video
first let's have a look at what is
selenium
selenium is a very popular open source
tool that is used for automating the
test carried on the web browsers there
may be various programming languages
like Java C sharp python Etc that could
be used to create selenium test Scripts
this testing that is done using selenium
tool is referred to as selenium testing
we must understand that selenium allows
the testing of web applications only
we can neither test any computer
software nor any mobile application
using selenium
selenium is composed of several tools
with each having its own specific role
and serving its own testing needs
moving forward let's have a look at the
features of selenium which will help us
understand the reason behind its
widespread popularity
so here we have a set of features of
selenium
selenium is an open source and portable
framework that has a playback and record
feature it is one of the best
cloud-based testing platform and
supports various OS and languages
it can be integrated with several
testing Frameworks and supports parallel
test execution
we will be talking about all these
features in detail as we move forward
so here the first feature that we have
is open source and portable framework
this feature states that selenium is an
open source and portable framework for
testing web applications
in addition to that selenium commands
are categorized in terms of different
classes which make it easier to
understand and implement
the second feature is the playback and
record feature
the feature states that the deaths can
be authorized without learning a test
scripting language with the help of
playback and record features
the next feature says that selenium is a
cloud-based testing platform
selenium is a leading cloud-based
testing platform that allows testers to
record their actions and Export them as
a reusable script with a simple to
understand and easy to use interface
moving forth the next feature states
that selenium supports various operating
systems browsers and programming
languages
the tool supports programming languages
such as c-sharp Java python PHP Ruby
Pearl and JavaScript
if you talk about the operating systems
then selenium supports operating systems
like Android iOS Windows Linux Mac and
Solaris
and the tool also supports various
browsers like Google Chrome Mozilla
Firefox Internet Explorer Edge Opera
Safari Etc
then the next feature we have is the
integration with testing Frameworks
selenium can be well integrated with
testing Frameworks like test NG for
application testing and generating
reports and also selenium can be
integrated with Frameworks like ant and
Maven for source code compilation
the last feature in our list is the
parallel test execution selenium enables
parallel testing which reduces time and
increases the efficiency of the tests
selenium requires fewer resources as
compared to other automation testing
tools
now let's move further and get to know
different selenium tools by now we know
that there are four different tools that
come under the selenium suit
the four tools are selenium remote
control selenium grid selenium IDE and
selenium Webdriver
we shall have a look at these four Tools
in detail one after the another
beginning with selenium remote control
selenium remote control enables the
writing of automated web applications in
languages such as Java c-sharp Perl
Python and PHP to build complex tests
such as reading and writing files
querying a database and emailing the
test results
selenium RC was a sustainable project
for a very long time before selenium
Webdriver came into existence and hence
selenium RC is hardly in use today as
the Webdriver offers more powerful
functionalities
the second tool we shall see is the
selenium grid
selenium grid is a smart proxy server
that enables the running of tests in
parallel on several machines
this is made possible when the commands
are rooted to the remote web browser
instances and one server acts as the Hub
The Hub here is responsible for
conducting several tests on multiple
machines
selenium grid makes cross browser
testing easy as a single test can be
carried on multiple machines and
browsers all together
making it easy to analyze and compare
the results
here there are two main components of
selenium grade Hub and the node
Hub is a server that accepts the access
request from the Webdriver client
routing the Json test commands to the
remote drivers on nodes and here the
node refers to the Remote device that
consists of a native OS and a remote web
driver
it receives the request from the Hub in
the form of a Json test commands and
executes them using the Webdriver moving
for the third tool is the selenium IDE
if you want to begin with this selenium
IDE it needs no additional setup except
installing the extension of your browser
it provides an easy to use tool that
gives instant feedbacks
selenium IDE records multiple locators
for each element it interacts with if
one locator fails during the playback
the others will be tried until one is
successful
the IDE makes the test debugging easier
with features like setting breakpoints
and pausing exceptions
through the use of the Run command you
can reuse one test case inside one
another
and also selenium IDE can be extended
through the use of plugins they can
introduce new commands to the IDE or
integrate with a third-party service
the last and the fourth tool in the
selenium suit is the selenium Webdriver
selenium Webdriver is the most critical
component of the selenium tools suit
that allows cross browser compatibility
testing
the Webdriver supports various operating
systems like Windows Mac Linux Unix Etc
it provides compatibility with a range
of languages including python Java and
Perl along with it it provides supports
different browsers like Chrome Firefox
Opera Safari and Internet Explorer
the selenium Webdriver completes the
execution of test scripts faster when
compared to other tools and also it
provides compatibility with iPhone
driver HTML unidriver and Android driver
now after you know about the tools the
question arises which tool to choose
so in the next topic we shall see
different factors on the basis of which
we may decide which tool would be more
suitable for us here we shall have a
look at the reasons why one should
choose that particular tool we shall
begin with selenium remote control
selenium remote control or selenium RC
should be chosen to design a test using
a more expressive language than Celanese
is a set of selenium commands that are
used to test our web applications
then the selenium RC might be chosen to
run tests against different browsers on
different operating systems the RC may
be chosen to deploy tests across
multiple environments using selenium
grid it helps in testing the
applications against a new browser that
supports JavaScript and web applications
with complex Ajax based scenarios
now the second tool for which we shall
see the reasons are selenium grid
selenium grid as we learned in the
reasons for selenium RC is used to run
selenium RC scripts in multiple browsers
and operating systems simultaneously
the grid is used to run a huge test suit
that needs to be completed at the
earliest possible time now comes the
third tool the third tool is selenium
IDE
Al IDE is used to learn about Concepts
on automated testing and selenium these
Concepts include selenis commands such
as type Open click and weight assert
verify Etc
the concepts also include locators such
as ID name XPath CSS selector
Etc
selenium IDE enables the execution of
customized JavaScript code using run
script and exporting test cases in
several different formats the IDE is
used to create tests with a limited
amount of knowledge in programming and
these test cases and test suits can be
exported later to RC or Webdriver now
finally let's have a look at the reasons
to choose a last tool that is selenium
Webdriver
selenium Webdriver uses a certain
programming language in designing a test
case the Webdriver is used to test
applications that are rich in Ajax based
functionalities execute tests on HTML
unit browser and create customized test
results so if you are interested in
taking your career to the next level
look no further than a postgraduate
program in devops this comprehensive
course is designed to empower you with
the skills and knowledge needed to excel
in the dynamic world of devops this
program offers 50 hours of self-paced
learning master classes led by Caltech
ctme 20 plus real life projects in
integrated labs and opportunity to
acquire 40 plus in demand skills and
master 15 plus essential tools top it
all off with a Capstone project spanning
three domains and you will be well on
your way to a successful devops career
so don't wait and enroll now so this is
what we're going to be covering in this
session we're going to cover what life
is like before using pins and the issues
that Jenkins specifically addresses then
we'll get into what Jenkins is about and
how it applies to continuous integration
and the other continuous integration
tools that you need in your devops team
then specifically we'll Deep dive into
features of Jenkins and the Jenkins
architecture and we'll give you a case
study of a company that's using Jenkins
today to actually transform how their it
organization is operating so let's talk
a little bit about life before Jenkins
let's see this scenario I think it's
something that maybe all of you can
relate to as developers we all write
code and we all submit that code into a
code repository and we all keep working
away writing our unit tests and
hopefully we're running our unit tests
but the problem is that the actual
commits are actually gets sent to the
code repository aren't consistent you as
a developer may be based in India you
may have another developer that's based
in the Philippines and you may have
another team lead that's based in the UK
and another development team that's
based in North America so you're all
working at different times and you have
different amounts of code going into the
code repository there's issues with the
integration and you're kind of running
into a situation that we'd like to call
development how where things just aren't
working out and there's just lots of
delays being added into the project and
the bugs just keep mounting up the
bottom line is the project is delayed
and in the past what we would have to do
is we'd have to wait until the entire
software code was built and tested and
before we could even begin checking for
errors and this just really kind of
increased the amount of problems that
you'd have in your project the actual
process of delivering software was slow
there was no way that you could actually
iterate on your software and you just
ended up with just a big headache with
teams pointing fingers at each other and
blaming each other so let's jump into J
Jenkins and see what Jenkins is and how
it can address these problems so Jenkins
is a product that comes out of the
concept of continuous integration that
you may have heard of as power
developers where you'd have two
developers sitting next to each other
coding against the same piece of
information what they were able to do is
to continuously develop their code and
test their code and move on to new
sections of code Jenkins is a product
that allows you to expand on that
capacity to your entire team so you're
able to submit your codes consistently
into a source code environment so there
are two ways in which you can do
continuous delivery one is through 90
builds and one is through continuous so
the approach that you can look at
continuous delivery is modifying the
Legacy approach to building out
Solutions so what we used to do is we
would wait for nightly builds and the
way that our nightly builds would work
and operate is that as code developers
we would all run and have a cut off time
at the end of the day and that was
consistent around the world that we
would put our codes into a single
repository and at night all of that code
would be run and operated and tested to
see if there were any changes and a new
build would be created that would be
referred to as the nightly build with
continuous integration we're able to go
one step further we're able to not only
commit our changes into our source code
but we can actually do this continuously
there's no need to race and have a team
get all of their code in at arbitrary
time you can actually do a continuous
release because what you're doing is
you're putting your tests and your
verification Services into the build
environment so you're always running
Cycles to test against your code this is
the power that Jenkins provides in
continuous integration so let's dig
deeper into continuous integration so
the concept of continuous integration is
that as a developer you're able to pull
from a repository the code that you're
working on and then you'll be able to
then at any time submit the code that
you're working on into a continuous
integration server and the goal of that
continuous integration server is that it
actually goes ahead and validates and
passes any tests that a tester may have
created now if on the continuous
integration server a test is a pass then
that code gets sent back to the
developer and the developer can then
make their changes it allows the
developer to actually do a couple of
things it allows the developer not to
break the build and we all don't want to
break the builds that are being created
but it also allows the developer not to
actually have to run all the tests
locally on their computer write tests
particularly if you have a large number
of tests can take up a lot of time so if
you can push that service up to another
environment like a continuous
integration server it really improves
the productivity of your developer
what's also good is that if there are
any code errors that have come up that
may be Beyond just the standard CI test
so maybe there's a Code the way that you
write your code isn't consistent those
errors can then be passed on easily from
the tester back to the developer too the
goal from doing all this testing is that
you're able to release and deploy and
your customer is able to get new code
faster and when they get that code it's
simply just works
so let's talk a little bit about some of
the tools that you may have in your
continuous integration environment so
the cool thing with working with
continuous integration tools is that
they are all open source at least the
ones that we have listed here are open
source there are some that are private
but typically you'll get started with
open source tools and it gives you the
opportunity to understand how you can
accelerate your environment quickly so
bamboo is a continuous integration tool
that specifically runs multiple builds
in parallel for faster compilation so if
you have multiple versions of your
software that runs on multiple platforms
this is a tool that really allows you to
get that up and running super fast so
that your teams can actually test how
those different builds would work for
different environments and this has
integration with and Maven and other
similar tools so one of the tools you're
going to need is a tool that allows you
to automate the software build test and
release process and buildbot is that
open source product for you again it's
an open source tool so there's no
license associated with this so you can
actually go in and you can actually get
the environment up and running and you
can then test for and build your
environment and create your releases
very quickly so buildbot's also written
in Python and it does support parallel
execution jobs across multiple platforms
if you're working specifically on Java
projects that need to be built and test
then Apache Gump is the tool for you it
makes all of those projects really easy
it makes all the Java projects easier
for you to be able to test with API
level and functionality level testing so
one of the popular places to actually
store code and create a versioning of
your code is GitHub and it's a service
that's available on the web just
recently acquired by Microsoft if you
are storing your projects in GitHub then
you'll be able to use Travis continuous
integration or Travis CI and it's a tool
design specifically for hosted GitHub
projects and so finally we're covering
Jenkins and Jenkins is a central tool
for automation for all of your projects
now when you're working with Jenkins
sometimes you'll find there's
documentation that refers to a product
called Hudson Hudson is actually the
original version of the product that
finally became Jenkins and it was
acquired by Oracle when that acquisition
happened the team behind
um Hudson was a little concerned about
the direction that Oracle May
potentially go with Hudson and so they
created a hard Fork of Hudson that they
renamed Jenkins and Jenkins has now
become that open source project it is
one of the most popular and continuously
contributed projects that's available as
open source so you're always getting new
features being added to it it's a tool
that really becomes the center for your
CI environment so let's jump into some
of those really great features that are
available available in Jenkins so
Jenkins itself is really comprised of
five key areas around easy installation
easy configuration plugins extensibility
and distribution so as I mentioned for
the easy installation Jenkins is a
self-contained Java program and that
allows it to run on most popular
operating systems including Windows Mac
OS and Unix you can even run it on Linux
it really isn't too bad to set up it
used to be much harder than it is today
the setup process has really improved
the web interface makes it really easy
for you to check for any errors in
addition you have great built-in help
one of the things that makes tools like
Jenkins really powerful for developers
and continuous integration teams in your
devops teams as a whole when you have
plugins that you can then add in to
extend the base functionality of the
product Jenkins has hundreds of plugins
and you can go and visit the update
Center and see which other plugins that
would be good for your devops
environment certainly checking out
there's just lots of stuff out there in
addition to the plugin architecture
Jenkins is also extremely extensible the
opportunity for you to be able to
configure Jenkins to fit in your
environment it's almost endless now it's
really important to remember that you
are extending Jenkins not creating a
custom version of Jenkins and that's a
great differentiation because the core
Foundation remains as the court Jenkins
product the extensibility can then be
continued with newer releases of Jenkins
so you're always having the latest
version of Jenkins and your extensions
mature with those core Foundation the
distribution and the nature of Jenkins
makes it really easy for you to be able
to have it available across your entire
network it really will become the center
of your CI environment and it's
certainly one of the easier tools and
more effective tools for devops so let's
jump into the standard Jenkins pipeline
so when you're doing development you
start off and you're coding away on your
computer the first thing you have to do
when you're working in the Jenkins
pipeline is to actually commit your code
now as a developer this is something
that you're already doing or at least
you should be doing you're committing
your code to a git server or to an SVN
server or a similar type of service so
in this instance you'll be using Jenkins
as the place for you to commit your code
Jenkins will then create a build of your
code and part of that build process is
actually going through and running
through tests and again as a developer
you're already comfortable with running
unit tests and writing those tests to
validate your code but there may be
additional tests that Jenkins is running
so for instance as a team you may have a
standard set of tests for how you
actually write out your code so that
each team member can understand the code
that's been written and those tests can
also be included in the testing process
within the Jenkins environment assuming
everything past the the test you can
then get everything placed in a stage
and release ready environment within
Jenkins and finally getting ready to
deploy or deliver your code to a
production environment Jenkins is going
to be the tool that helps you with your
server environment to be able to deploy
your code to the production environment
and the result is that you're able to
move from a developer to production code
really quickly this whole process can be
automated rather than having to wait for
people to actually test your codes or
going through a nightly build you're
looking at being able to commit your
code and go through this testing process
and release process continuously as an
example companies Etsy will release up
to 50 different versions of their
website every single day
so let's talk about the architecture
within Jenkins it allows you to be so
effective at applying a continuous
delivery devops environment so the
server architecture really is broken up
into two sections so on the left-hand
side of section you have the code the
developers are doing and submitting that
code to a source code repository and
then from then Jenkins is your
continuous integration server and it
will then pull any code that's been sent
to the source code repository and we'll
run tests against it it'll use a build
service such as MAV into actually then
build the code and then every single
stage that we have that Jenkins manages
there are constant tests so for instance
if a build fails that it feedback is
sent right back to the developers so
that they can then change their code so
that the build environment can run
effectively the final stage is to
actually execute specific test scripts
and these test scripts can be written in
selenium amp so it's probably good to
mention here that both mavin and
selenium are plugins that run in the
Jenkins environment so before we were
talking about how Jenkins can be
extended with plugins mavin and selenium
are just two very popular examples of
how you can extend the Jenkins
environment the goal to go through this
whole process again it's an automated
process is to get your code from the
developer to the production server as
quickly as possible have it fully tested
and have no errors so it's probably
important at this point to mention uh
one piece of information around the
Jenkins environment that if you have
different code builds that need to be
managed and distributed this will
require that you need to have multiple
builds being managed Jenkins itself
doesn't allow for multiple files and
builds to be executed on a single server
you need to have a multiple server
environment with running different
versions of Jenkins for that to be able
to happen so let's talk a little bit
about the Master Slave architecture
within Jenkins so what we have here is
an overview of the Master Slave
architecture within Jenkins on the left
hand side is the remote source code
repository and that remote source code
repository could be GitHub or it could
be a team Foundation services or the new
Azure devops code repository or it could
be your own git repository the Jenkins
server acts as the master environment on
the left hand side and that Master
environment can then push out to
multiple other Jenkins slave
environments to distribute the workload
so it allows you to run multiple builds
and tests and production environments
simultaneously across your entire
architecture so Jenkins slaves can be
running the different build versions of
the code for different operating systems
and the server Master is controlling how
each of those builds operate so let's
step into a quick story of a company
that has used Jenkins very successfully
so here's a use case scenario um over
the last 10 or 15 years there has been a
significant shift within the automotive
industry where manufacturers have
shifted from creating complex Hardware
to actually creating software we've seen
that with companies such as Tesla where
they are creating a software to manage
their cars we see the same thing with
companies such as General Motors with
their OnStar program and Ford just
recently have rebranded themselves as a
technology company rather than just a
automotive company what this means
though is that the software within these
cars is becoming more complex and
requires more testing to allow more
capabilities and enhancements to be
added to the core software so Bosch is a
company that specifically ran into this
problem and their challenge was that
they wanted to be able to streamline the
increase increasingly complex Automotive
software by adopting continuous
integration and continuous delivery best
practices with the goal of being able to
delight and exceed the customer
expectations of the end user so Bosch
has actually used Cloud bees which is
the Enterprise Jenkins environment so to
be able to reduce the number of manual
steps such as building deploying and
testing Bosch has introduced the use of
cloud bees from Jenkins and this is part
of the Enterprise Jenkins platform it
has significantly helped improve the
efficiencies throughout the whole
software development cycle from
automation stability and transparency
because Jenkins becomes a self-auditing
environment now the results have been
tangible previously it took three days
before a build process could be done and
now it's taken that same three-day
process and reduced it to less than
three hours that is significant
large-scale deployments are now kept on
track and have expert support and there
is clear visibility and transparency
across the whole operations through
using the Jenkins tools now what is the
purpose of Jenkins here now Jenkins is
normally a kind of a CI tool which we
use for performing the build automations
and the test cases automation there it's
one of the open source tool which is
available there and one of the most
popular CI tool also available into
their Market
now this tool makes it easier for the
developers to integrate the changes to
the project here so we can easily
integrate the changes and whatever the
modifications we want to manage we will
be able to do that with the help of
Jenkins
now Jenkins also achieves The Continuous
integration with the help of couple of
plugins each and every tool which you
want to integrate have its own plugins
which is available there for example you
want to integrate Maven we have a maven
plugin in Jenkins which you can install
you can configure in that case you will
be able to use the maven there now you
can deploy the mavener to build tool
onto the Jenkins server and then you can
prepare or you can configure any number
of Maven jobs in case of Jenkins
so uh what exactly the maven or the
Jenkins really do is the mavenwen
integrates with Jenkins through the
particular plugin so you can able to
automate the pills because for
automation the build you require some
integration with the maven and that
integration is what we are getting from
the maven plugin so in Jenkins you have
to install the maven plugin and once the
plugin is installed so what you can do
is that you can proceed with the
configurations you can proceed with the
setup and this a particular plugin can
help you to build out some of the Java
based projects which is available there
in the git repositories and once that is
done you will be able to go ahead and
you will be able to process a complete
integration of Maven within Jenkins
right so let's see that how we can go
for the integration now I have already
installed the maven onto the Linux
virtual machine which we are using so
using the app utility or using the Yum
utility you can actually download the
Jenkins package and the maven package
onto the server onto the virtual machine
and now I'm going to proceed further
with the plugin installation and the
configuration of a maven project so I
have a GitHub repository which is having
a maven project Maven uh source code and
the mavenized test cases over there so
let's see let's log into the uh Jenkins
and see that how it works
so this is the Jenkins interface which
we have over here now in this one what
we can do is that we can create some
Maven jobs over here and once those jobs
are created we will be able to do a
custom build onto of these Jenkins so
first of all we have to install the uh
particular plugin here
for that we have to go to the manage
Jenkins in manage Jenkins you have the
manage plugins option there so you have
to click on that now here you will be
having different tabs like updates
available installed Advanced all these
different tabs are available there so
what you can do is that you can click on
the available one when you go to the
available tab so what will happen that
here you can actually put up that what
exactly uh plugin you want to fetch here
so I can put a plugin called maven
now you can see that the very first one
the maven integration tool is available
so I'm going to select that particular
plugin and click on download now and
install after restart
now once that is done so what will
happen that the plugin will be
downloaded but in order to reflect the
changes we have to do a couple of
restart now for that you don't have to
go to the virtual machine you have the
option here itself that will allow you
to do the restart over here when you
click on this button so you check this
option and say that restart Jenkins when
the installation is done so what will
happen that the installation will be
automatically attempted whenever the
particular plugin installation is
completed here so you just have to
refresh the page again and you will be
able to see that the particular Jenkins
is being processed as such here
right so you can see that the screen is
coming up that Jenkins is restarting so
it will take a couple of five to six
seconds to do the restart and the login
screen to come up again over there
you can do the refresh also if you feel
automatically it will be reloaded once
the Jenkins is ready but sometimes we
have to refresh it so that we can get
the screen over there
so once the login is done so my Maven
integration is done so next thing which
I will be doing is that I will be
creating a maven related project so I'm
going to put the admin user
and the password so whatever the user
and password you have created you are
going to put that so that you will be
able to log into the Jenkins portal now
this is the Jenkins which is available
here so all you have to do is that you
have to click on create a new job or new
item so both the option is pretty much
same only
so here you will be able to see a maven
project here so I'm going to select like
maven build that's the name which I'm
going to give here and the maven project
I'm going to select here
and then press ok
now here you will be providing the first
of all the repository from which you
will be checking out the source code now
I can have a discard old builds over
here so if I feel that I want to have
like low rotation so all the previous
builds should be deleted so I'm just
saying that dates to keep a build should
be 10 over here and the number of bills
which I need to keep over here is 20.
you can adjust these settings according
to your requirement but uh over here we
are you know doing a kind of
configurations which we are trying to do
a lot of configurations and settings
here so these are the particular
settings which we are looking forward as
such over here so now we are going to
have the log rotation here so we can
have it like how many days we want to
keep and how many number of bills we
want to keep here so both the values we
are providing over here and then now I'm
going to put the get uh integration here
like the repo URL so I have this
repository here in which I have the Java
source code and some uh particular uh
genuine test cases and all I also have
the particular source code and it's kind
of a maven project so that's what I'm
trying to clone over here with the help
of this plugin so this plugin will
download this repository it will clone
it onto the Jenkins server and then
depending on our integration with May 1
the maven build will be triggered here
so now I'm going to process with the uh
Maven here so you can see here that it's
saying that uh Jenkins needs to know
that where the maven is installed
because that Maven version it needs to
configure it needs to process on that
part so I'll just do the save over here
and or I can click on this tool
configuration so I'll just save or do
the apply click on this tool
configuration here
now here you have the options like where
you can have the jdk installation but
what happens that thing Jenkins is
running there so jdk is automatically
installed so in the tools configuration
you don't have to put the jdk
configuration but at least for the maven
configurations you have to provide that
where exactly the maven is available
there so I'm just saying that maven
three I want to process and the latest
Maven Apache web server I want to
configure here so I just want to have
like I just want to save this settings
so that it will be automatically
download the latest version Apache 3.6.3
version there and that same should be
utilized over here in this case now I'm
just going to the maven Builder
configuration here and click on the
configure part so these git repositories
available here and in the build step it
automatically builds up that uh what a
maven environment you want to select so
you see that previously since I did not
configure my Maven environment so it was
throwing an error but once I have
configured that I have to download it
during the build process or before the
build that utility should be downloaded
so instead of doing the physical
installation of Maven on the server what
I have chosen over here is that I have
selected the particular version like I
have selected that a particular
3.6.3 version should be installed for
the maven purposes over here now once
the that is done I'm going to put the
particular
steps over here you can have it like
clean install you can have clean compile
test clean test or test alone you can
give it's just a part of the setup or
the goals which you want to configure
here it by default says that pom.xml
file is the current one in the current
directory you need to refer you need to
pick on that one what it's up to you
only that how you want to configure and
how you want to process as such this
information so according to your
requirement you can say that okay I'm
just want to go for these particular
goals and you can say like save over
here the particular configuration will
be saved now you can just click on the
build now and you will be able to see
that the first of all the git clone will
happen and then the desired Mi 1
executable will be uh the build tool
will be configured and according to that
it will be processed here so you can see
here that uh the maven is uh getting
downloaded it's getting configured here
and once it's configured because I have
explained over there that 3.6.3 version
I have to select so that specific
version will be configured and will be
picked up over here now even if you
don't have the maven installed on the
physical machine on which the Jenkins is
running still you will be able to do the
processing using this particular
component here so you can see here that
we have some particular test cases
executed and in the end we are able to
get a particular artifacts also there
since I did not call upon the package or
install goal that's the reason why the
particular artifacts was not generated
War file or jar file whatever the
packaging mode is available at Palm
level but still what happens that my
test cases do gets executed and that's
what I have got over here in this case
so this is a kind of a mechanism where
we feel that how we can configure a git
repository once the git repository is
configured you are going to integrate
the maven plugin in the maven plugin you
are going to configure in the tools
configuration that this and so and so
version I want to configure to run my
build and once that is done after that
you just have to trigger the build and
click on the bill now option and once
that is done you will be able to get a
particular full-fledged build or
compilation happened onto the Jenkins
and this log will give you the complete
details that what are the different
steps which has happened on this one so
let's take a slow scenario of a
developer and a tester before you had
the world of Docker a developer would
actually build the code and then they
send it to the tester but then the code
wouldn't work on their system encoders
will work on the other system due to the
differences in computer environments so
what could be the solution to this well
you could go ahead and create a virtual
machine to be the same of the solution
in both areas I think Docker is an even
better solution so let's kind of break
out what the main big differences are
between Docker and virtual machines as
you can see between the left and the
right hand side both look to be very
similar what you'll see however is that
and on the docker side what you'll see
as a big difference is that the guest OS
for each container has been illuminated
Docker is inherently more lightweight
but provides the same functionality as a
virtual machine so let's step through
some of the pros and cons of a virtual
machine versus Docker so first of all a
virtual machine occupies a lot more
memory space on the host machine in
contrast Docker occupies significantly
less memory space the boot up time
between both is very different Docker
just boots up faster the performance of
the docker environment is actually
better and more consistent than the
virtual machine Docker is also very easy
to set up and very easy to scale the
efficiencies therefore are much higher
with a Docker environment versus a
virtual machine environment and you'll
find it is easier to Port Docker across
multiple platforms than a virtual
machine finally the space allocation
between Docker and a virtual machine is
significant when you don't have to
include the guest OS you're eliminating
a significant amount of space and the
dock environment is just inherently
smaller so after Docker as a developer
you can build out your solution and send
it to a tester as long as we're all
running in the docker environment
everything will work just great so let's
step through what we're going to cover
in this presentation we're going to look
at the devops tools and where Docker
fits within that space we'll examine
what Docker actually is and how Docker
works and then finally we'll step
through the different components of the
docker environment so what is devops
devops is a collaboration between the
development team the operation team
allowing you to continuously deliver
Solutions and applications and services
that both delayed and improve the
efficiency of your customers if you look
at the Venn diagram that we have here on
the left hand side we have development
on the right hand side we have operation
and then there's a crossover in the
middle and that's where the devops team
sits if we look at the areas of
integration between both groups
developers are really interested in
planning code building and testing and
operations want to be able to
efficiently deploy operate a monitor
when you can have both groups
interacting with each other on these
seven key and elements then you can have
the efficiencies of an excellent devops
team so planning and code base we use
tools like jit and gearer for building
we use Gradle and Maven testing we use
selenium the integration between Dev and
Ops is through tools such as Jenkins and
then the deployment operation is done
with tools such as docker and Chef
finally nagis is used to monitor the
entire environment so let's step deeper
into what Docker actually is so Docker
is a tool which is used to automate the
deployment of applications in a
lightweight container so the application
can work efficiently in different
environments now it's important to note
that the container is actually a
software package that consists of all
the dependencies required to run the
application so multiple containers can
run on the same Hardware the containers
are maintained in isolated environments
they're highly productive and they're
quick and easy to configure so let's
take an example of what Docker is by
using a house that may be rented for
someone using Airbnb so in the house
there are three rooms and only one
cupboard and kitchen and the problem we
have is that none of the guests are
really ready to share the cupboard and
kitchen because every individual has a
different reference when it comes to how
the cupboard should be stocked and how
the kitchen should be used this is very
similar to how we run software
applications today each of the
applications could end up using
different Frameworks so you may have a
framework such as rails perfect and
flask and you may want to have them
running for different applications for
different situations this is where
Docker will help you run the
applications with the suitable
Frameworks so let's go back to our
Airbnb example so we have three rooms
and a kitchen and cupboard how do we
resolve this issue well we put a kitchen
and cupboard in each room we can do the
same thing for computers Docker provides
the suitable Frameworks for each
different application and since every
application has a framework with a
suitable version this space could also
then be utilized for putting in software
and applications that are long and since
every application has its own framework
and suitable version the area that we
had previously stored for a framework
can be used for something else now we
can create a new application and this
instance a fourth application that uses
its own resources you know what with
these kinds of abilities to be able to
free up space on the computer it's no
wonder Docker is the right choice so
let's take a closer look to how Docker
actually works so when we look at Docker
and we call something Docker we're
actually referring to the base engine
which actually is installed on the host
machine that has all the different
components that run your Docker
environment and if we look at the image
on the left hand side of the screen
you'll see that Docker has a client
server relationship there's a client
installed on the hardware there is a
client that contains the docker product
and then there is a server which
controls how that Docker client is
created the communication that goes back
and forth to be able to share the
knowledge on that Docker client
relationship is done through a rest API
this is fantastic news because that
means that you can actually interface
and program that API so we look here in
the animation we see that the docker
client is constantly communicating back
to the server information about the
infrastructure and it's using this rest
API as that Communication channel the
docker server then will check out the
requests and the interaction necessary
for it to be the docker Daemon which
runs on the server itself well then
check out the interaction and the
necessary operating system pieces needed
to be able to run the container okay so
that's just an overview of the docker
engine which is probably where you're
going to spend most of your time but
there are some other components that
form the infrastructure for Docker let's
dig into those a little bit deeper as
well so what we're going to do now is
break out the four main components that
comprise of the docker environment the
four components are as follows the
docker client and server which we've
already done a deeper dive on Docker
images Docker containers and the docker
registry so if we look at the structure
that we have here on the left hand side
you see the relationship between the
docker client and the docker server and
then we have the rest API in between now
if we start digging into that rest API
particularly the relationship with the
docker Daemon on the server we actually
have our other elements that form the
different components of the docker
ecosystem so the docker client is
accessed from your terminal window so if
you are using Windows this can be
Powershell on Mac it's going to be your
terminal window and it allows you to run
the docker Daemon and the registry
service when you have your terminal
window open so you can actually use your
terminal window to create instructions
on how to build and run your Docker
images and containers if we look at the
images part of our registry here we
actually see that the image is really
just a template with the instructions
used for creating the containers which
you use within Docker the docker image
is built using a file called the docker
file and then once you've created that
Docker file you'll store that image in
the docker Hub or registry and that
allows other people to be able to access
the same structure of a Docker
environment that you've created the
syntax of creating the image is fairly
simple it's something that you'll be
able to get your arms around very
quickly and essentially what you're
doing is you're creating the option of a
new container you're identifying what
the image will look like what are the
commands that are needed and the
arguments for within those commands and
once you've done that you have a
definition for what your image will look
like so if we look here at what the
container itself looks like is that the
container is a standalone executable
package which includes applications and
their depend agencies it's the
instructions for what your environment
will look like so you can be consistent
in how that environment is shared
between multiple developers testing
units and other people within your
devops team now the thing that's great
about working with Docker is that it's
so lightweight that you can actually run
multiple Docker containers in the same
infrastructure and share the same
operating system this is its strength it
allows you to be able to create those
multiple environments that you need for
multiple projects that you're working on
interestingly though within each
container that container creates an
isolated area for the applications to
run so while you can run multiple
containers in an infrastructure each of
those containers are completely isolated
they're protected so that you can
actually control how your Solutions work
there now as a team you may start off
with one or two developers on your team
but when a project starts becoming more
important and you start adding in more
people to your team you you may have 15
people that are offshore you may have 10
people that are local you may have 15
Consultants that are working on your
project you have a need for each of
those developers or each person on your
team to have access to that Docker image
and to get access to that image we use a
Docker registry which is an Open Source
server-side service for hosting and
distributing the images that you have to
find you can also use Docker itself as
its own default Retreat and Docker Hub
now something that has to be bear in
mind though is that for publicly shared
images you may want to have your own
private images in which case you would
do that through your own registry so
once again public repositories can be
used to host the docker images which can
be accessed by anyone and I really
encourage you to go out to Docker and
see the other Docker images that have
been created because there may be tools
there that you can use to speed up your
own development environments now you
will also get to a point where you you
start creating environments that are
very specific to the solutions that you
are building and when you get to that
point you'll likely want to create a
private repository so you're not sharing
that knowledge with the world in general
now the way in which you connect with
the docker registry is through simple
pull and push commands that you run
through terminal window to be able to
get the latest information so if you
want to be able to build your own
container what you'll start doing is
using the pull commands to actually pull
the image from the docker repository and
the command line for that is fairly
simple in terminal window you would
write Docker pull and then you put in
the image name and any tags associated
with that image and use the command
pause so in your terminal window you
would actually use a simple line of
command once you've actually connected
to your Docker environment and that
command will be Docker pull with the
image name and any Associated tags
around that image what that will then do
is pull the image from the docker
repository whether that's a public
repository or a private one now in
Reverse if you want to be able to update
the docker image with new information
you do a push command where you take the
script that you've written about the
docker container that you define and
push it to the repository and as you can
imagine the commands for that are also
fairly simple in terminal window you
would write Docker push the image name
any Associated tags and then that would
then push that image to the docker
repository again either a public or a
private repository so if we recap the
docker file creates a Docker image
that's using the build commands Docker
image then contains all the information
necessary for you to be able to execute
the project using the docker image any
user can run the code in order to create
a Docker container and once a Docker
image is built it's uploaded to a
registry or to a Docker Hub where it can
be shared across your entire team and
from the docker Hub users can get access
to the docker image and build their own
new containers so let's have a look at
what we have in our current environment
so today when you actually have your
standard machine you have the
infrastructure you have the host
operating system and you have your
applications and then when you create a
virtual environment what you're actually
doing is you're actually creating
virtual machines but those virtual
machines actually are now sitting within
a hypervisor solution that sits still on
top of your host operating system and
infrastructure and with a Docker engine
what we're able to do is we're able to
actually reduce significantly the
different elements that you would
normally have within a virtualized
environment so we're able to get rid of
the the bins and the so we're able to
get rid of the guest OS and we're able
to eliminate the hypervisor environment
now this is really important as we
actually start working and creating
environments that are consistent because
we want to be able to make it so it's
really easy and stable for the
environment that you have within your
Dev and Ops environment now critical is
getting rid of that hypervisor element
it's just a lot of overhead so let's
have a look at a container as an example
so here we actually have a couple of
examples on the right hand side we have
different containers we have one
containers running Apache Tomcat in a
with Java a second container is running
SQL server and microsoft.net environment
the third container is running python
with mySQL these are all running just
fine within the docker engine and
sitting on top of a host OS which could
be Linux it really could be any host OS
within a consistent infrastructure and
you're able to have a solution that can
be shared easily amongst your teams so
let's have a look at an example that
you'd have today if a company is doing a
traditional Java application so you have
your developers working in JBoss on his
system and he's coding away and he has
to get that code over to test and now
what will happen is that tester will
then typically in your traditional
environment then have to install JBoss
on their machine and get everything
running and cool and hopefully set up
identically to the developer chances are
they probably won't have it exactly the
same but they're trying to get it as
close as possible and then at some point
you want to be able to test this within
your production environment so you send
it over to a system administrator who
would then also have to install JBoss on
their environment as well yeah this just
seems to be a whole lot of duplication
so why go through the problem of
installing JBoss three times and this is
where it things get really interesting
because the challenge you have today is
that it's very difficult to almost
impossible to have identical
environments if you're just installing
software locally on devices the
developers probably got a whole bunch of
development software they could be
conflicting with the JBoss environment
the tester has similar testing software
but probably doesn't have all the
development software and certainly the
system administrator won't have all the
tools that the developer and tester have
their own tools and so what you want to
be able to do is kind of get away from
The Challenge you have of having to do
local installations on three different
computers and in addition what you see
is that this uses up a lot of effort
because when you're having to install
software over and over again you just
keep repeating doing really basic
foundational challenges so this is where
Docker comes in and Docker is the tool
that allows you to be able to share
environments from one group to another
group without having to install software
locally on device you install all of the
code into your Docker container and
simply share the container so in this
presentation we're going to go through a
few things we're going to cover what
Docker actually is and then we're going
to dig into the actual architecture of
Docker and kind of go through what
Docker container is and how to create a
Docker container and then we'll go at
through the benefits of using Docker
containers and then the command and
finalize everything out with a brief
demo so what is darker so darker is as
you'd expect because all the software
that we cover in this series is an open
source solution and it is a container
solution that allows you to be able to
containerize all of the necessary files
and applications needed to run the
solution you're building so you can
share it from different people in your
team whether it's a developer a tester
or system administrator and this allows
you to have a consistent environment
from one group to the next so let's kind
of dig into the architecture so you
understand why Docker runs effectively
so the docker architecture itself is
built up of two key elements there is
the docker client and then there is a
rest API connection to a Docker Daemon
which actually hosts the entire
environment within the docker host and
the docker demo you have your different
containers and each one has a link to to
a Docker registry the docker client
itself is a rest service so as you'd
expect a rest API and that sends command
line to the docker Daemon through a
terminal window or command line
interface window and we'll go through
some of these demos later on so you can
actually see how you can actually
interact with Docker the docker Dem then
checks the request against
um the docking components and then
performs the service that you're
requesting now the docker image itself
all it really is a collection of
instructions used to create a container
and again this is consistent with all
the devops tools that we have the devops
tools that we're looking to use
throughout this series of videos are all
environments that can be scripted and
this is really important because it
allows you to be able to duplicate and
scale the environments that you want to
be able to build very quickly and
effectively the actual container itself
has all of the applications and the
dependencies of those applications in
one package you can trying to think of
it as a really effective and efficient
zip file it's a little bit more than
that but it's one file that actually has
everything you need to be able to run
all of your Solutions the actual Docker
registry itself is an environment for
being able to host and distribute
different Docker images among your team
so say for instance you had a team of
developers that were working on multiple
different solutions so say you have a
team of developers and you have 50
developers and they're working on five
different applications you can actually
have the applications themselves the
containers shared in the docker registry
so each of those teams at any time check
out and have the latest container of
that latest image of the code that
you're working on so let's dig into what
actually is in a container so the
important part of a docking container is
that it has everything you need to be
able to run the application it's like a
virtualized environment it has all your
Frameworks and you your libraries and it
allows the teams to be able to build out
and run exactly the right environment
that the developer intended what's
interesting though is the actual
applications then will run in isolation
so they're not impacting other
applications that using dependencies on
other libraries or files outside of the
container because of the architecture it
really uses a lot less space and because
it's using less space it's a much more
lightweight architecture so the files
and the actual folder itself is much
smaller it's very secure highly portable
and the boot up time is incredibly fast
so let's actually get into how you would
actually create a Docker container so
the docker container itself is actually
built through command line and it's
built over file and Docker image so the
actual Docker file is a text file that
contains all the instructions that you
would need to be able to create that
docker image and then we'll actually
then create all of the project code with
inside of that image then the image
becomes the item that you would share
through the docker registry you would
then use the command line and we'll do
this later on select Docker run and then
the name of the image to be able to
easily and effectively run that image
locally and again once you've created
the document you can store that in the
docker registry making it available to
anybody within your network so something
to bear in mind is that Docker itself
has its own registry called Docker Hub
and that is a public restrict so you can
actually go out and see other Docker
images that have been created and access
those images as your own company you may
want to have your own private repository
so you want to be able to go ahead and
either do that locally through your own
repository or you can actually get a
licensed version of Docker Hardware you
can actually then share those files now
something that's also very interesting
to know is that you can can have
multiple versions of a Docker image so
if you have a different version control
different release versions and you want
to be able to test and write code for
those different release versions because
you may have different setups you can
certainly do that within your Docker
registry environment okay so let's go
ahead and we're going to create a Docker
image using some of our basic Docker
commands and so there are essentially
really you know kind of just two
commands that you're going to be looking
for one is the build command another one
is to actually put it into your registry
which is a push command so if you want
to get a image from a Docker registry
then you want to use the pull command
and a pull command simply pulls the
image from the registry and in this
example using nginx as our registry and
we can actually then pull the image down
to our test environment on our local
machine so we're actually running the
container within our Docker application
on a local machine we're able to then
have the image run exactly as it would
production and then you can actually use
the Run command to actually use the
docker image on your local machine so
just a you know a few interesting
tidbits about the docking container once
the container is created a new layer is
formed on top of the document layer is
called the container layout each
container has a separate read write
container layer and any changes made in
that docking container is then reflected
upon that particular container layout
and if you want to delete the container
layer the container layer also gets
deleted as well so you know why with
using Docker and containers be of
benefit to you well you know some of the
things that are useful is that
containers have no external dependencies
for the applications they run once you
actually have the container running
locally it has everything it needs to be
able to run the application so there's
no having to install additional pieces
of software such as the example we gave
with JBoss at the beginning of the
presentation now the containers are
really lightweight so it makes it very
easy easy to share the containers
amongst your teams whether it's a
developer whether it's a tester whether
it's somebody on your operations
environment it's really easy to share
those containers amongst your entire
team different data volumes can be
easily reused and shared among multiple
containers and again this is another big
benefit and this is a reflection of the
lightweight nature of your containers
the container itself also runs in
isolation which means that it is not
impacted by any dependencies you may
have on your own local environment so
it's a completely sandboxed environment
so some of the questions you might ask
us you know can you run multiple
containers together without the need to
start each one individually and you know
what yes you can with Docker compose
docking compose allows you to run
multiple containers in a single service
and again this is a reflection on the
lightweight nature of containers within
the docker environment so we're going to
end our presentation by looking at some
of the basic commands that you'd have
within Docker so we have here on the
left hand side we have a Docker
container and then the command for each
item we're actually going to go ahead
and use some of these commands and the
demo that we're going to do after this
presentation you'll see that in a moment
but just you know some of the basic
commands we have are committing the
docker image into the Container kill is
a you know standard kill command to you
know terminate one or more of the
running containers so they stop working
then restart those containers but
suddenly you can look at all the image
all of the commands here and try them
out for yourself so we're going to go
ahead and start a demo of how to use the
basic commands to run Docker so to do
this we're going to open up terminal
window or command line now depending
whether you're running Linux PC or Mac
and we're going to go ahead and the
first thing we want to do is see what
our Docker image lists are so we can go
sudo Docker images and this will give us
well first we'll enter in our password
so let's go enter that in and this will
now give us a list of our Docker images
and here are the dock images I have
already been created in the system and
we can actually go ahead and actually
see the processes that are actually
running so I'm going to go ahead and
open up this window a little bit more
but this will show you the actual
processes and the containers that we
actually have and so on the far left
hand side you'll see under names we have
learn simply learn be unscore cool these
are all just different ones that we've
been working on so let's go ahead and
create a Docker image so I'm going to do
sudo
docker
run
Dash D Dash p
0.0.0.0 go on 80 colon 80
Ubuntu and this will allow us to go
ahead and run an Ubuntu image and this
will run the latest image and what we
have here is a hash number and this hash
number is a unique name that defines the
container that we've just created and we
can go ahead and we can check to make
sure that the container actually is
present so we're going to do
pseudo.co.ps and this actually show us
down there so it's not in a running
state right now but that doesn't mean
that we don't have it so let's list out
all the containers that are both running
and in the exit state so let's do sucker
PS Dash a and this lists all the
containers that I have running on my
machine
and this shows all the ones that have
been in the running State and in the
exit State and here we see one that we
just created about a minute ago and it's
called learn
and these are all running Ubuntu and
this is the one that we had created just
a few seconds ago
let's open it up and
there we go so let's change that to that
new Doc container to a running state so
scroll down and we're going to type sudo
docker
run
Dash it Dash Dash
name my
um so this is going to be the new
container name it's going to be my
Docker so this is how we name our Docker
environment
and we'll put in the image name which is
Ubuntu
and dash bin Dash Bash
and it's now in our root and we'll exit
out of that
so now we're going to go ahead and start
the new my Docker container so sudo
docker
start
my
and we'll get the container image which
will be my docker
my docker
return and that started that Docker
image and let's go ahead and check
against the other running Docker images
to make sure it's running correctly so
sudo docker
PS
and there we are underneath name on the
right hand side you have to see my
Docker along with the other Docker
images that we created and it's been
running for 13 seconds
quite fast so we want to rename the
container let's use the command sudo
docker
rename we can take another Docker image
this to scrub this one and we'll put it
in rename
and we'll rename and put in the old
container name which is image and then
we'll put in the new container name and
let's call it
purple
so now the container image that had
previously been called image is now
called Purple
so do sudo Docker PS
to list all of our Docker images
and if we scroll up and there there we
go purple
how easy is that to rename an image
and we can go ahead and use this command
if we want to stop container so we're
going to write sudo docker
stop
and then we'll have to put in the
container name
and we'll put in my Docker the container
that we originally created
and that image has now stopped
and let's go ahead and prove that we're
going to list out all the docker images
and what you see is that it's not listed
in the active images it's uh not on the
list on the far right hand side
but if we go ahead and we can list out
all of the docker images so you actually
see it's still there as an image it's
just not in an active State just what's
known as in an exit state
so here we go
and there's my Docker it's in an exited
state so that happened 27 seconds ago
so if we want to to remove a container
we can use the following command
so sudo docker
RM
for remove
my docker and that will remove it from
the exited state
and we're going to go ahead and we're
going to double check that
and yep
yep it's not not listed there under exit
State anymore
it's gone
and there we go
there that's where it used to be all
right let's go back
so if we want to exit a container in the
running state so we do sudo kill and
then the name of the container
I think one of them is called yellow
let's just check and see if that's going
to kill it
oops no I guess we don't have one called
yellow so let's find out name of
container that we actually have
so sudo Docker kill oh we're going to
list out of the ones that are running oh
okay there we go now yellow isn't in
that list so let's take I know let's
take simply learn and so we can actually
go ahead and let's write sudo Docker
kill simply learn
and that will actually kill an active
Docker container
boom there we go
and we list out all the active
containers you can actually see now
that's they simply learn container is
not active anymore
and these are all the basic commands for
Docker container
so if you are interested in taking your
career to the next level look no further
than a postgraduate program in devops
this comprehensive course is designed to
empower you with the skills and
knowledge needed to excel in the dynamic
world of devops this program offers 50
hours of self-paced learning master
classes led by Caltech ctme 20 plus real
life projects in integrated labs and
opportunity to acquire 40 plus in-demand
skills and master 15 plus essential
tools top it all off with a Capstone
project spanning three domains and you
will be well on your way to a successful
devops career so don't wait and enroll
now
the field of devops has grown
exponentially over the past few years
and it's no surprise why devops is a set
of practices and tools that aims to
break down the silos between development
and operations team and streamline the
software delivery process by doing so
devops enables organizations to deliver
high quality software faster and more
efficiently than ever before
then this video on how to choose devops
as a career is for you with the proper
roadmap on how to get started with it
also do not forget to subscribe to our
YouTube channel and hit the Bell icon to
never miss an update from Simply learn
so without any further Ado let's get
started first is to understand the role
of a devops professional before diving
into the specifics of how to become a
devops professional it's essential to
understand the roles and
responsibilities of a devops
professional a devops professional is
typically responsible for Designing
implementing and maintaining the
infrastructure automation tools and
processes required for the efficient
delivery of software this includes
collaborating with development teams to
integrate automation into the software
development process setting up and
maintaining the infrastructure required
for the software delivery pipeline such
as servers databases and network systems
developing and implementing automation
scripts to improve the efficiency of
software delivery process
continuously monitoring and analyzing
the software delivery process to
identify and resolve bottlenecks and
improve efficiency
ensuring that the software delivery
process adheres to Industry best
practices and regulatory requirements so
now that you understand the roles and
responsibilities of a devops
professional let's take a a look at the
road map to become a devops professional
the first is to learn the basics of
software development to Be an Effective
devops professional it's essential to
have a good understanding of software
development processes including
programming languages software
development methodologies and Version
Control tools
you can start by learning programming
languages like python Java or Ruby well
a devops engineer course will prepare
you for a career in devops Technologies
through this devops engineer course you
will learn to review deployment
methodologies CI CD pipelines
observability and use devops tools like
gate Docker and Jenkins with this devops
engineer certification you can check out
the link for this course in the
description following that learn about
infrastructure and operations as a
devops professional you will be
responsible for managing infrastructure
including servers databases and network
system so it's essential to have a good
understanding of infrastructure and
operations this include learning about
servers operating systems Network
protocols and storage systems
then explore about automation tools and
cloud services
automation is a critical part of devops
learning automation tools such as
ansible puppet or shirt can help you
streamline the software delivery process
and improve efficiency
most organizations today use cloud
services to deliver software learning
cloud services such as AWS Azure or gcp
can help you design and Implement Cloud
architecture that are efficient and
scalable
well to help you with learning devops
and cloud services simply learn has to
offer Azure devops solution expert
master's program to help you become an
industry ready professional
in this course you will learn to plan
smarter collaborate better and ship
faster with a set of modern development
services the link for this course is
mentioned in the description do check
them out
once done with Skilling you need to gain
experience and get certified as
discussed before the best way to gain
experience in devops is to work on real
world projects this can be achieved
through internships volunteering for
open source projects or even taking on
small projects on your own this will
allow you to put your theoretical
knowledge into practice and gain
hands-on experience
along with it there are various
certification programs available such as
the devops Institute certification
program AWS certification devops
engineer and Microsoft Azure devops
Solutions certification once you have
gained experience and acquired the
necessary skills it's time to start
looking for job opportunities
various job titles in devops such as
devops engineer site reliability
engineer automation engineer and release
engineer in top hiring companies in the
field include Amazon Google Microsoft
IBM and many others well a postgraduate
program in devops in collaboration with
Caltech ctme is a professional
development option that will Square your
skills with industry standards in this
course you will learn how to formalize
and document development processes and
create a self-documenting system devops
certification course will also cover
Advanced tools like puppet soft stack
and ansible that will help
self-governance and automated management
at scale link is mentioned in the
description do check it out so if you
are interested in taking your career to
the next level look no further than a
postgraduate program in devops
comprehensive course is designed
design and knowledge needed to excel in
the dynamic world of devops this program
offers 50 hours of self-paced learning
master classes led by Caltech ctme 20
plus real life projects in integrated
labs and opportunity to acquire 40 plus
in-demand skills and master 15 plus
essential tools top it all off with a
Capstone project spanning three domains
and you will be well on your way to a
successful devops career so don't wait
and enroll now now let's understand what
is devops devops is like a teamwork
approach for making computer programs
better and faster it combines the work
of software developers and operation
team the goal is to help them work
together and use tools that speed up the
process and make fewer mistakes they
also keep an eye on the programs and fix
problems early This Way businesses can
release programs faster with few errors
and everyone works better together if
you want to learn more about this then
check post graduate program in devops to
understand from the basics two advanced
concepts this postgraduate program in
devops is crafted in partnership with
Caltech ctme this comprehensive course
aligns your expertise with industry
benchmarks experience or Innovative
Blended learning merging live online
devops certification sessions with
immersive labs for practical Mastery
Advance your career with Hands-On
training that meets industry demands
alright now let's move on to the first
question of devops interview question
which is what do you know about devops
so think of a devops like teamwork in
the IIT world it's become really
important because it helps teams work
together smoothly to make computer
programs faster and with fewer mistakes
imagine a soccer team everyone works
together to win the game devops is
similar it's when computer developers
and operations people team up to make
software better they start working
together from the beginning when they
plan what the software will be like
until they finish and put it out for
people to use this Teamwork Makes sure
things go well and the software works
great so this was about devops now
moving on to the second question which
is how is devops different from agile
methodology devops is like a teamwork
culture where the people who create the
software and the people who make it work
smoothly join forces this helps in
always improving and updating the
software without any big bricks agile is
a way of making softwares that's like
taking small steps towards instead of
big jumps it's about releasing small
parts of the software quickly and
getting feedback from the users this
helps in solving any issues or
differences between what users want and
what developers are making so you can
answer this question in this way all
right moving on to the third question
which is what are the ways in which a
build can be scheduled slash run in
Jenkins so as you can see there are four
ways by source code management commits
second is after the completion of other
builds third is scheduled to run at a
specified time and fourth one is manual
build requests so if the interior asks
then you can answer these four ways in
which a build can be scheduled in
Jenkins all right now the fourth
question is what are the different
phases in devops so the various phases
of devops lifecycle are as follows so as
you can see first is plan so initially
there should be a plan for the type of
application that needs to be developed
getting a rough feature of the
development process is always a good
idea then code the application is coded
as per the end user requirements then
there is build build application by
integrating various codes formed in
previous steps after that there is test
this is the most crucial step of the
application development test the
application and rebuild if necessary
then there is integrate so multiple
codes from different programmers are
integrated into one after integrate
there is deploy so code is deployed into
a cloud environment for further usage it
is ensured that any new changes do not
affect the functioning of a high traffic
website after that there is operate so
operations are performed on the code if
required then there is Monitor
applications performance is monitored
changes are made to meet the end user
requirements so these all were the
different phases of devops and here we
have explained each one of these you can
go through it and if the interviewer
have asked you this question you can
answer it in a similar way all right now
moving on to the next question which is
mention some of the core benefits of
devops so the core benefits of devops
are as follows first of all we'll say
technical benefits first technical
benefit of devops's continuous software
delivery then second is less complex
problems to manage third is early
detection and faster correction of
defects then here comes the business
benefits first benefit of devops that is
business benefit of devops is faster
delivery of features so it allows faster
delivery of features then there is
stable operation environment
then third one is improve communication
and collaboration between the teams so
these were the business benefits so here
we have discussed both technical
benefits and business benefits all right
now the next question is how will you
approach a project that needs to
implement devops so here are the simpler
terms here's how we can bring devops
into a specific project using these
Steps step one would be first we look at
how things are currently done and figure
out where we can make them better this
makes about two to three weeks then we
can plan for what changes to make then
step two would be we create a small test
to show that our plan works when
everyone agrees with it we can start
making the real changes and put the plan
into action then third step would be we
are all set to actually use devops we do
things like keeping track of different
versions of our work putting everything
together testing it and making it
available to the users we also watch how
it's working to make sure everything
goes smoothly by doing these steps right
the keeping track of changes putting
everything together testing and watching
how it's going we are all set to use
devops in our project
so this is how you can approach a
project that needs to implement in
devops and this is how you can answer
this question all right moving on to the
question number seven which is what is
the difference between continuous
delivery and continuous deployment all
right so first would be continuous
delivery so it ensures code can be
safely deployed on production whereas
continuous deployment here every change
that passes the automated test is
deployed to production automatically
then in continuous delivery it ensures
business applications and services
functions as expected whereas in
continuous deployment it makes software
development and the release process
faster and more robust in continuous
delivery delivers every change to a
production like environment through
rigorous and automatic testing whereas
in continuous deployment there is no
explicit approval from a developer and
requires a developed culture of
monitoring so these were the three
points that you you can highlight while
differentiating between continuous
delivery and continuous deployment all
right so this was the question number
seven now moving on to the question
number eight which is name three
security mechanisms of Jenkins uses to
authenticate users so here we have to
name three security mechanisms so first
one would be Jenkins uses an internal
database to store user data and
credentials second is Jenkins can use
the lightweight directory access
protocol that is ldap server to
authenticate users third one is Jenkins
can be configured to employ the
authentication mechanism that the
deployed application server uses so
these were the three mechanisms that
Jenkins uses to authenticate users all
right now moving on to the question
number nine which is how does continuous
monitoring help you maintain the entire
architecture of the system so continuous
monitoring within devops involves the
ongoing identification detection and
reporting of any anomalies or security
risk across the entire system
infrastructure it guarantees the proper
functioning of services applications and
resources on servers by overseeing
server statuses it accesses the accuracy
of application operations this practice
also facilitates uninterpreted audits
transactions to utility and regulated
surveillance
so this was about the question number
nine that was how does continuous
monitoring help you maintain the entire
architecture of the system now moving on
to the question number 10 which is what
is the role of AWS in devops so in the
realm of devops AWS assumes several
rules first one would be adaptable
services so it offers adaptable
pre-configured Services eliminating the
necessity for software installation or
configuration second one is design for
expansion whether managing one instance
or expanding two thousand AWS Services
accommodate seamless scalability then
there is automated operations AWS
empowers task and process automation
freeing up valuable time for inventing
Pursuits the next one is enhanced
security
AWS and entity and accesses management
that is IAM allows precise user
permissions and policy establishment
then the last one is extensive partner
Network so AWS Fosters a vast partner
Network that integrates with and
enhances its service offerings so these
were the role of awfs in the realm of
devops all right now moving on to the
question number 11 name three important
devops kpis the three important kpis are
as follows first one is mean time to
failure recovery this is the average
time taken to recover from a failure
second gapy is deployment frequency the
frequency in which the deployment occurs
is called deployment frequency third one
is percentage of field deployments the
number of times the deployment Fields is
called percentage of field deployments
so these were the three important devops
kpis this question can also be asked all
right moving on to question number 12
which is how is IAC implemented using
AWS
so comments by discussing traditional
methods involving scripting commands
into files followed by testing in
isolated environments prior to
deployment notice how this practice is
giving way to infrastructure as code IAC
comparable to code for various Services
IAC aided by AWS empowers developers to
craft accesses and manage infrastructure
components descriptively utilizing
formats like Json or yaml this Fosters
streamline development and expeditious
implementation of alterations in
infrastructure all right now moving on
to question number 13 which is describe
the branching strategies you have used
they'll ask you this question so to test
our knowledge the purpose of branching
and our experience of branching at a
past job this question is usually asked
so here we will discuss topics that can
help considering this devops interview
question so release branching we can
clone the develop Branch to create a
release branch once it has enough
functionality for a release this Branch
kicks off the next release cycle thus no
new features can be contributed Beyond
this point the things that can be
contributed are documentation generation
bug fixing and other release related
tasks the release is merged into master
and given a version number once it is
ready to ship it should also be merged
back into the development branch which
may have evolved since the initial
release so this was the first one then
there is feature branching this
branching model maintains all
modifications for a specific feature
contained within a branch the branch
gets merged into Master once the feature
has been completely tested and approved
by using tests that are automated then
the third branching is Task branching in
this branching model every task is
implemented in its respective Branch the
task key is mentioned in the branch name
we need to Simply look at the task key
in the branch name to discover which
code which tasks so these were the
branching strategies that you can say
that you have used or if you have really
used it otherwise you can say something
else
all right moving on to question number
14 which is can you explain the shift
left to reduce failure Concept in devops
so shifting left within the devops
framework is a concept aimed to enhance
security for performance and related
aspects to illustrate consider the
entity of diverse processes currently
security valuations occur before the
deployment stage by employing the left
ship approach we can introduce security
measures during the earlier development
phase denoted as the left
this integration spans on multiple
phases encompassing pre-development and
testing not confined to development
alone this holistic integration is
likely to elevate security measures by
identifying vulnerabilities at initial
stage leading to a more fortified
overall process now moving on to the
question number 15 which is what is blue
teen deployment pattern so this approach
involves seamless deployment aimed at
minimizing downtime it entails shifting
traffic from one instance to another
requiring the placement of outdating
code and a new version to integrate
fresh code
the updated version resides in a green
environment while the older one remains
in a blue environment after modifying
the existing version a new instance is
generated from the old one to execute
the updating instance ensuring a
smoother transition so the main focus of
this approach is smooth deployment
all right so this was question number
15. now moving on to the question number
16 which is what is continuous testing
so continuous testing involves the
automated execution of tests within the
software delivery pipeline
offerings immediate insights into
potential business rates within the
latest release
by seamlessly integrating testing into
every stage of the software developer
life cycle
I repeat by seamlessly integrating
testing into the every stage of software
delivery life cycle continuous testing
minimizes issues during transition
phases and empowers development teams
and instant feedback
this approach accelerates developer
efficiency by obtaining the need to
re-run all tests after each update and
project rebuild culminating in notable
gains in speed and productivity so this
was about continuous testing
now moving on to question number 17 what
are the benefits of automation testing
so some of the benefits of automation
testing includes
first it helps to save money and time
second is unattended execution can be
easily done
third one is huge test matrices can be
easily tested the next one is parallel
execution is enabled then there is
reduced human generated errors which
results in improved accuracy last one is
repeated test tasks execution is
supported
so these are the benefits of automation
testing
coming to question number 18 which is
what is a Docker file used for
so a Docker file is used for creating
Docker images using the build command
with a Docker image any user can run the
code to create Docker containers once
Docker image is built it's uploaded in
Docker registry
from the docker registry users can get
the docker image and build new
containers whenever they want so this is
what Docker file is used for
coming to question number 19 which is
what is the process for reverting a
commit that has already been pushed and
made public so there are two ways to do
that to revert a comment so first would
be remove or fix the bad file in a new
commit and push it to the remote
Repository
then commit it to the remote repository
using get commit minus M commit message
again git commit minus M commit message
so this is the first way then second is
create a new commit that undoes all the
changes that were made in the bad
comment
then use the command get reward
commit ID as you can see in the screen
First Command also on the second command
also the second command says get revert
commit ID commit ID will have to put the
commit ID all right so question number
19 this was now moving on to the last
question which is question number 20 how
do you find a list of files that have
been changed in a particular command
so the answer would be the command to
get a list of files that have been
changed in a particular comment is
git diff 3 minus r then there is commit
hash so this command as you can see on
the screen get div 3 minus r commit hash
you can put it then example is also
there like
commit hash
87e6735
f21b so this example as you can see on
the screen then there is minus r flag
instruct so the command to list
individual files and commit hash will
list all the files that were changed or
added in that command so there's telling
about the as you can see minus our
functionality minus r is a flag that
instructs the command to list individual
files and commit hash will list all the
files that were changed or added in that
comment
so these were the top 20 devops
interview questions that you must
understand if you are planning to give a
devops interview
so in this video we have explored key
Concepts methodologies and best
practices that are crucial in fostering
collaboration between development and
operation streams by understanding the
principles discussed here you are well
equipped to navigate the dynamic
landscape of devops as we wrap this
devops crash course you have taken a
significant step towards mastering the
world of devops you have learned the
essence of the and its in
its in with exciting salary prospects
exploring nine essential devop tools
including git GitHub selenium Jenkins
Docker and kubernetes has equipped you
with the valuable skills remember the
journey doesn't end here devops is a
dynamic field ever revolving and
continuously Innovative so keep
exploring practicing and embracing new
tools and techniques good luck on your
devops adventure staying ahead in your
career requires continuous learning and
upskilling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in Cutting Edge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know more
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
foreign