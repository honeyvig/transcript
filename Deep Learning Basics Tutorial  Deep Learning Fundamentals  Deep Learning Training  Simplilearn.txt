deep learning refers to a subset of
machine learning technique that involves
training artificial neural networks with
multiple layers to learn and extract
high level representation from complex
data it has been instrumental in
achieving breakthroughs in computer
vision natural language processing and
autonomous driving deep learning
algorithm handles vast amount of data
and have demonstrated remarkable
performance in various tasks
contributing to AI research and
application advancements deep learning
is a popular approach for building chat
boards and service boards as it enables
them to understand natural language
inputs recognize patterns and generate
appropriate responses
the ability of deep learning to
automatically learn complex
representations its advancements in
transfer learning and its resemblance to
the human brain makes it fascinating and
Powerful field of study that continues
to transform various domains of
artificial intelligence professionals
with deep learning skills can access
various career opportunities including
rules like deep learning engineer
machine learning engineer data scientist
computer vision engineer and natural
language processing engineer based on
the data from Glassdoor the average
annual salary for deep learning
Professionals in the United States is
around 109 000 the average annual salary
for deep learning Professionals in India
is 8 lakhs if you see expertise in deep
learning skills and aspect to be a part
of cutting its field of building chat
boots and service boats using deep
learning our pgp in Ai and ml is the
ideal choice this PCP in artificial
intelligence and machine learning
offered in partnership with IBM is your
pathway to becoming an expert artificial
in intelligence and machine learning
professional explore the latest tools
and Technologies from the AI ecosystem
through this comprehensive artificial
intelligence course gain Insight from
master classes conducted by Caltech
faculty and IBM expert participate in
hackathons and engage in informative ask
me anything sessions elevate your skills
and knowledge in the dynamic field of
artificial intelligence with this
industry leading program do check out
the course Link in the description for
more details now let's begin by
understanding the basics of deep
learning my
deep learning is a type of machine
learning that works on the basis of
functionalities of human brain it trains
machines to work on vast volumes of
structured and unstructured data
deep learning uses the concept of neural
networks that is derived from the
structure of a human brain
and if you've ever seen images of human
brain Network you have dendrites inputs
you have cell which is a nucleus as your
nodes you have synapses which create
weights and an axon which create an
output
now this is a very simplified version of
the human brain
and there are certain sets of cells that
are strung together very similar to how
today's neural networks perform but they
still have a long ways to go and the
human brain is still significantly more
complex with hundreds of different kinds
of cells and different interactions that
we don't see yet
so what is deep learning artificial
intelligence so you have your AI is a
method of building smart machines that
are capable to think like human and
mimic their actions
careful with that definition we're a
long way from a human cyborg coming in
and taking over when we talk about this
we're usually talking about automating a
single kind of instances or things going
on how do we automate a new process how
do we automate a small robot to do
something how do we get a drone to fly
out and when it loses contact to turn
around and come back in the direction it
came from
those are very simple processes and
significantly lower than the scale of
like what human thinking does now a
subcategory of artificial intelligence
is machine learning machine learning is
an application of AI that allows
machines to automatically learn and
improve with experience
there's a lot of tools out there to use
with machine learning
and you'll see really any regression
models and things like that they're much
more common when dealing with straight
numbers a linear regression model and
there's other models that just do basic
math functions quite well
but then we get into one of the
subcategories deep learning deep
learning is a subfield of machine
learning that is used to extract
patterns from data using neural networks
and again we're talking about
complicated patterns we talk about image
processing and things like that they're
not as straightforward as the numbers in
stock exchange so you start looking at
another way to solve these problems and
figure out is that a raccoon in the
picture or something much more
complicated if getting your learning
started is half the battle what if you
could do that for free visit skill up by
simply learn click on the link in the
description to know more
deep learning performance so when you
have we talk about performance of deep
learning and the amount of data the
performance goes up the more data you
have the higher the performance
when we talk about a lot of machine
learning platforms they kind of peek out
at a lower level so again you can think
of this as having thousands of pictures
of raccoons like I said before versus
the numbers in a stock exchange which
are very rigid and very clear there you
have a close and an open and the stock
exchange kind of thing where you have an
image of a raccoon the color shading all
kinds of things go into trying to figure
out is it a raccoon
so we look at what is a neural network
and we really look into kind of a nice
image of it today's neural networks
usually have a layer of inputs and
you'll see here we have input one two
and three with X1 X2 X3
so you have your input layer you have
your hidden layers this might have
multiple layers depending on what you're
working on
uh then you have your output layer which
in this case we have two outputs y1 and
Y2 and you can also see the connectivity
here so everything in the first row
connects with everything in the second
row in the hidden layer and if you had
another hidden layer everything in the
first hidden layer would connect to the
second hidden layer and then everything
in the second hidden layer would connect
to each of the outputs and then it
calculate calculations based on that and
this is interesting because there's so
many different aspects of neural
networks nowadays that are changing this
basic configuration they find that if
you skip a hidden layer or two with your
input going through that that actually
changes the results and works better in
some cases there's also convolutional
neural networks which look at windows
and adding up the numbers in them
there's a lot of complexity when we
start getting into the different aspects
of what you can do and how you build
neural networks and again it's very very
much in an infant stage so this basic
diagram does a great job of capturing
what it looks like now the input layer
is responsible to accept the inputs in
various formats the hidden layers again
there's that s could be multiple layers
it's responsible for extracting features
and hidden patterns from the data and
you know a hidden thing patterns and
features is important we want to look at
features you usually talk about features
as your input you have input one as one
feature input two is another feature if
you're looking at the iris data it might
be the width and length of the paddle
each one of those would be a feature you
have these nodes which generate a number
that doesn't really have a specific
representation but it becomes a way of
looking at it of the adding the features
together and creating an importance
value there and the output layer
produces the desired output after
completing the entire processing of the
data what is a perceptron a perceptron
is a binary classification algorithm
proposed by Frank rosenblot and you'll
see here we have our constants come in
and our inputs we have a constant one
for the bias the bias is important you
can go back to euclidean Geometry where
you have a line Y equals MX plus b b
being the y-intercept that's what that
constant is is there needs to be some
kind of adjustment that basically is the
y-intercept
and you have your inputs you have your
weights you have your weighted sum your
activation function and an output of 0
or 1. and so here we have we'll go ahead
and walk through these we have X1 X2 X3
are the inputs W naught W1 W2 W3 are the
weights weights are values that
determine the strength of the connection
between two neurons
and if we go back to this slide let me
just flash back here to this slide you
can see how each one of these nodes has
multiple inputs so when you hit look at
the hidden layer of the outputs they
have multiple inputs so each one of
these nodes has these inputs they might
be the original features coming in they
might be the layer of nodes before so we
have your X1 and X2 and X3 are the input
into your node your weights are the
weighted values that determine the
strength of the connection between two
neurons
the input neurons are multiplied with
the weight and a bias is added there's
that one which is weighted the
y-intercept in euclidean geometry
to get the resulted weight sum
the activation function applies a step
to check if the output of the weighting
function is greater than zero or not
there's a lot of ways to do an
activation function but this is the most
common or the most not common but the
most easy to see way of doing an
activation on here
so we look at here we go another glocken
back through this diagram and taking a
closer look at it we have the predicted
output is compared with the actual
output
so once you go through the step and it
adds everything together in there and
these are your weights are multiplied
they're just multiples so you have
feature of X1 times weight of one plus
feature of X2 times weight of two so on
plus the weight times the bias and we go
ahead and compute all those all the way
through the node comes out and we have
the actual output and then we go ahead
and have a predicted output what do we
think it's going to be
and this is on each note so keep in mind
we're 0 in and on the Node this isn't
the whole process because this actually
goes through all the notes but there's a
there's another step in here when you
get to that part the error in the
network is estimated using the cost
function and back propagation technique
is used to improve the performance
so when we look at the uh in the back
propagation algorithm the cost function
is minimized by changing weights and
biases in the network and you can see
here we have our input layer we have our
hidden layers and we have our output
layer and so you could think of this as
we have our actual output we predict
what it's going to be in this case we
have two outputs we might predict that
it's either nothing there or a raccoon
so one of them comes up and says I don't
see any kind of animal and the other one
says this is a raccoon
so it's either yes or no and if it gets
it wrong it says that's wrong and it
sends that error back and that error
goes through the first set of Weights
which then go to the hidden layers and
their set of Weights which goes back to
B1 the other layer and their set of
weights and it adjusts those weights as
it goes backwards and it says hey this
is the error on the output we kind of
adjusted a little bit the part that we
don't adjust in the first set of Weights
we say there's still an error so we send
it to the second set of weights and so
forth what happens is as we adjust these
weights each one of these nodes
starts creating kind of a category or
something to look for in whatever image
or data we have going in
and so for making a better predictions a
number of epics are executed where the
error is determined by the cost function
now epic is a important thing to keep
track of epics is if you have a large or
any data set and let's say you split out
your test date and your training data
set you're running your data through how
many times do you have to run all of
that data through
before you start getting something that
is usable until the error goes down to
you can't adjust the error anymore it's
the lowest error you can get so each
time you go through a full set of data
before you repeat and go through the
same data that's called an epic
the error is backward propagated until a
sufficiently small error is achieved
and again that's what we're talking
about we're trying to minimize the error
so we get to a point where that error
really isn't changing anymore you just
have the same error coming out
and there's other ways to weight that
error too
the cost function or loss function
measures the accuracy of the network the
cost functions tries to penalize the
network when it makes errors
and you can see here we have a formula
for the cost function C equals one half
of the Y predicted minus the Y the
actual y squared and of course the
squared value removes the sign because
we don't know whether it's plus or minus
when you look at an error and then the C
this is your cost function how much is
it going to cost how much of an error do
we really have that we're sending back
and so we have with the cost function we
can look at this we can look at it as a
loss in the Epic again that's every time
we go through the data that's an epic
how many epic runs are we going to go
and so we have a nice graph here that
shows like a very high learning rate low
learning rate different data is going to
change depending on what you're working
on they put the yellow as a good
learning rate because it has a nice
slope to it and you think yeah the more
epics that go in the lower the loss so
that means you're going in a good
direction there
and as you curves down it gets to the
the best answer has the lowest loss on
there
so uh looking at types of neural
networks we have the Gan we have the dbn
the rbfn the auto encoder the RNN the
lstm this is a flash of some of the main
ones that are out there there are now so
many variations that there's variations
on the variations
quickly jumping into these uh you can
see we have a number of them here listed
these are just like a flash of some of
the more common ones like the Gan
General adversarial Network where you
have two different models competing
against each other until they find which
one can beat the other one can I think
of a chess game where you keep flipping
who's in charge
uh and then we might look at the dbn
which is a deep belief Network
and they're used to recognize clusters
and generate image video sequences
generally
and we have the RBF Network rbfn which
is your radial basis function Network
which uses a different set of activation
functions
to figure out which is the right weights
Auto encoder the auto encoder is a
little different than the other ones in
that it looks for an error but the error
is based in finding data that groups
together so it's a way of sorting the
data out without knowing the answer
that's what an auto encoder does
RNN RNN has a couple different
definitions most of the time you now see
it as a recurrent neural network where
the output part of the output goes back
into the input so they call it this was
recurrent there's also another RNN which
deals with learning step by step as
opposed to over a set of data and
waiting it as you go
and there's the ls TM or long short-term
memory recurrent neural network it's
also an RNN which deals with how do you
sort something like a sentence out where
the last word depends on what the first
word was and so it slowly Waits things
as it goes through to figure out what's
being said
again these are just a quick Flash and
even as I was describing them you should
have been like well why don't you hook
an auto encoder into again and then run
that with a dbn well there are tools to
go ahead and mix and match these things
so you will actually see uh when you
start doing layers you might have the
layers of one type of neural network
feed into the next neural network and
that's actually pretty common
and we talk about deep learning
libraries there's a lot of different
things out there but the big ones that
they usually talk about and most of
these are dealing with larger data is
like tensorflow Karas cross and
tensorflow played nice with each other
and cross kind of is is usually
integrated with tensorflow Cafe Fino
pytorch DL for J these are all different
deep learning libraries and there's many
more out there there's a side kit has a
deep learning library in it under python
Scala has one that they've been building
on there so these aren't the only ones
tensorflow is probably the most robust
one at this time but that's not to say
the other ones aren't catching up and
there's not all kinds of other stuff out
there who their learner simply learn
brings you master's program in
artificial intelligence created in
collaboration with IBM to learn more
about this course you can find the
course Link in the description box below
now going through and talking about
these things is all fun but until you
get to roll up your sleeves and take a
look at the code and see what's going on
it's really hard to see what we're
talking about so we're going to do a
deep learning demo on text
classification now to do this there's a
lot of different editors you can use for
python
currently my favorite my favorites
always one I'm currently using which
changes from month to month or year to
year depending what projects we're doing
we want to look at Anaconda Navigator
which is a nice job building your
different packages and environments and
go into Jupiter notebook and so we'll go
and go to Jupiter notebook and create
our
uh are set up in there to run a neural
network
and so we open up our Jupiter we'll go
ahead and create a new python 3.
let's bring this out to the top
there we go
uh so now we already have our Jupiter
three and I like to always give it a a
setup on here so this is the text
classification we're doing today
we'll go ahead and rename that
and we were talking about if you
remember from the beginning of the thing
I said some of the the more the most one
of the most robust packages out there
right now is
um tensorflow along with Karas which
usually works nicely with tensorflow so
we're going to go ahead and be working
with the tensorflow and the cross
we want to go ahead and do a number of
imports in here
this is just set up on this a lot of
this really isn't necessary for what
we're going to do there's different
things we can do so we're going to go
ahead and from future we're going to
import absolute import division print
function
these really don't do a lot other than
they help us kind of display things
later on you don't really need them you
can just do some basic print on this if
you want to so I'm not going to dig too
deeply into that setup
but we do want to take a look at
iteration tools
because we're always iterating over
things again you don't really need the
iteration tools because a lot of
tensorflow will do this for you
um but it's a lot of times when we're
building these packages we don't think
about it we just bring in all our tools
that we might need the big ones though
is our pandasdb
and let me switch to a draw thing it's
always nice to kind of
give us arrows here uh there we go
uh so we're looking at you can see here
here's our pandas pandas sits on top of
uh numpy our number array appendez is
our data frame just makes it really easy
to bring in the data view the data and
kind of work with the file that's what
these two are for and of course our
matplot library appears for displaying
it and then we have the sklearn these
are for doing metrics and pre-processing
some of this you can actually pull off
of tensorflow I tend to balance back and
forth between the sklearn setup and by
the way the SK learning has its own
neural network a real simplified one
compared to what tensorflow does but we
want to go ahead and do this in
tensorflow where we're going to be using
tensorflow cross and cross is nice
because it sits on top of tensorflow
but it's easier to use it's a little bit
like you could think of tensorflow as
being the back end which you can program
in in Cross being a higher level
language which makes it easier to do
things in
and you can see here we went ahead and
printed out uh TF version we're using
tensorflow as a back end and you have
the tensorflow version 2.1.0 always
important to double check your versions
you never know what's not going to work
with what version of python and so forth
so it's important to check those this is
python36
I don't know if tensorflow is fully
integrated with three seven or three
eight yet I'm sure if it isn't it soon
will be
and so we're going to go ahead and bring
some data in this is
consumercomplaints.csv and if you want a
copy of this file just for your own to
play with you can put a note send a note
to Simply learn and they'll be happy to
send you a copy of that and then DF
stands for data frame that's very common
and we're using the pandas to go ahead
and read this
comma separated variable file in
and we'll go ahead and print what they
called the head if you've never worked
with data frames usually when you see
head it means the first five rows of
that data frame so you can see what's
going on in there
and give it just a moment to read that
in
there it goes and you'll see in here
that we have a date received product
mortgage credit reporting consumer loan
credit card sub product other mortgage
Nan Nan whatever that means whatever you
know it's just missing data right there
there's nothing in there vehicle load
and so forth uh then of course different
columns and this is a data Frame data
frame has rows and columns
and a lot of times when you're
processing data frames you can think of
mapping map and reduce as a terminology
we're mapping we're either mapping each
of the rows like you might be running a
process on each row or you're running a
process on each column
and then one of the most common things
would be say to some parts of the row
together and have a total value of cost
or maybe some the total uh you may come
over here where I'm sure there's a value
in here
complaint ID well they don't actually
have a value but it'd be something you
would process by column you might want
to find out how many different companies
are listed here so that might be
something you'd run on the column
uh so that gives you an idea we have our
data frame DF head on there
and if you remember we're looking at
text classification so that makes sense
that we don't have any dollar values in
there and we're going to look at just a
couple of these columns certainly you
can do this
with a number of different objects on
here but we're going to take just the
columns Consumer complaint narrative
and the product column
and in this case when we handle null
values there's a lot of different ways
to handle null values but in this case
we're just going to go ahead and do just
the not null we only want those that are
not null on the Consumer complaint
narrative and let's go ahead and print
that do the DF head
in Jupiter
the last line if you just have a
variable it automatically prints it out
so you could put print and put brackets
around DF head and do the same thing
and you can see here we have our
Consumer complaint narrative I have
outdated information on my credit report
and then you have your credit reporting
and so forth on each one of these an
account on my credit card was mistaken
the company refuses to provide me
verification the complaint regards to
square two something so we're going to
be looking at these complaint narratives
and trying to understand them
and can we write a code to
um optimize that
and then one of the things we always do
like up here you'll see where we already
removed the null values I would go ahead
and just double check
sum up the null values are there any
null values in our data frame
and this is just a simple way of looking
at every cell this includes Consumer
complaint the product and so forth and
you can see here we have uh zero null
value so that's good we're not we're not
going to play with the null values today
and then
we'll go ahead and take a look at our
end
aspect we might be looking for which is
our product we have credit reporting
consumer loan credit reporting debt
collection debt collection let's go
ahead and just take a look at this
and see how many counts we have and you
can see here that there is under debt
collection forty seven thousand entries
under mortgage there's 36 000 entries
under credit card
we have over 18 000 entries so this is a
pretty big database
and you can see it going down all the
way down here with the different uh
entries that go underneath of product so
we have and one of the things we don't
spend a lot of time on in some of these
demos is what is really defining what
we're looking for now if you're doing a
data science project usually start by
exploring the data and then you define
what you're really looking for and so
we're kind of skipping through that
really that particular process it does
constitute a small amount of time but as
far as the importance it's one of the
most important things you can do in data
science is ask the right questions so
even though you're spending 80 percent
of the time cleaning data building your
models and everything that 20 percent of
asking the right questions has a higher
impact and so you really should be
making sure that you understand what the
questions are that's domain knowledge so
we're talking in this case Banking and
so in this case it might be that as
Consumer complaint comes in
we can start looking at these consumer
complaints and what product they're
attached to what that means
we're not going to dig too much into the
domain in asking the question what you
know what what exactly are we looking
for we really want to look into the
process
um as far as building a neural network
and so the first thing I'm going to do
is go ahead and split our data we need a
train set and a test set the train size
in this case we're going to do 80 of the
data is going to be for training our
neural network
and then we'll go ahead and use the
we'll switch that over for the test size
to be the 20 let me go ahead and run
that
and you can see here train size has 15
159 000 entries and then we'll test that
on we've held out roughly 40 000 of
those entries for our test size
for this example uh we're just going to
split it
the first part of the data will be for
training our data and the second part
will be for testing our data so we're
not really doing a random setup in here
which is okay because this is an example
A lot of times you might split this one
of the things I do with neural networks
is I will split it into three sections
and I will test
two is training and the third as the
test this is obviously not big data you
don't want to do this on something that
might take days to process
and then I flip it and then so I'll
write it three times and those three
times will give me a nice Narrative of
how good my model is and then I'll
actually run the the final product on
all three sections to program it to
train it
so it gives me a good basis of what kind
of error I have versus you know on test
models while having a very robust model
to publish
in this case though we're just going to
go ahead and split it based on the first
part goes to the train and the second
part goes to the test
so one of the next things whenever we
deal with text and this is such an
important uh line I want to go ahead and
just highlight the tokenize you'll see
the word tokenize
tokenizer setting it up
we're taking the words and putting them
into a format the computer can
understand
there is a lot of ways to tokenize words
in some cases you call them zero one two
three four
depending on what you're doing it might
just be a zero or one
um so when we talk about the encoder
we're usually talking about as far as
tokenize
we actually are looking at in this
particular case we'll be looking at each
word as its own feature uh so when we
looked up here remember right up here
let's see here we go money transfer let
me let me go back up just a note here
turn that off so I can go back up if you
remember up here we have this right here
Consumer complaint narrative so I would
be a feature have would be a feature
outdated would be a feature information
would be a feature on feature my feature
credit feature report and you think wow
that's a lot of features uh yes in
running through bills put out by the
United States that are being voted on in
this in the government
it comes out roughly 2.4 million
different words are used in those bills
that's a lot of features and so we're
going to go ahead and look at uh Max
words tokenize and tokenizing words
there's a lot that is happening here in
the tokenize setup so we'll go ahead and
let me just put together some of this
code here
so the tokenized is an actual object in
here text tokenizer and it has number of
words equals Max words so we're going to
limit it to a thousand words character
level equals false fit on text train
narrative only fit on the training data
and so this is kind of interesting
because it drops a lot of these words
why would it drop a word well on on is
probably used a bunch and really doesn't
have any value so that would be one of
the words they'll probably drop and then
also there's other things it can do like
it can also combine combinations of word
it might be that this company this
complaint these might be always together
so at some point it might actually
bundle some of these words as a single
feature there's a lot of things that go
into that we're not going to go into too
much detail because you really have to
go through the API on these to
understand all the different encoding
and setups you have
this is just a really fast way to do
this and it works
I like easy and I like things that work
I don't know about you but what I'm
running through and doing a lot of
different models
I might start tweaking these once I have
an answer
but until then
we want to go ahead and run the encoder
and do something simple like this and so
using SK learn utility to convert the
label strings to number index and so
here's our encoder label encoder encoder
fit
and this is sklearn of course
everything's fit
and then we do y train equals the
encoder transform train product y test
equals encoder transform test product
now what's going on here now up here we
did the X where we have
let me view up here where we were
looking at
here we go I have outdated information
on my credit card report now we're
looking at this credit reporting
consumer loan credit reporting and if
you remember this is our list of those
debt collection here's our list of them
they're not a huge number
and so we're going to encode these
differently this is just 0 1 2 3 4 and
so on not necessarily in that order
order by the way so be a little careful
on order so we're going to convert the
labels to one hot representation as our
next step and that's what this is doing
so we have our number classes in pmax y
train plus one our weight y train equals
utilities to categorical
so here is where we're taking our y
value in the training set and we do the
same with the test set
and we go ahead and run that so now
we've created a y train and a y test
where we've numbered our categorical
we've created a categorical data so it's
easy to read the answer and translate it
back and we've encoded
up here our X and we use a tokenizer
so a little different encoder puts in 0
1 2 3 for the different listings an
encoder or token
you can get Tongue Tied on these a
tokenizer takes and creates a huge in
this case we've limited to 1 000 words
each word is its own token and to really
see what we're looking at let me go
ahead and we're going to inspect the
elements and see what we ended up with
let me run this and so we have our y
train shape there's our 1000 where does
that one thousand maximum words so we've
tokenized the top thousand words
as each as its own feature if
and the same thing with the X test so
those are X train and X test we have our
y train shape and our y train test shape
and I thought the encoder depending on
how you set it
um either to zero one two three or in
this case it actually puts it out as 18.
so it did the same thing as a tokenizer
as far as putting it as a zero one so
you have 18 choices there and if we
count these I'm guessing there's 18
there now this is the part which is we
look at the encoders and the tokenizers
these are the tools you need to process
text
the computer doesn't see the hello on
you have to give it a zero or one in
each one of those and so this is all
about the text classification part
this is how we classify text is we have
to give it something that has a number
representation so once we've sorted out
our data and we've converted it into
something the computer can read for
doing text we want to go ahead and
create our model
and when we create our model one of the
things to be aware of is this is what we
call a black box model now they've come
a long ways in understanding how these
different processes are created and so
they're starting to understand how you
can put these together and go back and
say why why does it pick this why does
it pick that how does it balance that
but it's black boxing that it's really
hard to do it's really hard to go in
there and figure out why it picks one
over the other just by looking at the
neural network itself
there are other machine learning tools
like decision tree which make it much
easier to see those changes and how it
branches down but they don't perform as
well in many cases
and so we're going to go ahead and build
a model we'll go ahead and run this just
because it builds the model doesn't
actually start fitting it yet let's take
a look at these different pieces so we
have our model which is going to be
sequential
that's a cross so we imported it at the
beginning from the cross setup this
tells us what's going on that we're
going to be running this from top to
bottom
and we're going to add a dense layer so
each time you see add let me do we look
at these ad each one of these
we're adding a row
so these are all rows
and then of course our rows you're like
what is a row so we talk about row these
are your layers we have you can see
right there input layer we're going to
add an activation to this layer and
we're going to use the ray Lou
activation then we're going to add a
dropout rate and what this does is they
found that when they process a neural
network instead of processing every
neuron each time you do the back
propagation to train it
you only process some of them so only
some of them are being trained by doing
that it's able to create the
differentiation between different
categories and it actually trains much
better instead of trying to train them
all at once
so that's what this is right here with
the Dropout 0.5 and then we added add
dense number classes
and the dense number classes is another
layer so up here we have our add an
activation Rayleigh layer so we have our
input our relu layer with a drop out of
0.5 then we have a dense layer
which uses a soft Max activation there
is a lot going on with Karas and these
models you can get lost in just the
activation here's our two activation
relu is one type of activation softmax
is another
they work differently and you can see
when we were talking about earlier we
talked about the different kinds of
neural networks
in Cross and tensorflow each of those
layers can be a different neural network
layer and can function differently and
you can stack them on top of each other
and feed them into each other
and then the final thing of course is
your compile we have what we're using
for loss
remember we want to minimize loss so
category this is a type of way of
minimizing that loss
atom is an optimizer
you'll find there's a number of
Optimizer atom is used for larger data
sets as you get to smaller data sets you
actually use a very different Optimizer
in here and we look at the accuracy
that's just when do we stop compiling
this data how far do we go until it
starts building what they call a bias it
starts it's overfitted we want to stop
at the right moment
and then finally we get to actually
training our model this is that black
box we're going to fit it to the data we
don't know what's actually going on in
there but we want to go ahead and run it
and I'm going to go ahead and start it
running because it takes a moment for it
to run through and you'll see the Epic
feed down here as it goes through epic
105 and so forth we'll freeze this just
for a second there we go so let's take a
look at this we have batch size batch
size Oops I meant to do that in an arrow
there we go uh batch size is how many
rows of data are we going to feed at a
time
so you can feed larger rows there's
there's different reasons to feed them
at different sizes
um
really a lot of times people just leave
this as a default depending on Let It
choose for them
I'm not sure why they picked 32 in this
case there's probably when they were
messing with this 32 probably was a good
batch size for fitting it
the back end math deals with
differential equations in some calculus
which we won't get into
uh so being aware of that that this
batch size affects how it does that that
back-end differential equation in the
reverse propagation
um tells us that this is actually a
pretty important number if you get it
too big it's going to not it's not going
to fit as well and if you get it too
small it takes way too long to process
and a lot of times uh there's what they
call a reinforced learning neural
network where it's batch size of one
what does that mean well that means
every action you take has a feedback you
program the neural network so you can
guess what your next action is that's
very common like in trying to beat video
games with a automated setup in a neural
network the Epic says we're going to go
through all the data in the training set
five times so that's what this remember
we talked about epics that's what the
Epic is
and we can see when I let it go out here
we're on Epic 2.5
so we'll go ahead and pause this I'm
going to go ahead and pause it for a
second let it finish running
or while we're waiting we can actually
take a look at some of this data here
and see what it's actually generating
for us
and so we look at this accuracy
loss so we want to minimize loss the
metrics is accuracy and you can see that
we're going to want the accuracy to go
up and we want the loss to go down and
the loss is going to be what we want to
minimize and this is just some of the
metrics and it talks about like value
loss
value accuracy and how they're changing
with each time we run it and so you get
to a point where it no longer gets
better or worse and at that point you
really want to stop running it
you can overfit it and overfit it means
it's going to miss some of the
generalities that you need when solving
some of these Solutions
and then I did talk too much about
what's actually going on here what are
we doing in the domain of this
particular one which it looks like it is
trying to figure out based on a Consumer
complaint narrative maybe they send in a
complaint
what product is it connected to so maybe
they get the complaints before the
product or something like that I I'm not
sure why you do this particular setup uh
that would be again a domain knowledge
in here so what we're doing is we're
using the Consumer complaint narrative
to predict what product they're talking
about if someone comes sends in an email
and says I have outdated information on
my credit report they're probably
talking about a credit card reporting if
you get a random email that says I
purchased a new car on blank the car
something something probably alone
that's what we're trying to do is if you
get a random input from the Consumer
complaint narrative you can point to the
product that they're referring to
without having to call them up and ask
them I guess
that might be useful not sure now that
we've gone through all five epics
let's go ahead and evaluate the accuracy
of our trained model and so we have
we're going to go ahead and do a score
for the model dot evaluate a nice caress
setup where we can do the X test and the
Y test the batch size verbose and then
we're going to it's going to generate a
score
and one of the things let's just go
ahead and print
so you can see why they broke out the
score let me just go ahead and print the
score in here let's go ahead and run
that
and so it's testing this score it's
going to take it a moment to go through
all the data and it gives us a nice
score and it says the test accuracy
these can mean a lot of different things
but we have a 0.5 to be honest without
looking at the data I would have to look
and see exactly which things it missed
on and how it how it scored on there but
it's it's getting about you know half of
it it's able to pull in half of it and
say hey if this is the complaint this is
what it's connected to
keep in mind when we're talking about
text that's really good can you imagine
some of the stuff I don't know if you've
ever worked in tech support or it or at
a counter in a in the mall or even in
taking order someplace as a waiter or
waitress or fast food or whatever
when you try to understand people it
gets pretty crazy so even an accuracy
score like this is probably pretty good
for understanding some of this text and
there might be steps you could do to
improve that and so let's go ahead and
go through here and look at actually
using it
and we'll punch this in it says how to
generate prediction on individual
examples we have our text labels and our
encoder class
uh and we'll just go through a
prediction equals model predict NP array
of X test of I
predict label text labels prediction
print test narrative so forth and this
is just let's go ahead and run it
because it's reading through print
statements can be very painful sometimes
unless you actually see what's going on
so when the president came out with the
HARP program dot dot dot the actual
label was mortgage the predicted label
was mortgage so when this person sent
this this complaint in maybe you don't
know what it's connected to yet you can
guess it's probably a mortgage I filed a
dispute with Capital One Bank on dot dot
dot dot and it says oh credit card well
it turns out it's actually debt
collection it was what it predicted
um so yeah missed a little bit uh if a
lot of times when you dispute something
it probably is a debt collection in this
case it was specific to a credit card
okay so we missed that one
I am disputing account number xxs with
Midland whatever actual test label debt
collection was debt collection I opened
a Barclay card on to help rebuild my
credit credit card credit card mortgage
mortgage so forth we can go through all
of these and you can see it does a
pretty good job it gets close or in fact
most of these it pulled in correctly
credit card reporting
this is a lot of what we talk about with
text
setup Cinnamon's a really big one
there's a whole sentimental libraries
out there uh whether someone is positive
or negative towards something if you're
pulling off Twitter feeds you might want
to look that up you might want to look
at connection towards postings in what
was it there was running stock I
actually do some some stock programming
code so I end up coming back to that a
lot
finding sentiment feeds on different
stock values and different stocks out
there on out of national Publications
really helps trying to predict what the
Stock's going to do what are people
going to do in buying and selling stock
same thing with this we can now predict
uh their complaints and try to figure
out what it is they're complaining about
hey there learner simple and brings you
master's program in artificial
intelligence created in collaboration
with IBM to learn more about this course
you can find the course Link in the
description box below
wraps up our deep learning demo on text
class of classification
gives you a nice internet
as opposed to shallow learning
bad joke on my part
but you can see how a deep learning
model can go in and do a lot of things
that you might not be able to do using
basic linear regression or many of the
other machine learning models
and text classification is one of those
where neural networks really shine
because they can pick up things that you
don't see you can't really measure in
other ways so what is a neural network
so hi guys I heard you want to know what
a neural network is here we have looks
like you just went shopping at a red tag
cell my robot's back so as a matter of
fact you have been using neural network
on a daily basis in today's world is
just amazing how much we use our new
technology we're not even aware of it
when you ask your Mobile Assistant to
perform a search for you you know like
saying you're Google or Siri or whoever
you use Amazon web self-driving cars so
that's the newest thing coming out
they're just now trying to make those
legal in different states in the U.S and
around the world even in the UK they now
have a self-driving cars going up and
down the street it's pretty amazing
these are all neural network driven
computer games use it there's a lot of
computer games are driven by neural
networks in the back end as part of the
game system and how it adjusts to the
players and it's also used in processing
the map images on your phone so every
time you do a navigation someplace and
it opens it up they now use neural
networks to help you find the quickest
way to get there neural network a neural
network is a system or Hardware that is
designed to operate like a human brain
in today's development this is so
important to understand because we don't
have anything else to compare it to I'm
sure someday in the future the computer
will redefine or the neural network or
the AI artificial intelligence will
redefine what these mean but as far as
we can today's world in today's
commercial development we have to
compare it to what humans do so as we
want to compare and how it operates to a
human brain and how it solves problems
like a human does what can a neural
network do and really we're just going
to dive in deeper to what we just
covered and look at other examples so
what can a neural network do well let's
list out the things neural networks can
do for you translate text boy we got
Google translate and Microsoft has their
own translate they they have some really
cool they actually have an earpiece it's
supposed to start translating as you
talk what a cool technology what a cool
time to live identify faces can you
imagine all the uses for facial
identification in the case of our sample
or our code that we're going to look at
later we'll be identifying dogs and cats
so not quite as detailed as
understanding whose face belongs to who
I'm waiting for the Google Glasses to
come out so I can see who's who and
identify faces as I'm walking around
have a little name tag over them not out
there yet but boy we are close we could
identify the faces and they have all
kinds of Technologies to bring that
information back to us recognize speech
goes along with the translate text so
now as you're talking into your
assistant it can use that to do commands
turn lights on all kinds of things you
can do with recognizing speech read had
written text they're starting to
translate all these old text documents
that they've had in storage instead of
doing it individually where somebody's
going through each text by themselves in
a room you can picture like an old
Raiders of the Lost Ark theme ways in
the back you know archaeologists
studying the text now it's fed into a
computer they take a picture they even
use neural networks to take a scroll
that is so messed up that they can't
undo the scroll and they x-ray it and
then they use that x-ray to translate
the text off of it without ever opening
the scroll I mean just way cool stuff
they're starting to do with all this and
of course control robots what would be a
neural network without bringing in the
robots and we have our own favorite
robot in the middle who goes to our red
tag cell and go shopping for us so you
know these are just a few of the
wonderful things that neural networks
are being applied to it's such an infant
stage technology what a wonderful time
to jump in and there are a lot of other
things it goes into I mean we could
spend just forever talking about all the
different applications from business to
whatever you can even imagine they're
now applying neural networks to help us
understand so now that we've talked a
little bit about all the cool things you
can do with the neural network let's
dive in and say how does a neural
network work so now we've come far
enough to understand and how neural
network works let's go ahead and walk
through this in a nice graphical
representation they usually describe a
neural network as having different
layers you'll see that we've identified
a Green Layer an orange layer and a red
layer the Green Layer is the input so
you have your data coming in it picks up
the input signals and passes them to the
next layer the next layer does all kinds
of calculations and feature extraction
it's called The Hidden layer a lot of
times there's more than one hidden layer
we're only showing one in this picture
but we'll show you how it looks like in
a more detail a little bit and then
finally we have an output layer this
layer delivers the final result so the
only two things we see is the input
layer and the output layer now let's
make use of this neural network and see
how it works wonder how traffic cameras
identify Vehicles registration plate on
the road to detect speeding vehicles and
those breaking the law that got me going
through a red light the other day well
last month that's like the horrible
thing they send you this picture of you
and all your information because they
pulled it up off of your license plate
and your picture they should have gone
through the red light so here we are and
we have an image of a car and you can
see the license plates on there so let's
consider the image of this vehicle and
find out what's on the number plate the
picture itself is 28 by 28 pixels and
the image is fed as an input to identify
the registration plate each neuron has a
number called activation that represents
the grayscale value of the corresponding
pixel range and we range it from zero to
one one for a white pixel and zero for a
black pixel and you can see down here we
have an example where one of the pixels
is registered as like 0.82 meaning it's
probably pretty dark each neuron is lit
up when its activation is close to one
so as we get closer to black on white we
can really start seeing the details in
there and you can see again the pixel
shows this one up there it's like part
of the car and so it lights up so pixels
in the form of arrays are fed to the
input layer and so we see here the
pixels of a car image fed as an input
and you're going to see that the input
layer which is green is one dimension
while our image is two Dimension now
when we look at our setup that we're
programming in Python it has a cool
feature that automatically does the work
for us if you're working with an older
neural network pattern package you then
convert each one of those rows so it's
all one array so you'd have like Row one
and then just tack row two onto the end
you can almost feed the image directly
into some of these neural networks the
key is though is that if you're using a
28 by 28 and you get a picture of this
30 by 30 shrink the 30 by 30 down to fit
the 28 by 28 so you can't increase the
number of input in this case Green Dots
it's very important to remember when you
work on neural networks and let's name
the inputs x y and X2 X3 respectively so
each one of those represents one of the
pixels coming in and the input layer
passes it to the hidden layer and you
can see here we now have two hidden
layers in this image in the orange and
each one of those pixels connects to
each one of those hidden layers and the
inter connections are assigned weights
at random so they get these random
weights that come through if x one
lights up then it's going to be X1 times
this weight going into the hidden layer
and we sum those weights the weights are
multiplied with the input signal and a
bias is added to all of them so as you
can see here we have X1 comes in and it
actually goes to all the different
hidden layer nodes or in this case
whatever you want to call them network
setup the orange dots and so you take
the value of X1 you multiply it by the
weight for the next hidden layer so X1
goes to Hidden layer one X1 goes to
Hidden Layer Two X1 goes hidden layer 1
node two hidden layer 1 node 3 and so on
and the bias a lot of times they just
put the bias in as like another Green
Dot or another orange Dot and they give
the bias a value one and then all the
weights go in from the bias into the
next node so the bias can change we
always just remember that you need to
have that bias in there there's things
that can be done with it generally most
of packages out there control that for
you so you don't have to worry about
figuring out what the bias is is but if
you ever dive deep into neural networks
you've got to remember there's a bias or
the answer won't come out correctly the
weighted sum of the input is fed as an
input to the activation function to
decide which nodes to Fire and for
feature extraction as the signal flows
within the hidden layers the weighted
sum of inputs is calculated and is fed
to the activation function in each layer
to decide which nodes to fire so here's
our feature extraction of the number
plate and you can see these are still
hidden nodes in the middle and this
becomes important we're going to take a
little detour here and look at the
activation function so we're going to
dive just a little bit into the math so
you can start to understand where some
of the games go on when you're playing
with neural networks in your programming
so let's look at the different
activation functions before we move
ahead here's our friendly red tag
shopping robot and so one is a sigmoid
function and the sigmoid function which
is 1 over 1 plus e to the minus X takes
the x value and you can see where it
generates almost a zero and almost a one
with a very small area in the Middle
where it crosses is over and we can use
that value to feed into another function
so if it's really uncertain it might
have a 0.1 or 0.2 or 0.3 but for the
most part it's going to be really close
to 1 and really close to this case 0 0
to 1. the threshold function so if you
don't want to worry about the
uncertainty in the middle you just say
oh if x is greater than or equal to 0 if
not then X is zero so it's either zero
or one really straightforward there's no
in between in the middle and then you
have the what they call the relu
function and you can see here where it
puts out the value but then it says well
if it's over 1 it's going to be 1. and
if it's less than zero it's zero so it
kind of just dead ends it on those two
ends but allows all the values in the
middle and again this like the sigmoid
function allows that information to go
to the next level so it might be
important to know if it's a 0.1 or a
minus 0.1 the next hidden layer might
pick that up and say oh this piece of
information is uncertain or this value
has a very low certainty to it and then
the hyperbolic tangent function and you
can see here it's a 1 minus E to the
minus 2x over 1 plus e to the minus 2X
and it's very much along the same theme
a little bit different in here and that
it goes between minus one and one so
you'll see some of these that goes zero
to one but this one goes minus one to
one and if it's less than zero it's you
know it doesn't fire and if it's over
zero it fires and it also still puts out
a value so you still have a value that
you can get off of that just like you
can with a sigmoid function and the relu
function very similar in use and I
believe the originally used to be
everything was done in the sigmoid
function that was the most commonly used
and now they just kind of use more the
relu function the reason is one it
processes faster because you already
have the value and you don't have to add
another compute the one over one plus e
to the minus X for each hidden node and
the data coming off works pretty good as
far as putting it into the next level if
you want to know just how close it is to
zero how close is it not to functioning
you know is it minus point one minus 0.2
usually their float value you so you get
like minus Point minus 0.00138 or
something so you know important
information but the real is most
commonly used these days as far as the
setup we're using but you'll also see
the sigmoid function very commonly used
also now that you know what an
activation function is let's get back to
the neural network so finally the model
would predict the outcome of applying a
suitable activation function to the
output layer so we go in here we look at
this we have the optical character
recognition OCR is used on the images to
convert it into a text in order to
identify what's written on the plate and
as it comes out you'll see the red node
and the red node might actually
represent just the letter A so there's
usually a lot of outputs when you're
doing text identification we're not
going to show that on here but you might
have it even in the order it might be
what order the license plates in so you
might have a b c d e f g you know the
alphabet plus the numbers and you might
have the one two three four five six
seven eight nine ten places so it's a
very large array that comes out it's not
a small amount of about you know we show
three dots coming in eight hidden layer
nodes you know two sets of four we just
saw one red coming out a lot of times
this is uh you know 28 times 28 if you
did 30 times 30 that's you know 900
nodes so 28 is a little bit less than
that uh just on the input and so you can
imagine the hidden layer is just as big
each hidden layer is just as big if not
bigger then the output is going to be
there's so many digits yeah it's a lot
it's a huge amount of input and output
but we're only showing you just you know
it would be hard to show in one picture
and so it comes up and this is what it
finally gets out on the output as it
identifies a number on the plate and in
this case we have 0 8
d-03858 error in the output is back
propagated through the network and
weights are adjusted to minimize the
error rate this is calculated by a cost
function when we're training our data
this is what's used and we'll look at
that in the code when we do the data
training so we have stuff we know the
answer to and then we put the
information through and it says yes that
was correct or no because remember we
randomly set all the weights to begin
with and if it's wrong we take that
error how far off are you you know are
you off but is it if it was like minus
one you're just a little bit off if it's
like minus 300 was your output remember
when we're looking at those different
options you know hyperbolic or whatever
and we're looking at the Rel the Rel
could doesn't have a limit on top or
bottom it actually just generates a
number so if it's way off you have to
adjust those weights a lot but if it's
pretty close you might have just relates
just a little bit and you keep adjusting
the weights until they fit all the
different training models you put in so
you might have 500 training models and
those weights will adjust using the back
propagation it sends the error backward
the output is compared with the original
result and multiple iterations are done
to get the maximum accuracy so not only
does it look at each one but it goes
through it and just keeps cycling
through these the data and making small
changes in the network until it gets the
right answers with every iteration the
weights at every interconnection are
adjusted based on the error we're not
going to dive into that math because it
is a differential equation and it gets a
little complicated but I will talk a
little bit about some of the different
options they have when we look at the
code if you see expertise in deep
learning skills and aspect to be a part
of Cutting Edge field of building chat
boards and service boats using deep
learning our P2P in Ai and ml is the
ideal Choice do check out the course
Link in the description for more details
so we've explored a neural network let's
look at the different types of
artificial neural networks and this is
like the biggest area growing is how
these all come together let's see the
different types of neural network and
again we're comparing this to human
learning so here's a human brain I feel
sorry for that poor guy so we have a
feed for forward neural network simplest
form of a they call it a n a neural
network data travels only in One
Direction input to Output this is what
we just looked at so as the data comes
in all the weights are added it goes to
the hidden layer all the weights are
added it goes to the next hidden layer
all the weights are added and it goes to
the output the only time you use a
reverse propagation is to train it so
when you actually use it it's very fast
when you're training it it takes a while
because it has to iterate through all
your training data and you start getting
into Big Data because you can train
these with a huge amount of data the
more data you put in the better train
they get the applications vision and
speech recognition actually they're
pretty much everything we talked about a
lot of almost all of them use this form
of neural network at some level radio
basis function neural network this model
classifies the data point based on its
distance from a Center Point what that
means is that you might not have
training data so you want to group
things together and you create Central
points and it looks for all the things
you know some of these things are just
like the other if you've ever watched
the Sesame Street as a kid that dates me
so it brings things together and this is
a great way if you don't have the right
training model you can start finding
things that are connected you might not
have noticed before applications power
restoration systems they try to figure
out what's connected and then based on
that they can fix the problem if you
have a huge power system cajonin
self-organizing neural network vectors
of random dimensions are input to
discrete map comprised of neurons so
they basically find a way to draw and
they call them they say Dimensions or
vectors or planes because they actually
chop the data in one dimension two
Dimension three dimension four five six
they keep adding dimensions and finding
ways to separate the data and connect
different data pieces together
applications used to recognize patterns
and data like in medical analysis the
hidden layer saves its output to be used
for future prediction recurrent neural
networks so the hidden layers remember
its output from last time and that
becomes part of its new input you might
use that especially in robotics or
flying a drone you want to know what
your last change was and how fast it was
going to help predict what your next
change you need to make is to get to
where the Drone wants to go applications
text-to-speech conversation model so you
know I talked about drones but you know
just identifying on Lexis or Google
assistant or any of these they're
starting to add in I'd like to play a
song on my Pandora and I'd like it to be
at volume 90 percent so you now can add
different things in there and it
connects them together the input
features are taken in batches like a
filter this allows a network to remember
an image in Parts convolution neural
network today's world in photo
identification and taking apart photos
and trying to you know have you ever
seen that on Google where you have five
people together this is the kind of
thing separates all those people so then
it can do a face recognition on each
person applications used in signal and
image processing in this case I use
facial images or Google picture images
as one of the options modular neural
network it has a collection of different
neural networks working together other
to get the output so wow we just went
through all these different types of
neural networks and the final one is to
put multiple neural networks together I
mentioned that a little bit when we
separated people in a larger photo and
individuals in the photo and then do the
facial recognition on each person so one
network is used to separate them and the
next network is then used to figure out
who they are and do the facial
recognition applications still
undergoing research this is The Cutting
Edge you hear the term Pipeline and
there's actual in Python code and in
almost all the different neural network
setups out there they now have a
pipeline feature usually and it just
means you take the data from one neural
network and maybe another neural network
or you put it into the next neural
network and then you take three or four
other neural networks and feed them into
another one so how we connect the neural
networks is really just Cutting Edge and
it's so experimental I mean it's almost
creative in its nature there's not
really a science to it because each
specific domain pain has different
things is looking at so if you're in the
banking domain it's going to be
different than the medical domain then
the automatic car domain and suddenly
figuring out how those all fit together
is just a lot of fun and really cool so
we have our types of artificial neural
network we have our feed forward neural
network we have a radial basis function
neural network we have our cohenen
self-organizing neural network recurrent
neural network convolution neural
network and modular neural network where
it brings them all together and know the
colors on the brain do not match what
your brain actually does but they do
bring it out that most of these were
developed by understanding how humans
learn and as we understand more and more
of how humans learn we can build
something in the computer industry to
mimic that to reflect that and that's
how these were developed so exciting
part use case problem statement so this
is where we jump in this is my favorite
part let's use the system to identify
between a cat and a dog if you remember
correctly I say we're going to do some
python code and you can see over here my
hair is kind of sticking up over the
computer a cup of coffee on one side and
then a little bit of old school a pencil
and a pen on the other side yeah most
people Now take notes I love the
stickies on the computer that's great
that's that is my computer I have sticky
notes on my computer in different colors
so not too far from today's programmer
so the problem is is we want to classify
photos of cats and dogs using a neural
network and you can see over here we
have quite a variety of dogs in the
pictures and cats and you know just
sorting out it is a cat it's pretty
amazing and why would anybody want to
even know the difference between a cat
and a dog okay you know why well I have
a cat door it'd be kind of fun that
instead of it identifying instead of
having like a little collar with a
magnet on it which is what my cat has
the door would be able to see oh that's
the cat that's our cat coming in oh
that's the dog we have a dog too that's
a dog I want to let in maybe I don't
want to let this other animal in because
it's a raccoon so you can see where you
could take this one step further and
actually apply buy this you could
actually start a little startup company
idea self-identifying door so this use
case will be implemented on python I am
actually in Python 3.6 it's always nice
to tell people the version of python
because that does affect sometimes which
modules you load and everything and
we're going to start by importing the
required packages I told you we're going
to do this in Cross so we're going to
import from Karas models sequential from
the cross layers conversion 2D or conv2d
Max pooling 2D flatten and dense and
we'll talk about what each one of these
do in just a second but before we do
that let's talk a little bit about the
environment we're going to work in and
you know in fact let me go ahead and
open a the website cross's website so we
can learn a little bit more about Karas
so here we are on the cross website and
it's a k-e-r-a-s DOT IO that's the
official website for Karas and the first
thing you'll notice is that cross runs
on top of either tensorflow
cntk and I think it's pronounced thano
or theano what's important on here is it
tensorflow and the same is true for all
these but tensorflow is probably one of
the most widely used currently packages
out there with the cross and of course
you know tomorrow this is all going to
change it's all going to disappear and
they'll have something new out there so
make sure when you're learning this code
that you understand what's going on and
also know the code I mean look when you
look at the code it's not as complicated
once you understand what's going on the
code itself is pretty straightforward
and the reason we like Karas and the
reason that people are jumping on it
right now is such a big deal is if we
come down here let me just scroll down a
little bit let me talk about user
friendliness modularity easy
extensibility work with python Python's
a big one because a lot of people in
data science now use Python although you
can actually access cross other ways
because if we continue down here is
layers and this is where it gets really
cool when we're working with cross you
just add layers on remember those hidden
layers we were talking about and we
talked about the r e l u activate
station you can see right here let me
just up that a little bit in size there
we go that's big I can add in an relu
layer and then I can add in a soft Max
layer in the next instance we didn't
talk about soft Max so you can do each
layer separate now if I'm working in
some of the other kits I use I take that
and I have one setup and then I feed the
output into the next one this one I can
just add hidden layer after hidden layer
with a different information in it which
makes it very powerful and very fast to
spin up and try different setups and see
how they work with the data you're
working on and we'll dig a little bit
deeper in here and a lot of this is very
much the same so when we get to that
part I'll point that out to you also now
just a quick side note I'm using
Anaconda with python in it and I went
ahead and created my own package and I
called it the cross python 36 because
I'm in python36 Anaconda is cool that
way you can create different
environments real easily if you're doing
a lot of different experimenting with
these different packages probably want
to create your own environment in there
and the first thing is you can see right
here there's a lot lot of dependencies a
lot of these you should recognize by now
if you've done any of these videos if
not kudos for you for jumping in today
pip install numpy scipy the sci kit
learn pillow and h5py are both needed
for the tensorflow and then putting the
cross on there and then you'll see here
in PIP is just a standard installer that
you use with python you'll see here that
we did pip install tensorflow sensor
we're going to do Karas on top of
tensorflow and then pip install and I
went ahead and used the GitHub so git
plus git and you'll see here github.com
this is one of their releases one of the
most current release on there that goes
on top of tensorflow you can look up
these instructions pretty much anywhere
this is for doing it on Anaconda
certainly you'd want to install these if
you're doing it in Ubuntu server setup
you'd want to get I don't think you need
the H5 py in Ubuntu but you do need the
rest in there because they are
dependencies in there and it's pretty
straightforward and that's actually in
some of the instructions they have on
their website so you don't have to
necessarily go through this just
remember their website on there and then
when I'm under my anaconda Navigator
which I like you'll see where I have
environments and on the bottom I created
a new environment and I called it cross
python36 just to separate everything you
can say I have Python 30.5 and python
36. I used to have a bunch of other ones
but it kind of cleaned house recently
and of course once I go in here I can
launch my Jupiter notebook making sure
I'm using the right environment that I
just set up this of course opens up my
in this case I'm using Google Chrome and
in here I could go and just create a new
document in here and this is all in your
browser window when you use the Anaconda
do you have to use anaconda and Jupiter
notebook no you can use any kind of
python editor whatever setup you're
comfortable with and whatever you're
doing in there so let's go ahead and go
in here and paste the code in and we're
importing a number of different settings
in here we have import sequential that's
under the models because that's the
model we're going to use as far as our
neural network and then we have layer so
we have conversion 2D Max pooling 2D
flatten dense and you can actually just
kind of guess at what these do we're
talking we're working in a 2d photograph
and if you remember correctly I talked
about how the actual input layer is a
single array it's not in two Dimensions
it's one dimension all these do is these
are tools to help flatten the image so
it takes a two-dimensional image and
then it creates its own proper setup you
don't have to worry about any of that
you don't have to do anything special
with the photograph you let the cross do
it we're going to run this and you'll
see right here they have some stuff that
is going to be depreciated and changed
because that's what it does everything's
being changed as we go we don't have to
worry about that too much if you have
warnings if you run it a second time the
warning will disappear and this is just
imported these packages for us to use
Jupiter is nice about this that you can
do each thing step by step and I'll go
ahead and also zoom in there a little
control plus that's one of the nice
things about being in a browser
environment so here we are back another
sip of coffee if you're familiar with my
other videos you notice I'm always
sipping coffee I always have a my case
latte next to me and espresso so the
next step is to go ahead and initialize
we're going to call it the CNN or
classifier neural network and the reason
we call it a classifier is because it's
going to classify it between two things
it's going to be cat or dog so when
you're doing classification you're
picking specific objects you're specific
it's a true or false yes no it is
something or it's not so first thing
we're going to create our classifier and
it's going to equal sequential so
there's sequential setup is the
classifier that's the actual model we're
using that's the neural network so we
call it a classifier and the next step
is to add in our convolution and let me
just do a Let Me shrink that down in
size you can see the whole line and
let's talk a little bit about what's
going on here I have my classifier and I
add something what am I adding well I'm
adding my first layer this first layer
we're adding in is probably the one that
takes the most work to make sure you
have it set correct and the reason I say
that this is your actual input and we're
going to jump here to the part that says
input shape equals 64 by 64 by 3. what
does that mean well that means that our
picture is coming in and there's these
pictures remember we had like the
picture of the car was 128 by 128 pixels
well this one is 64 by 64 pixels and
each pixel has three values that's where
these numbers come from and it is so
important that this matches I mentioned
a little bit that if you have like a
larger picture you have to reformat it
to fit this shape if it comes in as
something larger there's no input notes
there's no input neural network there
that will handle that extra space so you
have to reshape your data to fit in here
now the first layer is the most
important because after that Karas knows
what your shape is coming in here and it
knows what's coming out and so that
really sets the stage most important
thing is that input shape matches your
data coming in and you'll get a lot of
Errors if it doesn't you'll go through
there in picture number 55 doesn't match
it correctly and guess what it does it
usually gives you an error and then the
activation if you remember we talked
about the different activations on here
we're using the relu model like I said
that is the most commonly used now
because one it's fast doesn't have the
added calculations in it it just says
here's the value coming out based on the
weights and the value going in and from
there you know it's uh if it's over one
then it's good or over zero it's good if
it's under zero then it's considered not
active and then we have this conversion
2D what the heck is conversion 2D I'm
not going to go into too much detail in
this because this has a couple of things
it's doing in here a little bit more in
depth and we're ready to cover in this
tutorial but this is used to convert
from the photo because we have 64 by 64
by 3 and we're just converting it to
two-dimensional kind of setup so it's
very aware that this is a photograph and
that different pieces are next to each
other and then then we're going to add
in a second convolutional layer that's
what the conv stands for 2D so these are
hidden layers so we have our input layer
and our two hidden layers and they are
two-dimensional because we're doing with
a two-dimensional photograph and you'll
see down here that on the left when we
add a Max pooling 2D and we put a pool
size equals 2 2. and so what this is is
that as you get to the end of these
layers one of the things you always want
to think of is what they call mapping
and then reducing wonderful terminology
from the Big Data we're mapping this
data through all these layers and now we
want to reduce it to only two sets in
this case it's already in two sets
because it's a 2d photograph but we had
you know two Dimensions by we actually
have 64 by 64 by three so now we're just
getting it down to a two by two just the
two Dimension two-dimensional instead of
having the third dimension of colors and
we'll go ahead and run these I'm not
really seeing anything on our run script
because we're just setting up this is
all set up and this is where you start
playing because maybe you'll add a
different layer in here to do something
else to see how it works and see what
your output is that's what makes cross
so nice is I can with just a couple
flips of code put in a whole new layer
that does a whole new processing and see
whether that improves my run or makes it
worse and finally we're going to do the
final setup which is to flatten
classifier add a flattened setup and
then we're going to also add a layer a
dense layer and then we're going to add
in another dense layer and then we're
going to build it we're going to compile
this whole thing together so let's flip
over and see what that looks like and
we've even numbered them for you so
we're going to do the flattening and
flatten is exactly what it sounds like
we've been working in a two-dimensional
array of picture which actually is in
three dimensions because of the pixels
the pixels have a whole another
dimension to it of three different
values and we've kind of resized those
down to two by two but now we're just
going to flatten it I don't want to have
multiple Dimensions being worked on by
tensor and by Karas I want just a single
array so it's flat flattened out and in
Step 4 full connection so we add in our
final two layers and you could actually
do all kinds of things with this you
could actually leave out this some of
these layers and play with them you do
need to flatten it that's very important
then we want to use the dense again
we're taking this and we're taking
whatever came into it so once we take
all those different the two Dimensions
or three dimensions as they are and we
flatten it to one dimension we want to
take that and we're going to pull it
into units of 128. they got that you can
see where did they get 128 from you
could actually play with that number and
get all kinds of weird results but in
this case we took the 64 plus 64 is 128.
you could probably even do this with 64
or 32 usually you want to keep it in the
same multiple whatever the data shape
you're already using is in and we're
using the activation the relu just like
we did before and then we finally filter
all that into a single output and it has
how many units one why because we want
to know whether true or false it's
either a dog or a cat you could say one
is dog zero is cat or maybe you're a cat
lover and it's one is cat and zero is
dog and if you love both dogs and cats
you're gonna have to choose and then we
use the sigmoid activation if you
remember from before we had the relu and
there's also the sigmoid a sigmoid just
makes it clear it's yes or no we don't
want a any kind of in-between number
coming out and we'll go ahead and run
this and you'll see it's still all in
setup and then finally we want to go
ahead and compile and let's put the
compiling our classifier neural network
and we're going to use the optimizer
atom and I hinted at this just a little
bit before where does atom come in where
does an Optimizer come in well the
optimizer is the reverse propagation
when we're training it it goes all the
way through and says error and then how
does it readjust those weights there are
a number of them atom is the most
commonly used and it works best on large
data most people stick with the atom
because when they're test testing on
smaller data see if their model is going
to go through and get all their errors
out before they run it on larger data
sets they're going to run it on Adam
anyway so they just leave it on atom
most commonly used but there are some
other ones out there you should be aware
of that that you might try them if
you're stuck in a bind or you might blow
that in the future but usually Adam is
just fine on there and then you have two
more settings you have loss and metrics
we're not going to dig too much into
loss or metrics these are things you
really have to explore Karas because
there are so many choices this is how it
computes the error there's so many
different ways to on your back
propagation and your training so we're
using the atom model but you can compute
the error by standard deviation standard
deviation squared they use binary cross
entropy I'd have to look that up to even
know what that is there's so many of
these a lot of times you just start with
the ones that look correct that are most
commonly used and then you have to go
read the cross site and actually see
what these different losses and metrics
and what different options they have so
we're not going to get too much into
them other than reference you over to
the Karas website to explore them deeper
but we are going to go ahead and run
them and now we've set up our classifier
so we have an object classifier and if
you go back up here you'll see that
we've added in step one we added in our
layer for the input we added a layer
that comes in there and uses the relu
for Activation and then it pulls the
data so this is even though these are
two layers the actual neural network
layer is up here and then it uses this
to pull the data into a two by two so
into a two-dimensional array from a
three-dimensional array with the colors
then we flatten it so there's our Adder
flatten and then we add another dense
what they call dense layer this dense
layer goes in there and it downsizes it
to 128 it reduces it so you can look at
this as we're mapping all this data down
the two-dimensional setup and then we
flatten it so we map it to a flattened
map and then we take it and reduce it
down to 128 and we use the relu again
and then finally we reduce that down to
just a single output and we use use a
sigmoid to do that to figure out whether
it's yes no true false in this case cat
or dog and then finally once we put all
these layers together we compile them
that's what we've done here and we've
compiled them as far as how it trains to
use these settings for the training back
propagation so if you remember we talked
about training our setup and when we go
into this you'll see that we have two
data sets we have one called the
training set and the testing set and
that's very standard in any data
processing is you need to have that's
pretty common in any data processing is
you need to have a certain amount of
data to train it and then you got to
know whether it works or not is it any
good and that's why you have a separate
set of data for testing it where you
already know the answer but you don't
want to use that as part of the training
set so in here we jump into part two
fitting the classifier neural network to
the images and then from Cross let me
just zoom in there I always love that
about working with Jupiter notebooks you
can really see we're going to come in
here we do the cross pre-processing and
image and we import image data generator
it's so nice of Karas it's such a
high-end product right now going out and
since images are so common they already
have all this stuff to help us process
the data which is great and so we come
in here we do train data gen and we're
going to create our object for helping
us trainer for reshaping the data so
that it's going to work with our setup
and we use an image data generator and
we're going to rescale it and you'll see
here we have one point which tells it
it's a float value on the rescale over
255. where does 255 come from well
that's the scale in the colors of the
pictures we're using they're value from
0 to 255. so we want to divide it by 255
and it'll generate a number between 0
and 1. they have Shear range and zoom
range horizontal flip equals true and
this of course has to do with if the
photos are different shapes and sizes I
guess it's a wonderful package you
really need to dig in deep to see all
the different options you have for
setting up your images for right now
though we're going to stick with some
basic stuff here and let me go ahead and
run this code and again it doesn't
really do anything because we're still
setting up the pre-processing let's take
a look at this next set of code and this
one is just huge we're creating the
training set so the training set is
going to go in here and it's going to
use our train data gen we just created
dot flow from directory that's going to
access in this case the path data set
training set that's a folder so it's
going to pull all the images out of that
folder now I'm actually running this in
the folder that the data sets in so if
you're doing the same setup and you load
your data in there and you're doing this
make sure wherever your Jupiter notebook
is saving things to that you create this
path or you can do the complete path if
you need to you know C colon slash Etc
and the target size the batch size and
class mode is binary so the classes
we're switching everything to a binary
value back size what the heck is batch
size well that's how many pictures we're
going to batch through do the training
each time and the target size 64 by 64 a
little confusing but you can see right
here that this is just a general
training and you can go in there and
look at all the different settings for
your training set and of course with
different data we're doing pictures
there's all kinds of different settings
depending on what you're working with
let's go ahead and run that and see what
happens and you'll see that it found 800
images belonging to one classes so we
have 800 images in the training set and
if we're going to do this with the
training set we also have to format the
pictures in the test set now we're not
actually doing any predictions we're not
actually programming the model yet all
we're doing is preparing the data so
we're going to prepare a training set
and the test set so any changes we make
to the training set at this point also
have to be made to the test set so we've
done this thing we've done a train data
generator we've done our training set
and then we also have remember our test
set of data so I'm going to do the same
thing with that I'm going to create a
test data in and we're going to do this
image data generator we're going to
rescale 1 over 255 we don't need the
other settings just the single setting
for the test data gen and we're going to
create our test set we're going to do
the same thing we did with the test set
except that we're pulling it from the
test set folder and we'll run that and
you'll see in our test set we found 2
000 images that's about right we're
using 20 of the images as test and
eighty percent to train it and then
finally we've set up all our data we've
set up all our layers which is where all
the work is is cleaning up that data
making sure it's going in there
correctly and we are actually going to
fit it we're going to train our data set
and let's see what that looks like and
here we go let's put the information in
here and let's just take a quick look at
what we're looking at with our fit
generator we have our classifier DOT fit
generator that's our back propagation so
the information goes through forward
with a picture and it says oh you're the
right or you're wrong and then the error
goes backward and read programs all
those weights so we're training our
neural network and of course we're using
the training set remember we created the
training setup here and then we're going
steps per epic so it's 8 000 steps epic
means that that's how many times we go
through all the pictures so we're going
to rerun each of the pictures and we're
going to go through the whole data set
25 times but we're going to look at each
picture during each epic 8 000 times so
we're really programming the heck out of
this and going back over it and then
they have validation data equals test
set so we have our training set and then
we're gonna have our test set to
validate it so we're going to do this
all in one shot and we're going to look
at that and they're going to do 200
steps for each validation and we'll see
what that looks like in just a minute
let's go ahead and run our training here
and we're going to fit our data and as
it goes it says epic one of 25. you
start realizing that this is going to
take a while on my older computer it
takes about 45 minutes I have a dual
processor we're processing uh 10 000
photos that's not a small amount of
photographs to process so if you're on
your laptop which I am it's going to
take a while so let's go ahead and go
get our cup of coffee and a sip and come
back and see what this looks like so I'm
back you didn't know I was gone I was
actually a lengthy pause there I made a
couple changes and let's discuss those
changes real quick and why I made them
so the first thing I'm going to do is
I'm going to go up here and insert a
cell above and let's paste the original
code back in there and you'll see that
the original thing was steps per epic 8
000 25 epics and validation steps 2000
and I changed these to four thousand
epics or four thousand steps per epic 10
epics and just 10 validation steps and
this will cause problems if you're doing
this as a commercial release but for
demo purposes this should work and if
you remember our steps per epic that's
how many photos we're going to process
in fact let me go ahead and get my
drawing pin out and let's just highlight
that right here we have 8 000 pictures
we're going through so for each epic I'm
going to change this to four thousand
I'm going to cut that in half so it's
going to randomly pick 4 000 pictures
each time it goes through an epic and
the Epic is how many processes so this
is 25 and I'm just going to cut that to
10. so instead of doing 25 runs through
8 000 photos each which you can do the
math of 25 times 8 000. I'm only going
to do ten through four thousand so I'm
going to this forty thousand times
through the processes and the next thing
I know that you'll you'll want to notice
is that I also change the validation
step and this would cause some major
problems in releasing because I dropped
it all the way down to 10. what the
validation step does is it says we have
2 000 photos in our trainings or in our
testing set and we're going to use that
for validation well I'm only going to
use a random 10 of those to validate so
not really the best settings but let me
show you why we did that let's scroll
down here just a little bit and let's
look at the output here and see what
that what's going on there so I've got
my drawing tool back on and you'll see
here it lists a run so each time it goes
through an epic it's going to do 4 000
steps and this is where the 4000 comes
in so that's where we have we have epic
one of ten four thousand steps is
randomly picking half the pictures in
the file and going through them and then
we're going to look at this number right
here that is for the whole epic and
that's
2411 seconds and if you remember
correctly you divide that by 60 you
minutes if you divide that by 60 you get
hours or you can just divide the whole
thing by 60 times 60 which is 3600 if
3600 is an hour this is roughly 45
minutes right here and that's 45 minutes
to process half the pictures so if I was
doing all the pictures we're talking an
hour and a half per epic times 36 or no
25 they had 25 up above 25. so that's
roughly a couple days a couple days of
processing well for this demo we don't
want to do that I don't want to come
back the next day plus my computer did a
reboot in the middle of the night so we
look at this and we say okay let's we're
just testing this out my computer that
I'm running this on is a dual core
processor runs 0.9 gigahertz per second
for a laptop you know it was good about
four years ago but for running something
like this it's probably a little slow so
we cut the times down and the last one
was validation we're only validating it
on a random 10 photos and this comes
into effect because you're going to see
down here where we have accuracy value
loss value accuracy and loss those are
very important numbers to look at so the
10 means I'm only validating across 10
pictures that is where here we have
value this is acc's for accuracy value
loss not going to worry about that too
much and accuracy now accuracy is while
it's running it's putting these two
numbers together that's what accuracy is
and value accuracy is at the end of the
Epic what's our accuracy into the Epic
what is it looking at in this tutorial
we're not going to go so deep but these
numbers are really important when you
start talking about these two numbers
reflect bias that is really important
let me just put that up there and bias
is a little bit beyond this tutorial but
the short of it is is if this accuracy
which is being our validation per step
is going down and the value accuracy
continues to go up that means there's a
bias that means I'm memorizing the
photos I'm looking at I'm not actually
looking for what makes a dog a dog what
makes a catacat I'm just memorizing them
and so the more this discrepancy grows
the bigger the bias is and that is
really the beauty of the Cross neural
network it is a lot of built-in features
like this that make that really easy to
try
so let's go ahead and take a look at the
next set of code so here we are into
part three we're going to make a new
prediction and so we're going to bring
in a couple tools for that and then we
have to process the image coming in and
find out whether it's an actual dog or
cat we can actually use this to identify
it and of course the final step of part
three is to print prediction we'll go
ahead and combine these and of course
you can see me there adding more sticky
notes to my computer screen hidden
behind the screen and you know last one
was don't forget to feed the cat and the
dog
so let's go ahead and take a look at
that and see what that looks like in
code and put that in our Jupiter
notebook all right and let's paste that
in here and we'll start by importing
numpy as NP numpy is a very common
package I pretty much imported on any
python project I'm working on another
one I use regularly is pandas they're
just ways of organizing the data and
then NP is usually the standard in most
machine learning tools as a return for
the data array although you know you can
use the standard data array from Python
and we have cross pre-processing import
image dish that all look familiar
because we're going to take a test image
and we're going to set that equal to in
this case cat or dog one as you can see
over here and you know let me get my
drawing tool back on so let's take a
look at this we have our test image
we're loading and in here we have test
image one and this one has a data hasn't
seen this one at all so this is all new
oh let me shrink the screen down let me
start that over so here we have my test
image and we went ahead and the cross
processing has this nice image set up so
we're going to load the image and we're
going to alter it to a 64 by 64 print so
right off the bat we're going to cross
this nice that way it automatically sets
it up for us so we don't have to redo
all our images and find a way to reset
those and then we use all those to set
the image to an array so again we're all
in pre-processing the data just like we
pre-processed before with our test
information and our training data and
then we use the numpy here's our numpy
that's uh from our right up here
important numpy as in p expand the
dimensions test image axes equal zero so
it puts it into a single array and then
finally all that work all that
pre-processing and all we do is we run
the result we click on here we go result
equals classifier predict test image and
then we find out well what is the test
image and let's just take a quick look
and just see what that is and you can
see when I ran it it comes up dog and if
we look at those images there it is cat
or dog image number one that looks like
a nice floppy eared lab friendly with
his tongue hanging out it's either that
or a very floppy eared cat I'm not sure
which but Corridor software says it's a
dog and uh we have a second picture over
here let's just see what happens we run
the second picture we can go up here and
change this from dog image one to two
we'll run that and it comes down here
and says cat you can see me highlighting
it down there as cat so our process
works you are able to label a dog a dog
and a cat a cat just from the pictures
there we go cleared my drawing tool and
the last thing I want you to notice when
we come back up here to when I ran it
you'll see it has an accuracy of one and
the value accuracy of one well the value
accuracy is the important one because
the value accuracy is what it actually
runs on the the test data remember I'm
only testing it on I'm only validating
it on random 10 photos and those 10
photos just happen to come up one now
when they ran this on the server it
actually came up about 86 percent this
is why cutting these numbers down so far
for a commercial release is bad so you
want to make sure you're a little
careful of that when you're testing your
stuff that you change these numbers back
when you run it on a more Enterprise
computer other than your old laptop that
you're just practicing on or messing
with and we come down here and again you
know we had the validation of cat and so
we have successfully built a neural
network that could distinguish between
photos of a cat and a dog imagine all
the other things you could distinguish
imagine all the different Industries you
could dive into with that just being
able to understand those two differences
of pictures what about mosquitoes could
you find the mosquitoes that bite versus
the mosquitoes that are friendly it
turns out the mosquitoes that bite us
are only four percent of the mosquito
population if even that maybe two
percent there's all kinds of industries
that use this and there's so many
industries that are just now realizing
how powerful these tools are just in the
photos alone there is a myriad of
Industries sprouting up and I said it
before I'll say it again what an
exciting time to live in with these
tools and that we get to play with hey
there simple and brings your master's
program in artificial intelligence
created in collaboration with IBM to
learn more about this course you can
find the course Link in the description
box below what is deep learning again
this video is not about deep learning
but there are other videos we have
created in detail about what is deep
learning in this video we'll just touch
upon the basics so that that's like a
nice segue into tensorflow so deep
learning is in a way a subset of machine
learning and we use primarily neural
networks in deep learning and the
underlying technology behind artificial
intelligence is deep learning and here
we teach them how to recognize let's say
images or voice and so on and so forth
so it is a learning mechanism but here
unlike traditional machine learning the
data is far more complicated and far
more unstructured like it could be
primarily in the form of images or audio
files or text files and one of the core
components of deep learning is neural
network and the neural network somewhat
looks like this there is something known
as an input layer and then there is an
output layer and in between there are a
bunch of hidden layers so typically
there would be at least one hidden layer
and anything more than one hidden layer
is known as a deep neural network so any
neural network with more than three
layers all together right is known as a
deep neural network all right so what
are the functions of the various layers
let's take a quick look so the input
layer accepts the input so this could be
in the form of let's say if it is an
image it could be the pixel values of
the images so that's what the input
layer does and then it passes on to the
hidden layers and the hidden layers in
turn perform certain computations and
they have what is known as as a part of
the training they have these weights and
bias that they keep updating till the
training process is complete and each
neuron has multiple weights and there
will be one bias and these are like
variables and we will see when we go
into the tensorflow code what we
actually mean by that and so that's what
the hidden layer does it does a bunch of
computation and passes its values to the
output layer and then the output layer
in turn gives the output it could be in
the form of a class so for example if we
are doing classification it tells us
which class a particular image maybe
belongs to for example let's say if this
is a image classification application
then the input could be a bunch of
images of maybe cats and dogs and the
output will be like it will say okay if
this is activated this gives a zero and
this gives a 1 that means it is a cat if
this gives a 1 and this gives a zero
that means it is a dock so that is a
kind of a binary classification and that
can be extended with multiple neurons on
the output side to have many more
classes for example or it can also be
used for regression as well not
necessarily only classification again
since this video is not about deep
learning or machine learning or neural
network we will probably not go into a
lot of details but you can check other
videos where we have given a lot more
details about neural networks and deep
learning and so on so in order to
develop a deep learning application how
do you go about primarily there are two
or three components that are required in
order to develop deep learning
application you need obviously a
programming language so typically python
is used and that's what we are going to
use in this particular video but you can
also use other languages like Java or C
plus plus and so on and there are some
libraries that are readily available and
for primarily for doing machine learning
and deep learning programming so these
are a list of libraries these are by no
means the exhaustive list but some of
the most common ones like Keras thiano
tensorflow and so on and so forth
tensorflow has nowadays become very very
popular this is developed by Google and
it is an open source library and keros
was there before now Keras has actually
now become a part of tensorflow as well
so it is one player about tensorflow so
in that sense they're well integrated
It's a combination of Keras and
tensorflow is pretty good then of course
you have a torch and dl4j and so on and
so forth so there are multiple libraries
but this video is about tensorflow and
we will be focusing on tensorflow what
are the benefits of tensorflow and what
are its components and our towards the
end we will show you a code in Python
we've written a code in Python by the
way tensorflow can be used with multiple
languages it supports multiple languages
though python is by far the most popular
language so let's take a look at what
exactly is tensorflow and why we are so
excited about tensorflow so tensorflow
offers apis now we can earlier without
when these libraries none of these
libraries were there even then we were
doing people were doing in machine
learning and deep learning and so on but
the coding mechanism was much more
complicated what these Library select
tensorflow offer is they provide kind of
a high level API so that we don't have
to go really deep into writing all the
stuff that is required let's say to
prepare a neural network and to even
configure or even to program a neuron
and so on right so these are done by the
library so all you need to do is they
offer a higher level API you need to use
that API and call that API and maybe
pass the data and that would pretty much
it's much easier rather than actually
going down and writing everything by
yourself so tensorflow that way it
offers apis for to write your code in
python or even C plus plus and and so on
other languages Java as well it has an
integration with r as well apparently
okay and it supports CPUs as well as
gpos now deep learning applications are
very compute intensive especially the
training process success needs a lot of
computation it takes very long as you
can imagine because the the data size is
large and there are so many iterative
processes there are so much of
mathematical calculations matrix
multiplication and so on and so forth so
for that if you perform these activities
on a normal CPU typically it would take
much longer but gpus are graphical
processing units you must have heard
gpus in the context of games and so on
because where you need the screen needs
to be of high resolution and the images
need to be of high resolution and so on
so gpus there as the name such as
graphical Processing Unit were
originally designed for that but since
they are very good at handling this kind
of iterative calculations and so on now
they are kind of they are being used or
leveraged rather for doing or developing
deep learning applications and
tensorflow supports gpus as well as CPUs
so I think that's one of the major
advantages of tensorflow as well now
again what is exactly tensorflow it's a
open source Library very developed by
Google and open source and primarily for
deep learning development but tensorflow
also supports traditional machine
learning by the way so if you want to do
some traditional machine learning we can
do it however it is probably a bit of an
overhead to use tensorflow for doing
traditional machine learning and this is
really good for performing deep learning
activities and again if you want to get
into more details about what's the
difference between machine learning and
deep learning there is another video
about it you can probably take a look at
that video what else it is developed
originally for large numerical
computations so originally when
tensorflow was developed they never
thought of it as a keeping deep learning
in mind but ultimately it so happened
that it's really very good for deep
learning development and therefore
Google has open sourced it and
tensorflow as the name suggests the data
is in the form of what is known as
tensors these are like multi-dimensional
arrays and they are very handy in
handling large amounts of data and we
will see that as well as we move forward
and then of course the execution
mechanism is in the form of graphs so
that makes it much easier to execute
this code in a distributed manner across
a cluster of computers and and also
using gpus and so on and so forth right
so that's a quick overview about what is
tensorflow we will see a little bit more
detail the two major components that is
basically the tensors and the graphs
let's take a look at what they are so
what are tensors tensor is as I
mentioned earlier it is like a
multi-dimensional array in which the
data is stored now when we are doing
deep learning especially the training
process you will have large amounts of
data and the data is in typically in a
very complicated format and it really
helps when you're able to put this use
this or store it in a compact way and so
tensors actually offer a very nice and
compact way of storing the data handling
the data during computation this is not
really for storing on your hard disk or
things like that but in memory when you
are doing that computation tenses are
really really very handy in terms of
keeping the data compact because they
are like multi-dimensional arrays so the
data is stored in tensors and then it is
fed into the neural network and then you
get the output all right so there are
some terms associated with tensors let's
get ourselves familiarized one is the
dimension and another is the rank so
what is dimension typically Dimension is
like the number of elements in a way so
for example this is a five by four
dimension tensor and then you can have
again multi Dimensions right so you can
this can be like three by three by three
so that is uh the dimension and then you
have ranks so what are tensor ranks
ranks are basically traditionally we
would have thought of as Dimensions that
is actually in this case it is called
rank so it becomes easier when we see
examples so a tensors rank is supposed
to be zero when there is only one
element we also call this as scale so
just one element and this is not really
a vector it's just an element like 200
it's also known as a scalar so such a
tensor is supposed to be having a rank
of zero then you have let's say one
dimensional array this is a vector with
a row of elements this has rank of one
now if you have traditionally what we
called as a two-dimensional like a
matrix for example then the rank is 2
and in this case the rank is three and
it can have more ranks as well as I
mentioned it is like a multi-dimensional
array so you can have rank five six and
so on okay so those are the
terminologies in tensorflow terms
dimensions and ranks so this is just to
make sure that we are taking the same
language so whenever we talk about the
rank of a tensor you understand what
exactly is meant by that now in addition
to tensors in which the data is actually
stored all right so the data is stored
in the tensors and then once you have
the data there is a a computation that
needs to be done now the computation
happens in the form of graphs so what we
typically in a tensorflow program what
we do is it's not like traditional
programming where you just write a bunch
of lines and then everything gets
executed in sequence here we prepare
graphs various nodes and then these are
executed in the form of a session and
they use the data from these tensors now
I know this is a slightly New Concept
for a lot of you so it may be a little
difficult to probably understand in the
first cut but when we look at the code
when we go into the tutorial The Code
walkthrough I think that time it will
become much clearer as well but to start
with just uh we need to keep in mind
that we have to first prepare a graph
and when you're preparing the graph none
of the code is actually getting executed
you write the code to prepare the graph
and then you execute that graph so
that's the way by creating a session
that's the way tensorflow program works
so and each of these computation is
represented as what is known as a data
flow graph and we will also see that
whenever you start a tensorflow when you
create an object tensorflow object there
will be what is known as a default graph
and then if required I know probably in
the beginning it may not be required but
in more advanced programming you can
actually have multiple graphs instead of
the default graph you can create your
own graph and have multiple graphs and
use it as well but there is always
whenever you create a tensorflow object
there will be a default graph and this
will be very nicely Illustrated in the
example code that we will take to
explain this so there it becomes much
clearer than in these slides so the
graph gets executed and it processes all
the data that we are feeding all the
external data will be fed in the form of
what is known as placeholders and then
you have variables and constants again
this will also become clear when we take
a look at the code and once you have the
graph then the execution can be enabled
either on regular CPUs or on gpus and
also in a distributed mode so that the
processing becomes much faster as I
mentioned the training of the models in
deep learning takes extremely long
because of the large amount of data and
therefore using tensorflow actually
makes it much easier to write the code
for gpus or CPUs and then execute it in
a distributed manner so this is how the
tensorflow program looks so there is a
you need to build a computational graph
that's the first step and then you
execute that graph so the first step is
to write the code for repairing a graph
and then you create what is known as a
session and then in that session you ask
the session to execute this graph so
this will again become much clearer when
we look at the code as some of you may
be aware it's not that easy to set up
the tensorflow environment there are
several components there are several
possibilities for example you can set up
on Windows you can set upon Ubuntu now
Ubuntu has multiple versions which
version to use and then you have python
which release of python to use whether
to do a pip install whether to do
install using anaconda and how do you
then link it up with jupyter notebook
these are multiple possibilities and it
takes up a lot of time to try all of
these so today what I'm going to do is
show you a tried and tested method of
setting up the tensorflow environment
and this will help primarily those who
are starting with tensorflow so that
they don't have to waste so much time on
setting up the environment in
experimenting with the insulation and
setting up of the environment now what
we are going to do is I will show you a
method by which you you know it is a
tried and tested method and of course
then flow home page has a install page
and it shows you some ways to install
but again the challenge is the same
there are multiple versions multiple
methods shown there so it's highly
confusing for somebody who is new as to
decide which one which path to take so
in today's session what we are going to
do is we will set up tensorflow on
Ubuntu and I'm going to show you in a
watchful box but then if you are using a
laptop with Ubuntu installed you can
straight away use the same method
however we need to keep one thing in
mind that the various releases and
versions of Ubuntu and Python and then
tensorflow not all of them are
compatible with each other so these
versions and releases need to be very
specific so I will tell you which is the
version and releases of what combination
is best suited for you to get started
and later on of course you can then
experiment with other possibilities and
other releases and so on once you get
familiar with tensorflow to start with I
would also like to mention that it is a
good idea to install or start with
Ubuntu environment or a Linux any other
Linux also but here we will focus on
Ubuntu rather than Windows so for those
who are already let's say using a
Windows system the question may arise
what do we do but there is an easy
option as you can see I am actually
using a virtual box so you need to
install virtualbox let me just show you
so this is the virtualbox Oracle via
virtualbox and there are tons of videos
on YouTube how to install a virtualbox
and how to create Ubuntu image I think
we will not spend time on that but if
you are using Windows and my preference
would be to set up a virtual box and set
up your environment in Ubuntu image so
we will start by assuming that you have
an Ubuntu environment especially release
14.04 LTS there are multiple one two
versions and again we will not try to
get the latest and the greatest versions
or latest and greatest releases but the
focus here is to take the releases and
versions which are working and where you
will not waste time so the setup process
will be smooth if you stick to these
releases you can of course experiment
later on with other versions and try out
but here we will be working with Ubuntu
1404 LTS and we will use Python 3.4 and
we will use tensorflow 1.5 this is a
tried and tested combination and I would
also recommend that you use the same if
you want a smooth start and so let's get
started with that let me log in to my
Ubuntu system okay so we have the Ubuntu
system running here now if you go to
tensorflow.org there is a page which
mentions how to install tensorflow and
as you can see there are multiple
possibilities you have Ubuntu you have
Windows and Mac OS and so on and so
forth and if you go to Ubuntu for
example further you will get multiple
options whether you want CPU or GPU and
whether you want to do a pip install or
using a virtual native pip virtual EnV
anaconda and so on and so forth so all
these options are very complicated or
rather very confusing I would say not
complicated depending on whether you're
expert or of course I am talking about
beginners here but if you click on some
of these options they may look very easy
so for example if we go back and if you
select for example native pip it may
appear like oh this is just you know one
single or two steps and that's about it
everything gets installed you see here
it should be just one step install it
you say bit three install tensorflow and
everything gets installed unfortunately
it doesn't work that way so it's not as
easy as so don't get kind of fooled by
the Simplicity of the of the
documentation here again it's not their
fault because of the multiple
combinations of releases and so on and
so forth it is not that easy so what we
will do is we will take a slightly
roundabout method which is using
Anaconda which has a few more steps but
you're sure that this is going to work
so that is what we are going to do and
that's what I am going to show you so
what are the steps involved of course I
will not go exactly by what they have
mentioned here as I said I will show you
the steps which are again Sure Shot to
work whereas uh here again if you follow
just this document there will be some
variations which they have kind of not
documented so that's the reason I will
show you the steps separately all right
so these are the the main four steps you
need to download and install anaconda
and then create a virtual environment
with python and release as I mentioned
is Python 3.4 and we will install
tensorflow version 1.5 and then we will
install and configure Jupiter which will
be our development environment all right
so let's get started this is our Ubuntu
and let's get a terminal started here
and
okay yes so
all right so we started the terminal and
then what we'll also do is we'll go to
the Anaconda website because we need to
install Anaconda we need to download and
install Anaconda so for that you need to
go to the Anaconda website so you can
just open a browser and
Google and you will find Anaconda
website
so this is the site
anaconda.org click on this link and it
will take you to the Anaconda website
you don't have to sign up or anything
like that just look for the download
Anaconda
so just click on that it will take you
to the download pages and it
automatically recognizes that you are on
Ubuntu so it will show you the links to
download all right so as you can see it
has recognized that you're on a Linux
operating system so you can just click
on this download and down
it will start the download there are two
versions of course python with python
3.6 and 2.7 I recommend you start with
python 3.6 now just want to clarify that
we will be actually using Python 3.4
this is just for the initial
installation but subsequently when we
set up the virtual environment you will
see there is one more step where we set
up the virtual environment there we will
actually be using Python 3.4 so just
that you are not confused all right so
it has started the download we just say
no thanks for this and it is downloading
while it is downloading you can click on
this link saying how to install Anaconda
so there are a couple of steps mentioned
there that we will be using so let me
just in the meanwhile click on this for
some reason the network is a little slow
so it's taking time all right so once
the download is done we will be using
the options that are mentioned here now
the first step of course is we we are
already doing the first step which is
downloading the installer for Linux the
second step is not mandatory so it's an
optional step most often you can
actually skip that and third step is
what we are going to do and since we are
using at this point we are using python
3.6 we should use this command so
basically the terminal we have opened
here is is to use or to run this command
that's the reason I open this terminal
however make sure that the download is
complete before you run this command so
we'll just wait for a couple of minutes
and we'll come back once the download is
done all right so as you can see the
download is done this is a fairly large
file so if you are on a slow internet or
low internet bandwidth then it might
take quite a while it's about
578 MB and so this is the file now what
you need to do is you need to go back to
this installation steps and let me
minimize this and
so this is the command that you need to
run so you can just directly copy and
paste this command from here
okay so I do copy right Mouse like
now it will ask you a bunch of questions
as documented here so most of them you
need to just say enter or yes and that's
about it except for the last step I will
just show you what I mean and here you
need to do press enter multiple times
just to make sure you agree to all these
agreement uh the license agreement and
so on and so forth so once you do
multiple
enters it will bring you to the next
step
all right so now here again they say do
you accept the terms you just say yes
and then press enter and here it will
ask you a couple of questions and pretty
much you need to just go for the default
version press enter to confirm you just
say press enter and this is primarily
when it will pretty much start the
installation process of anaconda this
might take a little while depending
again on your internet speed so you need
to have some patience we will also
probably come back once this
installation is done or if I if it asks
for any further questions
all right so here you'll get again one
more question leave us to wish the
installer to pre-pen so you just say yes
for this question and
keep going now the last question
I think we are pretty much at the end
and that is about Microsoft
VSS I guess yeah vs code so for this you
can just say no because we will not be
using this and that's it you're done so
this is a completion of installation of
anaconda so it's always a good idea to
exit and start a fresh terminal okay so
let's start again terminal
so we are done with the installation of
Anaconda the next step is to create a
virtual environment with Python 3.4 so
for that this is the command conda
create dash n and this is the name of
your virtual environment you can give
any name but for easy reference I have
given as tensorflow and we have to
specify the python version as I
mentioned earlier we will be using the
combination of Python 3.4 and tensorflow
1.5 now this is not the latest version
python has probably 3.6 at this point at
the time of creation of this video and
even tensorflow probably has the latest
version as 1.7 but then these
combinations sometimes may not work and
I found that after several trials and
errors 3.4 with 1.5 seems to be most
reliable and that's the reason I have
chosen this I would recommend you to try
with this first if you are a beginner
and later on maybe you can try out
other permutations and combinations all
right one step I just wanted to show you
is before we do the installation or
creation of the virtual environment if
you want to just clarify or confirm
whether Anaconda has been installed or
not you can run what is known as
Anaconda Navigator so just say Anaconda
Navigator
okay we need to edit this part hold on
one second I guess it is
all right so you see here when you start
Anaconda Navigator this
Anaconda Navigator will open up that is
the indication that Anaconda has been
installed properly so that's just a
quick check and you don't have to do
anything just you can go back and close
it once this comes up and then in the
meanwhile let me just open one more
terminal
for the creation of the virtual
environment and and
so this is your anaconda Navigator and
you can just say okay don't show me let
me just say okay and
um
then you can close this
and say file you can do a file exit or
you can click on the close button
whichever
all right so we will exit from here
we have a terminal open here we will use
this for our next steps all right so
what we have to do now is Type in this
piece off
code or this command
for some reason copy paste is not
working so it's a small command so it
shouldn't be that click of an issue so
we say conda
and we say
create
so this is the command for creating a
new environment slash
dash n and then we give the name of the
environment so we will we can give any
name for uh
convenience I'm just calling it
tensorflow but you can actually name it
anything and then we need to specify
with which version of python so we say
pip
Python and then that is equal
3.4 okay so it will create an
environment this will take a little
while it'll ask you a question just say
yes and then it will be done
so this is just a warning you can simply
ignore that
and for the question that I asked you
just say yes and I think you should be
good
all right so it says do you want to
proceed you say yes then enter
this will again take a little bit a
little time so we will probably move
forward and then come back once this is
done
all right so the environment has been
created now it's always a good idea to
whenever some of these steps get over or
each step gets over you just exit the
terminal and start afresh with a new
terminal for some reason sometimes it
causes problem if you continue so I
found that it is a safe practice too
each time exit the terminal and start
afresh now that we have an environment
created we can by the name tensorflow
you can enter that environment by
calling the command Source activate
tensorflow or the name whatever name you
have given so you'll see here once you
do that you will get this the name of
the environment will be shown here so it
is like your own again a special
environment within within the system and
uh from here onwards you can do other
stuff like actually installing
tensorflow and so on and you will see
that it has installed Python 3 here so
3.4 right so it is installed that's what
we during creation of this environment
we wanted Python 3.4 so that is what it
is showing but we are not yet done so we
need to still install tensorflow so in
this environment you need to run a
command which will actually install the
crit combination of tensorflow now this
is as you can see it's a slightly
complicated command with install
dash dash ignore instance so what we can
do is from within here we can go and go
to tensorflow install page and there is
a sample code there you can pick up from
there so that you don't have to type in
so much so we say tensorflow
tensorflow let's dark and I'll also open
up probably a notepad here so that we
can construct that command and go here
is a Ubuntu if you come further down
there will be by the way we are doing
CPU installation I think
clear we are not there is a possibility
to do GPU as well we will probably
create a separate video for that and if
we go here yeah so again you probably if
you follow the entire document out here
it may still not work so I do not
recommend that for now at least for
beginners but just to copy this command
because this is a fairly complicated
command so this is much easier to copy
it from here that's the reason I'm on
this page
and we will then modify it according to
yes okay let's see
copy
and don't do it directly here because
that is probably not yet the right
command let us go and create a notepad
page
empty document I'll just say TF Dot txt
okay this is just a temporary file what
we need to do here is we need to adjust
this command to suit the versions that
we are installing here so let me just
explain what it is what you need to do
so first thing you can get rid of this
one yes okay now here these parts you
can simply ignore here you need to go
and change it to version 1.5
and I think we are good with that okay
so the cp34 indicates your Python 3.4 so
later on when you're trying to
experiment and if you want to do other
combinations of tensorflow and python
version then you can change this for
example you can change this to 3 6 both
these places you need to change this to
3 6 if you are using with python 3.6 and
similarly if you want the latest version
which is of tensorflow which is 1.7 you
need to change this to 1.7 and so on but
at this point to start with if you are a
beginner I would recommend that you
stick to this tensorflow 1.5 with Python
3.4 this is tried and test it and it
works okay just word of course question
that just by changing these versions the
whole thing will not work there may be
other places where you may have to some
of the steps may change so just a word
of caution that just by changing this if
you want to try out with a different
version some of the steps may also
change so you don't think that just by
changing this just one particular
command you will be uh fine okay so
that's been my experience so to start
with I would recommend use this
particular combination all right so it
looks good uh tensorflow 1.5 and let me
just reconform with my notes and I think
it looks good so this is install ignore
1.5 cp34 cp34 and so on okay
okay so let's hit enter
it started the process again this might
take a little while so we will pause the
recording and then get back when the
installation is done okay so looks like
this installation is done and as always
let us
exit and then come back now you're
currently in the environment virtual
environment
um and it's a good practice before you
exit out of the terminal to come out or
out of the virtual environment so just
like you did Source activate you need to
do Source deactivate
and the name tensor
flow okay we will come back to the
original command prompt and then from
here you can do an exit now we need to
validate whether the tensorflow
installation has gone through properly
or not so the way to validate is to
first start Python and then import
tensorflow Library see if it Imports
without error so that's the best way to
validate so we'll open a terminal
and whenever you want to get into the
environment this is how you have to do
come to environment and then say source
and obviously you can also use existing
so Source activate
tensorflow okay so we do this and you
need to run python from here remember
you should say Python 3 because you have
installed Python 3.4 now here when you
say import
tensorflow
SDF when you get back up prompt like
this without any errors that means
tensorflow installation has been
successful if tensorflow was not
installed properly you will get an error
saying tensorflow is not available our
module doesn't exist or something like
that so tensorflow installation is done
but now we will go through the process
of installing jupyter notebook so that
you can do your development so let me
just exit and in order to install your
jupyter notebook which is your which is
going to be your development environment
we will first install IPython using
conda so conda install IPython and then
pip install Jupiter so let's first do
the
I button Honda
install
Pi python again it will ask you a
question saying these are the components
do you want to install you just say yes
right you say yes
once again it will take a little while
so we might pause the recording and come
back once it is done
right so they did not take much
time anyways so we will as usual we will
exit which is Source deactivate
tensorflow
exit and then we will start a fresh
terminal in order to install Jupiter
now uh in some places uh they may say
um install Jupiter using Pick 3 but in
my experience it did not work with P3
and that's the reason we will use pip
however just keep in mind you need to
first go to the environment so Source
activate
tensorflow
and you need to use this command which
is PIP install Jupiter usually when
you're on Python 3 you use bit three but
as I said it did not work so
in my experience it works with the
install
and stop three so pip install
Jupiter
I hope you already noticed that it is
j-u-p-y-d-e-r
and not JPI
so again it's probably taking a little
while we will pause the recording and
come back once it is done okay so
Jupiter installation is also done once
again
you need to it's a good idea not you
need to but it's a good idea to come out
of this terminal session and then start
afresh
so we exit and we have a fresh terminal
already available here and we will once
again
go to tensorflow
and we say Jupiter
notebook
that's how you start your Jupiter
notebook
so this looks good now only thing is
since we are running uh for the first
time what you need to do is if you come
back to your command prompt it will show
you that if you're running it for the
first time it will say that you need to
copy paste this link so you you can just
for the first time only once you need to
do this copy this
and paste it in your browser and
remember this is only the first time
when you're running Jupiter you need to
do this
subsequently you don't have to it will
automatically
open up all right so now that we have
Jupiter installed and it starts up we
need to test whether tensorflow is
working fine or not and in order to do
that you can create a new
notebook
and you say import
ant tensorflow as TF shift enter if
everything is fine then Supply is
installed properly you don't get any
errors and then it works fine this is an
indication that tensorflow got installed
successfully and that's about it so
that's all about installing tensorflow
with Python 3.4 so remember this is
installation of tensorflow 1.5 with
Python 3.4 on Ubuntu and once again in
case you have a Windows system in fact I
have also done it on a Windows system
you can use Virtual blocks to create a
virtual environment Ubuntu environment
and then follow these steps and there
are tons of videos to create virtualbox
and that's the reason we have not
included those steps in this and if
there are any comments there are better
ways to do this please mention it in the
comment section below or if you need any
further help just mention it and with
your email we will respond to you hey
there learner simply learn brings you
Masters program in artificial
intelligence created in collaboration
with IBM to learn more about this course
you can find the course Link in the
description option box below now what
are the various elements of a tensorflow
program as I mentioned tensorflow
program is slightly different from
regular programming that we do so even
if you're familiar with python this may
still be new for you the way you write a
tensorflow program is different from the
regular Python Programming that you
would have done or even machine learning
program you would have written some
machine learning program using
scikit-learn or regular python libraries
this is different from even that so let
us see what are the various elements so
first of all the way we handle data
inside of a program itself is a little
different from how we normally do in a
normal programming language a variable
is a variable in your program right so
you have anything that can keep changing
you just create as a variable or even
constants in fact are actually created
as variables but in tensorflow the
storage in the program consists of three
types one is constants another is
variable and the third is a placeholder
so and they are there is a lot of
difference between the these types and
we will see how they vary and how they
are used and so on so constants are like
variables which cannot be changed so for
example if you define a constant like
this this is how you define a constant
by the way the simplest format is like
this like for example B is equal to TF
dot constant and then you give a value
here a slightly more advanced version is
you also specify the type so you say TF
dot constant 2.0 TF float32 so the type
is of type float now in case of
constants you cannot during the
computation you cannot really change
these values so for example if you want
to change the value of B from 3 to 5 or
any other number it is not possible so
that is the meaning of constant all
right so then we have variables so
variables we are all familiar with what
are variables whenever we use
programming we use variables so this is
pretty much the same this is the way you
define variables TF dot variable now one
thing you need to note is this is the
only type in which we have V capital
okay constant has C the small C and
placeholder as small P but variable as
capital V and TF dot variable and then
you give the a value and then you can
specify what is the type and then you
can use the variable change the variable
at any point in time with a different
value and so on you can update the
variable and so on so we will see all of
this in the code I will illustrate how a
variable is defined and how it can be
changed whereas a constant cannot be
changed and so on and so forth and then
we have placeholders placeholders are
really a special type and this may be
something completely new for many of us
who have been doing programming but in
tensorflow this is a completely new
concept so this is very important to
understand this placeholders are like
variables but only thing is that they
are used for feeding the data from
outside so typically when you are
performing some computations you need to
load data from a file or from an image
file or from a CSV file or whatever so
there is a provision with the special
kind of variables which can be fed in a
regular basis because the reason one of
the reasons for having this kind of
provision is that if you get the entire
input in one shot typically it may be
very difficult to handle the memory and
so on so I think that was the reason
they came up with this mechanism where
you can feed in patches and there is a
certain way of populating the
placeholder we call this free dick feed
underscore dick and this is a parameter
Name by the way and you feed the
placeholders right that is the meaning
here so there is a certain way of
feeding the placeholders and we will
again see this in the example code as we
move forward okay so there are three
types one is the constant which cannot
be changed once you assign a value then
you have variables which are like normal
variables we are all familiar with and
then you have placeholders this is
primarily for feeding data from usually
from outside but of course you can also
for temporary testing purpose you can
feed within the program as well but
primarily the purpose of a placeholder
is to get data from outside so all right
so those were the constants variables
and placeholders that is how you handle
data within a tensorflow program and
then you create a graph and once you
create a graph then you have what is
known as a session and then you create a
session object and you create a session
and then you run a particular
computation or a node or an operation
and so typically what you need to do is
every variable or a computation that you
perform is like an operation or a node
within a graph so initially the graph
will be what is known as the default
graph the moment you create a tensorflow
object or TF this TF here you see this
is the tensorflow object and again in
the code when we go into the code it
will become much easier to understand so
when you create a tensorflow object
there is like a default graph which
doesn't have any operations no nodes or
anything like that so it's like a clean
slate the moment you assign variables or
constants or placeholders each of them
is in tensorflow terms it is known as an
operation again you need to get familiar
with these terms and they are not very
intuitive this is not really an
operation you're just creating a
constant or a variable but and this C is
equal to a into B would traditionally or
would intuitively be an operation here
you're actually performing an operation
but in tensorflow terms each of these
everything is an operation so if you're
creating a constant that is an operation
another constant or variable that's an
operation so you can actually run each
of these and they are also known as
referred to as nodes and when you're in
your session you will actually run each
of these you can potentially run each of
these nodes okay so a typical example
would look like this so you have two
constants created a is equal to TF dot
constant its value is 5 b is equal to
this and then you say C is equal to a
into B and then you create a session now
remember all this you're just creating a
graph at this point no execution has
happened all right so only at this point
once you create a session and then you
say session or assess dot run C is when
actually this whole thing will get
executed all right so that is a
different way of programming compared to
our traditional way of writing program
so you need to get used to this new
format and when we look at the code as
we move forward and when we look in the
jupyter notebook it will become much
easier probably to understand this
rather than in the slide so these are
the slides showing the code but what we
can do is go straight into the lab and
take a look at the various examples that
are there and starting from the very
basic one how you create variables and
so these are some of the slides that are
showing so this is about how to create
variables how to create constants and
the variables or constants can also be
strings so this is like our first hello
world program and we will talk about
placeholders how to define a placeholder
and how to execute and populate the
placeholder values into placeholder we
will see these examples in the lab
actually I will run the code and we will
also perform a small computation of
adding and multiplying these variables
and we will in the end we will take up
our use case implementation using
tensorflow so let's first go and see
those examples and then come back and
I'll explain this use case and then we
will execute the use case in the lab if
you see expertise in deep learning
skills and aspect to be a part of
Cutting Edge field of building chat
boards and service boats using deep
learning our PCP in Ai and ml is the
ideal Choice do check out the course
Link in the description for more details
so let's go and check our lab okay so
I'm in the Jupiter notebook environment
and this is one of the development
environments you can use this is regular
python anytime you do Python Programming
we use Jupiter notebook or there are
other of course there are other ways of
using other tools like pycharm and so on
but for this particular tutorial I'm
comfortable using jupyter notebook so I
will show you in jupyter Notebook so
this is the very basic example to
demonstrate how to create variables and
constants and placeholders and what is
the difference between them how they
behave and so on and as I mentioned the
Assumption here is that you know at
least some basic Python Programming or
some programming language so at least
you understand the code here and first
two pieces of code you will not need
machine learning background but when we
do the use case of the case study there
it is expected that you know at least
some basic machine learning Concepts so
in case you need to brush up the machine
learning part you may have to do that
before you go to the third one but here
just at least some idea of programming
will be sufficient so what are we doing
here in this particular line or in this
particular cell we are importing
tensorflow it is uh as I mentioned it is
a library so we are importing tensorflow
and we are calling it TF so this is this
just a name you can give it any name but
it is very common and everywhere
wherever tensorflow programming is done
it is always named as TF but you can
name anything actually okay so this will
import the tensorflow into my session
now this is the way to create a variable
so let's start by creating a variable
and this is the name of my variable I am
starting by giving a name called 0 and I
say 0 is equal to TF dot variable and
then I'm giving the value of the
variable here so this is the very basic
way and the simplest way to create a
variable we will see a little later
there are other formats as well but the
bare minimum way of creating a variable
is this so I'm creating a variable by
the name zero and as I mentioned you
need to pay attention to the capital V
here in case of variable it is a capital
V uppercase constant and placeholders
are lowercase now I'm creating a
constant and the constant I'm naming it
as 1 and the way to create a constant is
TF dot constant and then you give the
value of the constant so the value of
the variable here is 0 and the value of
constant here is 1 and again constant
you can also have additional parameters
like a name and so on probably you must
have seen in other tutorials or in other
places where the code is written but the
basic format to construct to create a
constant is this this is sufficient to
create a constant so I created one
variable and one constant now what else
can we do again here there is no real
execution happening we are just building
a graph we have not yet executed any
tensorflow code we are building a graph
and I will show you how the graph looks
as well first let us understand the
constants and variables and so on and
then I will show you how the graph is
generated and so on so what I want to do
here is I want to add 0 and 1 and put it
into a new variable called new value so
what I do here tensorflow offers these
methods like add assign multiply or you
have matrix multiplication mat model and
so on right so I will use one of those
methods which is TF dot add and then
pass these two as parameters so we can
add a variable and a constant there are
no restrictions on that so this is a
variable and this is a constant so if I
do this it will be okay I did not
execute this code so it is giving an
error now we are good okay so what it
has done is new value will be equal to
TF dot add 0 1. so then we have one
variable and one constant now let's say
I want to change the value of the
variable because that is possible right
so we want to change the value of the
variable to something some new value so
we have in this case new value has 1
because we added a 0 and a one so new
value has one now I want to assign this
to this originally what we called as 0.
so I will call that as update is equal
to TF dot assign so assign is basically
changing the value so that's what we are
going to do here now it has not
complained we'll just say fine right
there are no issues now let us try this
something similar with this one as well
so 0 is a variable so we were able to
change the value and we will in a little
while we will see what exactly those
values are but before doing that now
let's say I will uncomment this part and
I want to do something similar for my
constant right one is a constant now I
want to let's say change the value of
the constant and make that also
something different okay so if I execute
this piece of code it will give an error
now again the error message may not be
very intuitive this doesn't say that you
cannot do this for a constant it will
just say tensor object has no attribute
assigned now that sometimes may be
confusing especially when you are
starting but the meaning here is that
one is a constant and you're trying to
modify the constant so it will not allow
that's the reason it is complaining so
let's put that back in the comment and
okay so that is done so only variables
you can modify now what you have to do
let's skip this piece of code I'll come
back to this in a bit but let's say we
start by creating you remember I told
you we need to create a session so the
way to create a session there are a
couple of ways of creating a session but
this is for beginners this is the
easiest way all you need to do is assign
a variable called SAS or you can give
any name and that is equal to TF dot
session that is the session method here
and you create a session object by the
name SAS now what I'll do is I'll skip
this as well I there is a purpose behind
that now let's go ahead and run this
piece of particular operation remember I
mentioned that everything you need to
run so as of now the code has not really
got executed tensorflow code has not got
executed you only created the graph so
only when you run through the session is
when the actually the program gets
executed so when you do this now you see
observe that it is giving an error Okay
the reason behind that is remember we
skip these two lines of code for
variables okay this and then this now
this is something very very important to
observe if you have variables in your
code what I mean by that is let's say
you are not using variables but you have
only constants and placeholders then
this will not complain and you will not
get this error but in our case we also
have variables so whenever you have
variables in that case you need to do an
initialization and this is just a
standard code there is nothing that we
need to add or modify or anything like
that this is a standard piece of code
you create this name of course can be
anything you can give us any name but
this Global variables initializer is
what you need to call TF dot Global
variables initializer this will kind of
initialize all the variables that you
may be using and then again remember
this doesn't execute anything right so
all you're doing here is you're creating
an operation but in order to run that
operation you need to also run this
known as run init underscore op so this
operation you need to run after creating
the session all right so we executed
this now let us execute this and now
when I run this piece of code it will
run successfully okay I hope you
observed that so it is whenever you use
variables these two lines of code one is
the operation you need to create an
operation for saying Global variables
initializer and then you need to run
before running anything else you need to
do a session dot run this operation okay
again only if you're using variables of
course you invariably in all your
programs you will use variables you
cannot just write a program with
constants and placeholders so you can
pretty much assume that this has to be
there in pretty much all the programs
now again why this has not been taken
care of in the library that's a
different question but you need to keep
in mind and always remember remember if
you don't do this for whatever reason if
you have forgotten you will get an error
and the error won't be intuitive so you
need to remember this that this could be
because of the variables okay good so we
have seen how to create a constant and
we have seen how to create a variable
and we have seen that you cannot modify
or update a constant and we have also
seen if you have variables that you need
to execute or have these two lines of
code to initialize the variables and
then we have seen that after creating a
graph how to run the graph in a session
and I will show you a little bit more in
detail how exactly the graph gets
created and how it gets executed but
this was the first very quick code on
creating variables and constants now we
will keep going and we will also show
you or I will show you the placeholders
but before that one more small piece of
code so this is how you write a for Loop
okay so we are saying five times you run
the this piece of code now like any
Python program this is nothing different
so you have a for Loop and this is
indented that's the reason and therefore
you say session dot Run update so this
will run five times and it will get
printed that's all so update will run
for five times each time it will come
and do this particular operation that's
all we are asking the system to do so
let me just run that and show you let me
yeah so it will okay so it has done five
times
one that's for it is basically adding if
you recall it starts from zero it adds
one to zero then one to one one to two
and so on so it's nothing but it is
generating five numbers one two three
four five that's it okay so then you can
also have your constants as strings for
example so you can also work in a
similar way you can work with string so
most of the computation happens mainly
on numerical values so we normally had
handle numbers but there will be
situations where you may have to handle
strings as well or text as well so this
is an example of text operation so you
can similar to numbers you just create
strings or store in constants instead of
a number you say hello is equal to TF
dot constant and then you assign the
string that you want it to be assigned
and then the next one is one more string
which is for world and then you say add
these two so it is nothing but
concatenation of these two words hello
and word so we will do that and remember
this is only creation of the graph right
so in order to actually execute this
graph you need to run the session now
you need to keep in in mind we don't
have to create the session once again
because once you create a session till
you close the session it remains valid
okay so we have created this session
here now till you say SAS Dot close it
will remain for you so that's why we did
not create one more so we are just
reusing that session says dot run hello
world so in this hello world what is the
operation that we are doing here the
operation is add these two concatenate
these two so if I run this it will print
hello world okay now that's string
operation now we will take a look at
placeholders so this is a slightly more
complicated and something new even for
people who have been writing programs so
but I'll just explain it with uh with a
quick example first of all how do you
declare or create a placeholder you just
do TF dot placeholder and the p is
lowercase only in case of variables the
B is uppercase but otherwise constant at
least one as it is lowercase and you
just say because as I mentioned this is
a placeholders it doesn't have any value
so you unlike a constant or your
variable you can't usually you will not
specify any value you just say what type
of placeholder you want and very often
most of the cases it is a floating point
so we just say placeholder of type
float32 so this TF dot floor 32 tells
the system that it is a floating Point
okay okay and then so let me just run
this and then you can do some or Define
some computation like for example B is
equal to a into two now remember here
right now A has no value but what we are
saying here is at any point later on
when a gets some value then B should be
equal to twice the value of a that's all
we are saying here okay and also as I
mentioned earlier we are not yet
executing anything we are just creating
a graph here okay so now you have that
now how do you feed the value or
populate the placeholder there is a
certain syntax in which you can populate
the placeholder and you do that using
what is known as a dictionary you're all
probably familiar those with python
specially must be familiar with the
dictionaries so you need to create a
dictionary and then pass that in order
to feed the placeholder so in this
particular example if I want to run
something if I want to calculate or
compute B obviously I need to feed the
value of a so the value of a I'm feeding
using this dictionary and I'm saying
that a is equal to 3. now there are
multiple ways of populating the value of
a because it typically this won't be
just a single a scalar value like in
this case right remember scalar so it's
just one value one number typically it
would be a vector or a tensor very often
it is a tensor so we will see step by
step so we can start with a very basic
example where a can be a scalar so in
this case a is equal to 3 that's what we
are saying feed underscore date in this
case feed underscore dick is the name of
a variable by the way so you can't give
anything else here so if you just say
feed is equal to this it will give you
an error all right so keep that in mind
either you specify feed underscore dick
is equal to and give the dictionary pass
the dictionary or use straight away like
in this case this is another format so
let's see first let me execute this it
has given 6 okay I think let me see if I
can clear out this particular cell so it
becomes easier to understand okay so it
is giving you six let me do that once
again clear out this and then okay and
let me change the format so this is one
way of doing it another way is you don't
even have to specify the name of this
parameter feed underscore deck you just
pass the dictionary and it will execute
okay but typically this makes the code
readable so that's the reason most of
the places whenever you find tensorflow
code you will see that they explicitly
mention feed underscore date so that the
code is readable otherwise it can get
confusing so I will also comment this
and retain the other format okay so I
hope it's clear so what we are doing
here we are running the operation B and
what exactly is BB is twice a a into two
so therefore what we are doing and a
therefore b gets a value of 6 and that's
what is being fed into result and that's
what is getting printed here okay now as
I mentioned a need not be just a scalar
value it can be a vector with a
multi-dimension array and so on and so
forth so let's start showing you
examples of that again let me just clear
out the outputs let me clear this and
let me also clear this for now okay you
can also do before running up this you
could we could also do clear all output
all output clear it will clear
everything but that will we'll have to
start all over again so I'm just doing
individually all right so in this
example we are taking a one dimension
array where rank is equal to one and I'm
feeding three four five now let us see
what happens if I run this right as you
can imagine it gets multiplied each
number gets multiplied so the result is
6 8 10 okay now as I said it can get
more complicated now let's say this is a
multi-dimensional vector so if you what
happens if you feed this okay so as you
can see this is pretty complicated now
but if you feed the this again all
you're doing is there is okay there is a
slight change here again you can also
Create Your Dictionary outside and feed
that here you don't have to directly do
it here like in this case you see there
are variations of the exact syntax in
the way you write it right so there is a
slight variation so in this case you are
creating the dictionary straight away
here kind of in line but here you
created the dictionary separately
because it's more complicated and then
you are passing the dictionary here so I
said dictionary is equal to a and then
I'm saying this is what should be the
value of a this is and I'm feeding that
and then I am using directly that
dictionary here okay so this is
equivalent to putting this code here
right so if we execute this what happens
same so you get because a was uh let's
say this is a 3 by 4 by 2 right so that
is one yeah three by four by two right
so you see here this is a my
multi-dimensional array so you feed that
and the values you see here it is one
two three it became two four six four
five six became eight ten twelve seven
eight nine became 14 16 18 and so on and
so forth okay I hope now you got an idea
about placeholders and in real life what
happens is you typically won't create
these dictionaries manually in since
this is a quick demo we did this you
will actually read the data into this
dictionary and then you feed that when
you're doing the computation okay so it
could be a CSV file or it could be a
image file so the input is basically red
and fed in usually it is also done in
patches so you don't read the entire
thing because they can be large amount
of data so that is the idea behind
having these placeholders and the way we
feed these placeholders there is a
provision there's a deliberately that
has been made a provision for this kind
of getting data into the program in
chunks okay now in this particular
example we will close the session here
and then I will show you what is other
way of creating a session right so now
if I close the session here now if I try
to do anything with the session it will
give us an error so I will probably not
do that right now now there is another
way or another format and this is a very
common way of using the session but I
didn't want to start off with this
because this could get a little
confusing the a simpler way was to
create session saying s is equal to TF
dot session but typically you will do it
in what is known as a width block this
is a very common way of creating session
so once you have the graph you would
execute the graph in a session using a
width block and this is the syntax so
you say with TF dot session as says all
right and then you put all your code
here so what will happen is you don't
have to explicitly close the session the
moment this with block gets completed
the session gets closed okay so let's
just take a quick look at this example
now if I uh let me clear this current
output be clear okay so this is the
hello world example a little bit earlier
we did that so I'm doing with TF dot
session assassin result is equal to SAS
dot run hello Plus World and then I say
print result okay so within this with
block I'm doing all my computation and
this gets executed now let's say I try
to run remember I created a session here
now if I want to let's say do something
like run search dot run a it is giving
an error because the session is no
longer valid right so you cannot either
you have to run this if you want to run
this you have to run this in this with
block or you shouldn't have closed the
session here right you remember this we
said session Dot close so since we have
closed the session there is no active
session therefore it is giving an error
okay now there is a third way of
creating a session but for now I will
skip it because that is very very
specific to the notebooks and and so on
so we will just avoid it you start with
for beginners I think the best way is to
start with creating a separate session
and then writing your code and then
closing your session now of course
remember if you don't do this part says
Dot close it will not give any error or
anything like that only thing is that
it's a good practice and your resources
will get released otherwise if you do
multiple of these programs if they are
running then your resources will get
blocked that's the only thing and to
start with it is it is simpler to do it
this way so initially in you're doing
you start with this but as you move
forward as you become familiar with the
tensorflow programming I would recommend
all of you to get into this model most
of the programming tensorflow
programming is done like this with the
app.session SS okay all right so that
was one book or one page of code that we
have done explaining about variables
constants and placeholders now let me
explain how the graph works so in this
example we are going to see how the
graph is created how the graph gets
executed and so on okay now that we got
a understanding of constants and
variables and placeholders okay so we
will start as usual by importing our
library and as I said whenever you
create a TF or tensorflow object there
will be what is known as a default graph
and you can get a handle of that by
saying get default graph so let's do
that and you can display the operations
remember every node in the graph is
considered as an operation right so as
of now as you can see we have not done
anything we have not created a constant
or variable or nothing right so there
was there is no operation so by the way
let's do one thing let's start with the
all output clear so that I don't have to
again do individually so I'll just run
these two lines of code okay now so yeah
so we have the default graph and as of
now there are no operations so there is
a method called get underscore
operations which which will kind of
display or show what are the various
operations that have been performed on
the graph so as I said to start with
there have no operations being performed
so when I try to display it's empty
there are no nothing is there no
operations okay now let's see what
happens when we slowly step by step
start writing or building the graph so
my first step may be to create a
constant okay remember in the previous
example I have shown you how to create a
constant in the most simple or the
simplest way which is like constant and
then the number right now a small
extension of that is you can actually
give a name to your constant now again
this need not be the same I can say a is
equal to name and x y z I can give
anything here okay this is just for your
reference but here for Simplicity
because there is a purpose why I am
using the same name as the name of the
variable and there are other reasons as
well why you would give a name again to
a constant or a variable this is again
useful when we are using what is known
as tensorboard otherwise it doesn't have
much value this additional name doesn't
have much value in this our case also we
will be seeing the graph and that's why
I'm using this but otherwise this having
additionally a name usually doesn't add
much value all right so let's create a
constant I created a constant and now
let us see what's going on in the graph
so I will just say operations is equal
to get operations and if you see here
now you remember here it was blank now
you see the first operation is showing
up and this a that's the reason I put
here so it is showing as a and it says
that okay you have performed an
operation and there is a constant you
have created a constant okay all right
so let's move on let's say we want to
create a second constant and this is B
so if I see what does what are the
operations list of operations that have
been performed you see here there are
two of them so you have operation a
which is a constant then you have an
operation B another constant you recall
we in the term operation here is not
very intuitive right so we are just
creating constants or numbers or
variables and it is saying it's an
operation but that's that's a
terminology in tensorflow everything is
an operation here okay all right so then
what else let's say I want to create a
third operation and that is basically
adding which is this is a real operation
in normal sense as well so let's say I'm
creating C which which is adding a and b
and the C value obviously will be a plus
b okay now if we want to see what is in
C and we say C you see here it is not
showing 30 it just says it is a tensor
and it is of type into 32 right because
we have not executed the graph we are
still only building the graph I hope
this makes it easy to understand okay so
we are just building the graph we have
till we do remember till we do a session
and run a particular operation you're
not actually executing anything okay
you're just building the graph all right
so now let us see what are the
operations we have done a b and now C so
you will see there are three of them A B
and C okay now let's do one more
operation which is D and here I am doing
multiplication remember we did a TF dot
add now I'll do TF dot multiply and I am
multiplying A and B and I'm calling this
operation as D so a has uh I think uh n
and B has 20 so D will be at actually
200 when we execute it right now it is
just a tensor and it is of shape in 32
okay and once again we can see what are
the operations A B C D and let's do one
last operation here which is e which is
once again multiplication of c and d now
that c has 30 which is a plus b and d
has 200 which is a into B so now C into
D will be 30 into 200 will is which is
equal to 6000 but as we have seen
earlier that multiplication will not
happen right now so now you have built a
fairly simple but operation graph which
consists of a b c d e one two three four
five operations okay so you built a
small it's not a very complicated one
but a simple graph now you need to
execute these operations so what you
need to do you need to create a session
okay so says dot TF is equal to session
and then you can run this session print
says dot run e now am I missing
something do I have to initialize the
variables that's a question for you
think and tell me do I have to
initialize the variables then remember I
ran one piece of code I think you must
have got the answer I don't have to
because I'm just using all constants
here I have not used any variable here
right so I don't have to write execute
that piece of code to initialize the
variables so I can just create a session
and run any of the operations that I
want to run so in this case I am running
session e operation e right this
operation e now I can actually run
individually instead of doing directly E
I could have run only a or I could have
run B and so on and so forth but in this
case I have run e which in turn will do
all the required computations which is
for example assigning the value to a and
B and C doing a multiplication and so on
all those operations will be performed
which are required to do the computation
of e okay now there is a small piece of
code that you can use to find out what
are the various operations that are
there in this particular graph in this
case of course it was very easy very
straightforward I have created a b c d e
but in normal programming in real life
this can sometimes help to see how what
is there in your graph what kind of
operations are going on in your graph so
you can write this piece of code and it
may sometimes help debugging as well
okay all right so I finished my session
what do we have to do we need to close
the session right because I did not use
the width block so I need to close the
session as a good practice so I close
the session good so that's how the graph
Works uh I hope it was helpful in
understanding how variables and
constants and placeholders are created
in the previous example we saw and how
exactly you create a graph and then you
actually execute that graph that's the
way tensorflow program works that's the
structure of tensorflow program okay so
now that we understood the structure of
tensorflow programming let's take an
example and a classification example and
take some real data from from outside
external data CSV file and try to solve
a classification problem for that let's
first go back to the slides and
understand what is the problem statement
and then we will come back
so what is the case here we have some
census data and which has all these
features or variables like the age of a
person what class work class what is
this education what is this marital
status gender and so on and so forth so
there are a bunch of a bunch of features
that are available and we have to
basically classify or not classify we
have to build a model for classifying
whether the income of this person is
above 50k or below 50k okay so there is
actually a label data available we will
then try to build a model and then see
what is the accuracy so that this is a
typical machine learning problem
actually but as I said tensorflow can be
used for doing machine learning as well
so we will as a simple example we will
take a machine learning X so this is how
the high level the code looks and in in
tensorflow programming we also take a
help of regular python libraries like
for example it could be numpy or it
could be scikit-learn or it could be
pandas in this case and because before
you actually develop or before you train
your model create your model you need to
prepare the data into a certain format
and sometimes if you're doing machine
learning activity you all are familiar
those familiar with machine learning
will know you need to split your data
into training and test data set so all
that can be done using regular
non-tensorflow libraries right so these
are like regular python libraries and
psychic learn and so on so we use that
and then prepare the data and then use
tensorflow for performing either the
machine learning or deep learning
activity now so what is the advantage of
using tensorflow right main advantage is
that tensorflow offers high level apis
right we talked about that so in this
case we will be using I'll explain what
is in this slide but before that I
wanted to just show you what is the API
that we will be using so we will be
using the estimator API we will be using
the estimator API to create a classifier
what is known as a linear classifier now
in order to do that you need to prepare
the data and you need to prepare the
data into a certain structure or format
and the API itself needs to be fed in a
certain way so that is exactly what we
are doing before we get into the regular
tensorflow code okay so these slides
will quickly show you what exactly is
happening but I will take you into
jupyter notebook and run that code and
and show you so here we are importing
the data the data is the name of the
file is sensors underscore data.csv it's
a CSV file and then there is a income
bracket is one of the columns which
basically is our Target that is what is
used that is the label rather but this
doesn't have numeric values so it has
the income so we need to convert that
into binary values either 0 or 1. so
that's what we are doing here and then
we are splitting using scikit-learn we
are doing splitting of the data into
test and training right so this is a
standard psychic learn people some of
you who are familiar with machine
traditional machine learning and you
have done machine learning in Python
using scikit-learn will immediately
recognize this this is a SK learns
psychic learn and there is a readily
available method to split the data into
training and test that's what we are
doing here and then you need to create
what is known as the feature columns and
input functions before you can call the
API estimator API that's what we are
doing here and data and also there are
different ways of creating the feature
columns for numeric values and for
categorical values or rather for
continuous values and categorical values
okay so that's what has been two types
we are doing here we will see again in
the code as well and then you create
your model so this is basically model is
equal to TF dot estimator linear
classify and then you're feeding the
feature columns and also subsequently
you will call the training by passing
the input function and you specify how
many iterations to be done for the
training process and again this is all
typical machine learning process so
there is nothing specific for tensorflow
here and then you evaluate your model
because this is classification model so
you can again take help of the
scikit-learn to test the accuracy and
get the reports evaluation perform the
evaluation and get the report like this
one right so this is the classification
report and you will see what is the
Christian recall F1 score and so on
again this is a standard machine
learning process all right so that's
pretty much as far as the code here is
concerned once again let me take you
into our lab and we will get started
with this particular code now let me
clear out the outputs so that you can
step by step you can see the output so
this is our data and we will actually
this is just a display here the code
starts from here so before even we start
with our tensorflow part of it as I
mentioned we now remember in the
previous examples the data was kind of
cooked up internally we created some
data and we were using it right the
constants variables and so on here is a
real life example so you are actually
getting data from outside so you know
before we get into tensorflow this is
actually regular python code so we are
using pandas for example to create our
data frame so we will import pandas and
then we will basically read our file
into our pandas data frame and this is
how the data looks right and if you do a
head it will give you about six
readings or observations five or six
observations and then you can take a
look at how your data is looking so for
example you have age work class
education and so on and so forth and you
see here this last column income
underscore bracket it says whether the
income is greater than 50k or less than
50k so this cannot be understood or this
cannot be fed into our model or or you
know so that's the reason we need to
convert this this will not be understood
right so this needs to be converted
saying okay maybe less than 50k 0 and
150k is 1 so that conversion needs to be
done before we feed it into our model so
that's what we are going to do as we
move forward so this is the code for
doing that okay so we take this and then
we Define a function and then we run
that function so now we have wherever it
is less than 50k it will have 0 and
greater than 50k it will have 1. so we
have now updated the data then the next
part is to split our data into training
and test data set so how are we going to
do that we again will take the standard
python Library which is scikit-learn SQL
on library and we use an existing
function there which is train test split
and what we are doing here is basically
splitting the data into I'm sorry I need
to run this and then this okay sorry
about that so once I run this what is
what are we doing here again those are
familiar with scikit-learn will
immediately recognize I am splitting
this into test and train I'm saying test
size is 30 so 30 of the data should go
into test and training should have 70
percent okay so that's all we are doing
0.3 indicates thirty percent so you can
again this can be individual preferences
some people do it like 50 50 some people
2080 and in our case we are doing uh 70
30. so training is 70 and test is 30.
all right so we have so far we've been
doing regular
non-tensorflow stuff so preparing the
data so that we can now use for
tensorflow and in tensorflow what we are
doing is we will be using the API called
estimator now estimator in order to call
estimator you need to prepare what is
known as feature columns so feature
columns have to be in a certain format
basically it is nothing but the columns
we have to put them in a certain format
and then when we are calling the the
training we need to pass what is known
as an input function again a certain way
in which you need to pass that function
so that is what we need to do before we
create the model and run the model for
training so the next few lines will be
doing that quick look at before we even
get in once again into the tensorflow
code so this is just showing us what are
the various columns names of The Columns
of this data Okay so so that is what is
shown here and this is more for when I
was writing the code I could copy paste
from here that's the reason I this line
of code is there instead of typing in
these values okay good so now is where
the actual tensorflow action starts so I
import tensorflow and now I will use
this feature column functionality to
create my feature columns and there is a
certain construct how you create the
feature columns again probably the
details of that is out of scope here but
just that you need to know that we need
to create feature columns and the way
you create feature columns for
continuous values and for categorical
values is slightly different and that's
the reason you have two different blocks
okay so for categorical values you need
to use what is known as TF dot feature
column dot categorical column with
vocabulary list and again within this
again there are two ways in which you
can create for categorical values one is
a vocabulary list where you know how
many types are there so for example
gender column can have only male and
female so in such a case there are let's
say a finite number of values for a
given column then you use a column with
vocabulary list okay so and then you say
what is the name of the column and what
are the possible values so in this case
male and female now there will be
situations where the the values that
this column can have okay first of all
it is a categorical column that means it
can have names or some non-numerical
values but the number of values is
probably unknown or undefined so for
example occupation now occupation can
have any value it can be self-employed
it can be software Professional Bank a
teacher doctor so so many possibilities
are there and we don't know in the given
data how many occupations have been
listed right so so that is where you use
what is known as categorical column with
hash bucket and you specify the hash
bucket size so what it what this says is
there can be a maximum of 1000 such
values so typically if you give a safe
number safely large number so that you
don't run out of these number of
occupations so I think thousand is a
safe number I don't think there will be
more than thousand occupations in a
given data so that's the idea behind it
but that's a judgment call we need to
take what should be the size of this
bucket size right hash bucket size so
these are two different types for
categorical values for creating the
feature column so we will execute this
for all the categorical columns
obviously you need to know the type of
the columns and that is where if you see
right at the beginning this information
was provided what is the data type and
you see here the column name and what
whether it is continuous or categorical
this information is provided in the
Census Data wherever we click the Census
Data from by the way this was picked
from online from a government website
and we stored locally so on that website
it is mentioned the details of this data
were mentioned and they have mentioned
whether each of these columns is
categorical or continuous so that
information we have to use and we have
to create feature columns using this
categorical method for the categorical
type of columns and then for numeric or
continuous values you just use what is
known as feature column dot numeric
column okay so these are all continuous
or numeric values like age education
number capital gain Capital loss hours
pervert these are all numerical values
or continuous values right so you just
say TF dot feature underscore column dot
numeric underscore column and then give
the name of the column so once you do
that
I have executed the previous one yeah
this one now so your feature columns are
ready and uh you need to basically
create a vector of all these columns and
we call that as feet underscore calls
which is which is like a short form for
feature columns now as you have seen
I've mentioned earlier also there are
two things that are needed one is the
feature columns and another is the input
function so feature columns to First
create the model and then to train the
model you need to create an input
function so now we have the feature
columns ready next is to create an input
function and there is a certain way in
which you need to this construct is
pretty much very common construct so we
will use that so the input function
takes these X and Y values for training
and tests you remember these are the X
strain is the X values of the training
data set and Y train is the labels of
the training data set so that's what we
are using here x train y train in and
then we specify what is the batch size
batch size you remember I mentioned this
is what will say how many records need
to be read each time right so typically
in when we are doing the training
process we don't get the entire data in
one shot we do it in batches so what
size you specify here number of epochs
in this case we just say none but number
of epochses again uh probably the
definition is not required here but at a
very high level how many times the
entire data has to be passed through the
model right so if you have 1000 records
and you pass all the Thousand records
three times for training purpose then
you call that as three epochs right so
there is a difference between the batch
size and epochs so when let's say we
have thousand records and you're saying
batch size is 100 that means there will
be 10 batches right so if you take 10
batches then that is one Epoch gets come
completed okay then you the training
obviously doesn't get completed in one
round okay so you need to pass this data
again maybe a second time or a third
time till you get the right accuracy
that is known as epochs okay again if
you need more details about this you may
have to go through the machine learning
tutorial I think it becomes much clearer
okay so you have your input function as
well let's execute that okay and now we
are ready to create the use the API
right so what will we do we will create
the linear classifier using
tf.estimator.linear classifier and this
guy needs feature columns right so
that's why we created we did all that so
much of manipulations to create this
feature columns so we have feature
columns so we pass that and we create
our model there may be a few warnings
don't worry about that so we will just
ignore that and now we use that model
and we actually what we did is we
created an instance of the model we have
not really run anything we just created
a instance of the model now we will
create the node for training so model
dot train and this needs the input
function remember we we created the
input function so we need to pass the
input function and then we say the steps
is now telling how many iterations this
training has to be run so we are saying
5000 and this may be it will probably
take a little long so we will okay but
that's fine I think we will leave it
5000. sometimes if you have probably a
less powerful machine you can cut it
down to maybe 1000 or something like
that but in this case I have a fairly
powerful system so I think it shouldn't
take much long let's uh go ahead and
give couple of minutes for it to get
over okay so let us go back yeah I think
this is done you see here the the r
Glass has disappeared that means this
computation is done yes okay this is
done right saving checkpoint for 5000
that means it is done so training is
done so now what we have to do we need
to do the evaluation so again this is a
standard machine learning method so a
methodology rather so training has done
has been done with the training data set
now you need to evaluate using your test
data set so that's what we are doing
here for example you see here x
underscore test is your the test data
set remember we used scikit-learn to
split the data into training and test
data set so X underscore test and a
quick question why are we not using Y
underscore test here like we did in case
of training obviously we are expecting
the model to predict the values y values
right so that's why we don't pass the Y
values here all right so let's execute
this for doing the evaluation all right
and then we put them in a proper format
because the output doesn't come out in a
proper format so we put it in a this
format and then we can just check yeah
so this is done and we can check what is
the first element in this what is the
probabilities and so on so it's just a
quick way to see if the values are there
or not you can take any value here right
any element here all right now that the
testing is done what we need to do again
we will probably put them in some kind
of this is more of a formatting thing we
will just uh run through quickly and
then we will use once again scikit-learn
for finding out the accuracy and so on
because scikit-learn offers what is
known as the classification report
functionality so we will use that and
find out how well our model has
performed so you see here again people
with machine learning background will be
immediately able to recognize this so it
gives us what is the Precision what is
the recall and F1 score this is pretty
much like you're getting like almost 85
percent accuracy which is pretty okay
and there are other ways to increase the
accuracy for example you can run the
training for more iterations that is one
way get more data and so on and so forth
or user this is a linear classifier we
could do have used a non-linear
classifier and so on so again multiple
ways of doing it to increase the
accuracy but the idea here was to
quickly show you a piece of uh
tensorflow code and that's what we have
done here hey there learner simply learn
your master's program in artificial
Intel is created in collaboration with
IBM to learn more about this course you
can find the course Link in the
description box below we had the
original tensorflow release of 1.0 and
then they came out with the 2.0 version
and the 2.0 addressed so many things out
there that the 1.0 really needed so we
start talking about tensorflow 1.0
versus 2.0 I guess you would need to
know this for a legacy programming job
if you're pulling apart somebody else's
code the first thing is that tensorflow
2.0 supports eager execution by default
it allows you to build your models and
run them instantly and you can see here
from tensorflow 1 to tensorflow 2 we
have almost double the code to do the
same thing so if I want to do with
tf.session or tensorflow session as a
session the session run you have your
variables your session run you have your
tables initializer and then you do your
model fit X train y train and then your
validation data your X value y value and
your epics and your batch size all that
goes into the fit and you can see here
where that was all just compressed to
make it run easier you can just create a
model and do a fit on it and you only
have like that last set of code on there
so it's automatic that's what they mean
by the eager so if you see the first
part you're like what the heck is all
this session thing going on that's
tensorflow 1.0 and then when you get
into 2.0 it's just nice and clean if you
remember from the beginning I said cross
on our list up there
and cross is the high level API in
tensorflow 2.0 cross is the official
high level API of tensorflow 2.0 it has
incorporated across as tf.corross cross
provides a number of model building apis
such as sequential functional and
subclassing so you can choose a right
level of abstraction for your project
and uh we'll hopefully touch base a
little bit more on this sequential being
the most common form that is your your
layers are going from one side to the
other so everything's going in a
sequential order
functional
is where you can split the layer so you
might have your input coming in one side
it splits into two completely mod
different models and then they come back
together and one of them might be doing
classification the other one might be
doing just linear regression kind of
stuff or neural basic reverse
propagation neural network and then
those all come together into another
layer which is your neural network
reverse propagation setup
subclassing is the most complicated as
you're building your own models and you
can subclass your own models into cross
so very powerful tools here this is all
the stuff that's been coming out
currently in the tensorflow cross setup
a third big change we're going to look
at is it in tensorflow 1.0 in order to
use TF layers as variables you would
have to write TF variable block so you'd
have to pre-define that in tensorflow 2
you just add your layers in under the
sequential and it automatically defines
them as long as they're flat layers of
course this changes a little bit as a
more complicated tensor you have coming
in but all of it's very easy to do and
that's what 2.0 does a really good job
of and here we have a little bit more on
the scope of this and you can see how
tensorflow 1 asks you to do these
different layers and values if you look
at the scope and the default name you
start looking at all the different code
in there to create the variable scope
that's not even necessary and tensor 2.0
so you'd have to do one before you do do
what you see the code in 2.0 in 2.0 you
just create your model it's a sequential
model and then you can add all your
layers in you don't have to pre-create
the um
variable scope so if you ever see the
variable scope you know that came from
an older version and then we have the
last two which is our API cleanup and
the autograph in the API cleanup
tensorflow one you could build models
using TF Gans TF app TF contrib TF Flags
Etc and tensorflow 2 a lot of apis have
been removed and this is just they just
clean them up because people weren't
using them and they've simplified them
and that's your TF app your TF Flags
your TF logging are all gone
so there's those are three Legacy
features that are not in 2.0 and then we
have our TF function and autograph
feature in the old version tensorflow
1-0 the python functions were limited
and could not be compiled or exported
re-imported
so you were continually having to redo
your code you couldn't very easily just
put a pointer to it and say hey let's
reuse this
in tensorflow 2 you can write a python
function using the TF function to mark
it for the jit compilation for the
python jit so that tensorflow runs it as
a single graph autograph feature of TF
function helps to write graph code using
natural python syntax
now we just threw in a new word and you
graph graph is not a picture of a person
you'll hear graph x and some other
things graph is what are all those lines
that are connecting different objects so
if you remember from before where we had
the different layers going through
sequentially each one of those white
lined arrows would be a graph x that's
where that computation is taken care of
and that's what they're talking about
and so if you had your own special code
or python way that you're sending that
information forward you can now put your
own function in there instead of using
whatever function they're using in
neural networks this would be your
activation function although it could be
almost anything out there depending on
what you're doing if you
be in Ai and ml is the ideal Choice do
check out the course Link in the
description for more details with this
we have come to the end of this basics
of deep learning if you like this video
subscribe to our YouTube channel and hit
the Bell icon to never miss any updates
from Simply learn if you need any help
opting for this course please let us
know in the comment section below we
will get back to you as soon as possible
until next time thank you keep learning
and get ahead
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
thank you
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
foreign