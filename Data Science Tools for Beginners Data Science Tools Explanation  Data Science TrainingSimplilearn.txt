welcome to the ultimate data science
tools tutorial video in this
comprehensive guide we will delve into
the world of data science equipping you
with the essential skills and knowledge
to harness the power of various tools
this course is your gateway to
harnessing the immense power of Cutting
Edge tools and technologies that drive
the field of data science discover the
essential tools used by industry experts
to extract insights analyze fast data
sets and make data driven decisions with
confidence we will explore a wide area
of indispensable tools including Excel
python numpy pandas cbone psych Lear
Metro lip tensorflow W and MySQL Excel
the ubo stretched software serves as an
excellent starting point for data
manipulation and Analysis Python A
versatile programming language take
Center Stage as we dive into its
extensive ecosystem of data science
libraries number five that enables
numerical computations and provides a
solid foundation for scientific
Computing while pandas offers powerful
data manipulation capabilities and to
unleash the full potential of your data
visualizations we will explore cbone and
M plot lip enabling you to create
stunning and informative graphs and
plots psyit learn psych learn will be
your goto library for machine learning
Enthusiast empowering you to implement
various algorithms and build predictive
models but your journey doesn't stop
there we'll Di into the realm of deep
learning with tens oflow a Cutting Edge
library for neural network development
next we will explore table a popular
data visualization tool to transform row
data into insightful dashboards lastly
we'll Master the art of data storage and
retrieval using MySQL a robust and
scalable database management system get
ready to unlock the full potential of
data science as we embark on this
thrilling tutorial equipping you with
the skills and knowledge to concer any
data driven challenge
in this data science tools course we
will learn the following join us as we
demystify the world of data science and
Empower you to harness the power ofit
tool to drive strategic success let's
dive in if these are the types of videos
you would like to watch then hit the
Subscribe button like and press on the
Bell icon to never miss any further
content if you are one of the aspiring
data scientists looking for online
training and graduating from the best
universities or a professional who
elicits to switch careers with data
analytics by learning from the experts
then try giving a short to Simply learn
skel Tech postgraduate program in data
science in collaboration with IBM the
link is mentioned in the description box
and you should navigate to the homepage
where you can find a complete overview
of the program being offered hello and
welcome to the lesson formatting
conditional formatting and important
functions in this lesson we will learn
about custom formatting how to use
conditional formatting and formulas and
Implement logical statistical and
mathematical functions let's say we need
to different differentiate the sales
value from the given data set as above
Target below Target and met
Target as a data set can be huge and
complex differentiating each value
manually would consume a lot of effort
and time here comes the Excel features
such as formatting and conditional
formatting which enables us to perform
this task easily formatting as the name
suggests helps us to format the data
using different techniques making the
data easy to read in the required format
and analyze them the right formatting
technique applied to the worksheets can
help the users present the data
efficiently conditional formatting a
type of formatting helps us visually
explore and analyze data detect critical
issues and identify patterns and Trends
conditional formatting helps to add
patterns and Trends to the raw
information using different colors icons
and formula
Etc at times we also need to perform
mathematical operations and calculations
within our data set Excel provides a
large number of logical statistical and
mathematical functions such as vlookup
hookup and if not rank quartile and many
more that can help the users to perform
calculations ranging from basic to
complex
operations these inbuilt functions help
us to manage data and perform
descriptive statistical
analysis Excel also offers a wide range
of important formulas that help to
perform many common tasks these formula
may be as simple as basic addition or it
could be a complex combination of
inbuilt excel functions before going
into details let us start this lesson by
defining the objectives of the
lesson the worksheets appear more
polished and easy to read if proper
formatting techniques are applied to it
we can format the cells in Microsoft
Excel manually by selecting fonts font
color font size background colors and
borders we can also use numerous
predefined table Styles or quick styles
to quickly format a table automatically
this is the basic formatting in
Excel if these built-in functions do not
meet our needs or do not display the
data in the format that we require we
can create our own custom format in this
topic we will focus on custom formatting
in Excel let's say we want to display 1,
532,000 as 1.5 3M or the 1st of January
2016 date with a weekday as the 1st
Friday January
2016 here we can use custom formatting
feature of excel let's see how we can
accomplish this in
Excel in this lesson we shall consider
the following fields from our master
table dates order ID product salesman
region number of customers net sales and
profit and loss here in the given
worksheet we have a column named net
sales suppose we want to display the
values of net sales in terms of
thousands for example
7,164 should be displayed at
7.16k select the entire column for net
sales under the Home tab click the drop
down present in the numbers panel
select the option more number formats a
format cells dialogue box appears from
the category list select the type as
custom in the type text box enter number
column number number Z decimal 0 0 comma
open double quote capital K close double
quote here number is the digit holder
with thousand comma
separation each comma after
decimal 0 0 indicate divide value by
1,000 the two zeros after decimal point
indicate number of value after decimal
places the K component displays a
literal K character to denote thousands
here we can see the preview in sample
box the value shown is
7.16k finally click okay
button the values in column net sales
are now displayed in terms of
thousands now let us consider another
example of date
formatting in the column dates we have a
date value written as 1st Jan
2012 we want to display it as first
Sunday Jan
2012 first select the column containing
dates under the Home tab click the
drop-down present in numbers panel
select the option more number formats a
format cell dialogue box appears from
the category list select the type as
custom in the type text box enter dd-
DDD dd-
mmm- y
y here d stands for date m stands for
month and y stands for year the first
2dd is the date
the next four D D D D is for weekday say
Thursday then mmm is for the month in
short form and YY is for year here we
can see in the preview in the sample box
the value shown is 1 Sunday Jan 12
finally click on okay button
thus the custom formatting helps us in
changing the appearance of the cell
value according to our needs a worksheet
may contain thousands of rows of data by
simply examining the raw information it
would be difficult to see patterns and
Trends conditional formatting helps us
visualize data that make worksheets
easier to understand it quickly
highlights important information in a
spreadsheet by using colors icons and
datab bars it changes the appearance of
one or more cells when the cell value
meets certain criteria to do this we
need to create a conditional formatting
rule for example a conditional
formatting rule can be if value is
greater than 5,000 color the cell Yellow
by applying this rule we'll be able to
quickly see which cells contain values
that are greater than 5,000 let us
understand this with the help of
examples consider the given sales table
which depicts the amount of sales done
by each salesman here we want to
highlight the duplicate order ID
values first select the order ID
column click Home tab under Styles panel
select the option conditional
formatting from the drop-down menu hover
the mouse over highlight cells rules and
select duplicate values a duplicate
values dialogue box appears
select a formatting style from the
drop-down menu in our example we will
choose green fill with dark green text
click okay here we can see that the
conditional formatting is applied to the
column order ID Excel highlights all the
duplicate cells with green fill with
dark green
text we can also explore other options
available in conditional formatting like
greater than less than between equal to
text that contains a date occurring and
top/ bottom rules all these rules work
in the similar
format Excel provides predefined styles
to quickly apply conditional formatting
to our data they are data bars data bars
are horizontal bars added to each column
much like a bar graph color scales color
scales change the color of each cell
based on its value
each color scale uses a two or three
color gradient icon sets icon sets add
specific icon to each cell based on its
value let us consider another example to
explain how icon set rules are used
consider the given sales
table here on the basis of the sales
amount we want to apply the icon set
rule if sales amount is greater than
8,000 then a green arrow should appear
in the upward direction if sales amount
is between 5,000 and 8,000 then show a
yellow arrow in the horizontal Direction
and if sales amount is less than 5,000
then show a red arrow in the downward
direction to perform conditional
formatting select the desired
cells click on Home tab under Styles
panel select the option conditional
formatting
from the drop- down menu hover the mouse
over icon set rules and select the first
rule again click on conditional
formatting select the option manage
rules from the dropdown a conditional
formatting rules manager dialogue box
appears go to Icon set Rule and select
three icon set rule click on edit Rule
and an edit rule formatting dialogue box
appears by default for three icon set
rule Excel uses the 67th percentile and
the 33 percentile rule this means that
it shows a green arrow for values
greater than
67% yellow arrows for values between 67
and
33% and red arrow for values less than
33% now let us customize this rule for
our criteria in the typee drop down
select the option as
number in the value reference box enter
value is 8,000 for green color arrow and
5,000 for yellow color
Arrow Excel automatically picks the
criteria for the red color Arrow click
okay in the conditional formatting rules
manager window click
okay we can see that the cells are
highlighted as per our criteria cells
which have a value greater than 8,000
have a green arrow in the upward
Direction values between 5K and 8K are
highlighted with a yellow arrow and
values less than 5K are highlighted with
a red
arrow we can also apply conditional
formatting rule with the help of
formulas consider the given table which
shows the profit done by each salesman
here we want to highlight the rows in
the table whereever profit is greater
than
4,000 select the entire table along with
headings click on conditional formatting
select the option new rule from the
dropdown a new formatting rule. dialog
box
appears under select rule type select
the option use a formula to determine
which cells to format under format
values reference box enter the formula
as equals if Open Bracket dollar sign H2
greater than 4,000 comma true comma
false close bracket
this will apply formatting to a sell if
the profit is greater than 4,000 click
on format button from the format cells
dialogue box select the fill tab from
the color dialogue box select the color
as green finally click
okay the conditional formatting is
successfully applied to the cells here
we can see that the cells with the
profit greater than 4 th000 are
highlighted with the green
color logical functions evaluate a cell
or cells for a criteria and returns a
Boolean value true or false in this
topic we will cover various logical
functions like if and or not true
false consider the given sales table
here on the basis of the sales amount we
want to determine the commission
percentage of each salesman
following is the criteria if net sales
greater than or equal to 10,000 set
commission as
5% if net sales greater than or equal to
5,000 and less than or equal to 10,000
set commission as 2.5% if net sales less
than 5,000 set commission as
1.5% this is a common condition where
we're required to create a between
formula that picks all values between
the two given values in conditions we
use if with the and function let's write
the formula in cell I5 like any other
formula we will start with the equal
sign then type our function name if and
then type an open parenthesis the first
parameter we need to pass is a logical
test here The Logical test is if sales
cell G5 greater than or equal to 10,000
assign value as
5% then we will pass the second logical
test here here we will use and function
if Open Bracket and Open Bracket Sal
cell G5 greater than or equal to 5,000
comma cell G5 less than or equal to
10,000 comma assign value as
2.5% in the else parameter we will type
1.5% this means that if the first two
cases become false then the formula will
assign the value
1.5 finally close the two braces for the
two ifs thus Excel Returns the result as
2.5% for cell G5 now drag the formula
with the help of the fill handle we can
see the results for the other cells as
well the or function returns true if any
of the conditions are true and returns
false if all of the conditions are false
in the given table we are required to
assign performance remarks for each
salesman as per the scaling table
in scaling table we have the criteria
that if net sales greater than 10,000 or
profit greater than 5,000 set remark as
good else set remark as needs
Improvement let's write the formula
using IF and or to check sales and
maximum sales in cell
J5 so our formula becomes equals if Open
Bracket or Open Bracket G5 greater than
10,000 comma H5 greater than or equal to
5,000 comma double quote good double
quote comma double quote need
Improvement double quote close
bracket let's drag this formula across
the cells with the help of the fill
handle this formula return true if any
of the condition is true thus in the
given example the cell j9 returns good
even if if profit is less than
5,000 our next function is true/ false
true/ false returns true and false
respectively if we write them in any
cell the not function reverses the
result of the function if result is true
it returns false else true let's write
formula as equal not open bracket K5
close bracket in cell L5
this Returns the result as
false let us now study about different
lookup and reference functions used in
Excel such as vlookup hookup match index
and offset function the vlookup function
lets us search for specific information
in the current worksheet say we have a
list of students with marks we can
search for the marks of a specific
student if our table is in horizontal
format and we want to determine a
specific value then we use the H lookup
function consider the extract from our
Master sales
table the table depicts sales
information based on order ID product
sold salesman and net sales now we want
to determine the net sales corresponding
to order ID
1121 let's understand how we can
accomplish this using the vlookup
function note that vlookup function is
only used for vertical tables let's use
cell I2 to add the formula like any
other formula we will start with the
equal sign then type our function name V
lookup and then type an open
parenthesis now we will add the
arguments the first argument is look up
value lookup value corresponds to the
value or item we are searching for so in
our example it will be order ID
1121 cell
H2 the second argument is table array t
T array is the cell range that contains
the data in this example our data is in
cell B2 to
E40 the third argument is call index num
column index number the column index
number is the column number in which we
are looking for information here in our
table array net sales is the fourth
column so our third AR arent will be
four the fourth argument is range lookup
this value corresponds if we are looking
for approximate matches or not and can
have value either true or false true
denotes the approximate match and false
denotes the exact match here we will
pass false finally close the parentheses
so our function has the form equal V
lookup Open Bracket H2 comma B2 colon
E40 comma 4 comma false close bracket we
can see that the function Returns the
value as
2277 note that the vup function works
from left to right that means the lookup
value should always be present in the
leftmost column of our table
array if our table is in horizontal
format and we want to determine a
specific value then we use the H lookup
function consider the given table let's
write the formula in cell I4 we will
start with the equal sign then type our
function name H lookup and then type
open parenthesis the first argument is
the lookup value so in our example it
will be order ID equals
1121 cell
84 the second argument is table array in
this example our data is in cell G8 to
as1 the third argument is row index
number here our table array net sales is
the fourth row so our third argument
will be four the fourth argument is
range lookup here we will pass false
finally close the parentheses so our
function has the form equals H lookup
Open Bracket H4 comma G8 colon
a11 comma 4 comma false close bracket
the function Returns the value
2277 note that H lookup function works
from left to right that means means
lookup value should always be in the
topmost row of your table
array the match function searches for a
specified value in an item in a single
dimensional array and then Returns the
relative position of that item in the
array let us understand its functioning
with the help of an example consider the
given sales table shown here we want to
determine the position of the
corresponding date in the original table
so let's write the formula in cell
E5 type equal sign then type the
function name match and then type an
open
parenthesis the first argument is the
lookup value so in our example it will
be date in cell
B5 the second argument is lookup array
the lookup array is a range of cells
that contains the value we are searching
for in this example we will select the
cells B17 to b1342
the third argument is match type match
type can be either minus1 0 or one the
value should be one if we want to find
the largest value less than or equal to
the lookup
value the value should be zero if we
want to find the first value exactly
equal to lookup value it should be minus
one if we want to find the smallest
value greater than or equal to lookup
value here we will pass the match type
as zero as we require an exact match
finally close the
parentheses so our function should look
like match Open Bracket B5 comma dollar
sign B dollar sign 17 colon dollar sign
B dollar sign
1342 comma 0 close
bracket this Returns the value as three
now we can drag the formula across cells
with the help of the fill
handle similarly we can use the match
formula in cell E3 to determine the
position of the shown field name cell C3
in range b 17 to i7 for example net
sales is at 7th position in the range B7
to
I17 let's write the function in cell E3
write formula as equal sign match and
open parentheses in the first argument
select cell C3 for second argument
select cells b77 to
I17 past third argument is zero finally
close the
parentheses so our function looks like
equal match Open Bracket C3 comma dollar
sign B dollar sign 17 colon dollar sign
I dollar sign 17 comma 0 close bracket
so for profit loss field the value
returned is
eight net sale the value returned is
seven the index function Returns value
from a specified position in a specific
column in a list suppose we want to
determine the value of a field mentioned
in cell C3 from the s table on a
specified date the table should
dynamically fetch the value if the value
of the cell C3 is changed if we enter
profit loss formula should fetch the
profit loss value for the specified
date if we write net sales in cell C3
then formula should fetch net sales
value for specified date so let's use
the index function to determine the
value in this worksheet select cell C5
formula as equal sign index and open
parentheses the first argument is the
array for our example enter B17 colon I
1342
the second argument is the row num this
corresponds to the row number that has
the value we want to return here we will
enter E5 which denotes the required
date the third column is col num this
corresponds to the column number that
has the value we want to return
here we will enter E3 which denotes the
position of the field name in cell C3 in
table finally close the
parenthesis thus our formula Returns the
value as
6528 based on the position at cell's E5
this is the net sales value on the date
the 2nd of January
now drag the formula with the fill
handle to the other cells as
well the offset function returns a
reference to a range that is the offset
number of the rows and columns from
another range or cell now let's
understand how to use it select cell D5
write formula as equal sign offset an
open parentheses the first argument is
the reference this denotes the starting
r range from which the offset will be
applied for our example we will select
the reference cell as
B17 the second argument is the rows this
denotes the number of rows to apply as
the offset to the reference this can be
a positive or A negative number so for
our example we will enter E5
min-1 here we are subtracting one to
match number of row which have the value
and the offside set from the reference
cell the third argument is the calls
this corresponds to the column number
that has the value we want to return
here we will enter E3 which denotes the
position of the field name and subtract
by one to match with the offset
referencing finally close the
parentheses the function Returns the
value based on the position of date from
cell B5 and field from cell C3 now drag
the formula across the cells with the
help of the fill handle if we enter
profit loss formula should fetch the
profit loss value for the specified
date a variety of statistical functions
are available in Excel to perform
calculations ranging from basic function
mean median and mode to more composite
distribution and test binomial or chai
square in this topic we will cover
various statistical functions like sum
if count IFS percentile quartile
standard deviation and
median some ifs and cifs are the most
frequently used functions in Excel these
functions allow us to perform count and
sum based on one or more criteria the
sum ifs function calculates the sum of
cells based on certain
criteria let's understand how to use the
function with the help of an example
consider the given sales table say we
want to determine the net sales done by
salesman just
here we will use the sum ifs function to
calculate net sales so let's write the
formula in cell N4 type equal sign then
type the function name sum ifs and then
type an open parenthesis the first
argument is the sum range this is the
input range that contains the required
data so in our example we will select
cells H4 to H
1,328 the second argument is the
criteria range this denotes the range
that contains the required criteria here
we will pass the range that holds
salesman's name sales E4 to e
1,328 the third argument is criteria
this corresponds to our required
criteria we only require net sales of
the salesman Justin so we will select
cell
M4 finally close the parenthesis note
that we can pass multiple criteria in
this function the function Returns the
net sales done by
Justin countifs function Works in a
similar manner like some ifs it counts
the number of sales in a given range
when certain criteria is met say in the
given sales table we want to determine
the total number of sales for net sales
greater than 8,000 to determine the
count we will use the count ifs function
so let's write the formula in cell N5
type the equal sign then type the
function name count ifs and then type an
open parenthesis the first argument is
the criteria range one this denotes the
range that contains the required
criteria here we will pass the cells
containing net sales H4 to H
1328 the second argument is criteria one
this corresponds to our required
criteria we only require entries with
net sales greater than 8,000 so we will
enter greater than 8,000 finally close
the
parentheses note that the function
allows you to enter multiple criteria
cell N5 reflects the total number of
sales as
445 for net sales greater than
8,000 in this topic we will learn how to
use the functions percentile quartile
standard deviation and median in
Excel consider the given sales table
let's see how to use the percentile
function on N Sales to find its 40th
percentile select the cell n6 write the
formula as equal sign percentile Open
Bracket select the column net sales H4
to H
1328 as our first argument pass the
second argument as 0.4 in order to find
the 40th percentile this Returns the
result
5,220 this means that 40% of the net
sales is less than or equal to 5 ,
220 the next function is quartile
quartile function depicts the results
for the first second third and fourth
quartile say we want to find the total
number of sales which are less than or
equal to
25% let's use the quartile function to
calculate this select cell N7 to write
the
formula type equal sign function name is
quartile and open
braces select net sales column H4 to H
1328 as the first
argument type one as the second
argument the second argument must be a
number between 0 and four the function
Returns the value as
3,822 which means that 25% of the sale
is less than or equal to
3,822 note that here we can use use a
percentile function to perform the same
task the function percentile H4 to H
1328 comma
0.25 gives us the same result it's up to
us which function we would like to use
for the
quartiles Excel has a function to
calculate sample standard deviations
stde V standard deviation determines how
dispersed the data is from the mean say
we want to calculate the standard
deviation for number of customers
so write the formula in cell N8 as
equals the function name
STD open
braces select number of customers
columns as its
argument the function Returns the result
1.4 the median function Returns the
value from the range for example we want
to find the mid value for net sales to
calculate median in cell N9 write the
formula as equal sign the function name
median then open braces select net sales
column as its
argument this calculates the mid value
of net sales as 6,192
rank function is used to compare a
number from the rest of the numbers in a
list in any order whether ascending or
descending this function Returns the
rank of the number relative to the other
values in the list let's use rank
function in cell j4 to calculate rank on
the basis of net sales select cell j4
and write the formula as equal sign then
type the function name rank and then
type in open parenthesis the first
argument is the number this is the value
whose rank we are required to calculate
here we will pass the argument cell H4
the second argument is ref this
corresponds to the array or reference
list here we will select the column net
sales I.E sales H4 to H 1328
the third number is order this specifies
how to rank the number zero means
descending order and one denotes
ascending order this argument is
optional if we don't pass any argument
Excel automatically takes the argument
zero here we will pass zero finally
close the parenthesis here we can see
that Excel has determined the rank for
net sales
7,164 as 553
finally we can drag the formula and can
see the rank for the other cells based
on net
sales so what is data science data
science is the field of study that deals
with modern scientific techniques
statistical methods and algorithms to
derive insights from vast volumes of
data companies using data science are
very numerous with companies generating
data every minute there is a need to
make sense of this data uh Mo Sigma
fractal Fidelity Intel gimpa cubala
Amazon trendin connect the
dots just about every business out there
nowadays uses some form of data science
even if it's indirectly to help govern
their business and help them growth and
compete in the business world and we use
the data science to find unseen patterns
within the data analyze and draw
insights from our data and solve
business problems and make decisions and
the data science has a life cycle we
start with our data Discovery then we go
through data preparation exploratory
data analysis data modeling interpret
the results and back to data Discovery
when we talk about data Discovery data
Discovery involves asking the right
questions identifying the business
problems finding the best solution and
Gathering correct data from relevant
sources for
[Music]
analysis this is in the data science
life cycle the most important
step now in general it only involves
like a small percentage of the time you
spend on this but if you're not asking
the right questions you can't go in the
right direction data preparation data
preparation deals with cleaning the data
finding the inconsistencies tackling
missing values converting it into the
right format and making it ready for
analysis when I'm running the data
science life cycle this is probably
where the most time is spent data can be
so
messy you have to break it down into
different parts you have to import the
different pieces from different
databases are you dealing with straight
numbers off of spreadsheets and cells or
are you dealing with text documents and
natural token language tokenization
exploratory data analysis analyze and
visualize the data using different
charts and graphs to find significant
patterns and hidden trends that helps to
understand the data better the building
of the charts is very creative probably
one of the more enjoyable Parts is be
able to come in there and describe the
data and show that to the shareholders
then we dive into Data modeling use
classification regression and clustering
algorithms to build machine learning
models that can help predict and
forecast future Trends this is where we
start start getting into predictions
what's next where's it going to go and
then interpret the results validate the
model results draw the final conclusion
and check if the performance of the
model is in line with the
requirements the final step is the one
that you set up in front of the board
members and give them suggestions as to
what's next where we going to spend our
marketing money where are we going to
start preparing our firefighters for the
next fire breaks next year uh what is
the weather patterns coming in this is
where you interpret the results and say
how accurate they are are they valid is
it something we're going to keep or
we're going to throw it out and have to
start over we look at data science
components there's the mathematics
statistics domain expertise data
engineering data visualization and
machine learning when we sum this up in
data science really uh this point in all
of this you want to focus on first is
domain expertise now when you're taking
your classes obviously you want to go
through the mathematics the statistics
the data engineering the data
visualization the machine learning and
learning all those
tools but it really in the job field the
domain expertise comes in because if
you're really good with business and you
go into business with these tools uh
then you're able to ask the right
questions and deal with it when we talk
about the tools in data science and
where they sit uh there is a data
analysis data analysis focuses more on
statistics and mathematics where the
data science size where the data science
side focuses more on the machine
learning data
visualization and then there's database
management which really focuses on the
data engineering part of it and how to
manage that data and the data flow you
can see all these components come
together and then they form the data
science and we talk about Tools in data
science uh we have SAS we have matlb we
have SQL uh you have your Apache spark
your r blue uh python right now in the
programming languages python is probably
the most widely used with r coming in a
close second the reason for that is
these are both open source and anybody
can use them when we look at SAS and
taboo those are two packages that are
paid for which have a lot of automation
behind them so a lot of larger companies
will pay for those packages because they
help work out a lot of the the backend
stuff before you even need to use them
and of course we have Apache spark you
almost have to know Apache spark in the
growing data uh Lakes today that's your
big data how do you distribute this if
you're going to process it over a large
amount of data when we look at data
science applications there's internet
search very common you enter your Google
thing and it uses the data science uh
algorithms to figure out what to return
Voice assistance analyzing The Voice
coming in and connecting it with
whatever patterns they need to activate
so you can ask ask to turn your lights
on in the living room Health Care
biggest Phil growing right now how do we
analyze Health Data from a smartwatch
and figure out how to know when a a
disease is being spread across the world
healthc care how do we identify cancer
versus non-cancer so we know who to
prioritize and get them in for surgery
where other people might not need that
immediate care and even identifying uh
problems somebody might have with their
health long before they even happen what
a neat thing to do instead of waiting
till have a heart attack uh to know
months ahead of time that you're having
heart conditions Logistics how do we get
from point A to point B if you ever look
at some of the major Distributors they
pre- ship Things based on what they
guess people are going to need this way
you get your stuff right in the mailbox
the day after you order it
Ecommerce huge in there because you have
to know what kind of marketing we're
going to put out uh who are we selling
to what's our target audience robotics
one of the funner ones when you're
coming up into the future is how do we
automate the different uh um tasks we
have from self-driving cars to
manufacturing lines to maybe even a
self- robotic Chef in the kitchen so we
talk about solving problems with data
science we have to ask possible
questions and the desired algorithms and
these really are uh kind of an overview
of some of terminology you'll want to
know in data science so we talk about
that how much or how many we're talking
about regression regression basically
means a number if you've ever seen stock
charts you see uh um the guess it what
the next sales price is or buy price is
that's all on your regression
side is it a or b
classification is that a dog or a cat
that my photo took a picture of how is
this organized clustering this is where
we take things that look alike and put
them together so that we can then make
predictions on it
how difference is this just like you
have clustering you have the opposite
anomaly detection how do we find things
that don't fit in what are the outliers
what are things we might need to look at
that aren't in our model and what to do
next reinforced learning this is
probably the newest Market is how do we
set something up whereas the data comes
in it learns from the new data as to
what the next action's going to take
most of these are combined so when we
talk about regression classification
clustering anomaly detection reinforced
learning a lot of times when you put
your model together you might have
multiple parts of this we might look at
the clustering of data and then feed it
into a classification is this a uh bad
loan to make at the bank or is this a
good loan to give and from there we
might start looking at when we start
clustering these things as good and bad
and what kind of uh setup is we need
might run it through a regression model
to say hey what is the amount dollar
amount this person uh should be allowed
to take a loan out and you can see that
all of these start coming in and then a
lot of the models uh reinforced learning
we talk about like bank loans and things
like that once a year they have to rerun
those algorithms and so that's like a
human run reinforced learning uh now
we're starting to automate that so that
as the data comes in real time the
models start updating themselves they
start figuring out a way to solve new
problems as they occur and of course in
robotics reinforced learning is one of
the biggest growing Fields alogorithms
used in data science linear regression
logistic regression when you're looking
at both of these uh think numbers we're
talking regression models and so linear
regression figures out the best line
through the data Maybe it's looking for
a curv line logistic regression starts
looking at data um on a Continuum so
that
it goes to an exponential you have a
spot where it might be one or the other
and then you know what it is as it goes
to one side or the other decision tree
wonderful tool if you uh used to call it
a hack now it's becoming mainstream to
look at the back end what made the
decisions so it's really easy to trace
back to how you arrived at the decision
decision trees are kind of nice that way
nearest neighbors K nearest neighbors is
your clustering K means clustering
hierarchial clustering uh so hierarchal
starts dividing them and builds a
hierarchy where the K means and the K
nearest neighbor look for things that
kind of connect each other how close are
they in data format DB scan another form
of clustering your data principal
component analysis so we start breaking
up and looking at the different
components in our data coming in these
are all algorithms used in data science
and there are more these are kind of the
mainline ones and each one of these has
many settings which you can play with to
build a better model to predict your
data and where you want it to go let's
go ahead and take a Hands-On demo and
just see what does this look like now
I'm going to go ahead and use the
Anaconda Navigator and the um which is
your jupyter notebook which will then
open up in Google Chrome this is one of
the IDE or ways of editing python code I
like this one CU it one protects my
computer and creates its own kind of
container and two it's got a really nice
visual demo for doing python certainly
you can use a number of other different
tools like py charm um there another
real popular one you can actually access
py charm code through the anacon
Navigator now and there's a lot of other
tools on there that each have their
benefit and we come into our Jupiter
notebook I go and create a new python
I've already done that and we'll start
with a logistics regression model just
so you can see what this looks like in
code we'll actually do two of these uh
as we go through this and take a look
and see what that code looks like in
Python now the first thing we want to go
ahead and do is um load up all our parts
modules that we need in our python code
uh I'll go and run that and you'll see
here I'm going to uh ignore warnings
because some of the stuff we're doing uh
we're just looking at it and as you go
from one version of python to one
version of whatever uh the site kit or
in this case the SC package you'll see
some warning come up they're not really
important right now um and then we're
going to go ahead and use the German
Credit Data
CSV and so when I run this this is all
loaded up and let's go ahead and just
open up this CSV file so you can see
what that looks like and here he is
here's our German Credit Data file uh
we'll go ahead and just open that up
with uh let's do uh word pad just fine
because it's a Word
document and when we open that
up you can see right here here is just a
bunch of comma separated values we have
across the top actually has columns on
here so our first row is a bunch of
columns um and then of course we look
down here oops I just moved stuff around
but that's okay I'm not going to save it
and you can see the values uh al1 al4
whatever that means or A14 I guess um
and then some sorted numbers on
here and we'll take a closer look when
we get into the
program and looking at the data for
right now we've loaded that under credit
DF and at this point I want to just kind
of take a detour back to the life cycle
of data science so we can see how that
fits in there and so we've already done
our data Discovery um we have a little
data prep but the data is pretty clean
um it already comes in preset on there
we'll might do a little data preparation
in here we're going to start looking at
exploring the data uh analysis and
building a data model and then
interpreting the results so really we're
focusing on the last three steps of this
the data Discovery would be asking uh
start asking questions of what do we
want in there is this going to be is
this going to be a good loan a bad loan
what do we want to get out of this data
where are we going with it that would be
part of the data Discovery and a lot of
times that might be um a meeting you
have with the management in the office
or whatever and then data preparation is
of course cleaning the data up uh making
sure you know where the source came from
um all those kind of fun things that go
into making sure you have good data
coming
in now you'll notice here uh we've got
we're using uh pandas whoops drew it
through the pandas and we're going to go
ahead and read it as a pandas data frame
and of course pandas sits on numpy so a
lot of stuff you do with pandas can go
right into numpy very easily uh and the
reason I bring this up is because pandas
has a lot of tools in there for just
looking at the data real quick and
easy and so the first one we want to go
ahead and do is do credit. DF or
creditor DF our data we read in info and
just see what that looks
like and when we go ahead and print this
out you can see here we have um 13
columns remember we were looking at our
data and it had those that row you
couldn't read across the top um then it
has non-null count how many values don't
have null what kind of objects are they
they're an object an
integer um different things like that
and we look at object is it a checking
account this is probably text on here uh
duration how long credit history amount
savings account present employe employed
sense uh so a lot of different numbers
coming in here and these are just the
columns we're looking
at and if you're going to look at the
columns it's kind of nice to actually
look at the data and so we'll use the
pandas to go ahead and print it out just
like you would on a
spreadsheet um hard to see them all on
here but you can see we have here's our
row across the top because Panda does
such a good job sorting out
columns and the actual data uh checking
account A1 a12 A14 not sure what that
means duration how long they've had it
credit history um not sure what the a33
or a34 means I'm sure it has a special
meaning on there the amount uh here's
our savings account and all the
information on
that and this is actually a very uh much
more easy to r view than what we had
before as far as like that text file
when you're just looking at data through
the database sometimes those can be
really hard to read this you can really
see and if I take it down a notch I can
get all the columns in here um we'll go
and keep it zoomed in just so we can see
that what's going on a little easier
and then we can also do uh credit DF do
iocation the iocation feature in the
pandas uh is going to go ahead and do
rows 0 through five and we're going to
look at columns 1 through 7 and let's go
and just run that and you can see right
here uh how we just split it up so it's
really easy to manipulate our data and
get a a smaller view of different parts
of the data as we're going through it
and just like we can look at the first
uh seven columns we can also look at the
last seven
columns uh same kind of format we're
looking at rows 0 through 5 column 7 to
the end if you leave the number out
there and you can see we can easily now
look at this on a smaller screen uh much
better for displaying those are just
tricks to help you
display your information so that when
you present this in front of a group of
people it makes it very easy to read or
you can just copy this out put it into a
document that kind of thing uh and then
we also like to go ahead and do like um
credit status value counts and things
like that again we're just looking at
the different uh uh sets of information
on here and in um our Panda data frame
value counts is we're looking at unique
values so there's a lot of unique values
in here that's um
7300 that's a significantly uh diverse
set of data
and if we look at the data one of the
things we're looking at is the uh status
01
01 um and I'm going to guess that that
means that it is either default or not
default so that's probably what we're
going to want to look for um hopefully
you've had your um specialty in domain
remember we talked about the different
parts of data science domain is what
filled it is in uh this would be in
Banks and loans and so if we want to
figure out what the status of a new loan
is we're going to separate the status
out and so if we take our um X features
equals the list of the columns that's
all this is creditor DF doc columns we
remove
status uh and then we go ahead and this
will print our features out and you can
see it's everything but the status on
the end because that's what we want to
predict we want to predict the new data
uh whether that status is zero or one or
not and next we want to do a little bit
more data prep preparation we're going
to go ahead and
incode um and we're going to take an
encode credit DF we're going to create
that off of our um setup on here and
you're going to ask well what what are
we encoding on here well when we go up
to here let me just go back all the way
when we're looking at all this
information up
here you can see there's uh
object object object well if I'm doing a
prediction and something's just a random
object it doesn't really mean anything
there's no numbers to
process and so when we come down a
little further uh we can see down here
that we are looking at um a65 I'm not
sure what a65 means um we're looking at
a93 personal status we're looking at
a143 these aren't actual numbers there's
some kind of code that the bank put in
probably and so for our model they
really don't mean anything unless we
convert them to
numbers and you convert them to numbers
by um literally creating a bunch of yes
NOS so instead of thinking it as um as a
173 and a172 is being 01 and if you had
A7 174 maybe 012 you would think of this
as a label a 173 is true or false and
then you'd have another label a172 is
true or false so for every unique job in
here that it becomes another feature
added in and it's on or off it's true or
false that's what this encoding is doing
and we'll go ahead and list our encoding
as it comes out and if you come up here
uh you can see it did exactly what I was
describing we've now encoded it so
here's checking account a13 checking
account um
a1214 a31 here's all of our uh
checking account numbers so there was
three of them uh here's our credit
history a31 a832 833
834 again the computer has no idea of
knowing what those are all it knows is
those are each unique features and the
same thing with the um savings account
862 and so on going down uh and so once
we have our nice list of there we can go
ahead and do
um let pull this in here we go let's
look at our new data set
and if we go ahead and run the new data
set we have our inone credit DF uh
checking and we're just going to look at
these and look at the head this is just
the first few of these so we can see
what what it actually looks like in the
data and just like I said it's now uh 0
0 1 1 0 0 this First Column is just the
count 0 1 2 3 4 that's not part of the
data uh but you can see here that it
just puts a check mark of one uh because
it's an AC
a12 and we have a checking account AC
A14 where it's also a check mark of one
going across so we've done a little data
preparation uh we've come in here and
done some exploratory data
analysis and now we want to start
building our data model and we'll kind
of go back and forth between exploratory
data analysis and data model and then
interpreting the results on this and so
when we go ahead and import our stats
models as API
um as SM now a lot of times we use the
um sklearn or the S kit models in this
one we're going to go ahead and use the
SM model uh from stats models which is
pretty common for doing a very basic
Logistics regression model uh and then
from the S kit we're going to have our
train test split with any data you want
to go ahead and train your model and
then you need something to test it on
some unknown data so you can see how
well you did very common in your D your
um setting up your models is to split it
up into your training set and your um
testing set and what I want you to
notice on here is we have XT train well
what is XT Train That's all the columns
except for the last one which was the
status was it default or was it um in in
good standing and then of course our X
test is part of the data we're going to
split that up um by 0 three so 30% of
the data is going to be 30% of our rows
are going to be put in the X test and
the Y train and Y test is just the
status and then we create our um once
we've created our data and split it up
we go and create our model and it's a
logit we're just going to call it logit
but you a lot of times you end up seeing
logit
model um with logit it equals the sm.
logit XT Trin um oops y train XT train
in sklearn it's always XT train y train
uh but in the U stats model it's
reversed on there
uh and then they go ahead and run the
fit so we load the data up and then we
fit
it uh and then we can go ahead and
summarize that model and see what it
looks like let's go ahead and run
that it takes it just a moment uh and we
get a nice summary of our data again
we're still looking at the data um and
we have um takes just a moment to to
kind of feed through and you can
certainly see where we have our
coefficient um which has to do with the
weights in the model standard error uh
that's very important as we look at this
coefficient and how it affects the model
um p is greater than the absolute value
of Z 025 these are all your statistical
notes on here uh so as you look at these
you might look at like uh how big is a
standard error that's one of the big
ones that I usually look at um as we go
up and we start looking at the different
things uh the higher the error the less
uh the more we want to be a little
careful of it the lower the error the
better in more general terms they also
have um uh dealing with the model and
its iterations you can see some general
status on here how many iterations did I
have to go through uh before I was able
to come up with the numbers because it
kind of guesses it's playing a high low
game and it takes it six iterations till
it finds what it considers the best fit
uh and you can see there's more
information up here as far as uh when it
convered how it conver and so on now
when we have our um summary up there one
of the questions starts coming in
remember I told you we're looking at
like our standard error and you know how
these numbers all are telling us how
important each one of these features is
in running our prediction and if you
have a ton of features and some of them
are just kind of all over the place all
they do is create noise uh so we really
want to get rid of them they also cause
issues in that if you're running a huge
amount of data and you have to rerun
this model over and over again uh this
can really tax your time time and so
you're losing both time money and your
results might not be as good because
they might be weighted based on some of
these very fuzzy
numbers uh so we're going to look at
significant variables and put together a
little routine for that again we're uh
um we've come in if you remember from
back here uh we're still in kind of an
exploratory data analysis and data
modeling they kind of go hand inand
sometimes we haven't quite got to
interpreting the results in running
predictions and so we'll go ahead and
build this thing on here it looks at the
um LM values so that's coming up here
where this was
generated I have to actually look that
up for the this particular model because
I usually use the sklearn version of
this uh so as we look at this we're
going to look at the P values um of our
variables and basically we're looking to
see how good the P values are if they're
less than or equal to 0.05 we probably
um uh those are probably solid values
and if they're greater than that then
probably we're not interested in them
and so let's go ahead and run this and
we see that when we pull out all the
values that are less than or equal to
0.05 for the P values and if we go up
here uh here we go here's our P I knew
it was up here I just wasn't seeing it
correctly um so you can probably see
duration that's going to be good amount
0.01 that looks good instant
instant ins rate
which I'm guessing is the percentage
rate or the interest um trying to say in
ins and interest at the same time so we
have our interest rate and all this if
we come down here we see duration amount
um our instant rate age uh checking
account a13 checking account A14 credit
history a34 savings account
a65 these are our primary features these
ones are related when we look at the
data these are the ones that are going
to predict whether it's a good or bad
bad loan and so now that we know which
ones work we want to go ahead and create
our recreate our SM model or our final
logit model and you'll see here we now
have the um XT train the Y train and
then instead of the X train it's XT
train significant variables right there
uh so all we're doing is is running this
just on the significant variables in the
setup and we'll go ahead and fit it and
you can see here you can actually um
double bracket and do it all in one line
above we had to do it in the two lines
which just how the um setup is for this
particular module and then we'll go
ahead and do a final legit summary and
just see what that looks like and we can
see here here's our P scores remember
they're all supposed to be under 0.0 um
underneath the 0.05 these are all under
0.05 you can see up here are different
statuses as far as how it ran it still
took six iterations to go through the
data um the date and so forth and now
that we have a solid model uh and let's
if you remember correctly let me flip
back on over here we've done our data
modeling uh We've created a solid model
and so now we need to actually run a
prediction so that we can interpret the
results and uh weigh in on that and see
how good they are and so for that I'm
going to go ahead and let's see we find
it over
here uh y predict uh so we have our
model that we created we want to go ah
and create our y predict ICT and it's
based on our final adct and here's the
magic word predict or do predict um and
so this is where we're going to come in
and actually get our predictions on here
that our model comes up and remember
we're we're testing um X test is going
in there's our X test and it's only the
significant variables so we've created
our model now we want to push in our
test version that's never seen this data
before so this is all new and then we're
just going to take a random sample to
see what it looks like like uh what do
those predictions look
like in this random sample we're just
looking at 10 of these uh and you can
see here is the actual number it says
0.08 gives us a one
007 this is our probability number which
is um I would have to actually look to
see whether um higher or lower is better
I'm guessing the closer
to zero the better it is but I'm not
completely sure on that I have to
actually look up on and see what this
particular
framework uh but you can see the actual
prediction 1 0 0 0 so I guess this is a
good loan a bunch of defaults and some
good loans that's what it predicts it's
going to come out as now just looking at
the data um you know it's confusing this
is you have different models they have
different setup they have the U
predicted probability comes out on this
in other models it doesn't so you really
need a tool to dig deeper into the setup
to understand what this means and so
what we're going to do is we're going to
take our our um predicted values which
you can see right here has an actual
predicted
probability um and then we're going to
go ahead and insert on
there um from a sample let me just run
this because it'll make a lot more sense
we have our actual value and our
predicted value so this is what we
predicted and this is the actual setup
um if we just look at this right off the
bat we can see that we predicted zero
and this was a one these all came out
correct
uh number 332 came out correct 9917 was
an actual they were good on their loan
and they predicted it's going to be a
default you start to see that there's
some um variance so how do we measure
that how do we know whether our model
was any good or
not well for that we're going to go
ahead and load up um a nice graph and
we'll use uh for our graph we'll import
met plot Library pip plot and caborn
caborn sits on map plot Library
and this uh M plot Library inline is
just because I'm using Jupiter notebook
uh with the new version I'm in it
doesn't even need it uh but I tend to
leave it in for older versions and then
we import from our sklearn metrics this
is your s kit uh s kit is the main
package and SK learn is how it looks on
here and we'll go ahead and um run that
so it's nicely loaded and then we'll go
ahead and create a little
routine me just a moment here
to print this out so we can see what
that looks like and this metric is kind
of a fun metric because it it tells you
uh like how many you have correct on the
correct
one how many you had wrong on the wrong
one um and this top part is just all a
draw routine so it comes in here and has
our actual and our predicted and has our
CM metrics it breaks it up puts it into
the heat map the SN heat map gives it
some colors
add some labels um and then we go ahe
and show it and we're just going to dump
that in there and run that and you can
spend forever building Graphics is so
much fun we come in here we have bad
credit good
credit uh we have bad credit good credit
we have predicted label and so when you
look at this you can see here it it had
um on the good credit it tells you how
well I predicted it and how many false
positive it has and on our bad credit um
here we have bad credit bad credit it
said 30 of those were bad credit it
predicted them bad
credit well 21 were good credit that it
predicted would be bad credit and then
we have here we have 188 that were good
credit uh but the actual Val and then it
had 61 that were bad credit that it said
were good were predicted good credit uh
so at this point you look at this this
and you'd say well if I'm predicting uh
these
are this is our good credit here let me
flip this on over here and this is our
bad credit there we
go um so we have bad credit good credit
you start looking that and you say okay
can we afford that many defaults after
five years or whatever it is um and so
those are the questions you start asking
is does this model work good enough to
predict whether we should give these
people loans or not you're always going
to have somebody default on them and
again that starts going into domain uh
banking domain something I don't know
very much about clearly I know whether I
default that alone or not which I don't
and I try very hard not to and then if
we're going to draw the chart which is
um really nice to have just a clear
visual chart um metrics also has our
classification report and you can see
these are just the numbers we are
looking at uh they let us know the
Precision uh our fscore and recall and
all that stuff on here you can just
print a straight out metrics
classification report now we looked at a
regression model uh let's go ahead and
look at a clustering model to see what
that looks like and we'll go ahead and
create a new
ch
um python on there and this is going to
be
our clustering setup let me flip this
off and switch on over to my clustering
menu in this case we're going to be
looking at k mean means
clustering so let's just put that up
here uh K
means
clustering now the last one was a
regression model so it figured out
coefficients for lines you can go back
to your old ukian
geometry um or the algebra expression
for slope on a line yal MX plus b or
whatever um and it goes on from there as
far as finding distances and and
figuring out the lines on there this is
a little bit different when we talk
about clustering it uses similar uh
ideas but it has to do with edges um two
points and how do you figure out a
distance between two points and what
does that
mean now you'll see here we have our
Panda's PD our numpy as NP our matplot
library caborn sits on matplot Library
as we talked about um they've added some
perimeters here as far as colors and
then we did an inline for the map plot
Library again that's not really
necessary even in the new version of the
um Jupiter notebook in the old version
jupyter notebook if you didn't do that
it would just kind of hang or
disappear and then we're going to import
some data it's important to have our uh
data setup in
here and uh instead of looking at this
and um guessing what the data
is usually open up the CSV file at this
time let's just go ahead and print it
under the DS F
head because really those those text
files are so messy and you look at the
DF head and it's easy to look
at whoops had a path issue there um so
College data it's a CSV so automatically
knows there's a CSV on
there and we can see here that they
downloaded from a number of colleges um
abing Christian University adelphie
university Adrian
College and then it has whether it's
private uh I'm guessing yes or no apps
I'm not sure what that is acceptance
rate applications I'm bedding and then
the acceptance rate enrollment uh top
10% top two 5% undergrad so on so these
are just statistics based on the college
their college getting into a
college and just like we did before
let's explore the data if you can
remember from the chart we were looking
at we're going through our data
Discovery data
preparation uh and exploratory data
analysis so we're really kind of looking
at the data and what do we have here
what came in at this point the data's
already been prepared we not doing a
whole lot as far as um uh getting rid of
null values and finding out whether it's
bad data or good data and that kind of
thing and data Discovery would be
figuring out which data to send so that
we've already done so we're going to go
with the exploratory data jump into Data
modeling and interpret the results just
like we did with the other one
and we'll go ahead and use the panda
Tool uh to get the info uh DF you'll see
DF very commonly it just stands for data
frame a lot of people use it as a
generic setup when they import from
pandas and so we have our info and you
can see right here uh non-null numbers
here's our count uh so each one of these
has 777 non-null
values and again here's our type of data
we're dealing with
and if you remember from last time
here's an
object um this was our yes or no
everything else looks like it's integer
we do have a float value here uh energy
integer floats and integers usually
they're when you're running them on a
prediction they're pretty
interchangeable in a lot of cases but
whether it's private true or false
that's pretty important on here uh the
fact that they could probably be
switched to a zero or one uh is pretty
helpful also and if we're going to look
at all that data we can go ahead and
describe it so there's our data frame
describe um and you have the same thing
here's our count all these should be
777 we have the mean value so you can
actually look at what our average is
standard
deviation uh minimum different uh
quartiles 25 50 the the third quartile
that so on and of course our max value
on there Min and Max and as we explore
the data we can do a lot of fun things
on here um we're going to use our
Seaborn and we're going to look at uh
room and board versus graduation date
we're going to add this is going to be a
heat map so we're going to add um
whether it's a private college or not um
how it fits the size the aspect ratio
and all that that's what this line is
here seor you can almost do your own
class on map plot library and Seaborn
because there's so many features in
there uh but when you look at the very
Basics you can throw up some data and
have a nice colorful map set up and here
we have our graduation rate um compared
to room and board costs and in private
or um public uh so you can see that the
graduation rate uh compared to room and
board plot interesting thought how much
should I spend on room and board uh am I
going to graduate the more I spend on
room and board and you can see it
numbers go up there on graduation rate
and you can see that the um graduation
rate tends to go way up for your private
schools uh and the r and board also goes
continues to grow on private schools
kind of thing it's kind of just a fun
heat map and we can explore other things
in our um doing our heat Maps uh we can
look at um outstate whether it's out of
state undergrad um private again is one
of our setups on here and you can see
here that um for undergrad uh there
certainly is a lot more uh public
schools and whether it's out of state um
interestingly enough the out of state
number goes way up how many people
coming from out of state into the school
for the private schools uh just kind of
fun things if you're trying to figure
out where you're going to go maybe you'd
want to do this um for picking um a
bachelor's degree or a master's degree
or something or um certainly you could
also plot this with fun locations and
download information on um uh tourism
sites if you're traveling that kind of
thing you do the same kind of thing with
and we'll go ahead and throw out another
map on here um let me go and run this
this is a slightly different setup on
here with the out ofate um and so we're
running let's see pull this
up uh bins 20 the bins is each how many
uh uh bins are you going to split it
into and this is kind of a fun one where
you can see private versus public um we
have I'm many out of state
and i' have to look this up to see what
they picked on
here oh there we go okay so you can see
we have another chart that has to do
with the out of state and the number of
students coming in in private and
public uh so a lot of different setups
on here as we dig into the different
graphing so at this point we've just
been kind of looking at uh the graphs on
here let's go ahead and import our um
cluster our K means cluster
and you can see here we have we're going
to call it km uh I guess for K means or
maybe it's K means model um I always
label mine with model equals K means in
clusters equals 2 random State equals
90 uh and so we're when we cluster it
we're only going to Cluster into two
groups that's what this this me is
meaning here um across the different
settings on there and then we're going
to go ahead and fit um and we're going
tow drop private uh so we're don't we're
not going to be looking at whether it's
a private or public school we're just
going to go ahead and um run the cluster
on
there and then once we run the cluster
on there we can actually look at cluster
centers let's go ahead and do that on
this when we talk about K means let me
run this boom there we go um so we
have all these different columns and
each of those columns has a center and
we start looking how the data comes
together on those centers that's what
all these numbers this is the hidden
stuff behind the Clusters on here and so
we can now do maybe we'll create a
little converter here um if private
equals yes return one else return zero
we're going to run our cluster private
and apply the
converter uh and then we'll go ahead and
print that out let me just show you what
that looks like so you can see what
we're doing here and then we go ahead
and do uh DF cluster we'll create a new
column on here
private apply
converter uh and we can see here it
creates some private apps except we've
just added onto our data frame on here
and we did that so that we can run the
confusion
Matrix make sure we import our confusion
Matrix we're going to do a
classification report confusion Matrix
and we'll go ahead and run this on our
clustering and as you can see here as
we're predictions uh here's our Fusion
Matrix up here just like we had on the
other setup where it has how many um
false true false false uh true false and
false false as your accuracy and our
setup on there this is really uh if
you're looking at which where you want
to go maybe you're looking at where you
want to go in a um I think more of a
tourism thing but um for colleges this
is a great way for looking for a
bachelor's or a master's uh coming in
here and you can start finding out which
one you want to go to based on this
accuracy whether it's private or public
um or how many graduations are on there
if you are one of the aspiring data
scientists looking for online training
and graduating from the best
universities or a professional who
elicits to switch careers with data
analytics by learning from the experts
then try giving a short to Simply learn
skel Tech postgraduate program in data
science in collaboration with IBM the
link is mentioned in the description box
and you should navigate to the homepage
where you can find a complete overview
of the program being offered so let's
start with what is numpy numpy is the
core library for scientific and
numerical Computing in Python it
provides high performance
multi-dimensional array object and tools
for working with arrays and I'll go a
step further and say there are so many
other modules in Python built on numpy
so the fundamentals of numpy are so
important to latch on to for the python
so that you can understand the other
modules and what they're doing numpy's
main object is a multi-dimensional array
it is a table of elements usually
numbers all of the same type indexed by
a tuple of position integers in numpy
dimensions are called axes take a
one-dimensional array or we have and
remember dimensions are also called axes
you can say this is the first axis 0 1 2
3 4 five and you can see down here it
has a shape of six why because there's
six different elements in it in the one
dimension array and they usually denote
that as six comma with an empty node on
there and then we have a two-dimensional
array where you can see 0 0 1 2 3 4 5 6
7 and in here we have two axes or two
dimensions and the shape is 24 so if you
were looking at this as a matrix or in
other mathematical functions you can see
there's all kinds of importance on shape
we're not going to cover shape today but
we will cover that in part two did you
know the numpy's array class is called
ND array for numpy data array now we're
going to take a detour here because
we're working in Python and two of my
favorite Tools in Python python is the
Jupiter notebook and then I like to use
that sitting on top of anaconda and if
you flip over to jupiter. org that's
jupy t.org you can go in here you can
install it off of here if you don't want
to use the Anaconda notebook but this is
the Jupiter setup the documentation on
the Jupiter Jupiter opens up in your web
browser that's what makes it so nice is
it's portable the files are saved on
your computer they do run in I python or
iron Python and you can create all kinds
of different environments in there which
I'll show you in just a minute I myself
like to use Anaconda that's www.
anaconda.com if you install Anaconda it
will install the Jupiter notebook with
the Anaconda separate and you can
install jupyter notebook and it'll run
completely separate from anaconda's
jupyter notebook and you can see here
I've now opened up my anaconda Navigator
what I like about the Navigator and this
is a fresh install on a new computer
which is always nice I can launch my
Jupiter notebook from in here I can
bring other tool tools so the Anaconda
does a lot more and under environments I
only have the one environment and I can
open up the terminal specific to this
environment this one happens to have
python 37 in it the most current version
as of this tutorial and then you open a
terminal if you're going to do your pip
installs and stuff like that for
different modules you can also create
different environments in here so maybe
you need a python 36 python 35 you can
see we're having a nice framework like
Anaconda really helps so you don't have
to chat track that on your own in the
jupyter notebook and your different
jupyter notebook setups we'll go ahead
and launch this jupyter notebook and
then I've set my browser window for a
default of chrome so it's going to open
up in Chrome and you can see here this
opens up a folder on my computer we have
a couple different options on here
remember I set the environment up as
python 3.7 you would install any
additional modules that aren't already
installed in your python on this and it
keeps them separate so you do have to
for each environment install the
separate module so they match the
environment on there and in here we have
a couple things we can look up what's
running you have your different clusters
again this is I just installed this on a
new machine so I just have the one a
couple things in here that were run on
here recently and what we go on here is
we then have on the upper right new and
from the pull down menu you'll see
Python 3 and this will open up a new
window and now we're in Jupiter python
so this is a python window we'll just do
a print and this of course is let's go
hello
world and we'll run that and it prints
out hello world in the command line
there's a couple special things you have
to know we're not going to do today
which is on Graphics if you've never
seen this one of the things you can do
is you can also do a equals hello world
and if you just put the a in there now
if you do a bunch of these where you
have a equals hello world b equals
goodbye world and you put a b a then
return B it'll only run the last one but
you can see here if you put the variable
down here it will show you what's in
that variable
and that has to do with the jupyter
notebook inline coding so that's not
basic python that's just Jupiter
notebook shorthand which you'll see in a
little bit so back to our numpy numpy
array versus python list python list
being the basic list in your python why
should we use numpy array when we have
python list well first it's fast the
numpy array has been optimized over
years and years by multiple programmers
and it's usually very quick compared to
the basic python list setup it's
convenient so has a lot of functionality
in there that's not in the basic python
list and it also uses less memory so
it's optimize both for Speed and memory
use and let's go ahead and jump into our
Jupiter notebook since we're coding best
way to learn coding is to code just like
the best way to learn how to write is
write and the best way to learn how to
cook is cook so let's do some coding
here today and just like any modules we
have to import numpy we almost always
import it as NP that is such a standard
so you'll see that very commonly we can
just run that and now we have access to
our numpy module inside our Python and
then the most common thing of course is
to go and create a numb array and in
here we can send it a regular list and
so we'll go ahead and send this a
regular array uh let's go one two three
to make it simple and then I'm just
going to type in a and we'll run
this and so you can see down here the
output is an array of one two three and
we could also do print just a reminder
that this is an inline command so that
wouldn't work if you're using a
different editor you can see that's an
array one two three but we'll go and
leave it as a kind of a nice feature so
you can see what you're doing really
quick in the jupyter notebook and just
like all your other uh standard arrays I
can go a of
zero which is going to be a value of one
of course we do a of one you go all the
way through this a of one has a value of
two in it so whether using the numpy
array or the basic python list that's
going to be the same that should all
look pretty familiar and and be pretty
straight forward remember the first
value is always
zero and when we set on there so let's
take a look why we're using numpy
because we went over the slide a little
bit but let's just take a look and see
what that actually looks like and what
we want to look at is the fact that it's
fast convenient and uses less memory so
let's take a glance at that in code and
see what that actually looks like when
we're writing it in Python and what the
difference
are and to do this I'm going to go ahead
and import a couple other modules we're
going to import the time module so we
can time it and we're going to import
the system module so that we can take a
look at how much memory it uses and
we'll go and just run those so those are
imported so we'll do b equals oh range
of one yeah 1,000 is fine and so that's
going to create a list of 1,000 0 to 999
remember it starts at zero and it stops
right at the 1,000 with actually going
to the
1,000 and let's go ahead and print and
we want system. get size of and we'll
pick any integer because we have you
know zero to to a th we'll just throw
one in there five it doesn't matter CU
it's going to whatever integer we put in
there is going to generate the same
value we're looking the size of how how
much memory it stores an integer in and
then we want to have the link of the B
that's how many integers are in there
and if we go ahead and execute this and
run this at a line we'll see Oops I did
that wrong comma if we multiply them
together we'll see it generates
28,000 so that's the size we're looking
at is 28,000 I believe that's bytes that
sounds about right so let's go ahead and
create this in numpy and we'll go Cals
NP and this is a range so that's the
numpy command to do the same thing that
we were just doing in a list and we'll
also use the same value on there the
1,000
then once we've created the uh c um
value of C for np. AR range let's go
ahead and print we can do that by doing
c.
size times c. item size and that's very
similar we did before we did get the
size of so the C size is the size of the
array and each item size just reversed
so it's the size of a integer five item
size it's going to be the integer and C
size and let's just take a look and see
what that
generates and wow okay we got 4,000
versus
28,000 that's a significant difference
in memory how much memory we're using
with the array and then let's go ahead
and take a look at uh speed let's do um
oh let's do size we tried this with
lower values and it would happened so
fast that the NP kept coming up with
zero because it just rounded it off so
size and let's create an L1 equals range
of
size and we'll do an
L2 we'll just set up to the same thing
it's also
range of size on there there we go and
then we can do on
A1 equals NP do a range
size and then let's do an
A2 equals
np. a range we'll keep it the same
size and what we're going to do is we're
going to take these two different arrays
and we're going to perform some basic
functions on them but let's go ahead
actually just load these up now we'll go
ahead and run this so those are all set
in
memory EX for the typo
here quickly fix that
there we go so these are now all loaded
in here and let's do a uh start
equals
time.time so it's just going to look at
my clock time and see what time it is
and we'll do result equals and let's do
oh let's say we got an array and we're
going to
say let's do some addition here x +
[Applause]
y for X comma
Y and and we'll zip it up here two
different arrays so here's are two
different arrays we're going to multiply
each of the individual things on here L1
L2 there we go so that should add up
each um value so L1 plus L2 each value
in each array then we want to go ahead
and
print and let's say uh python list
took and then we'll
do time do
time we'll just subtract the start out
of there so time whoops I messed up on
some of the quotation marks on there
okay there we go time minus the start
and we'll convert that to Second so
we'll go gu in milliseconds or times
1,000 and let's hit the run on there
this is kind of fun because you also get
a view while we're doing this of some
ways to manipulate the script and as you
can see also my bed typing there we go
okay so we'll go ahead and run
this and we can see here that the python
list took
34 actually I have to go back and look
at the conversion on there but you can
see it takes roughly 34 of a second and
we can go ahead and print the result in
here too let's do that we'll run that
just so you can see what the what kind
of data we're looking
at and we have the 02 4 68 so it's just
adding them together it looks pretty
straightforward on there and if we
scroll down to the bottom of the answer
again we see python list took 46 little
different time on there depending on
what um core because I have this is on
an8 core computer so it just depends on
what core it's running on what else is
pulling on the computer at the time and
let's go back up here and do our start
time paste that into here and this time
we're going to do a
result equals and this is really cool
notice how elegant this is It's So
straightforward this is a lot of reason
people started using numpy is because I
can add the two erase together by simply
going A1 Plus A2 makes a lot of sense
both looking at it and it's just very
convenient remember that slide we're
looking at fast convenient and less
memory so look how convenient that is
really easy to read really easy to see
and I don't know if we don't need to
print the result again so let's just go
ahead and print the time on here and
we'll borrow this from the top part
because I really am a lazy
typer and this isn't the python list
this is the numpy list or numpy
array and let's go ahead and see how
that comes out and uh we get
2.99 so let's take a look at these two
numbers 46 versus 2.99 so we'll just
round this up to three that's a huge
difference that's that's like more than
10 times faster that's like 15 times
roughly at a quick glance I'd have to go
do the math to look at it and it's going
to vary a little bit depending on what's
running in the backgr on the computer
obviously so we've looked at this and if
we go back here we found out it's much
faster yes there's different going to be
different speeds depending on what
you're doing with the array very
convenient easy to read and it uses less
memory so that's the core of the numpy
that's why a lot of people base so many
other modules on numpy and why it's so
widely used so we did glance at a couple
operations while we were looking at
speed and size let's dive into a little
bit more into the basic operations
and these are always nice to see I mean
certainly you want to go get a cheat
sheet if you're using it for the first
time you know look things up Google is
your friend we did this where the most
basic numpy do array or np. array and
we'll go ahead and create an array let's
do pairs uh 1 comma
2 and then let's do 3 comma four and if
we're going to do that let's do 5 comma
6 there we go and if we go ahead and
take this and and run this I can go
ahead and do our a down here so it's in
line and it'll print that out you can
see it makes a nice array for us so we
have a and if you look at that we have
three different objects each with two
values in them and hopefully you're
starting to think well how many
dimensions or indexes is that and you'll
see 3x two so let's go ahead and take a
look and let's go how about a do in
Dimensions speaking of which we'll run
that and we have two dimensions for each
object and then we can do the item size
so a do we saw this earlier we looked up
how many items it was up here where we
wanted to multiply item size times the
actual size of object so the memory it's
being used versus the item size and we
should see four there memories
compressed down that's always a good
thing and then the shape the shape is so
important when you're working with data
science and you're moving it from uh one
format to another so we have our shape
we just talked about that we have three
by two three rows by two objects in each
one generally I don't look too much at
the size but the dimensions I'm always
looking up and this is nice you can
automate it so you might be converting
something you might need to know how
many dimensions are going into the next
machine learning package so that you can
automatically just have it send that
information over we looked at a shape
let's go and create a slightly different
array np. array let's go ahead and just
do as our original setup
here and one of the features we can do
which is really important is we can do
dtype
equals in this case let's do
NP float 64 and so what we've done is
converting all of these into a float and
we type in
a and now instead of having 1 2 3 4 5
six you see they're all float values 1.0
Z there's no actual zero in there just
there's the one dot or the one period
two three period four period five period
six period and this again data science I
don't know how many times I've had to
convert something from an integer to a
float so that's going to work correctly
in the model I'm using so very common
features to be aware of and to be able
to get around and use and we'll also do
let's just curiosity item size we'll go
and run that and we see that it doubled
in size so it's not a huge increase well
doubling is always a big increase in
computers but it's not a huge increase
compared to what it would be if you're
running this in the python list format
and then we did the shape earlier
without having it set to the float 64
let's go ahead and do a shape with it
set to 64 and it should be the same 3
comma 2 so it all matches so we've gone
through and remember if you really if
this is all brand new to you according
to the Cambridge study at the Cambridge
Bri University if you're learning a
brand new word in a foreign language the
average person has to repeated 163 times
before it's
memorized so a lot of this you build off
of it so hopefully you don't have to
repeat it 163 times but we did manage to
repeat it at least twice here if not a
little bit more and let's go ahead and
take this we're going to go look at one
more setup on here and let me just take
this last statement here on the
converting our properties of our data
and instead of float
64 let's do complex let's just see what
that looks like and let's go and print
that out and run
it and so we now have a complex data set
up and you'll see it's denoted by the
one. plus
z.j and if we flip over here and do a
basic search for numpy data types better
to go to the original web page but pull
up a bunch of these you can see there's
a whole list of different numpy data
types short and complex we have complex
complex 64 complex 128 complex number
represented by 264 bit floats real and
imaginary
components one option on there float 16
float 32 float Shand for float 64 most
commonly used and of course all the
different ones that you can possibly put
into your numpy array so we covered a
basic addition up there we're comparing
how fast it runs but some very basic
components how to set up a numpy array
how many dimensions it has item
size data type item again we went to
item size and there's also the shape
probably one of the more used I use a
shape all the time very commonly
used and then down here you can see
where we actually created a numpy
complex data type so let's look at some
other features in numpy one of them is
you could do
numpy dot
zeros and we're going to do 3 comma
4 there we go and we'll go ahead and run
this and you can see if I do np. zeros I
create an ny array of
zeros this is really important I was
building my own neural network and I
needed to create an array where I
initialize the weights and I wanted them
all to be the same weight in this case I
wanted them to start off as zero for the
particular project I was working on and
there's other options like you can do
nump
On's and we'll do the same thing 3 comma
4 we'll run that and you can see I've
created a an array of numpy ones in this
case it comes out as a float
array and then this is an interesting
the note because we have let's go back
to our Python and do L Range Five and
we'll print the L so there our list and
if I run that it doesn't create the
range until after the fact until you
actually execute it that's an upgrade in
python python 27 actually created the
array 0 1 2 3 4 this one actually
creates the script and then once it's
used it then actually generates the
array and if we do that in numpy a Range
remember that from
before and if we do numpy a range five
and let's do uh L
equals or we can just leave it as numpy
that's fine there we go and just run
that you can see there we actually get
an array 0 1 2 3 4 for the value the
numpy range a range five generates the
actual array and for part one we're
going to do just one more section on
basic setup and we're going to
concatenation do a concatenation
example there we go we're going to do
strings let's take a look at uh strings
and what's going on with there and let's
do oh let's see print Let's do an NP
character something new here and we're
going to add
and then here's our brackets for what
we're going to
add oh and let's say um let's
do
hello comma
hi and in the brackets on there and
let's create another one and this one's
going to
be
ABC and we'll do XYZ so we're just
creating some randomly making some up on
here and then we'll go ahead and just
print this if we run that and come down
here and of course make sure all your
brackets are open and closed correctly
and then you can see in here when we can
catenate the example in numpy it takes
the two different arrays that we set up
in there and it combines the hello with
the ABC and the high with
XYZ and if we can also do something like
print oh let's do NP character.
multiply so there's a lot of different
functions in here again you can look
these up it's probably good to look them
all up and see what they are but it's
good to also just see liit action let's
do hello space comma 3 and we'll run
this
one and run that without the error and
you'll see it does hello hello hello so
we multiplied it by three and we can
also let's just take this whole thing
here instead of retyping
it and we can do character Center so
instead of multiply let's do Center
and over here keep our hello going take
the space out of there and let's do
Center it
20 and fill
character equals we'll fill it with
dashes so if we run
this you can see it prints out the uh
hello with dashes on each side and we
keep going um with that we can also in
addition to doing the fill function we
can play with capitalize we can title we
can do lowercase we can do uppercase we
can split split line strip join these
are all the most common ones and let's
go ahead and just look at those and see
what those look like each one of them
here we're going to do the hello world
all-time favorite of mine I always like
to say Hello Universe and you can see
here we did a capital H with the world
but so we want to capitalize so
capitalize is the first one in the array
so we get Hello World on there and we
can also take this
and instead of
capitalizing another feature in here is
title and let's just change this to how
are we doing how are you doing instead
of we we'll do do you and let's run that
and you can see here because we created
as a title it capitalizes the first
letter in each word and in this one
we're going to do character lower two
different examples here we have an array
we have Hello World all capitalized and
we have just hello and you can see that
one is an array and one is just a string
if we run that you get a an array with
Hello World lowercase and hello
lowercase and if we're going to do it
that way we can also do it the opposite
way there's also
upper and let's paste those in there and
you can see here we have character. uper
opposite there python. dat and we'll do
python is easy hopefully you're starting
to get the picture that most of the
Python and the scripting is very simple
it's when you put the bigger picture
together and starts building these
puzzles and somebody asks you hey I need
the first letter capitalized unless it's
the title and then we have you start
realizing that this can get really
complicated so numpy just makes it
simple and we like that and so in this
case we did python data it's all
uppercase python is easy like shouting
in your messenger python is easy and
then if you're ever uh processing text
and tokenizing it a lot of times the
first thing you do is we just split the
text and we're just going to run this
np. character dosit are you coming to
the party if we do that returns an array
of each of the individual words are you
coming to the party splitting it by the
spaces and then if you're going to split
it by spaces we also need to know how to
split it by
lines and just like we have the basic
split command we also have split lines
hello and you'll see here the scoop in
for a new line and when we run that if
you're following the split part with the
words you should see hello how are you
doing the two different lines are now
split
apart and let's just review three more
before we wrap this up commonly used
string variable manipulations we have
strip and in this case we have Nina
admin Anita and we're going to strip a
off of there let's see what that looks
like and then you end up with nin dein
Nate it basically takes up all leading
and trailing letters in this case we're
looking for a more common would be a
space in there but it might also be
punctuation or anything like that that
you need to remove from your letters and
words and if we're going to strip and
clean data we also need to be able to
reformat it or join it together so you
see here we have a character join we'll
go ah and run
this and it has on the first one it
splits Isa letters up by the colon and
the second one by the dash and you can
see how this is really useful if you're
processing in this case a date we have
day month year year month date very
common things to have to always switch
around and manipulate depending on what
they're going into and what you're
working with and finally let's look at
one last character string we're going to
do replace if you're doing
misinformation this is good pulling news
articles and replacing is and what in
this case we're just doing he is a good
dancer and we're going to replace is
with
was and you can see here he was a good
dancer hopefully that's not because he
had a bad fall he just was from like you
know 19 20s and it's gotten old so there
we go we've covered a lot of the basics
in numpy as far as creating an array
very important stuff here when you're
feeding it in how do we know the shape
of it the size of it what happens when
we convert it from a regular integer
into a float value as far as how much
space it takes we saw that that doubled
it item size you have your in dimensions
and probably the most used is shape and
we'll cover more on shape in part two so
make sure you join us on part two
there's a lot of important things on
shaping in there and setting them up we
also saw that you can create a zeros
based array you can create one with ones
if we do a range you can see how it is a
lot easier to use to create its own
range or a range as it is in numpy you
saw how easy it was to add two arrays we
saw that earlier just plus sign then we
got into doing strings and working with
strings and how to concatenate so if you
have two different arrays of strings you
can bring them together we also saw how
you can fill so you can add a nice
headline Dash Das Dash we saw about
capitalize the first letter we saw about
turning it into a title so all the first
letters are capitalized doing lowercase
on all the letters upper for all the
letters just lower and upper nice
abbreviation we also covered how to
split the character set how to strip it
so if you want to strip all the A's out
from leading a A's and ending A's or
spaces you can do that very easily also
how to join the data sets so here's a
character join Co option for your
strings and finally we did the character
replace now let's go ahead and dive in
there since we're going right into part
two which is getting some coding going
under our belt and here in our Jupiter
notebook we can go under new and create
a new folder Python 3 I think I forgot
to do this last time we could just do
the um control Plus+ which in any
browser enlarges a page makes a lot
easier to see always a nice feature
another beautiful benefit of using
Jupiter notebook and let me go ahead and
show you a neat thing we can do in
Jupiter this is nice if you're working
with people and you're doing this as a
demo on a large screen I'm going to do
the hashtag or pound symbol array
manipulation kind of a title what we're
working on and then I'm going to call
this cell cell type markdown as opposed
to code and you'll see it highlights it
here and then if I run it it just turns
it into array manipulation and then
we're specifically going to be working
on array manipulation changing shape to
start with and we'll go ahead mark this
cell also a markdown so has a nice
little look there and then it comes up
and you can see it just like I said it
just highlights it and makes it in very
bold print just making it easier to read
not a python thing but a Jupiter thing
that's good to know about especially if
you're working with the shareholders
since they're investing money in you
course the first thing you want to do is
import we're going to import
numpy as in P that should be standard by
now by now you you start a Python
program you're doing some data science
numpy is just something you bring in
there and let's go ahead create our
array and we're going to do that as the
np. AR range remember that's uh zero
well we're going to do 0 to 9 and then
uh we'll go print put a little title on
the original array we'll just print that
array a remember from the first lesson
so we have our array which is 0 1 2 3 4
5 6 7 8 and let's add a print space in
between let's create a second array B
but we want this to reshape array a and
what does that mean and the command is
simply reshape and then we have nine
items in here and this is so important
right now so be very aware if I did some
weird numbers in here it's not going to
work and we want multiples of nine we
know that 3 * three is N9 so we're going
to reshape our a array by 3x3 and then
we're going to print oh let's give it a
title oops I had too many brackets in
there modified array and then let's go
ahead and print
RB and let's see what that looks like
and as we come down here you can see
we've taken this and it's gone from 01 2
3 4 5 6 7 8 to an array of arrays and we
have 0 1 2 3 4 5 6 7 8 and so we split
this into three by3 and you can guess
that if I tried to reshape this let's
just do a five by3 which is 15 that's
going to give me an error so it's not
going to work you're not going to be
able to reshape something unless the
shape all the the data in there matches
correctly so we can take this nine this
flat nine and we call it a flat CU it's
just a single array and we can reshape
it into a 3X3 array and first you might
think matrixes which this is used for
that definitely I use it a lot in
graphing because they'll come in that I
have an array that's XY comma X y1 y1
comma X2 Y2 and so the shape of it might
be two by the length of the number of
points
and I need to separate that into a x
flat array and a y flat array and you
can see this can be very easy to reshape
the array doing that and we can of
course go back we can do B do a print
and we'll do B do platin remember I said
it's called a flatten array and if we
run that you'll see it just goes back to
the original one it takes this 0 1 2 3 4
5 6 7 eight and flattens it back to a
single
array and then one other feature to be
aware of is if we flatten it one of the
commands we can put in there is order
let me just go ahead and do that order
equals F strangely enough F stands for
Fortran the whole Fortran days I
remember actually studying Fortran
programming language in this case you'll
see that it uses the first like
036 is the order so instead of
flattening it like we had before 0 1 2 3
4 5 6 7 8 it now does 0361
4
7258 and if you go to the numpy array
page you can see here that they have the
flatten I just open up the numpy and D
array flatten setup to look it up and
they have three different options they
have c f and a and it's whether to
flatten in C which was based on how the
C code works for flattening originally
worked which is row major fortrend which
is column major or preserve the column
for Trend ordering from a so whatever it
was in the default is the um C version
so the default that you saw you could
put order equals c and it have the same
effect as we saw there before you can
even do order equals a that would also
have the same effect because that's the
default so really the only other thing
you change on here is to change it to C
if you need it and you can see right
here or F I mean not C the only thing
you really want to change it to is to
your f for the fortron order which then
does it by column versus by row and
let's look at here we go reshape so
let's create a range of
12 and let's reshape it and we'll do
four comma three for this one and
remember this is numpy I forgot the NP
there np.
arrange and we can type in just a for
print or you can do full print a and of
course the Jupiter notebook even have a
little extra print at the beginning we
run this we'll see we create a nice
array of 012 it's reshaped it so we have
four rows and three columns or you could
call it three columns and four rows 0 1
2 3 4 5 6 7 8 9 10
11 this one is so important we'll do
NP
transpose a let's go ahead and run
that and it helps if I get all the s's
in there don't leave an S out and you'll
see here we've taken our array if you
remember correctly we had 0 1 2 3 4 5 6
7 8 9 10 11 and we've swapped it so
we've gone from a 3x4 or a 4x3 to a
3x4 and this really helps if you're
looking at like a huge number of rows
and the data all comes in like let's say
this is your features in row one your
features in row two and this is XYZ well
when you go to plot it you send it all
of X in one array all of Y in one array
and all Z in another array and so it's
really important that we can transpose
this rather
quickly this is kind of a fun thing I
can highlight it and do brackets around
it and if you remember correctly because
we're in Jupiter it doesn't matter where
we do the print or not it'll
automatically print it for us and you
see if I hit the Run button it comes up
at the same exact thing and let's play
with the reshape and you know what let's
Zoom this up a little bit
here make that even bigger so you can
really see what's going on and let's
play with the reshape just a little bit
more we'll do b equals np. range let's
do
eight and reshape we'll do 2 comma 4
let's go ahead and print B and then run
that and you'll see we have now the two
rows this is a little bit more like so
we have four maybe two rows of four
things so this might be all of our X
components and our y components so we
can switch it back and forth real easy
important to know here whether we do 2
comma 4 or in the case of 4 comma 3
this has 12 elements and so however you
split it up it's got to equal 12 so 4 *
3 equal 12 that's pretty straightforward
same thing down here 2 * 4 = 8 if I
change this and let's say I do 2 comma
three let's just run that in and you'll
find we get an error because you can't
split eight up into two rows by three
you have to pick something that it can
split up and arrange it in so let's go
ahead and run that and just for fun
let's go um reshape our B again if I can
type reshape our B again and what else
goes into eight well we could do 2x two
by two so we can take this out to three
different dimensions and then if of
course if we um because this is going to
come out you as a variable we can just
go ahead and run it and it'll print it
we could also do a print statement on
there just like we did before and you'll
see we have two different groups of two
variables of two different dimensions so
2x 2 by two and let's go ahead and
assign this to a variable Cal B reshape
and let's do something a little
different let's roll the axes roll
axes and we'll take our C and do two
comma
one and if we go ahead and run this it's
going to print that out whoops hit a
wrong button there let's do that one
again and you roll the axis and you can
see that we now have instead of 0 1 2 3
4 5 6 7 we now have the 0213
4657 so what's going on here we're
taking and we're rolling the numbers
around and let's just simplify this
we'll just do it with C comma 1 and run
that and so if we roll a single axis you
get
01 and then it rolled the four five up
and then we have 2 3 six7 and if we do
two let me see what happens there and
this is one of those things you really
have to play with and start filling what
it's doing we've now taken 02 4
61357 so you can see we've now rolled by
two digits instead of um rolling the one
set up we now rolled two digits up there
and so if we go back and we do the one
so we've rolled it up 0 1 4 five and
then we're going to take the two in
there and we've rolled the 0 1 2 3 4
five and
67 so we start rolling these things
around on here there's a lot of
different things you can do on this
but it's another way to manipulate the
numbers on your uh numpy and finally
let's go ahead and
swap axis we'll do c and let's just go
ahead and run that it's going to give me
an error on there that's because it
requires multiple arguments left out the
arguments so now we can swap and we get
the 02 1 3 4 6 5 7 so you can see
everything's been swapped around so next
thing we want to go over is we want to
go go over numpy arithmetic
operations how can we take these and use
these let me just go ahead and put this
cell as a markdown there we go we'll run
that so it has a nice thing all right
nice title on there that's always
helpful and let's start by creating two
arrays we'll do uh a is an EP
NP range a range n and let's reshape
this 3x3 so by now you should be seeing
this reshaped stuff and this should all
look pretty familiar we have our 0o 1
two 3 4 5 six 7even eight on there and
let's create a second one b and this
time instead of doing a range let's do
NP array we'll just create a straight up
array and we'll do an array of three
objects so it's going to be 3 by one and
if we go ahead and print a b out let me
run that this is actually pretty common
to have something like this where you
have a 3 by whatever it is in a 3 by by3
array when you're doing your math you
kind of have that kind of setup on there
and what we can do is we can go um np.
addab don't forget we can always put a
print statement on there so if we add it
you'll see that it just comes in there
and it goes okay we're adding 10 to
everything and we could actually do
something more oh make it more
interesting 11 10 111 12 so let's change
B is now 10 111 12 and let's run that
and you can see that we have 10 then you
had 1 + 11 is 12 2 + 12 is 14 13 so 10 +
3 is 13 11 + 4 is 15 and 12 + 5 is 17
and so on we'll put this back since
that's how the original setup was let's
do 10 by 10 by 10 and run that and run
that and get the original answer and if
you're going to add them together we
need to go ahead and subtract
a and we run that we get - 10 - 9 - 8
just like you would expect so we have
our subtraction 0 - 10 is minus 10 and
so on and if you're going to add and
subtract you can guess what the next one
is we're going to multiply and we'll
multiply ab and this should be pretty
straightforward you should expect this
if we multiply 10 * 0 we got 0 10 * 10
is 10 and so on
and finally if you're going to multiply
what's the last one we got is divide
what happens when we do divide a by B
and we run this and we're going to get
zero and this is um 0id by 10 is 0 1id
by 10 is 0.1 two divid by 10 is 0.2 and
so on and so on so the math is pretty
straightforward it just makes it very
easy to do the whole setup and again if
we went this and let's say oh let's
change this up up here instead of 10 we
do 100
and make this a thousand there we go and
if we run that and then we do the add
you can see we got 10 plus 100 plus a
th000 same thing with the
subtract same thing with the
multiply then you can also see the same
thing here with the Divide so a lot of
control there with your array and your
math again let's set this back to 10
oops it's right up here wrong
section there we go 10 we we'll just go
ahead and run these and get back to
where we
were and this brings us to our next
section which is slicing and let's put
in our just make this a cell cell type
markdown and we run that of course it
gives us a nice looking slicing there
and slicing means we're just going to
take sections of the array so let's
create an array NP a
range let's just do
20 and if remember if we do a we have a
0o to 19
and then we can do a and remember we can
always print these this can always be
put in a print but because I'm in
Jupiter if you're doing a demo in
Jupiter that is it's just so great that
you have all these controls on here so
we can slice four on and this should
look familiar this is the same as a
python and a lot of other different
scripting languages if we do four go 0 1
two 3 that's the first four in the thing
and a skip some and starts with this one
the first four skip then from there on
you can also do the opposite and go till
the fourth one if we run that we get 0 1
2 3 quite the opposite on there we can
do a single item so we can pick object
number five on the list run that and
five happens to be five because that's
the order they're in and then this one's
interesting so I can do s equals
slice and let's create a slice here and
let's do 2 comma 9 comma y let's leave a
two on there so we'll create an S Slice
on here and then if we take our array
and we do array of s we're taking our
slice in there and let's go ahead and
run that and let's take a look and see
what it generated here first off we
started with two so we have two at the
beginning we're going to end at nine
which happens to be eight so it stops
before the nine remember when we're
doing arrays in Python and then we step
two so 2 4 6 8 we could do this as three
let me run that and you can see that
changes
258 and we could do this as let's leave
this at three and if we change this to
10 oops let's make it 12 there we go and
we run that we have 2581 so that's
pretty straightforward a a very nice
feature to have on here we can slice it
and take different parts of the series
right out of the middle so now that
we've accessed to different pieces of
our array let's get into iterating
iteration and this is interesting
because my sister who runs a college
data science division the first question
she asks is how do you go through data
and she's asking can you do you know how
to iterate through data do you know how
to do a basic for Loop do you know how
to go through each piece of the data and
in numpy they have some cool controls
for that put this as a mark down there
we go and run it and it's called the
indd itter I'm not sure what the indd
stands for but indd iter for iterator so
before we do that though let's create an
array or something we can actually
iterate through we'll call it a equals
NP a range let's do something a little
funny here or
funky and we'll do
0455 I'm not sure why the guys in the
back picked this particular one but it's
kind of a fun one and if I run that we
do this you can see um we get 0 5 10 15
20 25 30 35 40 that's what this array
looks like and that's just from our
slice you could this is just a slice
that's all that is is we created a slice
of 045 0 to 45 step five so we can do
with this we can also do a equals
reshape let's go ahead and take and
reshape this and since there's nine
variables in there we'll do a reshape it
3x3 so if we run that oops missed
something there that is the a that
really helps so if we do the a reshape
and we'll go ahead and print that out we
get 0 5 10 15 20 25 30 35 40 and then we
simply do
4xn our numpy
ND of
a colon and we'll just go ahead and
print X and let's see what happens here
when we run through this and we print
each one of those it goes all the way
through the whole array so it's the same
thing we just saw before we got 0o 5 10
15 20 25 30 35 40 so prints out each
object in the array so you can go
through and view each one of these and
certainly if you're remember you could
also flatten the array and just do for a
and that also and get the same result
there's a lot of ways to do this but
this is the proper way with the ND
iterator because it'll minimize the
amount of resources needed to go through
each of the different objects in the
numpy array and hopefully you asked this
question when I just did that and the
question is how can I change this
instead of doing each object so first of
all let's go a and take my cell type and
we're Mark that down and run it and so
we're going to work on iteration order C
style and F style remember C because it
came from the C programming and F
because it came from the old fortrend
programming so let's give us a reminder
we'll do a print a and we'll do 4 xn NP
iterate a but we also want to do this in
a specific order and you know what I'm a
really lazy typer so let's go back up
here this it's the ND iterator I knew
missing the ND part of a and let's do
order
equals c we'll print X on there and
let's do that again and this time
order order
equals F there we go order equals F
let's go ahead and run this
and see what happens here and the first
thing you're going to notice our
original array 0 5 10 15 20 25 30
3540 when we do order C that's the
default 0 5 10 15 20 and so on and then
when you come down
here you'll see F order f is
0530 so it takes a first digit of each
the subarrays or the second dimension
and then it goes into the second one 520
35 102 240 so slightly different order
for iterating through it if you need to
do that so we've covered reshaping we
covered math we've covered
iteration we've covered a number of
things the next section we want to go
ahead and go
over is going to be joining arrays so we
need to bring them together let me go
ahead and take the cell and make it a
markdown cell type markdown there we go
and run that so let's work on joining
arrays so we can bring them together and
with different options we have and let's
do U we'll do an NP array one two comma
34 we'll go ahead and print let's do
oops
first these arrays aren't that big so
let's just go a and keep it all on one
line
a so if we run this first array 1 two 3
4 oops I forgot that it automatically
wraps it when you do it this way so
we'll go ahead and keep it separate and
print a there we go and let's go ahead
and do a
b and we'll do five six 7even 8 and
notice I'm keeping the same shape on
these two arrays depending on what
you're doing those shapes have to match
and let's go ahead and
print second
array do a print B we go and run that
oops missed something up there let me
fix that real
quick when I was reformating it to go on
separate lines I missed that up there we
go run all right so we have first array
1 2 3 4 second array five six
78 and we'll put a carried return on
there and the keyword we use is um
concatenate and if you're familiar with
linix it usually means you're adding it
to the end on there and we're going to
do what they call Long axis zero
so we have concatenate AB along axis
zero let's go ahead and run that and see
what that looks like and so we have 1 2
3 4 5 6 7 8 so now we have an array that
is 4X two has a nice shape of 4X two on
here and if we're going to do it along
the axis
zero you should guess what the next one
is we're going to do it along the one
axis and let's see how those differ from
each other let's just go ahead and run
that and again all we're doing is adding
in the axis equals one so we have our
concatenate we have ab and then axis one
remember a couple things one these are
the same shape so we have a 2 by two
same dimensions going in there you're
going to get an error if you're
concatenating and they're not if you
have something that instead of one two
is 1 two 3 4 five six with a 56 7 8
they'll give you an error on there in
fact let's take a look and see what
happens when we do that let me just take
this one two three three four five let's
run that and if we come down here oh we
got there it says all the input array
Dimensions except for the concatination
axes M must match exactly so it'll let
you know if you mess up that's always a
good thing let's go ah and take this
back here and let's go ahead and run
that and so we have our zero axis which
is one two 3 4 5 6 78 we bring them
together and you'll see a very different
setup here when we do it along the axis
one
we end up with instead of a 4X two we
end up with a
2x4 1 2 5 6 3 4 78 and that's just
changing which axes we're going to go
ahead and and concatenate on what I find
is when you're talking about the
concatenate or the joining arrays you
really got to play with these for a
while to make sure you understand what
you mean by the axes it looks very
intuitive when you're looking at it axis
0 1 2 3 4 5 6 7 8 a one is then
splitting it a different way 1 2 5 6 3 4
7 8 when you're actually using real data
you start to really get a feel for what
this means and what this
does so if we're going to do that let's
go ahead and look at splitting the
array and do that on a markdown and run
it there we go so you have a nice little
title
there and we'll go ahead and create an
array of nine let's do NP
split we'll do a and we're going to
split it by three
let's just see what that looks like so
if we split it we get an array 0 1 2 3 4
5 we get three separate arrays on here
got remember we're looking at let me
just print a up here so we're looking at
0 1 2 3 4 5 6 7even 8 and then we can
split it into three separate
arrays and let's take this we're going
to do this right down here just move the
a split down here instead of the three
let's do four comma five put that in
Brackets
and so we do it this way we have 0 1 2 3
4 five six 7 8 and that's kind of
interesting I wasn't sure what to expect
on that but we get when you split an a
by four comma five you get a totally
different setup on here as far as the
way to split the
array and to understand how this works
I'm going to change the five to a seven
and this will visually make this a
little bit more clear so so we had four
and five it went 0 1 2 3 4 5 6 7 8 and
you see the markers four and five when
we do four and seven I get 0 1 2 3 four
five six
78 and so what you're looking at here is
the first markers this is going to go to
four so there's our first split at the
four the marker of four and then the
second split is going to be at position
seven and this is the same thing here
four position five that's how we're
splitting it in those two sections we
could also do it seven just's to see
what that looks like run and you can see
I now have 0 1 2 3 4 five six 78 so we
can split in all kinds of different
arrays and create a different set of um
multiple arrays on here and split it all
kinds of different ways and before we
get into the graphs and other um
miscellaneous stuff let's go ahead and
look at resizing the array we go and
take this cell and set this the cells a
markdown and run
it give us a nice title there and we'll
do an array uh an NP array of uh one two
three and four five six here and let's
go and just
print let's go print a DOT shape and
we'll go ahead and run that whoops hit a
wrong button there hit the comma instead
of the dot so we have a shape of 2 comma
3 here and this is important to note
because when we start resizing it is
going to mess with different aspects of
the
shape and so we'll go and do a
print scoop in for a blank line there we
go now let's do b
equals np.
[Music]
resize we're going to resize
a and let's resize it with 3x two and
then we'll just go ahead and print B
and print b period shape not a comma we
run that oops forgot the uh quotation
marks around the end we'll go ahead and
run that and let's just see what that
looks like so we have 1 2 3 4 5 Six
Original array with a shape of 2 three
and then we want to go ahead and resize
it by 32 and we end up with one 2 3 4 5
six and we end up with a shape of 32
that shouldn't be too much of a surprise
you know we got six elements in there we
can resize it by two three was the
original one and then we're actually
just reshaping it is how that kind of
comes out as when you resize it like
that but what happens if we do something
a little different and let's go ahead
and just take this whole thing and copy
it down here so we can see what that
looks like and instead of doing 32
remember last time I did the um to
reshape it I messed with the numbers and
it gave me an error well when you resize
it you don't have to match the numbers
they don't have to be the same
dimensions so we we can instead of going
from a 23 to a 32 we can resize it to a
33 so let's take a look and see how it
handles that and we come down here to 33
we end up with 1 two 3 4 5 six and it
repeats one two three so it actually
takes the data and just adds a whole
another Block in there based on the
original data and repeating it all right
now this point you know we've been
looking at tons of numbers and moving
stuff around we want to go ahead and do
is get a little visual here because that
um certainly you can picture all the
different numbers on there but let's
look at histogram let's put this into a
histogram let me go ahead and run that
and to do that we're going to use the
Matt plot Library so from map plot
Library we're going to import pip plot
as
PLT that's usually the notation used see
for pip plot so if you ever see PLT in a
code it's probably pip plot in the matap
plot
library and then the guys in the back
did a nice job and gals too guys and
gals back there our team over has simply
learned put together a nice array for me
20 874 40 53 with a bunch of numbers
that way we had something to play with
and what we want to do is we want to do
plot the histogram now remember a
histogram says how many times did
different numbers come in and then we're
going to put them in bins and we have
bins 0 to 20 to 40 to 60 to 80 to 100
you might in here with the map plot
Library they call them bins you might
heard the term buckets or they put them
in buckets that's a really common term
and then we want to give it a title so
the way it works is you do your PLT
doist for histogram your PLT title and
your PLT show and we're doing just a
single array in here in the numpy array
of a and let's go ahead and run this
piece of
code take it a moment to come out there
a figure size so it's generating the
graph and you can see we have and let's
just take a look at this let me go down
to size there we go okay so now we can
see we're taking a look at here so
between 0o and 20 we have three values
so we have a 20 here we have a four and
a 11 and a 15 0 1 2 three it's actually
Four values but they start at zero
remember we always count from zero up
and from 20 to 40 we got 20 so it's 1
42 3
four five
six and so you can see in the histogram
it shows that the most common numbers
coming up is going to be between the 40
and 60 range least common between the 80
and 100 this looks like a age
demographics is what looks like to me
and you can see where they would have
put it in the buckets of different age
groups which would be a nice way of
looking at this histograms are so
important and so powerful when you're
doing demos and explaining your data so
being able to quickly put a histogram up
that shows what's common and how it's
trending is really important and using
that with a numpy is really easy and you
know what let's take the same data and I
want to show you why we do bins or why
we have buckets of data I'm used to call
of buckets why we have bins let's do it
instead of by 20 let's do it by 10 and
see what happens and what happens when
you do it by tens is you miss out on the
you can see a nice curve here on the
first one and on the second one it looks
like a ladder going up and a plummet a
ladder going up and a plummet and a
ladder going down so the first would be
more indictive of an age group and the
second one would be what you would get
if you divide it incorrectly you
wouldn't see the natural trend of
I don't know what this would be maybe
how much food they eat hopefully not
because I'm in in 50 so I'm right in the
middle there which means I get a ton of
food compared to everybody else but it's
some kind of demog maybe it's mental
maybe it's knowledge because we we hit a
certain point and we start losing our
marble start leaking out or something so
you start off knowing something and then
as you get older you grow more but you
can see here where you lose that you
lose that continuity in the thing if you
split the histogram into too many bins
or too many
buckets and if you actually plotted this
by the individual numbers it would just
be a bunch of dots on the graph and
wouldn't mean a whole
lot and we've looked at graphs there
turns out there are a ton of useful
functions in
numpy I'm sure there's even new ones
that are aren't going to be in here but
let's just cover some important ones you
really need to know about if you're
using the numpy
framework one of them is line space
function this is generating data so we
have a line space we have 1310
and when we do that we end up with 10
numbers so if you count them there's 10
numbers they're between one and three
and they're evenly spaced we get one
1.22 two but these are all there's a
total of 10 here and it's right between
the one and three
range that can be there's a lot of uses
for that but they're probably more
obscure than a lot of the other common
numpy array set
up then a real common one is to do
summation so we'll do summation where
you do in this case we create an umpy
array of one of um two different arrays
1 two 3 or two different dimensions 1 2
3 3 4 5 and we're going to sum them up
under axis zero which is your columns
and if you remember correctly columns is
the 1 + 3 2 + 4 3 + 5 so we have three
columns and if we change this we'll just
flip this to
one we get two numbers so we get 1 2 3
all added together which equals 6 and 3
+ 4 + 5 which equal 12 we'll set this
back to
zero there we go since it said just
we're looking at actually
zero and these probably could have been
some of these could be our math section
square root and standard deviation two
very important tools we use throughout
the machine learning process and data
science and simply we take the NP array
we have again the one two 3 four five
six 3 four five I don't know why I need
to keep recreating it probably could
just kept it but we can take the square
root of a so it goes through and it
takes a square root of all the different
terms in a and we can also take the
standard deviation how much they deviate
in value on there and there's a Rabel
function we can run that and in pray
it's X we're going to do x equals hey we
changed it from a to x x equals Ravel
and this sets it up as columns so we
have 1 two 3 four five this is all
columns on here very similar to the
flatten function so they kind of look
almost identical but we also have the
option of doing a ravel by column then
another one is log so you can do
mathematical log on your array in this
case we have one two three and we'll
find the uh log base 10 for each of
those three numbers there's a couple of
them they don't you can't just do any
number here after log but there is also
log base 2 log base 10 is pretty
commonly used on here run that there we
go before we go let's have a little fun
let's do a little practice session here
on some more challenging questions so
you start to think how this stuff fits
together right now we just looked at all
the basics and all the basic tools you
have so let's do some numpy practice
examples and let's start by figuring out
how to plot say a sine wave in numpy how
what would that look like and so in this
project we wouldn't have to do this cuz
I've already run these but we'd want to
go ahead and import our numpy as NP and
import our map plot Library pip plot is
PLT so we get our tools going here and
then we'll break it into two sections CU
we need our XY coordinates in here so
first off let's create our x coordinates
and our x coordinates we're going to set
to an a
range and we want this error a range
since we're doing s and cosine it's
going to be between 0o and
0.1 and then we use our NP and we
actually can look up numpy stores Pi so
you have the option of just pulling Pi
in there directly from numpy it has a
few other variables that it stores in
there that you can pull from there but
we have numpy pi and we generate a nice
range here and let's go ahead and run
this and just out of curiosity let's see
what x looks like I always like to do
that so we have 0.1 2 3 point4 so we're
going uh 0 to in this case 9.4 3 times
numpy pi remember Pi is like 3 point
something something something so that
makes sense it should be about
and we're doing intervals of 0.1 so we
create a nice range of data and then we
need to create our y variable and so Y
is going to Simply equal NP our numpy do
sin of X and then once we have our X and
Y and if we print let's go and just
print y see how that we'll do this let's
do this so it looks print X print y so
we basically have two arrays of data so
we have like our xaxis and our y AIS
going on
there and this is simply a pt. plot
because we're going to plot the points
and we'll do X comma Y and then we want
to actually see the graph so we'll do
plot. show show and we'll go ahead and
run that and you see we get a nice sine
wave and here's our number 0 through 9
and here's our sign value which
oscillates between minus1 and one like
we expected to then for the next
challenge let's create a 6x6
two-dimensional array and let one and
zero be placed alternatively across the
diagonals oh that's a little confusing
so let's think about that we're going to
create a 6x6 two-dimensional so the
shape is 6x6 two-dimensional array and
let one and zero be placed alternatively
across the
diagonals now if you remember from
lesson one we can fill a whole numpy
array with zeros or ones or whatever so
we're going to do NP create a numpy
zeros and we're going to do a 6x6 and
we'll go ahead and make sure it knows
it's an integer even though it's usually
the default and just real quick let's
take a look and see what that looks like
so if I run this you can see I get 6x6
grid so 6X 6 0 0000
0 now if I understand this correctly
when they say ones and zero placed
alternatively across the diagonals they
want the center diagonal maybe that's
going to stay zero all the way down and
then the next diagonal will be
ones all the way AC cross diagonally and
then the next one zeros the next one
ones and the next one zeros and so on
hopefully you can see my mouse lit up
there and highlighting it so let's take
a little piece of code here and we'll do
Z1 colon colon 2 comma colon colon 2
equal one and wow that's a mouthful
right there so let's go ahead and run
this and see what that's doing and so
what we're doing is we're saying hey
let's look at in this case Row one
there's one and then we're going to go
every other row two so we're going to
skip a row so skip here skip here skip
here so we we're going down this way and
we're going every other row going this
way it's hard to highlight
columns so you can see right here where
the that we're not touching
each row is like this row right here is
not being touched okay so we're going to
start with Row one and then we're going
to skip a row and another one and so
we're going every two rows and then in
every two rows we're looking at every
two starting with the beginning that's
what this thing blank means so we're
going to start with the beginning and
we're going to look at all of them but
we're going to skip every two so
starting with Row
one we look at all the rows but we do we
do it by two steps so we go one Skip One
you know one Skip One One skip one one
if you leftt this out it do every one
this would just be ones in fact let's
see what this looks like if I go like
this and run it you can see that I just
get
ones so this notation allows us to go
down each row row by row and we're going
to do every other row set up on there
and so if we're going to start with Row
one we also contr Z
try that there we go we'll start with
row zero again we're going to go each
row step two so we'll start with row
zero and we'll go every other row and
this time we'll start with one column
one and again we go every other one
going
down step that's what that step two is
skipping every other one we're going to
set that equal to one so let's see what
that looks like and you can see here we
get our answer we get 0 one0 0er but it
has the ones going in diagonals on every
other diagonal and zero on every other
one little bit of a brain teaser that
one trying to get that one to work out
so you can see how you can arrange your
rows and here's your step and your
different access on
there and then the next one is find the
total number and locations of missing
values in the array the first challenge
is to create some missing numbers so
let's create our array Z we're going to
do a num. random. Rand 10 comma 10 and
before we do the second part let me just
take this second part
out and let's just see what that looks
like so let's run that and there we go
so we have a 10x10 random array it
randomly is picking out numbers and next
we want to go ahead and take our random
integer size equals 5 and then we're
going to do a random random 10 size
equals 5 so in the Z we're going to
select a number of random spaces here
and set them equal to null value and
let's go ahead and run that so you can
see what that looks like and if we look
at the
array we've created one two three four
there should be a fifth one in here my
eyes may be failing me so we've created
a series of oh because zero 0 to five 0
1 2 3 4 so we got there are different
null values here and and this is kind of
a neat notation to notice that we can
Generate random integers size equals 5
so this generates 5x5 miniature grid
inside of this to tell it where to put
the nans at so that's kind of a cool
little thing you can do and then we want
to look up and see how many null values
are in there and this is simply just NP
is Nan of Z simple so if it's is Nan
then we want to sum it up so we're going
to sum up all of the different null
values on there now let's do one one
more feature in here which is really
cool let's go ahead and print the
indexes so NP arare NP is Nan of Z so
we're going to create our own another NP
array and let's run this and we'll see
here there comes up with the four
indexes so we did Count four of them up
there it tells you where they are 1 192
04654 and then let's go ahead and run
this again run run there we go this time
I got five that's what get for random
numbers another fun one that I always
like to do it's very similar because we
have NP is Nan z. suum so we're summing
the number of nans and we can get the
indexes and you can reshape the indexes
but you can also just do we'll do an IND
inds where NP is Nan of Z and let's just
print let's print that print inds let's
see what that look looks like and it's
very similar we have we have 0 13 0 6 3
8693 but I've split it into two
different
arrays so we have our X and our y kind
of coordinates going there and what I
can now do is I can now do Z
inds equals and at this point you can
also instead of getting the sum you can
get the means or the all the numbers and
that kind of thing or the average as it
is so that'd be one thing you could and
you can pick out the average that's very
common in data science to get the
average and just use that for a value
but we'll go and just set it to zero and
then let's go ahead and print our Z and
run that and you can see when we come
down here we have wherever there was a
null value it is now zero and you could
set this to whatever you want this is
another way to replace data or help
clean data depending on what it is
you're doing so wow we covered a lot of
stuff so quick reash going over
everything we went into there we looked
at array manipulation changing the shape
how to switch that around we even had
the flatten down there which remember we
have another command lower that's
similar we could change the order by F
remember F stands for Fortran very
strange connotation but there's C and F
C is the standard and F switches it to a
different order to be honest I usually
have to look it up because I almost
never use f but when you need it you're
like oh my gosh what was the other order
do a quick Google so we talked about
reshape making sure that the dimensions
are the same you don't want to have like
a something that has 12 objects in it
and reshape it
to 11 and five because it doesn't work
it doesn't divide into 12 we can
transpose so we can switch them so we
can go from a 4x3 to a 3x4 Oops I did
that the other way around 3x4 to
4x3 we covered reshaping the array we
did the roll the axes you can do some
weird things with swapping and rolling
axes and transposing the
numbers we dug a little bit into the
arithmetic so we talked about adding we
talked about subtracting multiplying
dividing and you know at this point it's
so important we just look up the numpy
mathematics and you can see here they
have just about everything your
trigonometry uh your hyperbolic
functions rounding sums products
differences there is some of all your
different miscellaneous mathematical
connotation so you know Google it go to
the main numpy page and look at the
different setups you can do on there so
we covered that and we did slicing how
just break it apart we did iterating
over the array we covered joining arrays
and how to concatenate remember
concatenate just means add on to it so
in this case how are you adding B onto a
is how you read that from Linux you
should catch the concatenate
that's used regularly there splitting
the array we talked about how to split
the array in different ways so you can
split it in U array of arrays all kinds
of different ways to split the array up
how to resize it and remember resize
does not have to have the same shape but
if you resize it it will take the data
and begin at the beginning and add new
rows on if the size is bigger if it's
smaller it truncates it it just cuts the
end off we looked at how to do a
histogram and how to plot that
uh we mentioned B buckets or bins as
they call them in PIP plot and then we
covered a lot of other useful functions
in numpy talked about the line space
setup for doing U numbers in a series
how to sum the axes up again that's part
of the mathematical formulas there that
we looked at there's a sum there's also
means and median all of those you can
compute in numpy and you can also do the
square root and standard
deviation the r function very similar to
the flat to be honest I almost always
just use the flat but you know the Ravel
has its own kind of functionality that
it does and then we went into some numpy
practice examples we challenged you to
create a sine wave in numpy and how to
do that we're kind of looking for that a
range remember how we do the a range and
you can have your uh beginning value
your end value which they did is three
times Pi nump Pi and we're going to do
intervals of 0.1
and then y just equals the numpy sign of
X there's our math from the math page we
were just looking at remember that it's
right at the
top and uh finally we went down here we
had this kind of a little brain teaser
how to do diagonal zeros and Ones
playing with the different connotations
of Z of the numpy
array and then we did a random size and
we played a little bit with how to with
the null values playing with null values
if you're doing any data science you
know null values are like a headache
what do you do with them big sets of
data you get rid of them small sets of
data you have to factor something in
there like figure out the average or or
the median there and then replace it
with
that pandas really is a core python
module you need for doing data science
and data processing there's so many
other modules that come off of it there
actually sits kind of on numpy so if
you've already had our numpy array
hopefully you've already gone through
through the numpy tutorial 1 and two So
today we're going to cover what is
pandas we'll discuss series we'll
discuss basic operations on series then
we'll get into a data frame itself basic
operations on the data frame file
related operations on a data frame
visualization and then some practice
examples roll up our sleeves and get
some coding underneath there and let's
start with just some real general what
is pandas pandas is a tool for data
processing which helps in data analysis
it provides functions and methods to
efficiently manipulate large data sets
now this is a step down from say using
spark or her dup in Big Data so we're
not talking about Big Data here but we
are talking about pandas when there is
some connections there's like an
interface going on with that so there is
availability but you really should know
your pandas because if you're working in
Big Data you'll know there's data frames
well pandas is a data frame primarily it
has a couple different pieces we'll look
at here and if you've never worked with
data frames before a data frame is
basically like an Excel spreadsheet you
have rows and columns you can access
your data either by the row or the
column and you have an index and
different that kind of setup and we'll
dig more into that as we get deeper into
pandas but think of it as like a giant
Excel spreadsheet that's optimized to
run on larger data on your computer and
then I said it that it's a data frame so
the data structures in pandas are series
one-dimensional array
and then we have data frame
two-dimensional array and it really
centers around the data frame the series
just happens to be part of that data
frame and here's a closer look at a
panda series series is a one-dimensional
array with labels it can contain any
data type including integers strings
floats python objects and more so it's
very diverse if you remember from numpy
we studied they had to be all uniform
not in pandas in pandas we can do a lot
more and pandas actually kind of sits on
numpy so you really need to know both of
them those if you haven't done the numpy
tutorials and you can see here we have
our index 1 2 3 4 5 and then our data a
b c d and e very straightforward it's
just two columns and we have a nice
index label and a column label for the
data and then a data frame is a TW
dimensional data structure with labels
we can use labels to locate data and you
can see here we had if we go back one we
had our index 1 2 3 4 5 so in each one
of these series they would share the
same index over there the row index IND
so you have your row index DF do index
and then you have a column index DF do
columns and this should look like I said
this should be really familiar if you've
done any work with spreadsheets Excel so
it kind of resembles that this does make
it a lot easier to manipulate data and
add columns delete columns move them
around same thing with the rows so you
have a lot of control over all of this
now we're of course going to do this in
our Jupiter notebook you can use any of
your python editors but I highly suggest
if you have an installed jupyter and
haven't worked with it it is probably
one of the best ways for easily
displaying a project you're working on I
skip between a lot of different user
interfaces or idees for editing my
Python and it's just simply jupiter. org
J py t.org and then I always let mine
sit on anaconda anaconda.com and just
real quick we'll open that up for you
oops offline mode don't show me that
again but you can see here that I have
different tools that I can actually
install in my um Anaconda including the
Jupiter notebook which comes by default
and then I have access to the
environments and again that's
anaconda.com named after the L large one
of the largest world's largest snakes
and then Jupiter notebook in this case
jupiter. org and when we're in our I'm
going to go in here to our Jupiter
notebook and we're going to go ahead and
just do new and a Python 3 and this will
open up a Python 3 Untitled folder so
diving right in let's go ahead and give
this a title pandas tutorial and we'll
go up to cell and we'll change the cell
type to markdown so it doesn't execute
it as actual code one of those wonderful
tools when you have Jupiter notebooks
you can do demos with this and let's go
ahead and import pandas and usually
people just call it PD that's has become
such a standard in the industry so we'll
go ahead and run that now we have our
pandas has been imported into our
Jupiter notebook and then oh we can go
ahead and let me do the Control Plus and
say Internet Explorer I can enlarge it
very easily so you have a nice pretty
view oops too big there we go and when
you you're working with a new module
it's good to check your uh version of
the module in pandas you just use the in
this case PD doore verore uncore that's
actually pretty common in most of our
python modules there's different ways to
look up the version but that's one of
the more common one and we'll go ahead
and run that we get
23.4 and if we go to the panda site we
see
0.234 is the latest release and of
course a reminder that if you're going
to environment you need to install it so
you'll need to do pip install pandas if
you're using the PIP installer we'll go
and close out of that and the first
thing we want to do is we're going to
work with series a lot of the stuff you
do in series you can then do on the
whole data set we need to do what create
one we need to manipulate it take pieces
of it so query query
it delete so you can delete different
parts of it so we want to do all those
things with the series and we'll start
with the series and then almost all the
code in fact all the code does transfer
right
into the actual data table so we go from
a series of a single list of one column
and then we'll take that and we'll
transfer that over to the whole table
and we'll start by creating let's put a
there we go creating a series
from list and let's just call this ARR
equals and we'll do 0 1 2 3 4 if you
remember from our last one we could
easily do R equals range of five which
would be 0 to 4 but we'll do r equal 0
to 4 and we'll call this S1 and we'll go
PD and series is capitalized this one
always throws me is which letters do you
capitalize on these modules they're
getting more and more uniform but you
got to watch that with python and we're
just going to go ahead and do AR RR so
we're just going to take this python
list and we're going to turn it into a
series and then because we're in Jupiter
we don't have to put the print statement
we can just put S1 and it'll print out
this series for us and let's go ahead
and run that and take a look and you'll
see we have two rows of numbers so the
first one is the index now it
automatically creates the index starting
with zero unless you tell it to do
differently so we get zero index row
zero is 0 1 1 2 2 3 3 4 4 and because
it's a ser it doesn't need a title for
the column there's only one column so
why title it and this also lets you know
that it's a data type of integer 64 so
we print this out this is our series our
basic series we've just created and
let's do second
series PD and we'll use the
same data list and let's go ahead and do
order we'll give it an order equals oh
let's do it this way let's go index
equals order and it helps if we actually
give it an order so we'll do order
equals and let's do one 2 3 4 five so
instead of starting with zero we're
going to give it an order starting with
one we're going to run that and we'll go
ahead and print it out down here
S2 and we'll see that we now have an
index of 1 2 3 4 5 and that represents 0
1 2 3 4 in the series and we're still
data type integer 64 and very common as
you're missing with numpy arrays is we
can import our numpy as in p remember
that from our numpy tutorials we can go
ahead and create a numpy out of random
with the random numbers of five and
let's just see what that n looks like so
we can see what our numpy looks like so
we have some nice random float values
here 2.33 so on and that's from our last
tutorial the numpy tutorial one and two
and instead of calling it order let's
call it index and we're going to set our
index equal to a b c d and e
I want to show you that the index
doesn't have to be an integer so it can
be something very different here and
then let's go ahead and create our we'll
do use S2 again and here's our NP for
numpy series capital s and n is our NP
for numpy PD for pandas there we go
switching my anacronym so we have pd.
series of in and we go and do our index
equals our index we just created
and then let's go ahead and see what
that looks like S2 is a print it and
let's run that and we can see here we
have a nice Series going on AB c d and e
for our indexes so instead of it being 0
1 2 3 or four we can make this index
whatever we want and you can see the
numbers here going down that we randomly
generated from the numpy array so we use
numpy to create our Panda Series right
here and so continuing on with creating
our Series this one I use so often we
create a series from a dictionary so we
have our dictionary in this case we went
ahead and did a of one B is 2 C of3 D4 e
of5 so each one of those is a key and
then a value and then we're going to use
oh let's use S3 equals PD for pandas
series and then we want to go ahead and
just do D in here print out S3 here and
let's go ahead and run this and you can
see we got a is one B is 2 C is 3 D is
four e is five and it's still of integer
64 because you actually ual data is 1 2
3 4 5 and it's all integer 64 type 64
and the last thing we want to do in the
creating section of our series is to go
ahead and modify the index because we're
going to start modifying all this data
so let's start with modifying the index
of the series and if you remember let's
do a print this time
S1 I'll go ahead and run this and the
reason I did print is because it only
prints out the last variable so if I put
S1 up here and we're going to do another
variable back down lower it won't print
the first one just the last one and
we're going to go ahead and take
S1 the index and we're just going to set
it equal to a new index and obviously
the number of objects in our index has
to equal the number of objects in our
data and then because it's the last
variable we can go ahead and just do an
S1 and let's run that and you can see
how we went from 0 to0 0 1 2 3 4 as our
index we've now altered it to A B C D
and E
so this can be much more readable or
might be representational of a larger
database you're working with so cool
tools we've covered creating database
based on my basic array python array
we've showed you how to to reset the
index then we showed you how to use a
numpy array so you can put a numpy array
in there it's all the same you know pd.
series numpy array and then we can set
the index on there and the same thing
with the dictionary so it's very
versatile how it pulls in data and you
can pull in data from different sources
and different setups and create a new
series very easily in the pandas and
then we looked on changing your index so
now we have a new index on here and then
we want to go ahead and do some
selection let's do some
basic slicing most common thing you'll
probably do on here we'll just do S1
this notation should start to look
really familiar again this is going to
put an output so I usually it doesn't
change S1 one this just selects it so we
might do a equals S1 and then print a
and you'll see that it just looks at the
first three 0 one 2 and we can do the
same thing by not having the a in there
I'll go ahead and take that out but just
a reminder that it's not actually
changing S1 it's just viewing S1 so
simple slicing on here and we can
likewise do an append O So before we do
aend let's just do a quick kind of fun
one we'll do to minus one and you'll see
it covers everything but the e of course
you can do minus two on this side so one
another way to select it is to go how
far from the end and likewise we can do
a two here CDE to the end so it starts
at the second one and another way we can
do this is we can do a minus two over
here and that looks at just the last two
in the slice so you can see how easy it
is to slice the data and of course
there's no reason to do this but you
could select all of them if you wanted
to view all of them on there oops 32
there's not 32 so it's just going to
show the first three there we go and
then we can also append so I can take
and oh let's create another uh series
and append one to it and if you remember
we had S3 there's our S3 and we have our
S1 go and do S1 and let's go ahead and
do oh let's call it
S4 equals
S1 Ain S3 so we're just going to combine
those two into
S4 and if we go ahead ahead and print S4
on here you'll now see that we have
ABCDE e ABCDE e 0 1 2 3 1 2 3 45 because
we started the data at one so very easy
to append one series to the next and if
we're going to append one series to the
next we need to go ahead and drop or
delete one and drop is a key word for
that and let's just do e or indexe and
so if I run this you'll see that it'll
print it out and ABC d there's no e and
remember all these changes if I type in
S4 again you'll see that S4 still has e
in it so this change does not affect the
series unless you tell it to so I'd have
to do like x S4 equal S4 drop e and
there's another way to do that which
we'll show you later on me just cut this
one
out there we go all right so we've
covered all kinds of Cool Tools here we
have a pending we have slicing we did
all the creating stuff earlier
so you can see here on the setup how
easy it is to manipulate the series so
next what we want to get into is we want
to get into operations that happen on
the series let me go ahead and change
this cell to markdown there we go and
run that so series operations what can
we do with the series and let's start by
creating a couple arrays we'll call it
array one and we'll do 0 through 7 and
array 2 6 through 6 7 8 9 5 I don't know
why we threw the five on the end let's
go ahead and run those so those load up
into Jupiter and we'll do this a little
backwards we're going to do S5 equals a
panda series of array two so I'm doing
this in reverse and then when we do S5
you'll see that we have 0 to four it
automatically assigned the index 678 95
for our series and let's go ahead and do
the same and we'll call this S6 and
we'll set this equal to PD series for
for our first array and if we do an S6
down here to print it out we'll see
something similar I got 0 through six 0
1 2 3 4 5 7 for the data so those are
two series we just created series six
five and six and one of the first things
we can do is we can add one series to
the next so I can do S5 do add
S6 and let's see what that generates and
just a quick thing if You' never used
pandas what do you think's going to
happen happen with the fact that this
only has five different values in it and
this one has seven values so let's see
what that does and we end up with 6 8 10
12 9 and it goes oh I can't add this
there's nothing there so it gives us a
null return very different than the
numpy that would have given you an error
this instead tells you there's no value
here because we couldn't generate one so
we can easily add S5 do add S6 and
likewise we can do S5
do sub for subtract S6 and we'll run
that and on the add the subtract and you
guessed it we're going to do multiply
and divide next again you can see
there's null values where it can't
subtract the two because there's no
values there to subtract we can also do
S5 multiply M they're all three letters
on these that's one of the ways to
remember how they figured out the code
for this so remember these are all three
letters mole we'll go and run this and
you can again you can see how they're
multiplied together and then we can also
do the S5 div three letters again S6 and
run that and you'll see here this goes
to Infinity because we have zero in the
wrong position so it actually gives you
a whole different answer here that's
important to notice and then in the null
values because there's no data and it
can't actually produce an answer off of
null off of missing data and since we're
in data science let's do
S6 median so this look up the median
data which is simply uh median sorry for
those who are following the three
letters because median is not three
letters and you can see in S6 is 3.0 and
let's do a print here and we'll do
median or average
S6 and let's print
Max comma S6 and just like median
there's max value and if we're going to
have a max value we should also have a
minimum value so let's pop in minimum
we'll go aad and run this and you're
starting to see something that would be
generated like say an R where you're
starting to get your different
statistics we have a medium value of
three max value of seven and a minimum
value of zero and what it does when it
hits these null values if there is no
values in there because we could still
do that we could actually you know what
let's go up here and
do let's pick this one where you
multiplied let's go
S7
equals I'll go and print the S7 so I
keep it nice and uniform so I still have
my S7 down there and run it and then I
want to take the
S7 CU S7 now has null values and an
Infinity value and let's see what
happens this is going to be interesting
because I want to see what it does with
infinity and we end up with a median of
six maximum of 27 and minimum of zero
which is correct it drops those values
so when it gets to there and it doesn't
know what to do with them it just drops
those values and then it computes it on
the remaining data on there so that's
important to know when you're making
these computations you're looking at Min
and Max and median you're not going to
know that there's null values unless you
double check your data for the null
values it's a very important thing to
note on there so just a real quick
review on there we've done our created
our PD series and we've gone ahead and
done addition subtraction multiplication
division all those are three letters so
sub Min div add and then we looked at
median maximum and minimum so we're
going to go ahead and jump into the next
big topic which is to create a data
frame so now we're going to go from
series and we're going to create a
number of series and bundle them
together to make a data
frame there we go cell type markdown and
me go and run that so we have a nice
title on there it's always good to have
a good title all right so our first data
frame we'll jump in with some stuff that
looks a little complicated we'll break
it down first I'm going to create some
dates and you know what let's just go
ahead and do this I want you to see what
that looks like what I'm creating here
I've created a series of dates PD date
range and we're going to use these for
the index okay so when you look at this
you'll see that it's just an basically
it comes out kind of like a basic python
list or numpy array however you want to
look at it with our different dates
going down and we've generated six of
them and it's going to have whatever
time it is right now on your on the
thing for the date for the time that's
that time stamp right there and then
you'll see we have 1119 2008 1120
1119 and looking into the future there
so that's all this is is generating a
series of dates that we're going to use
as our index and this is a pandas
command so we have a date range which is
nice that's one of the tools hidden in
there in the pandas that you can use and
next we're going to use numpy to go
ahead and generate some random numbers
in this case we'll do the np. random.
random in 6 comma 4 you can look at this
as rows and columns as we move it into
the pandas and of course you could
reshape this if you had those backwards
on your data but we want the six to
match the rows and we have six periods
so our indexes should match along with
the rows on there and then you know what
before we do the next one let's go ahead
and just print out our numpy array so
you can see what that looks like here we
have it 1 2 3 4 by 1 2 3 4 56 4X six so
it's a nice little setup on there and
since working with data frames can be
very visual let's give our columns we
have four columns and we're going to
give them names A B C and D so now we
have columns on there also and then
let's put this all together in a data
frame and we can actually you know what
let's do this since I did it with
everything else let's go ahead and do
columns and you can see there's our
columns on there and we'll go ahead and
do df1
equals pandas do datf frame and note
that the D and the f are capitalized
series it was just the S and I always
highlight this cuz you don't know how
many times these things get retyped when
you forget what's capitalized on there
it's a minor thing you'll pick it up
right away if you do a lot of it and the
first thing we want to do is we want to
go ahead and take our numpy array
because that's what we're going to
create our data frame off of is the
numpy array and then we want our index
equal to our dates so there's our index
in there and then we also have columns
equals columns and then finally let's
see what that looks like now remember we
had all that different data that just
look like a jumb of data we have our
column names and everything else our
numpy array kind of just a jumble array
over there 4x6 you can sort of read it
but look how nice this looks I mean this
is you come into a board meeting you're
working with your um
shareholders this is pretty readable
this is you know this is our date this
is our a b c d whatever it is maybe each
one of these dates has your leads
closures lost leads total dollar made
you know whatever it is if it's in a
business maybe it's measurements on some
scientific equipment weather searching
material you know where this is like a
high of the temperature low of the day
humidity of the day whatever it is so
you can see that we can really create a
nice clear chart and it looks just like
a spreadsheet you know we have our rows
and we have our columns and we have our
data in there now this one I use all the
time if we're going to create we can
create it like you saw here with our
numpy array very easy to do that and
reshape it you can also create it with a
dictionary array so here we have some
data and let me just go down Notch so
you can see all the data on there we
have an animal in this case cat cat
snake dog dog cat snake cat dog we have
the age so we have an array of Ages we
have the number of visits and the
priority was it a high priority yes no
and then we're going to take that we're
going to create some labels we have a b
CDE e f g hi I and what I want you to
notice on this is we have a title animal
and then we have basically a python list
and these lists they don't necessarily
have to be equal cuz we can have non
data you know np. Nan numpy array null
value but we want to go ahead and create
labels that are equal to the number in
the list so a the first cat B the second
cat C the snake D the dog and so on so
we'll go ahead and create our labels
which we're going to use as an index and
we'll call this DF let's do it this way
we'll call this
df2 equals PD for Panda's data frame and
then we have our data just like we did
before and then we have our index equals
labels and if we're going to go from
there let's go ahead and print it out so
we can see what that looks like df2 so
let's go ahead and run that and another
again you have a nice very clean chart
to look at we've gone from this mess of
data here to what looks like a very
organized spreadsheet very Visual and
easy to read animal age visits priority
and then a through J cats and all your
different animals so on and so on and
then when you do pro programming a lot
of times it's important to know what the
data types are so we can simply do
df2 D types and if we run that we can
see that our animal is an object because
it's just a string but it comes in as an
object age is a float 64 integer 64 and
then priority again is just an object
and exploring this this one's very
popular let's go
df2 head and if we print that out the
df2 head Returns the first five and we
can change this you don't have to do
five you might want to just look at the
top two maybe you want to look at let's
see let's do six so maybe we want look
at just the top six in the database in
your data frame and you can actually
this creates another data frame so I
could have uh
df3 equal to df2 and this now takes the
df2 and just the first six values so if
we do
df3 run get the same
answer and if we do a the head of the
data we can also do the tail it's the
same thing DF you can look at the last
we'll just do the tell which by default
does five the last five and of course
you can just look at the last three of
those real quick just to see what's at
the end of the data and this is I the
tell I love doing the tell of one
because I'll have like the index or
something like that and it will just
show me the last whatever the last entry
was you know looking at stock values and
I might want to look at just the last
five days of the stock values I can do
that with the data frame tail and some
other key things to look up are the
index so we can do df2 do
index and I want you to notice that this
isn't a call function so if I put the
brackets on the end it'll give me an
error because index is not callable it's
just an object in there so we do DF 2.
index there's also
columns so we can go ahead and let's do
a let's print this remember the first
one's not going to show unless I print
it and then df2 columns so now we can
see we have our indexes and we have our
columns listed here df2 do columns
animal age visits priority it tells you
what kind of object it is and or what
kind of data type it is and they're both
object and then finally df2 Dov values
and again there's no brackets on the end
of df2 Dov values because this is an
actual object is not a cable function so
we'll go aad and run that and it creates
just displays a nice array a very easy
way to convert this back to a numpy
array basically so before I go into the
next section let's just take a quick
look at what we covered so far with the
data frame we came up here we created
our data frame we did it from a numpy
array first setting the columns and the
index the index is setting it up is the
same as when we set up the series so
that should look very familiar so is the
whole format the numpy array the index
dates and the columns columns and
remember in our numpy array we're
looking at row comma column so six rows
four columns is how that reads in the
data frame and we went ahead and also
did that from a dictionary in this case
animal was the column name with all the
date data underneath that column and
then age with that data visits that data
priority that data and then of course we
added our labels in there for our index
so there's no difference in there but it
automatically pulled the column names
important to know when you're dealing
with a data frame and importing a data
frame this way and then we did looking
up dtype we looked at head and tail
looking at your data really quick we
also did index and columns and values
and note these don't have the brackets
on the end so the next thing we want to
do is go ahead since we're dealing with
data science is we want to go and
describe the data so we have DF 2.
describe to do that and we're going to
manipulate it in just a minute but let's
just see what this
generates and you can see right here we
have age and visits so looking at our
data from up above let me just go all
the way up here animal age visits
priority and it does a nice job
generating your age versus visits which
has all the data you have your count
your means your standard deviation your
minimum value 25% or in this group 50%
75 and your maximum value so this should
look familiar as a data science setup
with your describe for a quick look at
your data Frame data so let's start
manipulating this data frame and moving
stuff around and we'll start with
transposing and it is simply capital T
for
transpose and when we run that it flips
The Columns and the indexes so now the
indexes are all column names and the
columns are all indexes animal age
visits priority so if we had come in
here with our data shaped wrong up above
where we had a 4X six we can quickly
just swap it if we had it backwards not
a big deal and we can also sort our data
uh something that you can't deal which
is more difficult to do with a lot of
other packages in the data frame is
really easy to do take our data frame
df2 and we're going to sort underscore
values by equals Ag and so when we run
this you'll see the default is ascending
so we have 0.52 2.53 and everything else
is organized so if you look at your
indexes they've been moved around
because each index it moves a whole row
not just the one piece of data is not
being sorted so a very quick way to sort
by age our different data in the data
frame and in addition to sorting it we
can also slice the data frame so I could
do df2 and this should look familiar
from earlier we'll just do one to three
so we're going to pull out oops it does
help if I use a DF instead of just D and
we're going to pull up just between one
and three so we have not zero which is a
we have B which is two or B which is one
and C which is two so one two and then
it does not include three which is the
standard in Python and we can even do
something like this we can combine them
which is always fun because remember
this returns a data frame so if I take
df2 do
sort values and we'll do BU equals age
this is just kind of fun and then I'm
going to slice it it there we go double
check my typing and run it and now you
should see fa because fa are now one and
two on there so you can very quickly
create a whole string on here which
narrows it you know that you can sort it
then slice it and do all kinds of fun
things with your data frame we'll just
go back to the original one run there we
go and if we can slice it by row we can
also query the data frame so we can do
df2 and this is a little different cuz
I'm going to create an array Within
array and in this case we're going to
look at oh let's do um age
comma visits so look at the different
format in here we have 1 to three so
we've done this by slicing by an integer
value and then on here I've done df2 age
comma visits in an array and when I run
this you can see that we get just these
two columns on here we get age and
visits so it's a quick way to select
just two columns or select number of
columns you're working with and if you
saw up there we did the slicing almost
identical to slice is I location which
uses the integer location 1 comma 3
there's a push in pandas to move to this
particular setup instead of doing just a
regular slice and that's because this
can be confusing when we slice one to
three and then we select Ag and visits
so there is a push to go ahead and move
to an i location which does the same
thing you you can see here BC it's the
same as up above there's also a copy
command so we can do df3 equals df2 copy
we're just going to create a straight
copy of it and of course if we do
df3 it'll be the same as a df2 on there
so df3 equal df2 copy and then let's do
df3 do is null so we're looking for null
values and this will return a nice map
and you'll see that everything is false
except when you go up here under the cat
or H they had a null there and so if we
go they have a couple up here also
underneath of let's see the dog okay
there's a bunch of NES in here there's D
up here so let's like a d down here and
you'll see false true there it is
there's our null value so we can create
a quick chart of null values you can use
this to do other things we can leverage
that null value to maybe take an average
or something and fill those null spaces
with data and we can also modify the
location so here's our d
F3
location and notice this is location not
I location I location has I for integer
location uses the in this case the
variables on the left and what we can do
on here and we'll go and just set this
equal
to five and then let's um I'll pick a
spot let's go back up here where we had
let's do F A just let's see where what
was we looking at oh here we go let's do
F and a
and up here f is set to age of 2.0 and
we find out that that's incorrect data
so we go ahead and switch the df3 equal
and then we go and print out our
df3 and if we go to F and age it is now
1.5 so we're just changing the value in
the df3 and this is changing the actual
data frame remember a lot of our stuff
we do a slice and uh like it returns
another data frame this changes the
actual data frame and that value in the
data frame
so we've covered uh location and I
location is null making a copy here's
our ey location which is equivalent of a
slice and also selecting columns so now
we want to dive just take a little
detour here and let's look at t df3
means and this is kind of nice because
you can do this you can either do this
by as you can select a single column
here by the way you can just add the
column selection right here like we did
before so we could have age look up the
mean that just creates a series if I run
that there's our age but if I take that
out instead of selecting it we can do
the whole setup and it has age and
visits so why doesn't it have priority
or animal well those are not integers so
it's really hard they're non-numerical
values so what is the average I guess
you could do a histogram which um
probably we'll look at that later on but
the only two things we can really look
at is age and visits and we have the
average or the mean on the age is 3.375
and the mean on visits is
1.9 and let's do
df3 visits we'll go and steal the visits
again and you remember all those
different functions we looked at for a
series well we can do those here we can
do the sum so if we run that we'll see
that these sum up to 19 could also look
up minimum if you remember that from
before the minimum is One Max so all
that functionality is here we'll just go
back to summing it up and adding it all
together so real quick we've uh shown
you how to take the series operations
and put them into the data frame and
then we can actually this is interesting
one we can just do df3 sum run and
you'll see the different summations on
there it just combines them I like the
way it just combines the strings on
there for priority and animal we've
looked at is null we've also looked at
copying along with the different slices
which we talked about earlier so let's
talk about strings let's dive into the
string setup on there and let's go ahead
and create a string series string equals
PD series and we just put it right in
there we have a c d AA B A CA popped in
a null value cow and Owl I don't know
why they picked cow and owl in the
background some much like those animals
and of course we can just do string if
we run that you'll see leave the r out
we'll get an error but if we put it in
there you'll see that we have a simple
series 0 a 1 C 2D and it automatically
indexes it 0 to8 and then we can go
string. lower so when we're talking
about our data frame in this case or our
data series string in this case we're
use the string function St and we're
going to make it lower and if we go
ahead and put the brackets on there and
you'll see that we've gone from capital
a Capital C so on to ABC and Baka CBA
cow Al they were all lowercase already
and of course if you want to go lower
you can also do upper and we'll go ahead
and run that and you can see we now have
ACD AAA Baka everything's capitalized
except for the null value which is still
null all right so we looked at a few
basic string you can see that string
functions upper and lower we're going to
jump into a very important topic I'm
even going to give it its own header on
here because it's such an important
topic what do you do with missing values
Panda has some great tools for that so
we'll dive into those we'll call we'll
work with df4 and if you remember the DF
copy from above we're just going to make
a copy of df3 and let's just take a
quick look at the data we're working
with oops df3 forgot the three on there
there we go so here we have our cats
snakes and dogs hopefully not all in the
same container because that would be
just probably mean to all of them so we
made a copy we're going to be working
with df4 and the reason we made a copy
is we want to go ahead and fill the data
and we just simply do fill in a a and
then we're going to give it the value we
want to put in there we'll give it the
value four so I can run in here and
you'll see now that df4 now has where
the na was it's filled with a value of
four same thing down here a lot of times
we'll compute the mean first so I might
do a mean age equals df4 and then we
want to go ahead and do
age and mean
and then I'll do something like this df4
I only want to select the age and I want
to fill that with the mean age and I run
in there and you'll see that our df4 age
now has the means in there just a quick
way of showing you how you can combine
these let me go back to our original one
there we go and run that and keeping
with good practices df5 equals df3 do
copy and we'll print our df5 which
should be the original one and then on
the df5 we can now drop our missing
data I'm going to Simply drop na a and
we're going to use how equals any so I'm
going to drop any row that has missing
data in it and you'll see we had D here
with missing data and H and then let's
go ahead and see what df5 looks like
when we do
that there we go and there it is D is
gone and so is H so we create a new data
frame off of this missing those values
now if you have a lot of data dropping
values is a good way to take care of it
because you don't miss of data if you
have not a whole lot of data you're
working with like the iris data set or
something like that or something small
you want to start trying to find a way
to fill that data in so you don't lose
your computational power of the data you
got so just a quick look at processing
null values or missing values you can
fill them usually with the me
some people use medium or the mode
there's different ways you can fill it
one way is means and we can also just
drop those rows those are the two main
things we do with missing
data here we go uh we're going to cover
next this is I so love data frames for
this file operations it saved me so much
time because they have so many different
tools for bringing data in and saving
data so we're looking at the data frame
file operations
it's really streamlined I don't know how
many times they'll go on to different
data downloads and they'll have Panda
download standard on there just because
it's so widely used so let's start with
the most common file is a CSV so we have
df3 to CSV or animal and let me just
show you the folders going into right
now I have some Untitled and a few
things in here but nothing labeled
animal so we go ahead and run this and
this has now saved the animal to my hard
drive and you can now see the animal
folder up here and if I U let's do edit
with a notepad oh let's open up with
just a regular notepad there we go or
word pad if I open that up you can see
it's comma separated our titles they
don't have an index on the categories on
the top in the index comma then all the
different data separated by commas
standard CSV file on there and if we're
going to send it to CSV and notice a
format is 2or CSV and this just the name
of the file we're sending it to you can
also put the complete path by default
it's going to go whatever the active
directory this program is running on
that's why those other folders were in
there so we have our df3 to CSV and then
if we're going to put it in there we
want to also get it back out and we'll
call this one DF animal equals PD read
uncore CSV I always have to remember
it's 2core CSV and read uncore CSV I
always want to do like a capital in
there and not the underscore we're going
in here again it's the active directory
so if I now do print out my DF
animal and let's just do the ahead we
only look at the first three lines so if
I go ahead and run this we'll see the
first three lines and they should match
up here what we saved to our CSV so very
easy to save and import from our CSV
files on here and it turns out
df3 also has a 2XL they actually have a
lot of different formats but you know
old school Excel was real popular for so
long still is we can go ahead and save
it as animal. xlsx we're going to call
the sheet named sheet one and then I can
also do DF we'll call it animal 2 animal
2 and this one's going to come from and
the same format on here there we go so
we still have our animal
xlsx the sheet one that's where it's
coming from index columns equals none so
we're not going to we're going to
suppress the indexing on the columns na
value and it it'll just assign it's zero
on up on your indexes so if it says
index columns equals none that's what it
does and then we've added null values
because there's null values in here and
we want to just make sure that they're
marked as Na and we'll go ahead and just
print out the animal animal to there we
go and let's run that let's make this
let's just do the whole thing so we'll
go ahead and run that and it probably
doesn't help that I completely forgot
the read so animal 2 equals pd. read X
Excel there we go Excel so now we go
ahead and run it and what we expect is
happening here we have the same data
frame on here and if I flick back to my
folder you can now see that we have the
animal one of these is an Excel and one
of these is a um CSV on here and so
there's our two file Types on there and
they have other formats these are just
the two most common ones used and I
don't know how many times I've had stuff
from Excel I need to pull out if you've
ever played with Excel it's a nightmare
in the back end because because of the
way they do the
indexing so this just makes it quick and
easy to pull in an Excel spreadsheet so
we looked at two different ways to bring
data in and save it to files we've
looked at all kinds of different ways of
manipulating our data set and slicing it
and creating it for our data frame let's
get in there what you're
visualization always a big thing at the
end because one it lets you check to see
what you did make sure it looks right
and then also if you're going to show
somebody else it makes it very clear
what's going on if they see something
visual so this is where really important
part of data science is so let's go
ahead and bring in our tools we're going
to do import numpy as NP we want to make
sure we have our Amber sign Matt plot
library in line this just lets Jupiter
know that we're going to print it on
this page if you're using a different
IDE you don't really necessarily need
that but this does help it displays
correctly in Jupiter notebook and if you
remember for earlier we could create a
uh we call it TS we're going to create a
pandas which are cuute cuddly creatures
versus pandem short for pandemonium no
so we have TS equals PD series and we're
just going to create a random setup of
50 we'll do an index we'll set it equal
to the Panda's date range today periods
equals 50 so the 50 should match and I
want you to notice something here I did
not import the map plot Library why
because it's already in there pandas
already has its built-in connection and
interface with map plot Library so you
don't have to import it and we'll go
ahead and do TS equals ts. cumulative
sum we're going to do the cumulative sum
so a little reformatting there and we'll
go ahead and plot it and let's take a
look at what that looks like so we have
a nice graph here we have the dates on
the bottom we set this up so we have a
nice range between in this case minus 4
to looks like about two maybe or one
minus four and one so what we've done
here we've plotted a basic series just a
single row of data and we've seted
indexes on there but we can also do the
whole data frame on there and let's see
what that looks like so first let's go
ahead and create the data frame we have
here random numbers and we're going to
do 50 by4 and then we'll go and create
columns a b X and Y just because we can
index is a ts. index on there so we're
going to use the same index as before
just to keep it nice and uniform we've
already generated the dates to go with
it and then we can do just like we did
with the ser
we can also do with the data frame DF
equals DF cumulative sum so we're going
to sum the whole data frame and then
we'll do simply DF plot and let's put
that in and let's go ahead and run this
and look how easy and quick that was to
generate a nice graph with all the
different data on there so we have our
shared index we have the shared columns
and then we have the different data from
each one that we can easily look at and
compare so very quick way of displaying
data you can imagine imagine if you were
working in oh I think I mentioned stock
earlier because I've been doing some
analysis of stock lately so you'd have
your date down here and then you would
have stock a Stock B stock XY whatever
it is and you could put them all on one
chart and see how they what they look
like next to each other and this isn't
too far off from what some of those
graphs look like and this is just
randomly generated so stock has a lot of
Randomness in it which is one of the
reasons I actually play with it for
doing some of my models on for testing
them out now there are a lot of features
in pandas so we're going to show you one
more thing on here there's some of the
things like I didn't go too deep we
looked at the top two for importing data
from a CSV and from an Excel spreadsheet
showed you how to quickly plot the data
there's more settings in there you can
do we're going to do one more thing down
here and this is kind of a fun one
change this to a markdown and run that
so how would you remove repeated data
using pandas and this is where you have
a data set that comes in
and maybe it's feeding from one location
and instead of noting that it's repeated
the date like oh let's go back to stocks
that's a good visual we have the stocks
from the 23rd and it adds another row
and it's the same row it's it's
importing the 23rd again and again so
now you have that data repeated three
times and you need to go back and figure
out how to get rid of it how do you
track that down so let's start by
creating a quick database or data frame
not a database I keep saying databases a
data frame and what this make this data
frame has using our dictionary going in
this data frame only has one data Series
in it which is fine so if we do DF to
print it out you'll see A1 2222 4 4 four
5 six 7 and so on and so how would you
remove that well there is a a neat
feature in data frames called shift
along with another feature that lets us
select just certain dat information and
we'll go with the Loc location function
put that in Brackets remember that from
above location and then in the location
let me just spread this out a little bit
so it's real easy to read in fact I'm
going to go up scale on that since we're
doing some a little bit more complicated
here what you can see on this on the
location is I have DF a.shift so this is
going to shift up one by default you can
actually change this to two or three you
can even do a minus one and it shifts
the other way but it's to shift up by
one by default that's going to say if
that does not equal DF of a then we want
that and if you look down here we had
one two two two two two when we run this
Logic on here and we do the shift it now
gets rid of all the duplicates so we
went from 1 2 two two two four four five
whatever it was here it is 1 2 two2 4 4
4 5 five 5 666 to 1 2 4 5 6 7 8 and
you'll see on the index it just deletes
them out of there so the index stays the
same obviously you don't want the dates
to change if you're working with an
index data setup so it just deletes
those duplicates out of there this is
just a quick way to introduce you to one
the fact that you can add logic gates
into here and two the
iocation allows you to use shift so
there's the shift function and then the
ication selects that based on true or
false wow so we've actually covered a
lot today in pandas we've really covered
into the basics of selecting your
different series out of your column out
of your data frame how to index rows how
to slice how to plot hopefully you'll
take this beyond that and start
combining these different things and you
can create long strings and really
explore your data generate some nice
graphs if you're in jupyter Notebook
it's a great demo to show others and I
didn't know this about Jupiter notebook
you can do this in jupyter Notebook and
then you can download and I always I
never really look too closely at all the
download loads but you can download as
an HTML and post it to your blog so it's
got a neat feature in there but any of
this is really powerful tool all of this
is really powerful tools for doing your
data science if you are one of the
aspiring data scientists looking for
online training and graduating from the
best universities or a professional who
elicits to switch careers with data
analytics by learning from the experts
then try giving a short to Simply learn
skel Tech postgraduate program in data
science in collaboration with IBM the
link is mentioned in the description box
and you should navigate to the homepage
where you can find a complete overview
of the program being offered so let's
start with what is Matt plot library
matplot library is an open-source
drawing Library which supports Rich
drawing types it is used to draw 2D and
3D graphics and there are so many
packages in the matplot library we're
going to cover the basics and there are
so many packages that sit on top of the
M plot library that we can't even cover
them all today but we'll hit the main
ones so you have a good understanding of
what the map plot library is and what
the basics can do you can understand
your data easily by visualizing it with
the help of matplot Library you can
generate plots histograms bar charts and
many other charts with just a few lines
of code and here we have some basic
types of plots you can see here that
we'll go into we have the bar chart the
histogram boy I use a lot of histograms
in my stuff scatter plot line chart pie
chart and area graph let's start
plotting them and to do this I'm going
to be using Jupiter notebook you can use
any of your python interfaces for
programming or scripting and running it
of course we here really like the
jupyter notebook for doing basic a lot
of basic stuff because it's so visual
and in our jupyter notebook which opens
up in this case I'm using Google Chrome
you can go up here to New and we'll
create a new Python 3 and set that up if
you're not familiar with Jupiter
notebook we do have a tutorial that
covers some of the basics of that and
you'll look at any of our tutorials they
usually cover a number of them showing
how to set up Jupiter and Anaconda I
myself use Jupiter through anaconda in
fact let's go ahead and open that up and
just take a look that see what that
looks like you can see your anaconda
Navigator if you install it it will
automatically install the Jupiter
notebook and it also installs a lot of
other things I know some people like the
QT console for doing python or spider
I've never used them I actually use
notepad++ as one of my editors and then
I use the Jupiter notebook a lot because
it's so easy to have a visual while I'm
programming and even simple script in
Python I'll take it from the Jupiter
notebook and then do a save as you know
go under file and you can download as a
Python program so that'll download it as
an actual python versus the I python
that this saves it as so let's go ahead
and dive in and see we got going here
and let's go ahead and put map plot
Library tutorial and I'm going to turn
this cell into a markdown so it doesn't
actually run it when you can see it has
a nice little title there that's all
Jupiter notebook and then from Matt
Library let's
import P laab that back one and then
let's go ahead just print we'll go pylab
and the version let's go ahead and run
this so we're going to import our pylab
module from the matplot library and we
find out that we're in version
1.15.1 always important to note the
version you're in probably I was reading
an article that said the number one
thing that python programmers struggle
with is remembering what version they're
working in and making sure that they're
going from one platform to the other
with the same version and if we're going
to graph things I think we need some
data to graph so we're going to import
numpy as NP now if you're not familiar
with numpy definitely go back and check
out our numpy tutorial there's so many
different things you can do with it
dealing with reshaping the data and
creating the data we're just going to
use it to create some data for us and
there is a lot of ways to create data
but we're going to use the np. linespace
so we're going to create a numpy array
and the way you read this is we're going
to create numbers between 0 and 10 and
we're going to create 25 of these
numbers so we're just going to divide
that equally up between 0o and 10 and if
we have x coordinates we should probably
have some y-coordinates and we'll do
something simple like x * x + two and
let's just take a look we're going to
print X and print y let me go and run
this and let's see what we got going on
here so we have our x coordinates which
is 0 0
0483 Etc and you can look at this as an
XY plot so where we have zero we have
two or we have .416 we have
2.17 and just as a quick reminder we're
going to do print NPR X comma y.
reshaped 25 comma 2 and the reason I
want to do this is I want to show you
something here a lot of times a program
returns X comma Y and it's an array of X
comma y x comma y x comma Y and so when
you're you're working with the pi plot
you have to separate it out and reshape
it so if I start off with pairs like
this I can reshape them if I know
there's 25 pairs in there I can switch
the two and the 25 and this is kind of
goofy but we'll do it anyways reshape so
I'm going to reshape my 25x two back to
2x
25 and if I run that you'll see I end up
with the same output as the XY the two
different arrays in here and this is
important that we want x i me y separate
again that's all numpy stuff but it's
important to understand that this is a
format that map plot Library works with
it works with an array of x's and they
should match your array of y's so each
one has 25 different entities in it and
then for our basic plotting of this data
it only takes one command to draw graph
of this data and so we use our um from
up here we imported P laab we take our P
lab and the key under there is plot for
plotting a line and then we want our x
coordinates and our Y coordinates and
we'll throw in R and the r simply means
red so we're going to draw the line in
red let me go and run that you can
actually switch this around if you
wanted to do different there's B for
blue we have a lot of fun yellow hard to
see yellow there we go but we'll go
ahead and stick with red run and when
you're doing presentations with these
try to be consistent you know if the
business and the shareholders send you a
uh spreadsheet and they have losses in
red use red for losses in your graph try
to be consistent use green for profit
for money you don't have to necessarily
use green but it's whatever they're
using whatever the company's using try
to mirror that that way people aren't
going to be confused if you switch your
data around and every time one graph has
red for loss and one graph has blue for
loss it gets really confusing so make
sure you're consistent in your graphs
and your coloring and something to know
because we're going to cover this in a
minute this is your canvas size so we
have a canvas here and what we're going
to do next is we're going to look at
subgraphs okay so let's take our PI lab
and create a
subplot and one of the things also to
note when we're working with the um mat
plot Library I'm not setting when I do
this this is my drawing canvas the pi
lab so once I've imported the P laab I'm
drawing my images on there very
important to know and with the subplot
we're going to give it some different
values and we're going to represent by
rows columns and indexes and let's do
one 2 one so it's going to be the first
row second column and the index is like
you can stack your graphs and things
like that we don't worry too much about
indexes but rows and columns we want to
go ahead and use Row one and column two
and if we're going to have one object we
should probably have two but before we
do that we have to plot data onto the
subplot so the order is very important
and we'll go and stick with our X comma
Y and uh let's do this we're going to
add in a third parameter here remember
we did Red we're going to add shorthand
dash dash for dash lines so this plots
the data into Row one column two and if
we're going to do that let's do another
one pb. subplot and if we're going to do
Row one let's do column 2 and index 2
and this time we're going to add G for
green and this denotes a style and if
we're going to set up our py laab
subplot here we go P lab we got to go
aad and plot that Pi lab
plot and instead of XY we want y comma X
Oops I messed up this is in the wrong
spot there we go we'll move that down
here real quick because that goes in the
plot part so the subplot tells it the
row column and index and the pi plot
tells it what data in this case we
switch them and the color and then the
style shorthand and let's go ahead and
run that and you'll see it takes this
canvas splits it in two and now we have
two different graphs and we have the red
one with Dash lines and we have the
green one which is has a little stars
going up and if we take this and let's
just um just for fun let's change this
and run that with an index of one it
puts them both on the same index it also
gives me a warning because it's a
strange way of doing to sub plots there
it's depreciated there's another way to
do it but most people just ignore that
warning because it's not going to go
away anytime soon now that's using the
same setup what happens if we do instead
of this let's change the column on here
and find out what happens and if we do
the
column it didn't really like that on the
setup it just disappears so let's keep
our column as two and let's change the
row on the second one to two and run
that and you'll see again it kind of
squishes everything together it can
causes some issues so let's take the
index so each need a unique index and
you can see here where I made some
changes I said row two and look what
happens when I change to column two so I
now have row two column 2 index 2 it
squished it up here so you could put
another graph underneath is what that
does and there's all kinds of different
things you really have to just play with
these numbers till you get a handle on
them
because you know you have to repeat 164
times according to Cambridge University
if it's completely new to you and you
can see right here where here we go
three run there we go but you can see it
takes a little bit sometimes to play
with these and get the numbers right o I
hit the wrong one that's why let's go
three there three there run there we go
now it's overlapping so I have this
doubled over here on the right for now
we'll just go ahead and leave this with
the um where we have column and row to
and the two different indexes so they
appear nice and neatly side by side and
then as we just saw as we were flashing
through them we can put them on top of
each other and let me just highlight
that and copy it down
here paste it down there and here we
have one two one and then we'll do one
two one also for this one and that puts
the two subplot directly on top of each
other gives us that warning and you can
see we now have two different sets of
data graphed on top of each other and
you can also see how it did the indexes
since one of them is from 0 to 10 that's
the green one on the x- axis and the
other one is from0 to 10 on the y axis
so took the greatest value of either one
and then used those as a shared value so
let's next look at operator description
and we'll go ahead and turn this cell
into a mark down and run that so it
looks nice so fig and you remember I
talked about the canvas earlier I
briefly mentioned it we're going to look
a little bit more at the canvas later on
but that's what the figure is FID we're
going to add axes so we're going to
initialize a subplot add the subplot in
rows in columns and all kinds of
different things with this that you can
do let's look at that code and see
exactly what's going on and I want you
to notice that there's fig which is the
actual canvas in the matap plot library
and ax is commonly used to refer to the
subplots so we're creating subplots
you'll see ax equals PLT
subplot earlier we did the pi lab so
let's go ahead and import Pi plot from
map plot library and we're going to can
do it as PLT you'll see that a lot
that's really the standard in the
industry is to call it PLT just like
pandas as PD and nump array as NP
certainly you could import it as
whatever you want but I would stick to
the standards and we're going to do the
same graph as we did above with the p
lab but with the PLT so if it looks
familiar there's reason we're doing this
because we want to show you how the
figure part works and working with the
canvas goes but we're going to do the
same plot as we did before and we'll
call it fig and we're going to set that
equal to plot figure so there's our
figure or our canvas on there and let's
create a variable called axies and we're
going to set that equal to
fig. add
axis and in this we're going to control
the Left Right the width the height of
the camrs from 0o to one so we can go
ahead and I'm just going to put some
stuff in there I got 0.5.1 point88 so
when you're looking at this this is a 0
to one or you could say 50 % 10% 80 80%
but it's a control it's going to control
your left and your right along with the
width and the height so the width and
the height we're going to use 80% and
we're going to have like a little indent
on the left and the right and this
should look familiar from above xc. plot
X comma Y and then let's give it a color
how about red since we're recreating the
same graph let's keep it uniform oops
and it helps if I use axis instead of
axes I don't know where that came from
but this looks identical to the one we
had up above so here's our axis plot X
comma y of red same graph same setup but
this time we've added a variable equal
to the figure. add axis so our plot
figure is our canvas our axis is what
we're working in and then our axis. plot
X comma Y and again we can draw
subgraphs put that down
[Music]
here just like we did before and a
little different tonation here we're
going to have fig comma
axes equal PLT do
subplots and in here it's going to be
the number of rows we're going to do one
row in columns equals two so if you
remember before that's what we did we
had one row with two different graphs on
it we're going to do the same thing but
note how we did this here's our figure
or our canvas and our axes we're going
to create actually two different axes
we're going to create a row one column
two and so axes is an array of
information so we can simply do for
let's do ax in
axes this will now look familiar x. plot
we're going to do X comma y we'll go
ahead and make it red keep everything
looking the same remember nice uniform
graph so everything looks the same and
if we go ahead and run this you'll see
we get two nice side by-side graphs so
just as we had before the same look the
same setup and just for fun let's change
in columns to three we'll run that and
now you see we'll have three on there
and let's even make it a little bit more
interesting we'll do in rows equals to
two and you can see down here we're
going to get an attribute ER is trying
to scrunch everything together so it
does have a limit how much stuff it can
put in one small space that's important
to know you can fix that by changing the
canvas size which we'll look at in just
a minute and there's other ways to
change it on here U but here we go we
can do in rows two in columns equals 1
you can see two nice images right above
each other we'll go back to the original
one row two column side by side left to
right and we can also draw a picture or
graph inside another
graph and that's kind of a fun thing to
do it's important to note that we can
layer our stuff on top of each other
which makes for a really nice
presentation so let's start by uh fig we
create another figure so we're going to
start over again with our canvas we set
that equal to PLT do figure so there's
our new canvas and let's do axes we'll
call it axis one and two axes one equals
fig. add axis remember this from earlier
and this here similar numbers we used
before saying how big this axis is this
figure in the axis is so this is going
to be the big axis and let's do axis 2
equals another figure add axis and then
2.
5.4.3 and if we're going to do this they
need data on them so let's go and plot
some data on our axes so axis one. plot
and we'll make this simply X comma y
comma make it red and then let's go axis
2. plot and let's reverse them y comma X
comma green there we go doing what I
told you not to do you shouldn't be
swapping axes around and plotting your
data in five different directions
because it's confusing but let's go
ahead and run this and see what this
looks like and then let's talk a little
bit about this we talked about the. 2.
5.4.3 and let me just grab the
annotation for that uh that's left right
width and height so we have in here that
this is going to be left right so here's
our left is 0.1 in 0.5 and we you know
what let's just play with this a little
bit what happens when I change this to
0.1 moves it way over to the left so
there's our 0.1 so we can make this
point four run that there we go so you
can see how you can move it around the
the branches on here 0 2 0. five is the
left so that's the right so see what
happens when we do point oh let's make
this 0.1 that actually is they had it
down as left right I thought this was
wrong it's actually how far from the
bottom let me switch that on here bottom
there we go so we had here on this we
can go and put that back to 0. five and
run that and this is. three let's make
this. three also and that is the width
and then of course there's the height we
can make that really tiny let see let's
do
0.2 let's run that and you can see it
changes the height on there we can make
it even smaller 2 by way point 2 and as
you can see you can get stuck playing
with this to make it look just right it
can sometimes take a little bit
certainly once you have the settings if
you're doing a presentation you try to
keep it uniform unless it doesn't make
sense for the graph you're working on
try to keep the same colors the same
position and the same look and feel and
I mentioned earlier we can adjust the
canvas sides so this is from earlier I
just copied it down below we're going to
replot the same data we've been looking
at and what we can do is we can change
the figure size to 16 by9 let me run
that and show you what that looks like
so it fills the whole screen and then if
you are normally when you're working on
the screen you don't worry too much
about this but we can set the DPI to 300
run that there it goes this is your dots
per inch and if you are doing an output
of this and you're printing a hard copy
you want the higher quality I would
suggest nothing under 300 if it's a
professional print you might get a
little less than that but whenever I'm
doing professional graphics and printing
them out on on something 300 dots per
inch is kind of the minimal on there you
can go a lot higher too but keep in mind
the higher you get the more memory it
takes and the more lag time and the more
resources you use so usually 300 is a
good solid number to use in your dots
per inch and you can see it draws a nice
it draws a nice large canvas here which
is 16 by9 and then the DPI is 300 on
here so it's a little higher quality and
just out of curiosity I wonder how long
it takes to draw something double that
size 600 and you can see here where at
600 DPI it's going to take a while there
it goes just because it's utilizing a
lot more graphics on there and let me
just go back to the 300 now we'll
actually do let's do a 100 you're not
going to see a difference on this
because it is web based Graphics are
pretty low and up here you saw I did
this with the plot figure this works the
same if I do figure axis subplot figure
size and then we'll go ahead and do
axes plot
X comma y comma we'll stick to
Red let's go ahead and run this and you
should get almost the same thing here
here's our our axis on the subplot on
here with the fixed size and the DPI let
me take this all out let me just remove
all that real quick run it again there
we go now we're back to our original
figure and let's look at some of the
other things you can do with this one of
the things we can do is we can set a
title for the axis so axis set title
you'll see right here since I put this
on the a it's the main title for the
whole
graph and if you're going to have a
title you should also label so we can
label our X label and we can set our y
label in this case we're just going to
call it X and Y keep it nice and uniform
and if we run this you'll see that we've
added a nice X label and Y label whoops
where' they go and it turns out in this
environment that you have to put it
before the title so let me go ahead and
put it before the title and there's our
XY and let we run that and of course we
can also do up our size a little bit so
you can see what's going on on a little
better so here we have X label X if you
come down here you'll see our X label
and our y label we can of course change
this to X label we can change this to
Y be whatever you want on here of course
and our title graph there we go run so
here we have our title graph our y Lael
and our X label all set up on our nice
little plot and then before we move on
to the next section let's do one more
thing on here we have a thing called The
Legend and we're going to do we're going
to set our ax Legend label one label two
up here that's the format for it but
let's go down here and actually use it
I'm going to do two different plots
we're going to have axis plot X by x *
X2 and X cubed and if I run this you'll
see it puts two nice graphs on the setup
on there but it's nice to have a legend
telling you what's going on so for the
legend we can actually do axes since we
have the two plots Legend and on here
we've created an array and we have y =
x^2 y = x Cub you can actually put this
as whatever you want those are just
strings and then location two and let's
go ahead and run this and see what that
looks like and you can see it puts a
nice Legend on the upper leftand corner
location two we can do location three
and run it and it drops it down to the
bottom location one can't remember where
that's at there we go upper right so
each one of these is a number that
refers to the different locations on the
screen zero kind of have to play with
them or look them up to remember where
they're at but they do work it just kind
of moves around depending on where you
want your Legend at on there so on this
section we cover the title of the graph
the Y labels and legends this is we're
getting into some starting to look
really fancy here so we now have
something we can actually put out you'll
see the title of the graph looks a
little fuzzy so I might in a web setup
put the DPI up a couple notches maybe
put it at 200 100 might work fine just
so you know that's something to notice
on here when you're playing with these
different things we had our subplots DPI
equals oh let's do 200 just see what
that looks like so you can see now it's
a lot clearer it's also larger so it's a
nice little feature you can throw in
there with your DPI dots per inch so the
next section is let's look at some graph
features we're going to look at line
color transparency size and a few more
things on here and oops I forgot the
main title so we have our figure and our
axes equals our plot and subplots and
I'm go ahead and do a DPI equals 150 so
the graph comes out nice and large and
easy for you to see and let's go ahead
do three plots on here we'll do X by X+
one so it's just going to be a straight
line plot X+ x + 2 and axis. plot XX + 3
this looks like we're doing nearest
neighbor setup or showing how it uh
located data putting your lines on there
between the nearest neighbor there we go
so it draws a nice little graph with
three lines on it one of the things we
can do is we can control the alpha on
this oops and you can actually see the
um when they did these lines it
automatically pulls in different colors
for your setup so there's some automatic
automatic things going on in there and a
lot of times we do that comma R but
we're going to do color equals red
another notation on here let's go ahead
and run this now we have a bright red
line down there and with the map plot
Library you're not limited to Red you
can also use the one of many different
color references as you see here with
pound sign 1155 DD which just is just
blue and we can do the same thing with
another color on here which is turns out
to be green and I can just as easily do
this green blue oops there we go blue
and run that and you'll see here we have
red blue and green and what I want to do
is I want to make this we're going to
set What's called the alpha on this and
we're going to set this equal to
0.5 so this is halfway see- through when
I run this and it's almost going to look
pink because you can see through it and
let's change this just a little bit just
to make this kind of fun let's Square it
there we go run it so now we have this
nice square that comes up and you can
see when it crosses it because I plotted
these two lines after it and they have
no Alpha the red is behind those lines
or in this case pink because we did the
alpha halfway through so let's go ahead
and do this Alpha equals
0.5 and oh you know what instead of
squaring it let's take it to the 0. five
power that'll be kind of interesting to
see what that does we'll just go to keep
it squared there we go and run that and
let's go back and look at this where it
crosses over and the first thing you'll
see right here is on the blue is kind of
light blue now you can see how the two
colors add together you get almost a
purple on there so I can clearly see
where the Red Cross is the blue line and
then the green just blanks it over
because I didn't do any opaqueness no
Alpha on there so this is great if you
have lots of data that crosses over and
you need to be able to track those lines
better we'll go ahead and do this 0.
five and we'll run that oops I did uh
equals 05 let me go ahead and run that
and so you can see right here now you
can easily see the red line how crosses
the green and the blue down here and if
we want to we can do this as uh the
default is one is solid so we can change
this all to point8 let me just do that
oops 588 there we go run oops I must
have hit a wrong button there let me try
that again I accidentally get rid of a
bracket and let's go ahead and run that
and we come down here and look at this
you can still see where it passes behind
them but the green dominates and the
blue dominates because we're now at 80%
instead of 50% and you can do less
that's kind of fun although at some
point the lengths kind of
fade so 0 five is usually the best
setting on there we have a nice pastel
here at three and you can easily see
where they cross over and just like you
can play with the colors we can play
with line width and you know Let's do
let's try DPI 100 and see what that
looks like on my screen equals 100 and
we'll go ahead and just take our ax
plot let's do four of these lines just
so you can see how they look next to
each
other real quick here there we go and if
I run this they should all appear the
same it automatically does different
colors on there so let's do color equals
blue forgot my quotation marks there we
go and we'll go ahead and just make
these all blue just for purposes of
being nice and uniform and then what I
want to do is I want to do the line
width line width equals
0.25 and let's just copy and paste that
down
here let's do equals
[Music]
one about
1.5 and let's do one just make this
equal to two let's see what that looks
like and we do that you can see it goes
from a very thin line a 0.5 a one our
1.5 and two which is twice the width of
the one and if we're going to do
different sizes we had different colors
we had our Alpha scheme let's take this
whole thing here let's paste it down
here and do another one but instead of
line width let's look at Styles and
something to know here you can actually
abbreviate this with LW so line width
can also be Point let's just do
everything .2 and let's set up a line
style we'll do the first one dashes and
let me just paste that down here so I'm
not doing a lot of extra typing there we
go take this out so we have our dashed
we can do a dash dot let's do the dash
dot here and a colon here there we go
and there's a lot of different options
we'll look at a few more as we go down
for different ways of highlighting data
but when you look at this we have
everything is a line width of two and
now we have a straight line we have a
dash line or a dot dash and a dot dot
dotline and then another thing we can
add on here is we're going to do here's
our ax plot and we did X let's do X Plus
um four so goes right on the top going
do color black line width 1.5 so it's a
smaller line and we're going to take the
line and we're going to set dashes so
look how I've changed some of the
notation here for my line and my ax plot
so I can set my line comma equal to ax
plot and then I can change the line
settings this way and when I run this me
run that on here you'll see the 5 10 15
10 creates a series of dashes that are
varied in Link Link in this case they
alternate between a short Dash and a
long Dash we can play with these numbers
curiosity always has me what happens
when you play with the numbers just to
see what they look like let's do this
let's paste this down here I'll do two
of these just because they're kind of
fun to play with and let's change this
from 10 to three and we're going to
change this one from 15 to 4 and let's
run that and you can see the differences
in the lines oops very a little bit
confusing on there because I forgot to
change the lines are all on top of each
other so let me change that really quick
here and let's run that and now you can
see here's our original dash line
alternating when I change these numbers
on the second one the very end value to
three you can see now we have dashes of
five let's see I'm gonna guess this is a
dashes a five skip 10 dashes a 15 skip
three and then it goes back to the
beginning dashes five dashes skip 10 15
dashes skip three and of course the last
one we just switched up a little bit it
looks a lot more uniform because I'm
using two sets of 10 or if I did
something like this and changed it to 30
it really becomes pronounced as far as
the distances between them and instead
of four let's go oh let's put 30 here
also 30 by 30 there we go really
pronounce on that one and let's look at
one more important group for plotting
our data and in this we're going to
here's our plot we started with with the
X+ One X plus 2 X plus 3 and did it in
Blue on this one three or four different
blue lines and this property we want to
add the actual plots so you can see
where the plots are on the graph and for
that we might have marker equals o and
if we run this you'll see it puts a DOT
for each of these and there's 25 dots
because we have 25 x values so we have
actually have zero and each of the
different values of X Y are then plotted
here with the dots and we don't want to
just limit ourselves to
dots can also do plus sign that's
another option dots is most common I
actually like the dots the best if we do
the plus sign you see it puts a nice
crosshairs or plus sign on there and we
can do a marker there's a number of
different markers you can use and I
think this one was it s is another one
which is a nice square and that's
actually a good one s for square o for
period okay that's just kind of weird so
you can see that probably on these
markers another one is uh number one so
if we run that you'll see we now have
these little hatch marks and let's take
oh let's just go with the o on this
one by the way this works with square
really nicely some of the stuff we're
going to do here on just a second let's
do
Marker size equals 2 and change that to
five and run that and you can see here
it puts a nice little tiny dot versus uh
the size dot here and this is
interesting because it said two I
thought it would be bigger but if you do
0. five gets even smaller and let's just
do 10 to see what that looks like run
that looks huge so marker size a lot of
these are dependent on the DPI and the
setups there's things that switch around
as far as the way the size shows up you
got to be a little careful when you
change one setting it can change all the
other markers and then let's take our um
Square on here and we'll do we had
marker size we also have marker Bas
we'll set that equal to Red of course we
got made change the so it's up one notch
we'll run that whoops must have mistyped
something on here and I did is marker
face color equals red and so when I run
that you can now see I have the squares
on there with the marker face color of
course we can mix and match these come
down here and we'll make this instead of
Let's Make This plus seven and we'll
make this
size
15 marker face color equals and we'll do
what green just because there we go run
very hard to actually see what's going
on there still 25 dots they kind of
overlap as you can see they print them
over each other and of course if we
really wanted to make it look horrible
we could just make that really huge
generally though you want something a
little bit smaller and cuter we'll just
try doing it this way there we go that's
too small to even see the face so four
you start to see the face on there
around four maybe on eight eight might
be a good number for this there we go
eight again that all just depends on
what you're trying to show on display so
we've covered a lot of stuff here as far
as our lines we've covered opaque with
our Alpha setting on there give us some
nice pastels you can see how they
overlap and how they cross over we
covered the line width different size on
there different formats for the line
itself and these are all you can combine
all these so we can have our line width
equals two line style equals Dash you
can bring this down here also to the
markers and then we added markers in
just entered the circle a plus sign the
square a little tick which uses a one
then we add a marker size and a marker
color face and we combine those you see
we get a nice different series of
representations we also briefly
mentioned color where you didn't have to
use like in here we used color black
someplace up here i' have to find it we
Ed the actual number for the color as
opposed to I changed them to red and
blue so you can get very precise on the
color if you have very specific color
set that you need to match your website
or whatever you're working on all those
are tools in the map plot Library so we
have one more piece to formatting the
graphs so we want to show you and then
we have two big sections we're going to
go over the different graphs that they
have along with a challenge problem so
let's go in the last section we're going
to look at is limits we're going to
limit our data so as first I'm just
going to paste in there we're going to
create our subplots one two so one row
two columns we're going to do a figure
size of 10 comma five this should all
look familiar now since we've done a
number of them and we're going to go
ahead and plot and this is an
interesting notation you should notice
here our axis zero so one we've used
instead of you can just iterate through
them but they're just an array so it's
an array of zero is still the axes of
the first axies out of two and we're
going to plot X
x x x cubed line width two so we're
going to go ahe and just plot two graphs
right on top of each other without doing
multiple plots on here and we'll set the
grid equal to true on here let's go
ahead and run that and you can see here
are two plots with the x value going
across and I'm going to do something
similar and by the ways you can just if
you look at it you can see the Grid on
there that's all that is easier to spot
the data going across we're going to
take the same data for axis one so we
have our plot of X X2 X x and x cubed
line width two and this time we're going
to take our axis one and do y limit it's
actually setcor y limit this is the y
axis so it's going to be an array of two
two values and we'll do 0 comma 60 I'm
just making these numbers up the guys in
the back actually made them up I'm just
using their numbers and we're going to
set the x
limit and we'll set the x limit as um
don't forget our brackets there 2 comma
5 so it's the same data going in and but
we're setting a limit on it let's go
ahead and run that and let's see what it
comes out of and here we have the Y
limit 0 to 60 so we're looking at just
the lower part of this curve here up to
here and we have the X limit 2 to five
so that starts right here at two and you
can see very different graphs this is
kind of nice because you could actually
put one of these on top of the other if
you wanted to draw Focus to one part of
a graph remember how we did that earlier
one inside the other there but just a
quick note you can easily limit your
graph and re kind of reshape the way it
looks quite easily and we can also add
that grid down there if you want a
grid we'll run that and add the Grid in
there oops I guess you have to do the
grid
beforehand switch that there we go
sometimes the order on this is really
important so you may double check your
order when you're printing these things
out it also helps if I change it to one
so in this case might not be the order I
wonder if it'll go back here as one
there we go so doesn't matter the order
and grid but you can set the grid for
easy viewing here nice setup on there
but you can see how we can limit the
data so let's start looking at some
other 2D graphs and make this cell a
markdown so we run it has a nice pretty
title to it and let's go ahead and
create some data with an npay we'll do
zero to five on here there we go and
let's look at uh four common graphs
we'll put them side by side so we'll do
a figure our axis equals plot subplots
one four columns and then figure size
hopefully it'll fit nicely on here it
seems to do pretty good on here and I'll
go and just run that since we're in
there run you'll see I have four blank
plots on here and we'll start with axes
of zero let's set
title and we want this to be a scatter
plot a scatter plot just means it has a
bunch of dots on it so here's our axes
of zero Dot scatter easy to remember
scatter a bunch of plots on there we'll
do our n or we could do X or n there we
go and let's go ahead and do axis set
title scatter oh I already did that
we're just going to do
scatter that's how you do it on there
that is how you create a scatter plot
simply with the scatter control and
we'll do let's do the variable X X Plus
let's throw some Randomness in here
usually Scatter Plots are um have a lot
of random numbers connected to them
that's why they do them on there and so
the bigger the X gets the bigger the
randomness so 0.25 times the randomness
and what we should end up down here is
with the scatter plot and you can see as
you go up it just kind of has some
random numbers and moves up and down the
line but it plots just the points so if
you remember from back up here where we
did marker this is plotting basically
just the marker so it's a scatter plot
probably less use is a step plot so for
X's one we'll go ahead and do a step
plot so you can see what that looks like
and this time we use our n value instead
of X we generated that n value up here
and so for this we have n n * 2 our n
squar n * 2 N squ line width equals two
and if we run that it creates a nice
step
up let's see so we've got a scatter plot
we've got a steps plot let's do a bar
plot and we'll use the same formula N N
squared alignment centered because you
can have them to left or right withth
0.5 and Alpha Al if you remember
correctly that's how opaque it is let's
see what that looks like on there so we
have some nice you can see here a nice
bar plot it should look very similar to
the step plot but colored in and we can
change the width let's see what happens
we do 0.9 run and if we take width out
completely run that you can see it
starts coming together on there and we
can change the alpha we take the alpha
out too and run that see now you have
the solid colors and if we take out the
center and run that everything you
really can't see the shift on here CU
that's actually the default on this but
these are common settings for the bar
graph let me just put them back in there
there we go alignment center and Alpha
now I can't say I've used the step graph
very much there certain there certain I
guess domains of expertise require a
step graph but the scatter plot and the
bar graph very common especially the bar
graph and we'll look at histograms here
in just a minute so I use histograms a
lot especially in data science but this
is nice if you very concrete objects
somebody how many people are wearing
yellow hats that kind of thing but if
we're going to do that let's go ahead
and do the last one which I I see a lot
more in the Sciences certainly using the
data science but more like for mapping I
saw publication on uh solar flares and
they were discussing the energy and so
filling in the graph gives it a very
different look so we're going to do the
fill between it's just like you think
it'd be it's fill between but with a
underscore between them and we'll do x
and x SAR and x and x cubed and we'll do
color green and Alpha again in case you
had other data you wanted to plot on
there you can see it forms a nice
squared coming up here and also if you
look at the bottom one is your squared
value the upper line is your cubed value
and then it fills in everything in
between if you remember from calculus
this would be if you had like a car
motor and efficiency they would talk
about the efficiency going up and the
loss and you're looking for the space or
the area between the two lines so it
gives you a nice visual of that and
let's look at a few more basic two-
dimensionals so we're going to have our
figure figure size on here we're going
to do a radar chart to be honest I've
never used a radar chart in business or
in data science I have to find a reason
to use one now so the first line for
doing a radar chart we have to add axis
on the figure and with this this
actually creates our oh let's let's run
it so you can see what it creates it
creates a nice looks like you're on a
submarine and you're tracking The Hunt
for Red October or something like that
and it needs all of these the polar is a
fact that we're doing polar coordinates
0. 6.6 has to do with the size if you
take out any of these things and run
them you get just a box if you take out
the other half you pretty much get
nothing in there and if you change these
numbers change them a little bit you can
see it gets bigger they had 6 on here
I'll go ahead and leave it as one
because that's just kind of fun that's
all about the size on here the height
and the width and then let's create some
data T equals NP line space and this is0
to 2 * NP * pi so if you remember that
is the um distance across and we're
going to generate 100 points so this is
just a thing of data we're putting
together then we can simply do an ax.
plot and in this case let's do T comma T
which would be a diagonal line on a
regular chart and we'll give it a nice
color equal
blue and line width equals 3 Let's see
what that looks like and we can see here
a spiral coming out remember this would
be just a diagonal line on a regular
chart what happens if we take this and
instead of T time5 there we go and you
can see it slightly Alters the way it
spirals out we can do T * 2 spirals out
a little quicker so it's kind of just a
fun I've like I said I've never used a
uh radar chart it's a column but you can
always think of radar and a submarine
kind of looks like one of those or in an
airplane and none of this would be
complete if we didn't discuss histograms
oh my gosh do I use a histogram so much
and we'll use our numpy that we have set
as NP to generate oh looks like we have
100,000 variables we're going to set
equal to n and of course we create our
figure and our axis subplots one two
figure size 12 14 so we're going to look
at two different variations of the
histogram and we'll set a title default
histogram set our title there and then
this is simply hist for histogram and
we'll just go ahead and put in our n in
there and let me run this you can see
what that looks like and let's talk
about what is going on here so we
generated an array here of data 1,00
random arrays it looks like they're
mostly between minus 4 and four and then
it adds up each one it says zero you
have 35,000 that are zero so that's
what's most common on here and we have
20,000 that are somewhere in this range
right here between the minus two and
well it looks like one and minus two and
somewhere between 0 and one there's
30,000 numbers so all this is saying is
this is how common these variables are
and this gives you this will point you
in so many directions when you're
looking at data science to go ahead and
run your histogram so you should always
have your histogram and you can always
put limits and all the other different
things on your array just like you did
on the other graphs on there and then
we're going to do a cumulative detailed
histogram and all is is a histogram me
just do that and we set cumulative equal
to
true and bins equal 50 and I really want
to highlight the um the cumulative
equals true is important but we can now
choose how many bends we have in the
first one it kind of selected them for
us in this case let me go ahead and run
this and you'll see it has the prce of
data out for us and here's our whoops
must have missed oh there we go doesn't
help that I put it over the old one
there we go okay so now you have your
default histogram and now we have a
cumulative histogram and we should have
50 steps in there and let's just find
out if that's true not So Much by
counting them I'm not going to count
them if you want to you can count them
let's just change it to 10 and see what
happens and we see here we have now 10
counts of that and we could set that for
five and run that and then we have our
five on there and we go ahead and take
the cumulative equals true out just so
you can see what that looks like and let
me run that on here too that looks just
like it did before I think there's what
1 2 3 4 five 6 seven eight they have
eight different bins on here is what the
default came out of put that back in
there run and so now it should look
almost identical and it does and then we
can put the cumulative back in see what
that looks like with the
cumulative and run
that and we can see how that shifts
everything over and has a slightly
different look wait it shifts it all to
the right no it doesn't actually shift
it to the right it's cumulative so it's
a total of the different occurrences and
so with that means is like if you
consider this like for the year of
rainfall we have like day one you had a
little bit of rain day two we have more
rain and so if you look at the number
this is 100,000 35,000 so it's a
cumulative detail the histogram the
currence as it grows and rainfall is a
good one cuz that would be a cumulative
histogram of how much rain occurred
throughout the year and we're going to
look at two more graphs we've already
looked at a bunch of them we looked at
our radar graph we've looked at scatter
step bar fill in basic plots we've
looked at different ways of showing the
data you know we can increase the size
of the line the look the color the alpha
setting so let's look at contour maps
let put that in there there we go draw a
contour map and before we draw a contour
map we need to go ahead and create data
for it and if you have Contours your
data is all going to have three
different values
so let's go ahead and create the data
here we have our uh you'd import your
map plot Library your numpce so we have
our numbers array and we'll import map
plot. cm and that's your color Maps so
you have all these different color maps
you can look at there's like hundreds of
color Maps so if you don't want to do
your own color you can even do your own
color map they're pretty diverse and of
course our PLT we're going to our P plot
and to generate our different data we're
going to create a Delta 0.025
and we'll start with X we're going to
create an array between minus 3 and 3
and Delta increments of
025 and we'll have our y we'll do
something similar and then we'll create
our XY into a mesh grid again these are
all numpy commands so if you're not
familiar with these you'll want to go
back and review our numpy tutorial and
we'll do an exponential on here - x^2 -
y^2 for our Z1 we'll do a Z2 so we have
two different areas and Z equals Z1
minus Z2 * 2 so we've created a number
of values here and let me go ahead and
run this and let's plug that in so you
can see where those values are going so
once we've set these we're going to
create our figure and our X from our PLT
subplots we're going to create the
variable Cs and this is going to be our
Contour so right here CS is our Contour
surface and we're feeding it X Y and Z
and if you remember XY we created as our
X and Y components using our mesh grid
and you know what let's do this just
because it's kind of good to see this
let's go ahead and print X and let's
print Y and I always like to do this
when I'm working with something that's
either is really complicated in this
case is what we're looking at or you
don't understand yet so we've created a
mesh grid we have XY and when we're done
with this we end up with here's our X
and this set of values and our y so
those our X and Y coordinates and then
we've also created Z based on our X and
Y so we have X capital x capital Y and
capital Z is our three components X and
Y being the coordinates well Z is going
to be our actual height since we're
doing a contour map so we Creed our
contour map from our X Y and Z
coordinates we want to go ahead and put
in a c label maybe we want to go ahead
and do an title on
here we'll put that in our set title and
this is a contour there we go contour
map and let's go ahead and run this and
see what that looks like and you'll see
we generated a nice little contour map
there's different settings you can play
with on this but you can picture this
being you're on a mountain climb and
here we have a line that's represent
zero maybe that sea level and then
moving on up you have your Contours of
05 and then minus one and different
setups little Hills I guess if it's
minus that's like a pit so I guess
you're going down into a pit at minus5
and minus one when on the other side you
can see you're going up in levels so
here's a mountain top and here's like a
basin of some kind
and in data science this could represent
a lot of things this could also be
representing two different values and
maybe profits and loss I don't know if
I'd ever really do that as a contour map
but I'm sure you can be creative and
find something fun to do with a contour
map and then we're going to look at one
last map which is the 3D map and those
are can be really important as a final
product because they can show so much
additional information that you can't
fit on two-dimensional
graphs there we go draw a 3D image and
so we're going to import from our um MPL
toolkits the Imp plot 3D and the axis 3D
we're going to import axis 3D this is
what's going to let us work with the 3D
image and this should look familiar
we're going to create another figure
just like we did before figure size 14
by6 that's a good fit on the screen
we'll go ahead and run that so we have
our figure and let's go ahead and take
our X and we're going to set that equal
to fig. add subplot that should also be
familiar from earlier and we're going to
work with this sets the settings for the
projection and we're going to use one
two one projection 3D and we'll see what
that looks like in just a minute and we
just created some threedimensional data
here before where we had X Y and Z
capital X Y and Z so we're going to
reuse that data we're just going to use
that since this also this is also a
three-dimensional image so let's use
that for a threedimensional graph and we
simply do ax plot
underscore surface and our capital x
capital y capital Z so there 's our data
coming in and we're going to add some
settings in here we're going to do R
stride 4 C stride 4 and line width zero
and I'll show you what that is here in
just a minute let's go ahead and run
that so we can see our graph and of
course it helps if I don't add an extra
comma in there and you can see it
generates this really beautiful
three-dimensional graph so let's take a
little bit time to explore some of these
numbers we have going in here we have
the r stride four the C stride 4
and the projection 3D projection 3D is
the important one because that's telling
us that this is a 3D graph here so what
are these first numbers one two one
let's just change one of these I'm going
to change this to five and it's going to
give me an error let's change it to
one and O that didn't work let's change
this middle one to three instead and
you're going to see how it starts
reshaping the size and how it fits on
the screen and we'll change the first
one to two we we'll run that one and
again it's changed the dimensions and
the size and how it fits on here play
with these numbers so you get a nice
look and feel for it part of it is the
Tilt and the angle I'll do seven on this
one there we go you can see it really
shifted it there but again that changes
the size and outfits on the canvas but
we'll leave it at the one two and just
so you get a good look at what that
we're talking about here this is column
width and index from before if we do one
one you can see that it now spreads it
out all the way across uses the whole
setup on there so this has to do with
the size and how big you want it to be
now there's one term that we didn't
cover in this yet but we've used it
throughout the whole setup and I'm just
going to type that down here even though
we're not going to go into detail and
that's the term heat map you might see
that is kind of starting to lose ground
as far as a common reference but there
sure are a lot of people still talk
about heat Maps what is a heat map well
it is simply a color map that's all it
is so if you ever see the term heat map
that refers to the fact this is in
different colors representing different
heights that one is in a heat map but
you can see up here we switched into let
me go back up here here we go this one
has different colors for the different
values a lot of times you'll use like
instead of X and Y you might do a heat
map where you have a fourth value and
the fourth value represents the color
and so you'll see this 3D image in a
nice colors represented by heat map
that's all it is so if you see the term
heat map that only means we're plotting
some of the data in color to make it
stand out or to give it a fourth
dimension in this case so we've covered
a lot of things on map plot and that
brings us covered all the basics so that
brings us to practice example and this
is going to be the challenge for you and
let me go ahead and change our cell cell
type markdown and run that so it looks
pretty practice example write a p Python
program to create a pie chart of the
popularity of programming
languages okay excellent and if you're
going to have a challenge we need some
data and I'll just throw in our import
our map plot library at the beginning
you should do that automatically and so
for our data to plot we're going to have
our languages we're going to have python
we're going to have Java PHP JavaScript
C C++ so those are six categories and
then we have our popularity oops
misspelling there popularity we'll give
the first one 22.2 2% Java
17.6 and I don't know if these are real
numbers they pulled my guess is that
they might have just been made up
because I don't know if Python's really
that much more popular than the other
ones maybe specific to data science
because Python's very popular in data
science right now because it has so many
options the only other program that
that's highly used in exclusively for
data science is R so Python's big and
python also does a lot more it's a full
programming language where R is
primarily for data science science they
didn't even put our in here so we have
python we have Java we have our PHP and
you can see the different values they've
given it or different
percentages and I did add these up does
not add up to 100% it adds up to 71% or
something like that and then we're going
to give colors and we've chosen these
guys in the back brought in these colors
I'm not sure what these colors are we'll
find out in a minute so I'll be exciting
but you can see they're using the actual
color values you can pull off of a color
wheel or something like that you could
have just as easily done blue blue red
green if you're too lazy to pick the
exact colors and then let's go ahead and
solve this and see what we got here
we're going to do something a little
fancy just because we can the first
thing we're going to do is we're going
to use variable called explode and
you'll notice that there's six variables
in here so that matches our six
different categories and the first one
we've done is 0.1 and then
0000000000 0.1 when we put this in here
under the explode in the plot it will
actually push that Square out so it's a
really cool feature to highlight certain
information on a pie chart and this is
simply PLT dopy and we're
plotting popularity there we go and
before we add in all the really cool
settings for this let's go ahead and run
it and you'll see we generate a nice
flat pie not too exciting there and then
we'll go ahead and put in all the extras
I talked about explode where we can
explode one of the values out so here's
our explode equals explode labels as
languages cuz we want to know what the
different colors mean here's our colors
equals colors our Auto picture and this
is standard print format so that's a
python setup on there and that's just
going to put the value on the piie slice
and then we're going to add Shadow cuz
it just looks cooler with a shadow gives
a little 3D look and we'll do a start
angle of 140 let's go ahead and run this
and take a look and see what comes out
of that and look how that changes the
whole setup so here's our labels there's
our value we put on there there's our
slice that's pushed out there's our
shadow a 3d effect and then we started
at 140 we could also rotate this let's
just do this angle 90 and if we run it
you'll see the blue pie slice has moved
up a little bit we could actually do
actually let's just take the whole
starting triangle out and run it'll
default to zero this is what it looks
like if it defaulted to zero so
depending on where you want the
highlighted slice to appear usually you
want that to appear on the left because
people read left or right and so it
draws a focus onto in this case Python
and how great python is I'm a little
biased we're teaching a Python tutorial
so it should be understandable that
we're looking at Python and one last
reference before we close you can go
over to the map plot library. pyplot
setup and if you go underneath their the
different functions on there you can
look this up on their website you'll see
a full list and this this is why it's so
important to go through a tutorial like
this because this list is just so
massive I'm trying to figure out like
here's our bar plot there's a bar H you
can add barbs there's a box plot we
didn't cover C labels a totally
different kind of for your Contour plot
you can set up in there if you go down
here we have our figures we used on
there we showed you the basics of how to
do the figure you'll see some closer
references on those there's a histogram
down here H there's also the his 2D
makes a 2d histogram plot H lines all of
this these are all the different
commands that are underneath of here and
you can see it's pretty extensive we've
covered all the basic ones so that you
know have a solid ground to look at
these different options so when you come
to these functions some of them are
going to look a little off or not off
will look unfamiliar but you'll still
have the availability to probably
understand most of this and have a basic
understanding of your map plot Library
so what is the site kit learn it's
simple and efficient tool for data
mining and data analysis it's built on
numpy scipi and matplot Library so it
interfaces very well with these other
modules and it's an open-source
commercially usable BSD license BSD
originally stood for Burley software
distribution license but it means it's
open source with very few restrictions
as far as what you can do with it
another reason to really like the S kit
learn setup so you don't have to pay for
it as a commercial license versus many
other copyrighted platforms forms out
there what we can achieve using the pyit
learn we use class the two main things
are classification and regression models
classification identifying which
category an object belongs to for one
application very commonly used is Spam
detection so is it a Spam or is it not a
Spam yes no in banking you might be is
this a good loan bad loan today we'll be
looking at wine is it going to be a good
wine or a bad wine and regression is
predicting an attribute associated with
an object one example is stock price is
predicted
what is going to be the next value if
the stock today sold for
$235 a share what do you think it's
going to sell for tomorrow and the next
day and the next day so that'd be a
regression model same thing with weather
weather forecasting any of these are
regression models where we're looking at
one specific prediction on one attribute
today we will be doing classification
like I said we're going to be looking at
whether a Wine's good or bad but
certainly the regression model which is
in many cases more useful because you're
looking for an actual value is also a
little harder to find follow sometimes
so classification is a really good place
to start we can also do clustering and
model selection clustering is taking an
automatic grouping of similar objects
into sets customer segmentation is an
example so we have these customers like
this they'll probably also like this or
if you like this particular kind of uh
features on your objects maybe you'll
like these other objects so it's a
referral is a good one especially in
amazon.com or any of your shopping
networks model selection comparing
validating and choosing parameters and
models now this is actually a little bit
deeper as far as a site kit learn we're
looking at different models for
predicting the right course or the best
course or what's the best solution today
like I said we're looking at wine so
it's going to be well how do you get the
best wine out of this so we can compare
different models and we'll look a little
bit at that and improve the model's
accuracy via different parameters and f-
tuning now this is only part one so
we're not going to do too much tuning on
the models we're looking at but I'll
Point them out as we go two other
features dimensionality reduction and
pre-processing dimensionality reduction
is we're reducing the number of random
variables to consider this increases the
model efficiency we won't touch that in
today's tutorial but be aware if you
have you know a thousands of columns of
data coming in a thousands of features
some of those are going to be duplicated
or some of them you can combine to form
a new column and by reducing all those
different features into a smaller amount
you can have a you can increase the
efficiency of your model it can process
faster and in some cases you'll be less
biased because if you're weighing it on
the same feature over and over again
it's going to be biased to that feature
and pre-processing these are both
pre-processing but pre-processing is
feature extraction and normalization so
we're going to be transforming input
data such as text for use with machine
learning algorithms we'll be doing a
simple scaling in this one for our
pre-processing and I'll point that out
when we get to that and we can discuss
pre-processing at that point with that
let's go ahead and roll up our sleeves
and dive in and see what we got here now
I like to use the Jupiter notebook and
and I use it out of the Anaconda
Navigator so if you install the Anaconda
Navigator by default it will come with
the Jupiter notebook or you can install
the jupyter notebook by itself this code
will work in any of your python setups I
believe I'm running an environment of
3.7 setup on there i' have to go in here
environments and look it up for the
python setup but it's one of the 3xs and
we go and launch this and this will open
it up in a web browser so it's kind of
nice it keeps everything separate and in
this Anaconda you can actually have
different environments different
versions of python different modules
installed in each environment so it's a
very powerful tool if you're doing a lot
of development and the jupyter notebook
is just a wonderful visual display
certainly you can use I know spider is
another one which is installed with the
Anaconda I actually use a simple notepad
Plus+ when I'm doing some of my python
script any of your idees will work fine
jupyter notebook is iron python cuz it's
designed for the interface but it's good
to be aware of these different tools and
when I launch the Jupiter notebook it'll
open up like I said a web page in here
and we'll go over here to New and create
a new python setup like I said I believe
this is python 37 but any of the three
this the um site kit learn works with
any of the 3 XS there's even 27 versions
so it's been around a long time so it's
very big on the development side and
then the uh guys in the back guys and
gals developed they went ahead and put
this together for me and let's go ahead
and import our different packages now if
you've been reading some of our other
tutorials you'll recognize pandas as PD
pandas library is pretty widely used
it's a data frame setup so it's just
like columns in rows in a spreadsheet
with a lot of different features for
looking stuff up Seaborn sits on top of
matap plot Library this is for a
graphing we'll see that how quick it is
to throw a graph out there to view in
the Jupiter notebook for demos and
showing people what's going on and then
we're going to use the random Forest the
SVC or support vector classifier and
also the neural network so we're going
to look at this we're actually going to
go through and look at three different
classifiers that are most common some of
the most common classifiers and we'll
show how those work in the site kit
learn setup and how they're different
and then if you're going to do your um
setup on here you'll want to go ahead
and import some metrics so the SK learn.
metrics on here and we're use the
confusion metrics and the classification
report out of that and then we're going
to use from the SK learn pre-processing
the standard scaler and label encoder
standard scaler is probably the most
commonly used pre-processing there's a
lot of different pre-processing packages
in the sklearn and then model selection
for splitting our data up it's one of
the many ways we can split data into
different sections and the last line
here is our percentage matap plot
library in line some of the caborn and
matap plot Library will go ahead and
display perfectly in line without this
and some won't it's good to always
include this when you're in the Jupiter
notebook this is Jupiter notebook so if
you're in ide when you run this it will
actually open up a new window and
display the graphics that way so you
only need this if you're running it in a
editor like this one with the
specifically Jupiter notebook I'm not
even familiar with other editors that
are like this but I'm sure they're out
there I'm sure there's a Firefox version
or something jupyter notebook just
happens to be the most widely used out
there and we can go ahead and hit the
Run button and this now has saved all
this underneath the packages so my
packages are now all loaded I've run
them whether you run it on top or you
run it to left and the packages are up
there so we now have them all available
to us for our project we're working on
and I'm just going to make a little side
note on that when you're playing with
these and you delete something out and
add something in even if I went back and
deleted this cell and just hit the
scissors up here these are still loaded
in this kernel so until I go under
kernel and restart or restart and clear
or restart and run all I'll still have
access to pandas uh important to know
because I've done that before I've
loaded up maybe not a module here but
I've loaded up my own code and then
changed my mind and wondering why does
it keep putting out the wrong output and
then I realize it's still loaded in the
kernel and you have to restart the
kernel just a quick side note for
working with a Jupiter notebook and one
of the troubleshooting things that comes
up and we're going to go a and load up
our data set we're using the pandas so
if you haven't yet go look at our pandas
tutorial a simple read the CSV with the
separation on here and so let me go
ahead and run that and that's now loaded
into the variable wine and let's take a
quick look at the actual file I always
like to look at the actual data I'm
working with in this case we have wine
quality-- red I'll just open that up I
have it in my open Office setup
separated by semicolons that's important
to
notice and we open that up you'll see we
have go all the way down here looks like
1,600 lines of data minus the first one
so 15 1,599 lines and we have a number
of features going across the last one is
quality and right off the bat we see the
quality is uh has different numbers in
it 5 six 7 it's not really I'm not sure
how how high of a level it goes but I
don't see anything over a seven so it's
kind of five through seven is what I see
here 5 six and seven four five six and
seven looking to see if there's any
other values in there looking through
the demo to begin with I didn't realize
the setup on this so you can see there's
a different quality values in there
alcohol sulfates pH density total Sul
sulfur dioxide and so on those are all
the features we're going to be looking
at and since this is a pandas we'll just
do wine head and that prints the first
five rows rows of data that's of course
Panda's command and we can see that
looks uh very similar to what we're
looking at before we have everything
across here it's automatically assigned
an index on the left that's what pandas
does if you don't give it an index and
for the column names it has assigned the
first row so we have our first row of
data pulled off the our comma separated
variable file in this case semicolon
separated and it shows the different
features going across and we have what 1
2 3 4 4 5 6 7 8 9 10 11 features 12
including quality but that's the one we
want to work on and understand and then
because we're in uh Panda's data frame
we can also do w.info and let's go ahead
and run that this tells us a lot about
our variables we're working with you'll
see here that there is
1,599 that's what I said from the
spreadsheet so that looks correct non
null float 64 this is very important
information especially the non-null so
there's no null values in here that can
really trip us up in pre-processing and
there's a number of ways to process
non-null values one is just to delete
that data out of there so if you have
enough data in there you might just
delete your non-null values another one
is to fill that information in with like
the average or the most common values or
other such means but we're not going to
have to worry about that but we'll look
at another way because we can also do
wine is null and sum it up and this will
give us a similar it won't tell us that
these are float Val vales but it will
give us a summation o there we go let me
run that it'll give us a summation on
here how many null values in each one so
if you wanted to you know from here you
would be able to say okay this is a null
value but it doesn't tell you how many
are null values this one would clearly
tell you that you have maybe five null
values here two null values here and you
might just if you had only seven null
values and all that different data you'd
probably just delete them out where if
uh 90% of the data was null values you
might rethink either a different data
collection
setup or find a different way to deal
with the null values we'll talk about
that just a little bit in the models too
cuz the models themselves have some
built-in features uh especially the
forest model which we're going to look
at this point we need to make a choice
and to keep it simple we're going to do
a little pre-processing of the data and
we're going to create some bins and bins
we're going to do is two comma 6.5 comma
8 what this means is that we're going to
take those values if remember up here
let me just scroll back up here we had
our quality the quality comes out
between 2 and eight basically or 1 and
eight we have 5556 you can see just in
the just in the first five lines of
variation in quality we're going to
separate that into just two bins of
quality and so we've decided to create
two bins we have bad and good it's going
to be the labels on those two bins we
have a spread of 6.5 and an exact index
of eight the exact index is because
we're doing 0o to eight on there the 6.5
five we can change we could actually
make this smaller or greater but we're
only looking for the really good wi
we're not looking for the 012 3 four
five six we're looking for wines with
seven or eight on them so high quality
you know this is what I want to put on
my dinner table at
night I want to taste the good wine not
the semi good wine or mediocre wine and
then this is a panda so PD remember
stands for pandas pandas cut means we're
cutting out the wine quality and we're
replacing it and then we have our bins
equals bins that's the command bins is
the actual command and then our variable
bins 2 comma 6.58 so two different bins
and our labels bad and good and we can
also do uh let me just do it this way
wine quality since that's what we're
working on and let's look at unique
another pandas command and we'll run
this and I get this lovely error why did
I get an error well because I replaced
wine quality and I did this cut here
which changes things on here so I
literally altered one of the variables
is saved in the memory so we'll go up
here to the kernel restart and run all
it starts it from the very beginning and
we can see here that that fixes the
error because I'm not cutting something
that's already been cut we have our wine
quality unique and the wine quality
unique is bad or good so we have two
qualities objects bad is less than good
meaning bad's going to be zero and
Good's going to be one and to make that
happen we need to actually encode it so
we'll use the label quality equals label
encoder and the label encoder let me
just go back there since this is part of
K learn that was one of the things we
imported was a label encoder you can see
that right here from the SK learn.
processing import standard scaler which
we're going to use in a minute and label
encoder and that's what tells it to use
bad equals 0 and good equals 1 and we'll
go ahead and run that and then we need
to apply it to the data and when we do
that we take our wine quality that we
had before and we're going to set that
equal to label quality which is our
encoder and let's look at this line
right here we have do fit transform and
you'll see this in the pre-processing
these are the most common used is fit
transform and fit transform because
there's so often that you're also
transforming the data when you fit it
they just combined them into one command
and we're just going to take the wine
quality feed it back into there and put
that back in our wine quality setup and
run that and now when we do uh the wine
and the head of the first five values
and we go ahead and run this you can see
right here underneath quality 0000 0 I
have to go down a little further to look
at the better wines let's see if we have
some that are ones yeah there we go
there's some ones down here so when we
look at 10 of them you can see all the
way down to zero or one that's our
quality and again we're looking at high
quality we're looking at the seven and
the eights or 6.5 and up and let's go
ahead and grab our or was it here we go
wine quality and let's take another look
at what else more information about the
wine quality itself and we can do a
simple pandas thing value counts I typed
that in there correctly and we can see
that we only have two
27 of our wines which are going to be
the higher quality so 2117 and the rest
of them fall into the bad bucket the
zero which is uh 1382 so again we're
just looking for the top percentage of
these the top what is that it's probably
about a little little under 20% on there
so we're looking for our top wines our
seven and8 and let's use our uh let's
plot this on a graph so we take a look
at this and the SNS if you remember
correctly that is let me just go back to
the top that's our Seaborn Seaborn sits
on top of map plot Library it has a lot
of added features plus all the features
of the map plot library and it also
makes it quick and easy to put out a
graph and we'll do a simple bar graph
and then actually call it count plot and
then we want to just do count plot the
wine quality so let's put our wine
quality in there and let's go ahead and
run this and see what that looks like
and nice inline remember this is why we
did the inline so make sure it PE in
here and you can see the blue space or
the first space represents lowquality
wine and and our second bar is a high
quality line and you can see that we're
just looking at the top quality wine
here most of the wine we want to just
give it away to the neighbors no maybe
if you don't like your neighbors maybe
give them the good quality wine and I
don't know what you do with the bad
quality wine I guess use it for cooking
there we go but you can see here forms a
nice little graph for us with the
Seaborn on there and you can see our
setup on that so now we've looked at
we've done some pre-processing we've
described our data a little bit we have
a picture of how much of the wine what
we expect it to be high quality low
quality checked out the fact that
there's none we don't have any null
values to contend with or any odd values
some of the other things you sometimes
look at these is if you have like some
values that are just way off the chart
so the measurement might be off or
miscalibrated equipment if you're in the
scientific field so the next step we
want to go ahead and do is we want to go
ahead and separate our data set or
reformat our data set and we usually use
capital x and that denotes the features
we're working with and we usually use a
lowercase y that denotes what uh in this
case quality what we're looking looking
for and we can take this we can go wine
it's going to be our full thing of wine
dropping what are we dropping we're
dropping the quality so these are all
the features minus quality and make sure
we have our axis equals one if you left
it out it would still come out correctly
just because of the way it processes um
on the defaults and then our y if we're
going to remove quality for our X that's
just going to be one and it is just the
quality that we're looking at for y so
we put that in there and we'll go ahead
and run this so now we've separated the
features that we want to use to predict
the quality of the wine and the quality
itself the next step is if you're going
to um create a data set in a model we
got to know how good our model is so
we're going to split the data train and
test splitting data and this is one of
the packages we imported from sklearn
and the actual package was train test
split and we're going to do XY test size
point2 random State 42 and this returns
four variables and most common you'll
see is capital X train so we're going to
train our set with capital X test that's
the data we're going to keep on the side
to test it with Y train y remember
stands for the quality or the answer
we're looking for so when we train it
we're going to use x train and Y train
and then y test to see how good our X
test does and the train test split let
me just go back up to the top that was
part of the sklearn model selection
import train test split there is a lot
of ways to split data up this is when
you're first starting you do your first
model you probably start with the basics
on here you have one test for training
one for test our test size is 0.2 or 20%
and random State just means we just
start with a it's like a random seed
number so that's not too important back
there we're randomly selecting which
ones we're going to use since this is
the most common way this is what we're
going to use today there is and it's not
even an SK learn package yet so
someone's still putting it in there one
of the new things they do is they split
the data in thirds and then they'll run
the model on each of they combine each
of those thirds into 2/3 for training
and one for testing and so you actually
go through all the data and you come up
with three different test results from
it which is pretty cool that's a pretty
cool way of doing it you could actually
do that with this by just splitting this
into thirds and then or you know have a
test side one test set third and then
split the training set also into thirds
and also do that and get three different
data sets this works fine for most
projects especially when you're starting
out it works great so we have our X
train our X test our y train and our y
test and then we need to go ahead and do
the scaler and let's talk about this
because this is really important some
models do not need to have scaling going
on most models do and so we create our
scalar variable we'll call it SC
standard scaler and if you remember
correctly we imported that here wrong
with the label encoder the standard
scaler setup so there's our scaler and
this is going to conver convert the
values instead of having some values
that go from zero if you remember up
here we had some values are 54 60 40 59
102 so our total sulfur dioxide would
have these huge values coming into our
model and some models would look at that
and they'd become very biased to sulfur
dioxide it'd have the hugest impact and
then a value that had 0076 098 or
chlorides would have very little impact
because it's such a small number so when
we take the scaler we kind of level the
playing field and depending on our
scaler it sets it up between zero and
one a lot of times is what it does let's
go ahe and take a look at that and we'll
go ahead and start with our X train and
our X train equals SC fit transform we
talked about that earlier that's an
sklearn setup is going to both fit and
transform our X train into our X U train
variable and if we have an X train we
also need to do that to our test and
this is important because you need to
note that you don't want to refit the
data we want to use the same fit we used
on the training as on the testing
otherwise you get different results and
so we'll do just oops not fit transform
we're only going to transform the test
side of the data so here's our X test
that we want to transform and let's go
ahead and run that and just so we have
an idea let's go ahead and take and just
print out our X train oh let's do uh
first 10 variables very similar to the
way you do the head on data frame you
can see see here our variables are now
much more uniform and they've scaled
them to the same scale so they're
between certain numbers and with the
basic scaler you can fine-tune it I just
let it do its defaults on this and
that's fine for what we're doing in most
cases you don't really need to mess with
it too much it does look like it goes
between like minus probably Min - 2 to
two or something like that that's just
looking at the train variable we'll go
and cut that one out of there so before
we actually build the models and start
discussing ing the sklearn models we're
going to use we covered a lot of ground
here most of when you're working with
these models you put a lot of work into
pre prepping the data so we looked at
the data noticed that it's uh separated
loaded it up we went in there we found
out there's no null values that's hard
to say no no no values we have uh
there's none there's none no Val I can't
say it and of course we sum it up if you
had a lot of null values this would be
really important coming in here so is
there a null summary we looked at
pre-process the data as far as the
quality and we're looking at the bins so
this would be something you might start
playing with maybe you don't want super
fine wine you don't want the seven and
eights maybe you want to split this
differently so certainly you can play
with the bins and get different values
and make the bins smaller or lean more
towards the lower quality so you then
have like medium to high quality and we
went ahead and gave it labels again this
is all pandas we're doing in here
setting it up with unique labels and
group names bad good bad is less than
good could be so important you don't
know how many times people go through
these models and they have them reversed
or something and then they go back and
they're like why is this data not
looking correct so it's important to
remember what you're doing up here and
double check it and we used our label
encoder so that was um to set that up as
quality 01 good in this case we have uh
bad good 01 and we just double check
that to make sure that's what came up in
the quality there and then we threw it
into a graph because people like to see
graphs I don't know about you but you
start looking these numbers and all this
text and you get down here and you say
oh yes you know here this is how much of
the wine we're going to label as subpar
not good and this is how much we're
going to label as good and then we got
down here to finally separating out our
data so it's ready to go into the models
and the models take X and A Y in this
case X is all of our features minus the
one we're looking for and then Y is the
features we're looking for so in this
case we dropped quality and in the Y
case we added quality and then because
we need to have a training set and a
test set so we can see how good our
models do we went ahead and split the
models up X train X test y train y test
and that's using the train test split
which is part of the sklearn package and
we did um as far as our testing size 0.2
or 20% the default is 25% so if you
leave that out it'll do default setup
and we did a random State equals 42 if
you leave that out it'll use a random
State I believe it's default one I'd
have to look that back up and then
finally we scaled the data this is so
important to scale the data going back
up to here if you have something that's
coming out as 100 is going to really
outweigh something that's
0.0071 that's not in all the models
different models handle it differently
and as we look at the different models
I'll talk a little bit about that we're
going to only look at three models today
three of the top models used for this
and see how they compare and how the
numbers come out between them so we're
going to look at three different setups
oh let me change my sell here to mark
down there we go and we're going to
start with the random Forest classifier
so the three setups we're looking at is
the random Forest classifier support
vector classifier and then a neural
network now we start with the random
Force classifier because it has the
least amount of uh Parts moving parts to
fine tune and let's go ahead and put
this in here so we're going to call it
RFC for random force classifier and if
you remember we imported that so let me
go back up here to the top real quick
and we did an import of the random force
classifier from sklearn Ensemble and
then uh we'll all we also let me just
point this out here's our svm where we
uted our support Vector classifier so
svm is support Vector model support
vector classifier and then we also have
our neural network and we're going to
from there the
multi-layered pepotron classifier kind
of a mouthful for the P Patron don't
worry too much about that name it's just
it's a neural network a lot of different
options on there and setups which is
where they came up with the patron but
so we have our three different models
we're going to go through on here and
then we're going to weigh them here's
our metrics we're going to use a
confusion metrics also from the sklearn
package to see how good our model does
um with our split so let's go back down
there and take a look at that and we
have our um RFS equals random Forest
classify and we have n estimators equals
200 this is the only value you play with
with a random Forest classifier how many
Forest do you need or how many trees in
the forest so how many models are in
here that makes it pretty good as a
startup model because you're only
playing with one number and it's pretty
clear what it is and you can lower this
number or raise it usually start up with
a higher number and then bring it down
to see if it keeps the same value so you
have less you know the smaller the model
the better the fit and it's easier to
send out to somebody else if you're
going to distribute it now the random
four is classified
um everything I read says it's used for
kind of a medium-size data set so you
can run it in on Big Data you can run it
on smaller data obviously but tends to
work best in the mid-range and we'll go
ahead and take our RFC and I just copied
this from the other side fit XT Trin
comma y train so we're sending it our
features and then the quality in the Y
train what we want to predict in there
and we just do a simple fit now remember
this is sklearn so everything is fit or
transform another one is predict which
we'll do in just a second here fact
let's do that now predict RFC equals and
it's our RFC model predict and what are
we predicting on well we trained it with
our train value so now we need our test
our X test so this has done it this is
going to do this is the three lines of
code we need to create our random Forest
variable fit our training data to it so
we're programming it to fit in this case
it's got 200 different trees it's going
to build and then we're going to predict
on here let me go ahead and just run
that and we can actually do something
like oh let's do predict
RFC just real quick we'll look at the
first 20 variables of it uh let's go a
and run that and uh in our first 20
variables we have three wines that make
the cut and the other 17 don't so the
other 17 are bad quality and three of
them are good quality in our predicted
values and if you can remember correctly
U we'll goad and take this out of here
this is based on our test test so these
are the first 20 values in our test and
this has as you can see all the
different features listed in there and
they've been scaled so when you look at
these they're a little bit confusing to
look at and hard to read but we have
there's a minus 01 so this is
36-01 so1 164 minus
.09 or no it's still minus one so minus
.9 all between 0er and one on here I
think I was confused earlier and I said
zero between two negative -2 but between
minus one and one which is what it
should be in the scale and we'll go
ahead and just cut that out of there run
this we have our setup on here so now
that we've run the prediction and we
have predicted values well one you could
uh publish them but what do we do with
them well we want to do with them is we
want to see how well our model model
performed that's a whole reason for
splitting it between a training and
testing model and for that if you
remember we imported the classification
report that was again from the sklearn
there's our confusion Matrix and
classification report and the
classification report actually sits on
the confusion Matrix so it uses that
information and our classification
report we want to know how good our y
test that's the actual values versus our
predicted RFC so we'll go ahead and
print this report out and let's take a
look and we can see here we have a
Precision out of the zero we had about 0
92 that were labeled as uh bad that were
actually bad and out of precision for
the um Quality wines we're running about
78% so you kind of give us an overall
90% And you can see our F1 score our
support set up on there our recall you
could also do the confusion Matrix on
here which gives you a little bit more
information but for this this is going
to be good enough for right now we're
just going to look at how good this
model was because we want to compare the
random force classifier with the other
two models and you know what let's go
ahead and put in the um confusion Matrix
just so you can see that on there with Y
test and prediction RFC so in the
confusion Matrix we can see here that we
had
266 correct and seven wrong these are
the mislabels for bad wine and we had a
lot of Mis labels for good wine so our
quality labels aren't that good we're
good at predicting bad wine not so good
at predicting whether it's a good
quality wine important to note on there
so that is our basic random forest
classifier and let me go ahead oops cell
change cell type to markdown and run
that so we have a nice label let's look
at our svm classifier our support Vector
model and this should look familiar we
have our clf we're going to create
what's we'll call it just like we call
this an RFC and then we'll have our cf.
fit and this should be identical to up
above X train comma y train and uh just
like we did before let's go ahead and do
the prediction and here is our clf
predict and it's going to equal the
cf. predict and we want to go ahead and
use
xcore test and right about now you can
realize that you can create these
different models and actually just
create a loop to go through your
different models and put the data in and
that's how they designed it they
designed it to have that ability let's
go ahead and run this and then let's go
ahead and do our classification report
and I'm just going to copy this right
off of
here they say you shouldn't copy and
paste your code and the reason is is
when you go in here and edit it you
unbearably will miss something we only
have two lines so I think I'm safe to do
it today and let's go ahead and run this
and let's take a look how the svm
classifier came out so up here we had a
90% and down here we're running about an
86% so it's not doing as good now
remember we randomly split the data so
if I run this a bunch of times you'll
see some changes down here so these
numbers this size of data if I ran it
100 times it would probably be within
plus or minus three or four on here in
fact if I ran this 100 times you'd
probably see these come out almost the
same as far as how well they do in
classification and then on the confusion
Matrix let's take a look at this one
this had 22x 25 this one has 35 by 12 so
it's it's doing not quite as good that
shows up here 71% versus 78% and then if
we're going to do a svm classifier we
also wanted to show you one more and
before I do that I kind of tease you a
little bit here before we jump into
neural networks the um big save all deep
learning because everything else must be
shallow learning that's a joke let's
just talk a little bit about the svm
vers versus the random Forest classifier
the svm tends to work better on smaller
numbers it also works really good on um
because a lot of times you convert
things into numbers and bins and things
like that the random Forest tends to do
better with those at least that's my
brief experience with it where if you
have just a lot of raw data coming in
the svm is usually the fastest and
easiest to apply model on there so they
they each have their own benefits you'll
find though again that when you run
these like a hundred times difference
between these two on a data set like
this is going to just go away there's
Randomness involved depending on which
data we took and how they classify them
the big one is the neural networks and
this is what makes the neural networks
nice is they can do they can look into
huge amounts of data so for a project
like this you probably don't need a
neural network on this but it's
important to see how they work
differently and how they come up
differently so you can work with huge
amounts of data you can also many
respects they work really good with text
analysis especially if it's time
sensitive more and more you have an
order of text and they've just come out
with different ways of feeding that data
in where the series and the Order of the
words is really important same thing
with uh starting to predict in the stock
market if you have tons of data coming
in from different sources the neural
network can really process that in a
powerful way to pull up things that
aren't seen before when I say lots of
data coming in I'm not talking about
just the high lows that you can run an
svm on real easily I'm talking about the
data that comes in where you have you've
pulled off the Twitter feeds and have
word counts going on and you've pulled
off the uh the different news feeds the
business are looking at and the
different releases when they release the
different reports so you have all this
different data coming in and the neural
network does really good with that
pictures picture processing Now is
really moving heavily into the neural
network if you have a pixel 2 or pixel 3
phone put out by Google it has a neural
network for doing it's kind of goofy but
you can put Little Star Wars Androids
dancing around your pictures and things
like that that's all done with the
neural Network so has a lot of different
uses but it's also requires a lot of
data and is a little heavy-handed for
something like this and this should now
look familiar because we've done it
twice before we have our multi-layered
Patron classifier we'll call it an mlpc
and it's this is what we imported mlpc
classifier there's a lot of settings in
here the first one is the hidden layers
you have to have the hidden layers in
there we're going to do three layers of
11 each so that's how many nodes or in
each layer as it comes in and that was
based on the fact we have features
coming in then I went ahead and just did
three layers probably get by with a lot
less on this but you I didn't want to
sit and play with it all afternoon again
this is one of those things you play
with a lot because the more hidden
layers you have the more resources
you're using you can also run into
problems with overfitting with too many
layers and you also have to run higher
iterations the max iteration we have is
set to 500 the defaults 200 because I
use three layers of 11 each which is by
the way kind of a default I use I
realized that usually you have about
three layers going down and the number
of features going across you'll see
that's pretty common for the first
classifier when you're working in neural
networks but it also means you have to
do higher iterations so we up the
iterations to 500 so that means it's
going through the data 500 times to
program those different layers and
carefully adjust them and we do have a
full tutorials you can go look up on
neural networks and understand the
neural network settings a lot more and
of course we have uh you're looking over
here where we had our previous model
where we fit it same thing here mlpc fit
X train y train and then we going to
create our prediction so let's do our
predict and mlpc and it's going to equal
the mlpc and we'll just take the same
thing here predict X test let's just put
that down here do predict X test and if
I run that we've now programmed it we
now have our prediction here same as
before and we'll go ahead and do the
copy print again always be careful with
the copy paste because you always run
the the chance of missing one of these
variables so if you're doing a lot of
coding you might want to skip that copy
and paste and just type it in and let's
go ahead and run this and see what that
looks like and we came up with an 88%
we're going to compare that with the 86
from our tree or svm classifier and are
90 from the random forest classifier and
keep in mind random Forest classifiers
they do good on midsize data the svm on
smaller amounts of data although to be
honest I don't think that's necessarily
the split between the two and these
things will actually come together if
you ran them a number of times and we
can see down here the know of good wines
mislabeled with setup on there it's on
par with our random Forest so it had 22
25 shouldn't be a surprise it's
identical it just didn't do as good with
the bad wines labeling what's a bad wine
and what's not see yeah cuz they had 266
and seven we had down here 260 and 13 so
mislabeled a couple of the bad wines as
good wines so we've explored three of
these basic classifiers these are
probably the three most widely used
right now I might even throw in the
random tree if we open up their website
and we go under supervised learning
there's a linear model we didn't do that
almost most of the data usually just
start with a linear model because it's
going to process the quickest and use
the least amount of resources but you
can see they have linear quadratic they
have kernel Ridge there's our support
Vector stochastic gradient nearest
neighbors nearest neighbors is another
common one that's used a lot very
similar to the svm gazan process cross
decomposition naive Bay this is more of
an intellectual one that I don't see
used a lot but it's like the basis of a
lot of other things decision tree
there's another one that's used a lot
Ensemble methods not as much multiclass
and multi-label algorithms feature
selection neural networks that's the
other one we use down here and of course
the forest so you can see there's a in
sklearn there are so many different
options and they just developed them
over the years we covered three of the
most commonly used ones in here and went
over a little bit over why they're
different neural network just because
it's fun to work in deep learning and
not in Shallow learning as I told you it
doesn't mean that the svm is actually
shallow it's does a lot of it covers a
lot of things and same thing with the
decision the random for is classifier
and we notice that there's a number of
other different classifier options in
there these are just the three most
common ones and I'd probably throw the
nearest neighbor in there and the
decision tree which is usually part of
the decision Forest depending on what
the back end you're using and since as
human beings um if I was in the
shareholders office I wouldn't want to
leave them with a confusion Matrix they
need that information for making
decisions but we want to give them just
one particular score and so I would go
ahead and we have our sklearn metrics
we're going to import the accuracy score
and I'm just going to do this on the um
random Forest since that was our best
model and we have our CM accuracy score
and I forgot to print it remember in
Jupiter notebook we can just do the last
variable we leave out there it'll print
and and so our CM acurate score we get
is 90% And that matches up here we
should already see that up here in
Precision so you could e quote that but
a lot of times people like to see it
highlighted at the very end this is our
Precision on this model and then the
final stage is we would like to use this
for future so let's go ahead and take
our wine if you remember correctly we'll
do wine head of 10 we'll run that
remember our original data set we've
gone through so many steps now we're
going to go back to the original data
and we can see here we have our top 10
top 10 on the list only two of them make
it as having high enough quality wine
for us to be interested in them and then
let's go ahead and create some data here
we'll call it X new equals and this is
important this data has to be we just
kind of randomly selected some data
looks an awful lot like some of the
other numbers on here which is what it
should look like and so we have our X
new equal 7.3.5 eight and so on and then
it is so important this is where people
forget this step X new equals SC
remember SC that was our standard scaler
variable we created if we go right back
up here before we did anything else we
created an sc we fit it and we
transformed it and then we need to do
what transform the data we're going to
feed in so we're going to go back down
here and we're going to transform our X
new and then we were going to go ahead
and use the where are we at here we go
our random forest and if you remember
all it is is our RFC predict model right
there let's go Ahad just grab that down
here and so our y new equals here's our
RFC predict we're going to do our X new
in and then it's kind of nice to know
what it actually puts out so according
to this it should print out what our
prediction is for this wine and oh it's
a bad wine okay so we didn't pick out a
good wine for our ex new and that should
be expected moso wine if you remember
correctly only a small percentage of the
wine met our quality requirements so we
can look at this and say oh we'll have
to try another wine out which is fine by
me because I like to try out new wines
and I certainly have a collection of old
wine bottles and very few of them match
but you can see here we've gone through
the whole process just a quick re rehash
we had our Imports we touched a lot on
the sklearn our random Forest our svm
and our MLP classifier so we had our um
support Vector classifier we had our
random forest and we have our neural
network three of the top used
classifiers in the sklearn system system
and we also have our confusion matri
Matrix and our classification report
which we used our standard scaler for
scaling it and our label encoder and of
course we needed to go ahead and split
our data up in our imp plot line train
and we explored the data in here for
null values we set up our quality into
bins we took a look at the data and what
we actually have and put a nice little
plot to show our quality what we're
looking at and then we went through our
three different models and it's always
interesting because you spend so much
time getting to these models and then
you kind of go through the models and
play with them until you get the best
training on there without becoming
biased that's always a challenge is to
not overtrain your uh data to the point
where you're training it to fit the test
value and finally we went ahead and
actually used it and applied it to a new
wine which unfortunately didn't make the
cut it's going to be the one that we
drink a glass out of and save the rest
from
cooking of course that's according to
the random Forest on there because we
used the best model that it came up with
if you're one of the aspiring data
scientists for online training and
graduating from the best universities or
a professional who elicits to switch
careers with data analytics by learning
from the experts then try giving a short
to Simply learn skel Tech postgraduate
program in data science in collaboration
with IBM the link is mentioned in the
description box and you should navigate
to the homepage where you can find a
complete overview of the program being
offered so what is cbor in Python the
cabon library in Python is a widely
popular data visualization library that
is commonly used for data science and
machine learning tasks it is built on
top of the matplot LI data visualization
library and allows you to perform
exploratory data analysis using cbon you
can create interactive plots to answer
specific questions about your data it
helps you find Trends in your data that
you can't notice just by looking at the
data now to understand the seon library
in detail and how to create different
plots and know the data better we'll be
using two data sets in our demo the
first one is the empty cast data set and
the second is the popular Irish flow
data set now let's head over to our
jupyter notebook and start with our
demonstration so I on my Jupiter
notebook let me go ahead and rename the
notebook I'll
say
cbon demo we'll rename
it all right you can see I have the
first two cells already filled in so
while working on an explorat data
analysis project using python you will
need numpy pandas matplot Li and cbon
libraries for data manipulation and
visualization so let's import these
libraries first so I've already written
import nump as NP Pand as as PD matplot
le. P plot as PLT cbon as SNS all right
so I'll hit shift enter to run this
first
cell all right now using the read CSV
function that is present in the pandas
Library I'm importing my data set that
is empty cars I'll tell you what empty
cars is about this is a CSV data set and
here I have given the location where my
data set is
present let's run it all
right
now let me go ahead and print the first
five
rows of my empt cars data
frame for that I'm using the head
function if I run it okay you can see
here it has given me the first five rows
the index starts from zero till four so
total five rows of information and you
can see these are the columns that are
present in the empt cars data set the
first is the model of the car now just
let me give you a background about this
data set so the data is taken from the
1974 Motor Trend Us magazine it has
information about fuel consumption and
11 different aspects of automobile
design and performance of 32
cars all right so here we have the First
Column as the model of the car and then
we have MPG which is miles per gallon
then we have C yl which stands for the
number of cylinders in the vehicle or
the car then we have disp which is
displacement in cubic Ines so these all
values are in cubic
in after that we have HP which is cross
horsepower then we have something called
as dr8 which is basically the rear axle
ratio then we have WT which is the
weight of the car in pounds so think of
it as weight multiplied by
1,000 all right and then we have QC
which is4 mile time this is a measure
for acceleration then we have something
called as vs this is the type of engine
for each of the vehicles so zero stands
for v-shaped engine and one stands for
straight engine then we have am which is
basically the mode of transmission so
zero is for automatic transmission and
one is for manual transmission then we
have the number of forward gears present
in the vehicle and then we have
something called as carb or
CB this stands for the number of
carburetors so carburetors is a device
for supplying a spark ignition engine
with a mixture of fuel and
air all
right
now we'll use the info function to print
the summary of the data frame so the
info function returns information
regarding the index data type column
data types then we have the information
about nonnull values and memory usage so
I'll write my data frame name which is
Mt cars dot I'm going to use the info
function let's run it and here you can
see we have all the columns there are
total 32 entries from 0 to 31 so data
about 32 cars then here you can see the
data types and the memory usage
now moving ahead let me just print the
shape of the data frame that will give
you the number of rows and columns
present I'll use the shape
attribute you can see here there are
total 32 rows and 12 columns in my data
set now the cbon library provides a
range of plotting functions that makes
the visualization and Analysis of data
easier so we'll cover some of the
crucial plots that are available in cbon
so let's start with a bar plot I'll give
a comment as bar
plot
now first understand what a bar plot is
so a bar plot gives an estimate of the
central tendency of a numeric variable
with the height of each rectangle it
provides some indication of the
uncertainty around the estimate using
error bars now to build this plot you
usually choose a categorical column in
the x-axis and it numeric column on the
y-
AIS so we'll create a bar plot between
the number of cylinders and the miles
per
gallon so let's
start I'll declare a variable called RS
which essentially stands for result then
I'll use
SNS which is for the cbon
library I'll say SNS dot followed by my
barplot
function and inside the
function I'll pass in
my data frame name that is empty
cars and then for xaxis within the
bracket using quotation I'll give my
column name as cylinder so this is going
to be my x-axis and for y axis I'll say
my data frame name empty C
then the square bracket followed by my Y
axis column that is miles per
gallon now to display I'm going to use
the mat plotly function I'll say PLT do
show let's run it and we'll see the
result there you go so here you can see
on the xaxis we have the number of
cylinders so in our data we have
vehicles with four 6 or eight 8
cylinders and on the y- axis you can see
the miles per gallon now what
essentially this graph tells us is that
when the number of cylinders in the
vehicle increases my miles per gallon on
MPG for the vehicles
decreases and you can see these black
lines so these lines indicate the
uncertainty around the estimate using
error
bars all right
now you can create
the same bar plot using another way now
we will exclusively Define the X and ya
axis columns and also pass the name of
the data frame using the data argument
let me show you how to do it I'll use
the same variable name RS which stands
for result I'll say SNS do
barplot now inside the bar plot function
I'm going to exclusively Define my axis
I'll say x equal to I'll pass in my
column
as
cylinder then I'll say y
AIS which is miles per gallon and then
using the data attribute I'll pass in my
data frame name that is empt
cars and finally we'll write PLT do
show let me run it you see here we have
the exact same bar plot as the above one
so this is another way to create a bar
plot now python cbor allows the users to
assign colors to the bars so the bar
chart that we are going to create will
convert all the bars to a particular
color so for that I'm going to use the
color
argument so I'll say RS equal to SNS do
my function that is
barplot I'll say x equal to
cylinder my Y
axis will have
MPG and then I'll say data is empty
cars give a comma and my final attribute
I'll say
color equal to I'll say
yellow close the
bracket and then
I'm going to write PLT do
show if I run it you can see here all
the bars have been converted to yellow
color now you can change the
color argument let's say I'll make it to
Red if I print it you can see here all
the bars have been changed to red color
now all right now cbor Library also has
the pallet argument
which you can use to give different
colors to the bars so if you go to
Google and search for cbon
pallet you have a nice link
here so here you have there are
different pallets provided by cbon you
can see here SNS do color palette and
these are the names of the palette you
can use whichever pallet you
want all
right so I'll say RS equal
to SNS do
barplot we'll use our first method
of
assigning a bar plot so I'm using my
data frame Mt cars and this time we are
going to change the X and Y labels let's
say I'm using
my X label
as transmission whether automatic or
manual transmission and then in my Y
axis I'll
have miles per
gallon then I'm going to
say
pallet equal
to I'm going to spelling mistake
here let's say our pallet is set set two
this is the name of the pallet I'll
write PLT do
show if I run it you can see
here we have the two modes of
transmission zero is for automatic
transmission of the vehicle one is for
manual transmission and you can see the
miles per gallon for each of the modes
of transmission if you want you can
change the pallet colors let me just
copy
this I'll P it in my new cell and
instead of set two let's see
we'll
use another pallet called as
Rocket and I'm also going to
alter my X and Y labels let's say in my
X label I'll have
gear and my y label will be
horar if I run it you see here we now
have a
different pallette and the bars are of
different colors on the x-axis you have
the
Gears 3 gear 4 gear or five gear vehicle
and on the y axis you have the
horsepower so from the graph you can
see the horsepower is low for vehicles
that have four gears while it is high
for the vehicles that have five gears
now let's understand how to create a
count plot so the count plot function in
the python cbon Library Returns the
count of total values for each category
using bars we will create a count plot
that Returns the total number of
vehicles for each category of
cylinders I'll just give a comment here
as count
plot all right so I'll
start SNS do I'll use the count plot
function
and I'll Define my x axis as
cylinders my Y axis won't have any label
or any column because Y axis will only
show the
count then I'm going
to use
my data parameter or the argument and
pass in my data frame name that is empty
cars let's say I'll set the
pallet
to set
one let me run
it okay there is some error let's debug
the
error says St strr object has no
attribute
get okay so this should
be without the single codes make sure
the data frame name is uh without the
single codes let's run it again there
you go so you can see here this is a
nice count plot that shows the number of
cylinders on the x- axis and on the y
axis you have the count you can see here
there are more number of vehicles that
have eight cylinders and there are less
number of vehicles or cars that have six
cylinders and actually you can count it
as well so you can see here there are 14
cars that have eight cylinders there are
11 cars that have four cylinders and
there are seven cars that have six
cylinders all
right now moving ahead the next count
plot will show the number of cars for
each
carburetor so for that I'm going to say
SNS
do count
plot my x-axis will now
have the variable carbon
I'll see data equal to empty
cars
and my
pallet I'll set it to set
one or let's say set two now let me run
it you can see it here on the x-axis we
have the different number of carburetors
1 2 3 4 6 and 8 and you can see we have
more number of cars that have two and
four carburetors and there are very few
vehicles that have six and8
carburetor cool now moving
ahead so the python seon Library allows
you to create horizontal count plots
where the feature column is on the Y AIS
and the count is on the x-axis let me
show you how to create a horizontal
count
plot I'll say SNS do
count
plot I'm going to Define my y label or
the y axis
as
gear I'll say data equal to empty cars
and set my pallet to let's say
rocket let me run it okay you see here
this is a horizontal count plot if you
compare it with the above one these were
all vertical count plot as in the Bars
were vertically aligned but here you can
see the bars are all horizontally
aligned
and on the Y AIS you have the gear
column and on the x-axis you have the
count so if you Mark here properly you
can see there are 15 vehicles with three
gears there are 12 vehicles that have
four gears and there there are total
five vehicles with five gear now you can
also create a grouped count plot using
the Hue parameter so the Hue parameter
accepts the column name for color
encoding now let's build a count plot
that shows the number of cars for each
category of Gears grouped by the number
of
cylinders so we are going to group based
on the number of cylinders let me show
you how to use the Hue parameter for
that I'll say
SNS do count plot in the x-axis I'll
have my gear
column then I am going to pass my Hue
parameter that will have the cylinder
column or the number of cylinders for
each
vehicle I'll pass my data frame using
the data argument as empty cars and say
pallet
I'll set my pallet to set
one let's run
it if I scroll down you see here we have
a count plot on the x-axis you have
three four and five gears of the
different vehicles and for each gear you
have the bars that represent the number
of cylinders so for vehicles that have
three gears we
have majority of the vehicles Having
Eight cylinders
similarly for vehicles that have four
gears we have most of the vehicles with
four cylinders and if you compare for
the cars that have five gears so we have
equal number of vehicles that have four
and 8 cylinders and
then we have one vehicle that has 6
cylinders cool now moving
ahead so the C bone Library supports the
disc plot function that creates the
distribution of any continuous data
let's create the distribution of miles
per gallon of the different vehicles or
the cars the mpg Matrix measures the
total distance the car can travel per
gallon of fuel so here I'll
give a comment as disc plot which stands
for distribution
plot I'll say SNS
dot disc
plot I'll pass in
my data frame name that is empty
cars give a period and
say MPG which is my column
name then I'm going to define the number
of bins that I want let's say 10 and
then I'll use
the color
argument and I'll see let's say G which
is for
green let's run it now there you go if I
scroll down you can see we have a
nice distribution plot so these are all
histograms
and you can see we have a smooth
curve and this shows the distribution of
miles per gallon that is present in our
data set all right now moving ahead you
can also create heat Maps using the seor
library so the heat Maps will let you
visualize Matrix like data the values of
the variables are contained in a matrix
and are represented as colors you can
create a correlation heat map which is a
graphical representation of a
correlation Matrix representing the
correlation between different
variables so in our example we'll create
a heat map where you can find the
correlation between each variable in the
empt cars data set just give a comment
as heat
map I'll say SNS and I'll use the heat
map
function I'll pass in my data frame name
that is empty cars followed by my
function called C RR which is for
correlation then I'll see c bar is equal
to true so the c bar argument is used to
draw the color
bar then I'll
say line widths
I'll pass it
as05 let's run
it there you go here you can see we have
a nice heat map and on the right side
you can see the color bar that starts
from -8 and goes up to +8 so this ranges
between -1 and +
1
and the bars or the cube you can see
that have a lighter color have a
positive
correlation and the ones that
are dark have a negative
correlation so for example you can see
here for cylinder and
MPG the color is really dark which is
around this region
minus8 this means when the number of
cylinders in the car increases
your miles per gallon decreases and
this we also had seen in our first bar
plot this one you can see here whenever
the number of cylinders increased your
miles per gallon had
decreased and this is what has been
depicted in our correlation map or the
heat map but if you see
for s cylinder and displacement there is
a positive
correlation which comes around this
color bar range similarly there are
other features in the data set for
example if you compare for the weight of
the vehicle and the number of
cylinders there is a positive
correlation which means when the weight
of the vehicle increases the number of
cylinders present in the vehicle also
increases and the you can also compare
and find out the correlation based on
this color bar choose from over 300 in
demand scales and get access to 1,000
plus hours of video content for free
visit scaleup by simply learn click on
the link in the description to know more
okay now let's move ahead the cbon
scatter plot function helps you create
plots that can draw relationships
between two continuous variables so to
understand about Scatter Plots and other
plotting functions hence for both we are
going to use Irish flat data set so
let's go ahead and load the Irish flat
data set I'll say
Irish equal to SNS do
loore data
set so this Iris flat data set is
already inbuilt into the SNS or the cbon
library now we'll go ahead and print the
first five rows from the data set
there you go here you can see here there
are
total four feature variables you have
the Supple length Supple width petal
length and petal width of the three
species of flow that is
setosa virginica and
versical so using this data set we are
going to see the rest of the cabon
plotting functions first let me go ahead
and print the different pieces of FL and
the
count for each of the species of FL that
we have in our data set I'll say
Irish within square brackets I'll pass
in
the output or the target variable that
is species and I'll use the function
that is value underscore
counts let's run it there you go so we
have 50 fls of setosa type again 50 fls
of bicular species and then we have
another 50 fls of type virginica so
there are total 150 fls present in our
data
set all
right we'll create a scatter plot to
show the relationship between seel
length and petal length for different
species of Irish
flaws so I'll say SNS dot followed by
the
scatter plot function in the x-axis I'll
see Supple
length
column give a
comma I'll pass in my Y axis
with petal
length then I'll say
data equal to
Iris now let's run it
you can see
here this is my scatter plot between
Supple length and petal length now you
can classify the different species of FL
using the Hue parameter as species in
the function so what I'll do is I'm
going to copy
my
above code I'll just paste it here and
we are going to edit in this cell
itself I'll add a last parameter called
Hue and this will have my species column
let's run it and see the difference
there you go here you can see we have a
nice scatter plot the blue dots are for
Sosa the orange dots are for versy color
and for Virgin you have the green dots
from this scatter plot you can see that
for virginica fls the Supple length and
the petal length are higher than your
versical and
Sosa and this region is completely for
setosa fls so you can see that the
setosa fls
have lower Supple length and petal
length all
right you can also so use another
parameter known as the size that will
differentiate the different pieces of
flower in terms of the size of the dots
I'll say size as
species let's run it you can see the
result there you go so here you can see
the
virginica fls are in green dots the size
is small followed by versical with
medium size dots and then we have SATA
whose size
is large okay now the python cbon
Library lets you visualize data using
pair plots that produces a matrix of
relationship between each variable in
the data set so let me show you how to
create a pair plot I'll say SNS
dot pair plot which is the function then
then I'll pass in my data frame name
that is Iris I'll put this Iris data
frame
within double
codes and we'll run it okay there is
some error here let's debug the
error data must be pandas TAA frame okay
what I'll do is I'll just remove the
double codes and we'll run it this might
take some time to create the pair plot
so just wait for a while they go here
you can see we have a nice pair plot on
the Y AIS you have the Supple length
Supple width petal length and petal
width similarly on the x-axis also you
have the same feature columns all the
diagonal plots are histograms and the
rest of the plots are Scatter Plots now
you can convert the diagonal visuals to
KDE or kernel density estimation plots
and the rest of the Scatter Plots using
the Hue parameter this makes the pair
plot easier to classify each type of FL
so I'll just copy this cell of
code paste it
here and I'll say
Hue equal
to species then I'll
say pallet
equal to set
one let's run it and we'll see the new
pair plot
now this might take some
time there you go so here you have the
pair plot
ready and on
the right side you can see the different
species the red color dots represent
setosa FL
the blue color dots represent versy
color and the green color dots represent
virin
FL now if
you look at this graph you will find
that your petal length is the best
feature to classify the three different
species of fls the reason being you can
see here they are very well categorized
and they are separated all the red dots
present here in The Petal length
represent Sosa fls around this range all
the blue dots
represent versic fls and the green dots
are all virgin fls so out of the four
features petal width petal length SLE
width and supple length you can take
petal length as the best feature
to differentiate between the three
species of
fls all right now coming to LM plot
function so the LM plot function in the
cbon library draws a linear relationship
as determined through regression for
continuous variables so let's create the
plot to show the relationship between
petal length and petal width of the
different species of Iris flaws
I'll just give a comment here as LM plot
I'll say SNS dot LM
plot in the x-axis we'll
have petal
length give a comma in the y
axis I'll pass in Petal
width and I'll pass in my data frame
using the
data argument or parameter which is
Iris let's run
it you can see it here we have a nice
plot the line you see here is a linear
regression
line now in this LM plot you can use the
Hue parameter to differentiate between
each species of FL and you can set
markers for different species let me
show you how to do it I'll say SNS dot
LM
plot we'll say
x as
our petal
length give a
comma I'll write
y
as
petal
width then I'm going to pass the Hue
parameter in Q I'll
see my column as
species then I'll give data equal to my
data set name that is Iris and now you
can pass
in markers
argument to differentiate the different
types of FL let's say I want to
represent setosa
with DOT or
O then my next PS which is versicle
should be represented with
a star
symbol and the final virginica species
of FL should be represented with let's
say this Arrow
Mark now let me go ahead and run
it there you
go you have three different linear
regression lines and the circle dots in
blue color represent
SATA the orange color stars represent
versic flowers
and the triangles represent virgin flers
so this is how you create a linear
relationship between the different types
of fls using the regression line all
right now let's take a look at the final
plotting function where we will create a
box plot a box plot also known as a box
and whisker plot depicts the
distribution of quantitative data the
Box represents the quartiles of the data
set the whiskers show the rest of the
distribution except for the
outliers now let's see the distribution
of the three species of
flowers based on their SLE width I just
give a comment as box
plot is the final plot that we are going
to see in our cbor
demo I'll say SNS dobox
plot in the x-axis we'll have the
species
column in the Y AIS we'll
have
Supple
width then I'll pass my
data as Iris let's run it
there you
go we have this box plot created at the
bottom in the xaxis you can see we have
the box plot for setosa flowers orange
for versic flowers and green for Virgin
CFS now this line
represents median which is inside the
box so this is also the median for
versic FL you can compare the
median SLE width for each of the fls
from the Y
AIS this line represents
the maximum Supple width value for
setosa and this line at the bottom
represents the
minimum length of the Supple width for
soplas now these dots that you see
outside
the box plot are called as
outliers the extended lines
that are outside the box plot are known
as
whiskers all right so that brings us to
the end of this demo session on
cbon library in Python so first we
imported our empty cars data set then we
saw how to create a bar
plot we learned how to use the color
argument or the parameter inside bar
plot we also saw how to use the pallet
attribute or the pallet parameter then
we learned about creating count plots
then we saw how to create horizontal
count
plot now moving
ahead we saw how to create distribution
plot then we saw a correlation heat map
after that we analyzed the other
functions such as the
scatterplot then we have had the pair
plot and we had the LM plot for finding
the linear relationship and later we saw
the box plot using the iris FL data set
so let's start with deep learning
Frameworks to start with this chart
doesn't even do the filled um Justice
because it's just exploded these are
just some of the major Frameworks out
there there's carass uh which happens to
sit on tensor flow so they're very
integrated there's tensor flow uh pie
torches out there Cafe piano uh
dl4j and chainer these are just a few of
the deep learning Frameworks we're
talking about neural networks if you're
just starting out and never seen a
neural network you can go into python in
the um s kit and do the neural network
in there which is probably the most
simplest version I know but the RO most
robust version out there the most top of
the ladder as far far as the technology
right now is tensor flow and that of
course is changing from day to day and
some of these are better for different
purposes uh so let's dive into tensor
flow let's see what is tensor flow what
is tensorflow tensorflow is a popular
open-source Library released in 2015 by
Google brain team for building machine
learning and deep learning models it is
based on Python programming language and
performs numerical computations using
data flow graphs to build model mod so
let's take a look at some of the
features of tensor flow it works
efficiently with multi-dimensional
arrays if you ever played with any of
the simpler packages of neural networks
you're going to find that you have to
pretty much flatten them and make sure
your your stuff is set in a flat model
tensorflow works really good so we're
talking pictures here um where you have
uh X and Y coordinates where the picture
is and then each pixel has three or four
different channels that's a very
complicated array very multi-dimensional
array it provides scalability of
computation across machines and large
data sets this is so new right now um
and you might think that's a minor thing
but when python is operating on one
computer and it has a float value and it
truncates it differently on each
computer you don't get the same results
and so your training model might work on
one machine and then on another it
doesn't this is one of the things that
tensorflow um addresses and does a very
good job on it supports fast debugging
and model building this is why I love
tensor flow uh I can go in there and I
can build a model with different layers
each layer might have different
properties um they have like the
convolutional neural network which you
can then sit on top of a regular neural
network with reverse propagation there's
a lot of tools in here and a lot of
options and each layer that it goes
through can utilize those different
options and stack different
and it has a large community and
provides tensor board to visualize the
model tensor board is pretty uh recent
but it's a really nice tool to have so
when you're working with other people or
showing your uh clients or the um
shareholders in the company you can give
them a nice visual model so they know
what's going on what are they paying for
and let's take a glance at some of the
different uses or applications for
tensor flow when we talk about
tensorflow
applications uh clearly this is data
analytics we're getting into the data
science I like to use data science is
probably a better term this is the
programming side uh and it's really the
sky is a limit um we can look at face
detection language translation fraud
detection video detection there are so
many different things out there that
tensorflow can be used for when you
think of neural networks because tensor
flow is a neural network uh think of
complicated chaotic data this is very
different than if you have a set numbers
like you're looking at the stock market
you can use this on the stock market but
if you're doing something where the
numbers are very clear and not so
chaotic as you have in a picture then
you're talking more about linear
regression models um and different
regression models when you're looking at
that when you're talking about these
really complicated data patterns then
you're talking neural networks and
tensor flow and if we're going to talk
about tensor flow we should talk about
what tensors are after all that is what
tensor um that's what this is named
after so we talk about tensors in tensor
flow tensor flow is derived from its
core component known as a tensor a
tensor is a vector or a matrix of in
Dimensions that represent all types of
data and you can see here we have like
the scaler which is just a single um
number you have your vector which is two
numbers might be a number in a direction
you have a simple Matrix and then we get
into the tensor I mentioned how a
picture is a very complicated tensor
because it has your x y coordinates and
then each one of those pixels has three
to four channels for your different
colors and so each image coming in would
be its own tensor and in tensor flow
tensors are defined by a unit of
dimensionality called as Rank and you
can see here we have our um scaler which
is a single number that has a rank of
zero because it has no real Dimensions
to it other than it's just a single
point and then you have your vector
which would be a single list of num
numbers uh so it's a rank one uh Matrix
would have rank two and then as you can
see right here as we get into the full
tensor it has a rank three and so the
next step is to understand how a tensor
flow works and if you haven't looked at
um the basics of a neural network in
Reverse propagation that is the basics
of tensor flow and then it goes through
a lot of different options and
properties that you can build into your
different tensors uh so a tensor flow
performs computations with the help of
data flow graphs it has nodes that
represent the operations in your model
and if you look at this you should see u
a neural network going on here we have
our inputs BC and d and you might have X
= B+ c y = d - 4 um a = x * Y and then
you have an output and so even though
this isn't a neural network here it's
just a simple set of computations going
across you can see how the more
complicated it gets the more be you can
actually one of the tensors is a neural
network with reverse propagation but
it's not limited to that there's so much
more you can do with it and this here is
just a basic uh flow of computations of
the data going across and you can see we
can plug in the numbers uh Bal 4 c = 3 D
= 6 and you get x = 4 + 3 so x = 7 y = =
6 - 4 so Y = 2 and finally a = 7 * 2 or
a = 14 like I said this is a very
simplified version of how tensor flow
Works each one of these layers can get
very
complicated um but tensorflow does such
a nice job that you can spin different
setups up very easily and test them out
so you can test out these different
models to see how they work now
tensorflow has gone through two major
stages uh we had the original tensor
flow release of 1.0 and then they came
out with the uh 2.0 version and the 2.0
addressed so many things out there that
the 1.0 really needed so when we start
talking about tensor flow 1.0 versus 2.0
um I guess you would need to know this
for um a legacy programming job if
you're pulling apart somebody else's
code the first thing is that tensorflow
2.0 supports eager execution by default
it allows you to build your models and
run them instantly and you can see here
from tensor flow 1 to tensorflow 2 uh we
have have um almost double the code to
do the same thing so if I want to do um
with tf. session or tensorflow session
um as a session the session run you have
your variables your session run you have
your tables initializer and then you do
your model fit um X train y train and
then your validation data your x value
yv value and your epics and your batch
size all that goes into the fit and you
can see here where that was all just
compressed to make it run easier you can
just create a model and do a Fed on it
uh and you only have like that last set
of code on there so it's automatic
that's what they mean by the eager so if
you see the first part and you're like
what the heck is all the session thing
going on that's tensor flow 1.0 and then
when you get into 2.0 it's just nice and
clean if you remember from the beginning
I said coros uh on our list up there and
cross is the high level API in tensor
flow 2.0 cross is the official highlevel
API of tensorflow 2.0 it has
Incorporated cross is tf. cross cross
provides a number of model building apis
such as sequential functional and
subclassing so you can choose the right
level of abstraction for your project
and uh we'll hopefully touch base a
little bit more on this sequential being
the most common uh form that is your
your layers are going from one side to
the other so everything's going in a
sequential order
functional is where you can split the
layer so you might have your input
coming on one side it splits into two
completely Mo different models and then
they come back together U and one of
them might be doing classification the
other one might be doing just linear
regression kind of stuff or neural basic
uh reverse propagation neural network
and then those all come together into
another layer which is your uh neural
network reverse propagation setup
subclassing is the most complicated as
you're building your own models and you
can subclass your own models in into
carass so very powerful tools here this
is all the stuff that's been coming out
currently in the ten orlow carass setup
a third big change we're going to look
at is it in tensor flow
1.0 uh in order to use TF layers as
variables uh you would have to write TF
variable block so you'd have to
predefine that in tensor flow 2 you just
add your layers in under the sequential
and it automatically defines them as
long as they're flat layers of course
this changes a little bit as some more
complicated tensor you have coming in
but all of it's very easy to do and
that's what 2.0 does a really good job
of and here we have um a little bit more
on the scope of this and you can see how
tensorflow one asks you to do um these
different layers and values if you look
at the scope and the default name you
start looking at all the different code
in there to create the variable scope
that's not even necessary in tensor 2.0
so you'd have to do one before you do do
what you see the code in 2.0 in 2.0 you
just create your model it's a sequential
model and then you can add all your
layers in you don't have to precreate
the um uh variable scope so if you ever
see the variable scope you know that
came from an older version and then we
have the last two which is our API
cleanup and the autograph uh in the API
cleanup tensor flow one you could build
models using TF Gans TF app TF contrib
TF Flags Etc in tensor flow 2 uh a lot
of apis have been removed D and this is
just they just cleaned them up cuz
people weren't using them and they've
simplified them and that's your TF app
your TF Flags your TF logging are all
gone uh so there's those are three
Legacy features that are not in 2.0 and
then we have our TF function and
autograph feature in the old version uh
tensorflow 10 the python functions were
limited and could not be compiled or
exported reimported so you were
continually having to redo your code and
you couldn't very easily just um put a
pointer to it and say hey let's reuse
this in tensor flow 2 you can write a
python function using the TF function to
mark it for the jit compilation for the
python jit so that tensorflow runs it as
a single graph autograph feature of TF
function helps to write graph code using
natural python
syntax uh now we just threw in a new
word in you graph uh graph is not a
picture of a person uh you'll hear graph
x and some other things graph is what
are all those lines that are connecting
different objects so if you remember
from before where we had uh the
different layers going through
sequentially each one of those wh lined
arrows would be a graph x that's where
that computation taken care of and
that's what they're talking about and so
if you had your own special code or
python way that you're sending that
information forward you can now put your
own function in there instead of using
whatever function they're using in
neural networks this would be your
activation function although it could be
almost anything out there depending on
what you're doing next let's go for
hierarchy and architecture and then
we'll cover three basic Tools in
tensorflow before we roll up our sleeves
and dive into the example so let's just
take a quick look at tensor flow tool
kits in their hierarchy at the high
level we have our objectoriented API so
this is what you're working with you
have your TF carass you have your
estimators this sits on top of your TF
layers TF loss TF metrics so you have
your reusable libraries for model
building this is really where tins or
flow shines is between the carass uh
running your estimators and then being
able to swap in different layers you can
your losses your metrics all of that is
so built into tensorflow makes it really
easy to use and then you can get down to
your lowlevel TF API um you have
extensive control over this you can put
your own formulas in there your own
procedures or models in there uh you
could have it split we talked about that
earlier so with the 2.0 you can now have
it split One Direction where you do a
linear regression model and then go to
the other where it does a uh neural
network and maybe each neural network
has a different activation set on it and
then it comes together into another
layer which is another neural network so
you can build these really complicated
models and at the low level you can put
in your own apis you can move that stuff
around and most recently we have the TF
code can run on multiple platforms and
so you have your CPU which is uh
basically like on the computer I'm
running on I have uh eight cores and 16
dedicated threads I here they now have
one out there that has over a 100 cores
uh so you have your CPU running and then
you have your GPU which is your graphics
card and most recently they also include
the TPU setup which is specifically for
tensorflow models uh neural network kind
of set up so now you can export the TF
code and you can run on all kinds of
different platforms for the most um
diverse setup out there and moving on
from the hierarchy to the architecture
in the tensorflow 2.0 architecture uh we
have uh you can see on the left this is
usually where you start out with and 80%
of your time in data science is spent
pre-processing data making sure it's
loaded correctly and everything looks
right uh so the first level in tensor
flow is going to be your read and
pre-processed data your TF data feature
columns this is going to feed into your
TF Cross or your pre-made estimators and
kind of you have your tensorflow Hub
that sits on top of there so you can see
what's going on uh once you have all
that set up you have your distribution
strategy where are you going to run it
are you going to be running it on just
your regular CPU are you going to be
running it uh with the GPU added in um
like I have a pretty high in graphics
card so it actually grabs that GPU
processor and uses it or do you have a
specialized TPU set up in there that you
paid extra money for uh it could be if
you're and later on when you're
Distributing the package you might need
to run this on some really high
processors because you're processing at
a server level for uh let's say you
might be processing this at a um a
distribute you're Distributing it not
the distribution strategy but you're
Distributing it into a server where that
server might be analyzing thousands and
thousands of purchases done every minute
um and so you need that higher speed to
give them a um to give them a Rec mation
or a suggestion so they can buy more
stuff off your website or maybe you're
looking for uh data fraud analysis
working with the banks you want to be
able to run this at a high speed so that
when you have hundreds of people sending
their transactions in it says hey this
doesn't look right someone's scamming
this person and probably has their
credit card so when we're talking about
all those fun things we're talking about
saved model this is we were talking
about that earlier where it used to be
when you did one of these models it
wouldn't truncate the float numbers the
same and so a model going from one you
build the model on your com machine in
the office and then you need to
distribute it and so we have our tensor
flow serving Cloud on premium that's
what I was talking about if you're like
a banking or something like that now
they have tensorflow light so you can
actually run a tensorflow on an Android
or an iOS or Raspberry Pi little
breakout board there in fact they just
came out with a new one that has a
built-in it's this a little mini TPU
with the camera on it so it can
pre-process a video so you can load your
tensor flow model onto that um talking
about an affordable way to beta test uh
a new product uh you have the tensorflow
JS which is for browser and node server
so you can get that out on the browser
for some simple computations that don't
require a lot of heavy lifting but you
want to distribute to a lot of end
points and now they also have other
language binding so you can now create
your uh tensorflow backend save it and
have it accessed from C Java
C rust r or from whatever package you're
working on so we kind of have an
overview of the architecture and what's
going on behind the scenes and in this
case what's going on as far as
Distributing it let's go ahead and take
a look at uh three specific pieces of
tensor flow and those are going to be
constants variables and sessions uh so
very basic things you need to know and
understand when you're working with the
tensor flow uh setup so constants in
tensor flow in tensorflow constants are
created using the function constant in
other words they're going to stay static
the whole time whatever you're working
with the Syntax for constant uh value
dtype 9 shape equals none name constant
verify shape equals false that's kind of
the syntax you're looking at and we'll
explore this with our hands on a little
more in depth uh and you can see here we
do Z equals tf. constant 5.2 name equals
x uh dtype is a float that means that
we're never going to change that 5 .2 is
going to be a constant value and then we
have our variables in tensor flow uh
variables in tensor flow are in memory
buffers that store tensors and so we can
declare a 2x3 tensor populated by ones
you could also do constants this way by
the way so you can create a um an array
of ones for your constant in here we
have V equals tf.
variables and then in tensorflow you
have tf. On's and you have the shape
which is 23 which is then going to
create a nice uh 2x3 um array that's
filled with ones and then of course you
can go in there and their variables so
you can change them it's a tensor so you
have full control over that and then you
of course have uh sessions in tensorflow
a session in tensorflow is used to run a
computational graph to evaluate to the
nodes and remember when we're talking
about graph or graph x we're talking
about all that information then goes
through all those arrows and whatever
computations they have that take it to
the next node and you can see down here
uh where we have import Tor flow is TF
if we do xal a TF do constant of 10 we
do yal a TF constant of 2.0 or 20.0 and
then you can do Z equals tf. variable
and it's a tf. addx comma y uh and then
once you have that set up in there you
go ahead and init your TF Global
variables initializer with TF session as
session you can do a session run and
knit and then you print this the session
run
y uh and so when you run this you're
going to end up with of course the uh 10
+ 20 is 30 and we'll be looking at this
a lot more closely as we actually roll
up our sleeves and put some code
together so let's go ahead and take a
look at that and for my coding today I'm
going to go ahead and go through
anaconda and then I'll use specifically
the Jupiter Notebook on there and of
course this code is going to work uh
whatever platform you choose whether
you're in a no book um the Jupiter lab
which is just the Jupiter notebook but
with tabs for larger projects we're
going to stick with Jupiter notebook pie
charm uh whatever it is you're going to
use in here uh you know you have your
spider and your QT console for different
programming environments the thing to
note um it's kind of hard to see but I
have my main Pi
36 right now when I was writing this
tensor flow Works in Python version 36
if you have python version 37 or 38
you're probably going to get some errors
in there uh might be that they've
already updated and I don't know it and
I have an older version but you want to
make sure you're in a python version 36
in your environment and of course in
Anaconda I can easily set that
environment up make sure you go ahead
and and um pip in your tensor flow or if
you're in Anaconda you can do AA install
tensor flow to make sure it's in your
package so let's just go ahead and dive
in and bring that up this will open up
up a nice browser window I just love the
fact I can zoom in and zoom out
depending on what I'm working on making
it really easy to adjust um a demo for
the right size go under new and let's go
ahead and create a new Python and once
we're in our new python window this is
is going to leave it Untitled uh let's
go ahead and import import tensorflow as
TF uh at this point we'll go ah and just
run it real quick no errors yay no
errors I do that whenever I do my
imports cuz I I unbearably will have
opened up a new environment and
forgotten to install tensorflow into
that environment uh or something along
those lines so it's always good to
double
check uh and if we're going to double
check that we also it's also good to
know uh what version we're working with
and we can do that simply by um using
the version command in tensor flow which
uh you should know is is probably
intuitively the TF um doore underscore
version uncore
uncore and you know it always confuses
me because sometimes you do tf. version
for one thing you do tf. uncore version
underscore for another thing uh this is
a double underscore in tensor flow for
pulling your version out and it's good
to know what you're working with we're
going to be working in tensorflow
version
2.1.0 and I did tell you the the um we
were going to dig a little deeper into
our constants and you can do an array of
constants and we'll just create this
nice array um aals tf. constant and
we're just going to put the array right
in there
4361 uh we can run this and now that is
what a is equal to and if we want to
just double check that uh remember we're
in Jupiter notebook where we can just
put the letter a and it knows that
that's going to be print um otherwise
you you surround it in print and you can
see it's a TF tensor it has the shape
the type and the and the array on here
it's a 2X two array and just like we can
create a constant we can go and create a
variable and this is also going to be a
2X two array and if we go ahead and
print the V out we'll run that uh and
sure enough there's our TF variable in
here uh then we can also let's just go
back up here and add this in here um I
could create another tensor and we'll
make it a constant this
time and we'll go and put that in over
here uh we'll have B TF constant and if
we go and print out uh V and B
let me go and run that and this is an
interesting thing that always that
happens in here uh you'll see right here
when I print them both out what happens
it only prints the last one unless you
use print commands uh so important to
remember that in Jupiter notebooks but
we can easily fix that by go ahead and
and print and Surround V with brackets
and now we can see with the two
different variables we have uh we have
the 3152 which is a variable and this is
just a flat a constant so it comes up as
a TF tensor shape two kind of two and
that's interesting to note that this
label is a tf. tensor and this is a TF
variable so that's how it's looking in
the back end when you're talking about
the difference between a variable and a
constant the other thing I want you to
notice is that in variable we capitalize
the V and with the constant we have a
lowercase C little things like that can
lose you when you're programming and
you're trying to find out hey why
doesn't this work uh so those are a
couple little things to note in here and
just like any other array in math uh we
can do like a concatenate or concatenate
the different values here uh and you can
see we can take um AB concatenated you
just do a tf. concat values and there's
our AB axis on one hopefully you're
familiar with axes and how that works
when you're dealing with matrixes and if
we go ahead and print this out uh you'll
see right here we end up with a tensor
so let's put it in as a const not as a
variable and you have your array 4378
and 6145 it's concatenated the two
together and again I want to highlight a
couple things on this our axis equals 1
this means we're doing the columns um so
if you had a longer array like right now
we have an array that is like you know
has a shape one Whatever It Is 2 comma 2
um axis zero is going to be your first
one and axis one is going to be your
second one and it translates as columns
and rows if we had a shape let me just
put the word shape here um so you know
what I'm talking about and it's very
clear and this is I'll tell you what I
spent a lot of time looking at these
shapes and trying to figure out which
direction I'm going in and whether to
flip it or whatever um so you can get
lost in which way your Matrix is going
and which is column which is rows are
you dealing with the third Axis or the
second axis um axis one you know zero 1
two that's going to be our columns uh
and if you can do columns then we also
can do rows and that is simply just
changing the concatenate uh we'll just
grab this one here and copy it we'll do
the whole thing over um control
copy contrl + V and changes from axis
one to axis zero and if we run that uh
you'll see that now we can catenate by
row as opposed to column and you have 4
3 6 1 7 8 4 7 so just brings it right
down turns it into rows versus columns
you can see the difference there your
output this really you want to look at
the output sometimes just to make sure
your eyes are looking at it correctly
and it's in the format um I find
visually looking at it is almost more
important than understanding what's
going on uh because conceptually your
mind just just too many dimensions
sometimes the second thing I want you to
notice is this says a numpy array uh so
tensor flow is utilizing numpy as part
of their form format as far as Python's
concerned and so you can treat you can
treat this output like a numpy array
because it is just that it's going to be
a numpy array another thing that comes
up uh more than you would think is
filling U one of these with zeros or
ones and so you can see here we just
create a tensor tf. Zer and we give it a
shape we tell it what kind of data type
it is in this case we're doing an
integer and then if we um print out our
tensor again we're in Jupiter so I can
just type out tensor and I run this you
can see I have a nice array of um with
shape 3 comma 4 of zeros one of the
things I want to highlight here is
integer 32 if I go to the um tensor flow
data types I want you to notice how we
have float 16 float 32 float 64 uh
complex if we scroll down you'll see the
integer down here of 32 the reason for
this is that we want to control how many
bits are used in the proc ision this is
for exporting it to another platform uh
so what would happen is I might run it
on this computer where python goes does
a float to indefinite however long it
wants to um and then we can take it but
we want to actually say hey we don't
want that high Precision we want to be
able to run this on any computer and so
we need to control whether it's a TF
float 16 in this case we did an integer
32 we could also do this as a float so
if I run this as a float 3 2 uh that
means this has a 32-bit Precision you'll
see 0 point whatever and then to go with
uh
zeros we have ones if we're going from
the opposite side and so we can easily
just create a tensor flow with ones and
you might ask yourself why would I want
zeros and ones and your first thought
might be to initiate a new tensor
usually we initiate a lot of this stuff
with random numbers because it does a
better job solving it if you start with
a uniform uh set of ones or zeros you're
dealing with a lot of bias so be very
careful about starting a neural network
uh for one of your rows or something
like that with ones and zeros on the
other hand uh I use this for masking you
can do a lot of work with masking you
can also have uh it might be that one t
a row is masked um you know zero is is
false one is true or whatever you want
to do it um and so in that case you do
want to use the zeros and ones and there
are cases where you do want to
initialize it with all zeros all ones
and then swap in different numbers as
the as the um tensor learns so it's
another form of control but in general
you see zeros and ones you usually are
talking about a mask over another array
and just like in uh numpy you can also
uh do reshapes so if we take our um
remember this is shaped 3 comma 4 maybe
we want to swap that to 4 comma 3 and if
we print this out you will see let me
just go and do that contrl V let me run
that and you'll see that the the order
of these is now switched instead of uh
four across now we have three acrossed
and four
down and just for fun let's go back up
here where we did the ones and I'm going
to change the ones to um tf. random
uniform uh and we'll go and just take
off well we'll go and leave that we'll
go and run this and you'll see now we
have uh
0441 and this way you can actually see
how the reshape looks a lot different uh
041 .15 71 and then instead of having
this one it rolls down here to the
0.14 and this is what I was talking
about sometimes you fill a lot of times
you fill these with random numbers and
so this is the random. uniform is one of
the ways to do that now I just talked a
little bit about this float 32 and all
these data types uh one of the things
that comes up of course is recasting
your data
um so if we have a dtype float 32 we
might want to convert these two integers
because of the project we're working on
um I know one of the projects I've
worked on ended up wanting to do a lot
of roundoff so that it would take a
dollar amount or a float value and then
have to round it off to a dollar amount
so we only wanted two decimal points um
and in which case you have a lot of
different options you can multiply by
100 and then round it off or whatever
you want to do there's a lot of or then
convert it to an integer was one way to
round it off uh kind of
cheap and dirty
trick uh so we can take this and we can
take the same tensor and we go ahead and
create a um as an integer and so we're
going to take this tensor we're going to
tf. cast it and if we
print tensor uh and then we're going to
go ahead and
print our tensor let me just do a quick
copy and paste and when I'm actually
programming I usually type out a lot of
my stuff just to double check it uh in
doing a demo copy and paste works fine
but sometimes be aware that uh copy and
paste can copy the wrong code over
personal choice depends on what I'm
working on and you can see here we took
um a float 32 4.6 4.2 and so on and it
just converts it right down to a integer
value uh it's our integer 32 setup and
uh remember we talked about um a little
bit about
reshape um as as far as flipping it and
I just did uh 4 comma 3 on the reshape
up here and we talked about axis zero
axis one uh one of the things that is
important to be able to do is to take
one of these variables we'll just take
this last one tensor as
integer and I want to go ahead and
transpose it and so I can do um we'll do
a equals tf.
transpose and we'll do our tensor
integer in there and then if I print the
A out and we run this you'll see it's
the same array but we've fli it so that
our columns and rows are flipped this is
the same as reshaping uh so when you
transpose you're just doing a reshape
what's nice about this is that if you
look at the numbers The Columns when
when we went up here and we did the
reshape they kind of roll down to the
next row so you're not maintaining the
structure of your Matrix so when we do a
reshape up here they're similar but
they're not quite the same and you can
actually go in here and there's settings
in the reshape that would allow you to
turn it into a
transform uh so when we come down here
it's all done for you and so there are
so many times you have to transpose your
digits that this is important to know
that you can just do that you can flip
your rows and columns rather quickly
here and just like numpy you can also do
multip your different math functions
we'll look at multiplication and so
we're going to take matrix
multiplication of 10 ensors uh we'll go
ah and create a as a constant
5839 and we'll put in a vector v 4 comma
2 and we could have done this where they
matched where this was a 2X two array um
but instead we're going to do just a 2x1
array and the code for that is your tf.
mat mole uh so Matrix multiplier and we
have a * V and if we go ahead and run
this oh let's make make sure we print
out our av on
there and if we go ahead and run this uh
you'll see that we end up with 36x 30
and if it's been a while since you've
seen The Matrix math uh this is 5 * 4 +
8 * 2 um 3 * 4 + 9 *
2 and that's where we get the 36 and 30
now I know we're covering a lot really
quickly as far as the basic function fun
ality uh so the Matrix or your Matrix
multiplier is a very commonly used
backend tool as far as Computing um uh
different models or linear regression
stuff like that one of the things is to
note is that just like in um numpy you
have all of your different math so we
have our TF math and if we go in here we
have um functions we have our cosiness
absolute angle all of that's in here so
all of these are available for you to
use in the tensor flow model and if we
go back to our example and let's go
ahead and pull um oh let's do some
multiplication that's always good we'll
stick with our um AV our um constant a
and our vector
v and we'll go ahead and do some bitwise
multiplication and we'll create an AV
which is a * V let's go and print that
out and you can can see coming across
here uh we have the 42 and the
5839 and it produces uh 20 32
68 and that's pretty straightforward if
you look at it you have 4 * uh 5 is 20 4
* 8 is uh 32 that's where those numbers
come
from uh we can also quickly create an
identity
Matrix which is basically um your main
values on the diagonal being ones and
Zer across the other side let's go ahead
and take a look and see what that
uh uh looks like we can do let's do this
uh so we're going to get the shape um
this is a simple way very similar to
your numpy you can do a. shape and it's
going to return a tuple in this case our
rows and columns and so we can do a
quick uh print we'll do rows oops
and we'll do
columns and if we run this uh you can
see we have three rows uh two
columns and then if we go ahead and
create an identity
Matrix the
scops the script for that hit a wrong
button there the script for that looks
like
this where we have the number of rows
equals rows the number of columns equals
columns and D type is a 32 and then if
we go ahead and just print out our
identity you can see we have a nice
identity column with our ones going
across here now clearly we're not going
to go through every math module um
available but we do want to start
looking at this as a prediction model
and seeing how it functions so we're
going to move on to a more of a um
direct setup where you can actually see
the full tensor flow in use for that
let's go back and create a uh new
setup and we'll go in here new Python 3
module there we go bring this out so it
takes up the whole window because I like
to do that hopefully you made it through
that first part and you have a basic
understanding of tensor flow as far as
being uh a series of numpy arrays you
got your math equations and different
things that go into them we're going to
start building a full um setup as far as
the numpy so you can see how uh K sits
on top of it and the different aspects
of how it works the first thing we want
to do is we're going to go and do a lot
of imports uh date times warning scipi
scipi is your uh
math so the backend scientific math uh
warnings because whenever we do a lot of
this you have older versions newer
versions um and so sometimes when you
get warnings you want to go ahead and
just suppress them we'll talk about that
if it comes up on this particular setup
and of course date time pandas again is
your data frame think rows and columns
we import it as PD numpy is your uh
numbers array which of course tensor
flow is integrated heavily with caborn
for our graphics and the caborn as SNS
is going to be set on top of our mat
plot Library which we import as MPL and
then of course we're going to import our
matplot library pip plot as PLT and
right off the bat we're going to set
some graphic colors um patch Force Edge
color equals true the style we're going
to use the 538 style you can look this
all up there's when you get into M plot
Library into Seaborn there are so many
options in here it's just kind of nice
to make it look pretty when we start the
um when we start up that way we don't
have to think about it later on uh and
then we're going to take we have our uh
mlrc we're going to put a patch Ed color
dim Gray Line width again this is all
part of our Graphics here in our setup
uh we'll go ahead and do an interactive
shell uh node interactivity equals last
expression uh here we are PD for pandas
options display Max columns so we don't
want to display playay more than 50 um
and then our map plot library is going
to be in line This is a Jupiter notebook
thing the map plot Library inline uh
then warnings we're going to filter our
warnings and we're just going to ignore
warnings that way when they come up we
don't have to worry about them not
really what you want to do when you're
working on a major project you want to
make sure you know those warnings and
then uh filter them out and ignore them
later on and if we run this it's just
going to be loading all that into the
background uh so that's a little backend
kind of stuff then what we want to go
ahead and do is we want to go ahead and
import our specific packages U that
we're going to be working with which is
under carass now remember carass kind of
sits on tensor flow so when we're
importing carass and the sequential
model we are in effect importing um
tensor flow underneath of it uh we just
brought in the math probably should have
put that up above and then we have our
cross models we're going to import
sequential now if you remember from our
uh slide there was three different
options let me just flip back over there
so we can have a quick uh recall on that
and so in coras uh we have sequential
functional and subclassing so remember
those three different setups in here we
talked about earlier and if you remember
from here we have uh sequential where
it's going one tensor flow layer at a
time you go kind of look think of it as
going from left to right or top to
bottom or whatever Direction it's going
in but it goes in one Direction all the
time where functional can have a very
complicated graph of directions you can
have the data split into two separate um
tensors and then it comes back together
into another tensor um all those kinds
of things and then subclassing is really
the really complicated one where now
you're adding your own sub classes into
the tensor to do external computations
right in the middle of like a huge flow
of data uh but we're going to stick with
sequential it's not a big jump to go
from sequential to functional uh but
we're running a sequential tensor flow
and that's what this first import is
here we want to bring in our sequential
and then we have our layers and let's
talk a little bit about these layers
this is where cross and tensor flow
really are happening this is what makes
them so nice to work with is all these
layers are pre-built uh so from carass
we have layers import Den from carass
layers import
lstm when we talk about these layers uh
carass has so many built-in layers you
can do your own layers the dense layer
is your standard neural network U by
default it uses railu for its
activation and then the lstm is a long
short-term memory layer since we're
going to be looking probably at
sequential data uh we want to go ahead
and do the
lstm and if we go
into um carass and we look at their
layers this is the Cross website you can
see as we scroll down for the cross
layers that are built in we can get down
here and we can look at let's see here
we have our layer activation our base
layers um activation layer weight layer
weight there a lot of stuff in here we
had the railu which is the basic
activation that was listed up here for
layer activations you can change those
and here we have our core layers in our
dense layers you have an input layer a
dense layer um and then we've added more
custom one with the long-term short-term
memory layer and of course you can even
do your own custom layers in carass
there's a whole functionality in there
if you're doing your own thing what's
really nice about this is it's all built
in uh even the convolutional layers this
is for processing Graphics there's a lot
of cool things in here you can do um
this is why cross is so popular it's
open source and you have all these tools
right at your fingertips so from Cross
we're just going to import a couple
layers the dense layer um and the long
short-term memory layer and then of
course from uh sklearn our s kit we want
to go ahead and do our minmax scaler
standard scaler for pre-editing our data
and then metrics just so we can take a
look at the errors and compute those let
me go ahead and run this and that just
loads it up we're not expecting anything
from the output and our file coming in
is going to be air quality. CSV let's go
ahead and take a quick look at that this
is in Open Office it's just a standard
you know you do excel whatever you're
using for your spreadsheet and you can
see here we have a number of columns uh
number of rows it actually goes down to
like
8,000 the first thing we want to notice
is that the first row is kind of just a
random number put in going down probably
not something we're going to work with
the second row um is Bandung I'm
guessing that's a reference for the
profile if we scroll to the bottom which
I'm not going to do cuz it takes forever
to get back up they're all the same uh
the same thing with the status the
status is the same we have a date so we
have a sequential order here um here is
the gem which I'm going to guess is the
time stamp on there so we have a date
and time we have our O3 Co NO2 reading s
SO2 no CO2 VOC um and then some other
numbers here pm1 pm2.5 pm4 pm1 10
uh without actually looking through the
data um I mean some of this I can guess
is like temperature humidity I'm not
sure what the PMS are uh but we have a
whole slew of data here uh so we're
looking at air quality as far as an area
and a region and what's going on with
our date time stamps on there and so
codewise we're going to read this into a
pandas data frame so our data frame uh
DF is a nice abbreviation commonly used
for data frames equals pd. read CSV and
then our the path to it just Happ Happ
to be on my D drive uh separated by
spaces and so if we go ahead and run
this we'll print out the head of our
data and again this looks very similar
to what we were just looking at um being
in Jupiter I can take this and go the
other way uh make it real small so you
can see all the columns going across and
we get a full view of it um or we can
bring it back up in size that's pretty
small on there overshot um but you can
see it's the same data we were just
looking at uh we're looking at the
number we're looking at the profile
which is the bendung the um date we have
a timestamp our 03 count Co and so forth
on here uh and this is just your basic
pandas printing out the top five rows we
could easily have done uh three
rows uh five rows 10 whatever you want
to put in there by default that's five
for pandas now I talk about this all the
time so I know I've already said it at
least once or twice during this video
most of our work is in pre formatting
data what are we looking at how do we
bring it together uh so we want to go
ahead and start with our date time uh
it's come in in two columns we have our
date here and we have our time and we
want to go ahead and combine that and
then we have uh this is just a simple
script in there that says combine date
time that's our formula we're
building our we're going to submit our
um Panda's data frame and the tab name
when we go ahead and do this um that's
all of our information that we want to
go ahead and create and then goes for
ION range DF shape zero so we're going
to go through um the whole setup and
we're going to list tab a pin DF
location I and here is our date uh going
in there and then return the numpy array
list tab D types date time 64 that's all
we're doing we're just switching this to
a date time stamp and if we go ahead and
do DF date time equals combined date
time and then I always like to uh print
we'll do DF head just so we can see what
that looks like and so when we come out
of this uh we now have our setup on here
and of course just added it on to the
far right here's our date time you can
see the format's changed uh so there's
our we've added in the date time column
and we've brought the date over and
we've taken this format here and it's an
actual variable with a 0000 on here well
that doesn't look good so we need to
also include the time part of this and
we want to convert it into hourly data
uh so let's go ahead and do that uh to
do that uh to finish combining our date
time let's go ahead and create a uh a
little script here to combine the time
in there same thing we just did we're
just creating a numpy array returning a
numpy array and C forcing this into a
datetime format and we can actually spin
hours just going through uh these
conversions how do you pull it from the
p 's data frame how do you set it up um
so I'm kind of skipping through it a
little fast cuz I want to stay focused
on tensor flow and carass keep in mind
this is like 80% of your coding when
you're doing a lot of this stuff is
going to be reformatting these things
resetting them back up uh so that it
looks right on here and uh you know it
just takes time to to get through all
that but that is usually what the
companies are paying you for that's what
the big bucks are
for and we want to go ahead and uh a
couple things going on here is we're
going to go ahead and do our date time
we're going to reorganize some of our
setup in here convert into hourly data
we just put a pause in there um now
remember we can select from DF our
different columns we're going to be
working with and you're going to see
that we actually dropped a couple of the
columns those ones I showed you earlier
they're just repetitive data uh so
there's nothing in there that exciting
and then we want to go ahead and we'll
create a uh second data frame here just
get rid of the DF head and df2 is we're
going to group by date time and we're
looking at the mean value and then we'll
print that out so you can see what we're
talking about uh we have now reorganized
this so we put in date time 03 coo so
now this is in the same order um as it
was before and you'll see the date time
now has our Z 0 uh same date one two
three and so on so it's group the data
together so it's a lot more manageable
and in the format we want and in the
right sequential order and if we go back
to um there we go our air quality uh you
can see right here we're looking at um
these columns going across we really
don't need since we're going to create
our own date time column we can get rid
of those these are the different Columns
of information we want and that should
reflect right here in the columns we
picked coming across so this is all the
same columns on there there that's all
we've done is reformatted our data
grouped it together by date and then you
can see the different data coming out uh
set up on there uh and then as a data
scientist first thing I want to do is
get a description what am I looking at
uh and so we can go ahead and do the df2
describe and this gives us our um you
know describe gives us our basic uh data
analytics information we might be
looking for like what is the mean
standard
deviation uh minimum amount maximum
amount we have our first quarter second
quarter and third quarter um numbers
also in there uh so you can get a quick
look at a glance describing the data or
descriptive analysis and even though we
have our quantile information in here
we're going to dig a little deeper into
that uh we're going to calculate the
quantile for each variable uh we're
going to look at a number of things for
each variable we'll see right here q1 uh
we can simply do the quantile
.25% which should match our 25% up here
and we're going to be looking at the Min
the max um and we're just going to do
this is basically we're breaking this
down for each uh different variable in
there one of the things is kind of fun
to do uh we're going to look at that in
just a second let me get put the next
piece of code in here um got clean out
some of our um we're going to drop a
couple thing our um last rows and first
row because those have usually they have
a lot of null values and the first row
is just are titles um so that's
important it's important to drop those
rows in here and so this right here as
we look at our different
quantiles again it's it's the same you
we're still looking at the 25 quantile
here we're going to do a little bit more
with this um so now that we've cleared
off our first and last rows we're going
to go ahead and go through all of our
columns and this way we can look at each
uh column individually and so we'll just
create a q1 Q3 min max uh Min IQR Max
IQR and calculate the quantile of I of
df2 we're basically doing uh the math
that they did up here but we're
splitting it apart um that's all this is
and this happens a lot because you might
want to look at individual uh if this
was my own project I would probably
spend days and days going through what
these different values mean one of the
biggest data science uh things we can
look look at that's
important is uh use your use your common
sense you know if you're looking at this
data and it doesn't make sense and you
go back in there and you're like wait a
minute what the heck did I just do at
that point you probably should go back
and double check what you have going on
now uh we're looking at this and you can
see right here here's our attribute for
our 03 so we've broken it down uh we
have our q1 5.88 Q3 10.37 if we go back
up here here's our 58 we've uh rounded
it off 10.37 is in
there so we've basically done the same
math uh just split it up we have our
minimum and our Max IQR and that's
computed uh let's see where is it here
we go uh q1 minus 1.5 * IQR and the IQR
is your Q3 minus q1 so that's the
difference between our two different
quarters and this is all uh data science
um as far as a hard math we're really
not we're actually trying to focus on
carass and tensor flow you still got to
go through all this stuff I told you 80%
of your programming is going through and
understanding what the heck uh happened
here what's going on what does this data
mean and so when we looking in that
we're going to go ahead and say hey
um we've computed these numbers and the
reason we've computed these numbers is
if you take the minimum value and it's
less than your Min minimum IQR
uh that means something's going wrong
there and they usually in this case is
going to show us an outlier uh so we
want to go ahead and find the minimum
value if it's less than the minimum
minimum IQR it's an outlier and if the
max value is greater than the uh Max IQR
uh we have an outlier and that's all
this is doing low outlier is found uh
minimum value High outlier is found
really important actually outliers are
almost everything in data sometimes
sometimes you do this project just to
find the outliers cuz you want to know
uh crime detection what are we looking
for we're looking for the outliers what
doesn't fit a normal business deal and
then we'll go ahead and throw in um just
threw in a lot of code oh my goodness uh
so we have if your max is greater than
IQR print outlier is found what we want
to do is we want to start cleaning up
these outliers and so we want to convert
uh we'll do create a convert Nan x max
IQR equals Max _ IQR Min IQR equal Min
IQR
so this is just saying this is the data
we're going to send that's all that is
in Python and if x is greater than the
max IQR and X is less than the Min IQR x
equals uh null we're going to set it to
null why because we want to clear these
outliers out of the data now again if
you're doing fraud detection you would
do the opposite you would be cleaning
everything else that's not in that
Series so that you can look at just the
outlier uh and then we're going to
convert the Nan hum again we have x max
IQR is 100% Min IQR is min IQR if x is
greater than Max IQR and X is less than
Min IQR again we're going to return a
null value otherwise it's going to
remain the same value x xals x and you
can see um as we go through the code if
I equals um our huum uh then we go ahead
and do that's the that's a column
specific to humidity that's your h M
column uh then we're going to go ahead
and convert do the run a map on there
and convert the
nonm uh you can see here it's this is
just cleanup uh we run we found out that
humidity probably has some weird values
in it uh we have our outliers um that's
all this is and so when we go ahead and
finish this and we take a look at our
outliers and we run this code
here uh we have a low outline lier 2.04
we have a high outlier
99.06 outliers have been
interpolated that means we've given them
a new value uh chances are these days
when you're looking at something like um
these sensors coming in they probably
have a failed sensor in there something
went wrong um that's a kind of thing
that you really don't want to do your
data analysis on uh so that's what we're
doing is we're pulling that out and then
uh converting it over and setting it up
method linear so we interpolate method
linear it's going to fill that data in
based on a linear regression model of
similar data uh same thing with this up
here with the um df2 interpolate that's
what we're doing again this is all data
prep uh we're not actually talking about
tensor flow we're just trying to get all
our data set up correctly so that when
we run it it's not going to cause
problems or have a huge bias
so we've dealt with outliers
specifically in
humidity and again this is one of these
things where when we start running
um we ran through this you can see down
here that we have our um outliers found
high low outliers um migrated them in we
also know there's other issues going on
with this data uh how do we know that
some of us just looking at the data
playing with it until you start
understanding what's going on let's take
the temp value and we're going to go
ahead and and use a logarithmic function
on the temp value and uh it's
interesting because it's like how do you
how do you heck do you even know to use
logarithmic on the temp value that's
domain specific we're talking about
being an expert in air care I'm not an
expert in Air Care um you know it's not
what I go look at I don't look at Air
Care data in fact this is probably the
first Air Care data setup I've looked at
but the experts come in there and they
come to you and say hey in data science
um this is a um exponentially VAR
variable on here so we need to go ahead
and do um transform it and use a
logarithmic scale on
that so at that point that would be
coming from your um uh data here we go
data science programmer overview does a
lot of stuff connecting the database and
connecting in with the experts data
analytics a lot of times you're talking
about somebody who is a data analysis
might be all the way usually a PhD level
data science programming level
interfaces database manager that's going
to be the person who's your admin
working on it so when we're looking at
this we're looking at uh something
they've sent to me and they said hey
domain Air Care this needs to be this is
a skew because the data just goes up
exponentially and affects everything
else and we'll go ahead and take that
data um let me just go ahead and run
this
just for another quick look at it um we
have our uh uh we'll do a distribution
DF we'll create another data frame from
the temp values and then from a data set
from the log temp so we can put them
side by side and we'll just go ahead and
do a quick histogram this is kind of
nice plot a figure figure size here's
our PLT from matplot library uh and then
we'll just do a distribution umore DF
there's our data frames this is nice
because it just integrates the histogram
right into pandas love pandas and this
is a chart you would send back to your
data analysist and say hey is this what
you wanted this is how the data is
converting on here as a data science
scientist the first thing I note is
we've gone from a
10230 scales at 2.5 3.0 3.5 scale um and
the data itself has kind of been um uh
adjusted a little bit based on some kind
of a skew on there so let's jump into
we're getting a little closer to
actually doing our uh um carass on here
we'll go ahead and split our data up um
and this of course is any good data
scientists you want to have a training
set and a test Set uh and we'll go ahead
and do the train
size we're going to use 75% of the data
make sure it's an integer don't want to
take a slice as a float value give you a
nice error uh and we'll have our train
size is 75% and the test size is going
to be um of course the uh train size
minus the length of the data set and
then we can simply do train comma test
here's our data set which is going to be
the train size the test size uh and then
if we go and print this let me just go
and run this we can see how these values
um split it's a nice split of 1298 and
then 433 points of value that are going
to be for our um setup on here and if
you remember we're specifically looking
at the data set where did we create that
data set from
um that was from up here that what we
called the uh logarithmic um value of
the temp uh that's where the data set
came from so we're looking at just that
column with this train size and the test
with the train and test data set here
and let's go ahead and do a converted an
array of values into a data set Matrix
we're going to create a little um setup
in here we create our data set our data
set's going to come in we're going to do
a look back of one so we're going to
look back one piece of data going
backward and we have our data X and our
data y for ION range length of data set
look back minus one uh this is creating
let me just go ahead and run this
actually the best way to do this is to
go ahead and create this data and take a
look at the shape of it uh let me go ahe
and just put that code in here so we're
going to do a look back one here's our
train X our train Y and it's going to be
adding the data on there and then when
we come up here and we take a look at
the shape there we go go um and we run
this piece of code here we look at the
shape on this and we have uh a new
slightly different change on here but we
have a shape of X 1296 comma 1 shape of
Y train y Test X test Y and so what
we're looking at is that um the X comes
in and we're only having a single value
out uh we want to predict what the next
one is that's what this little piece of
code is here for for what are we looking
for well we want to look back one that's
the um what we're going to train the
data with is yesterday's data yesterday
says Hey the humidity was at 97% what
should today's humidity be at if it's
97% yesterday is it going to go up or is
it going to go down today if 97 does it
go up to 100 what's going on there uh
and so our we're looking forward to the
next piece of data which says Hey
tomorrow's is going to you know today's
humidity is this this is what tomorrow's
humidity is going to be that's all that
is all that is is stacking our data so
that U our Y is basically x + one or X
could be y minus
one and then a couple things to note is
our X data um we're only dealing with
the one column but you need to have it
in a shape that has it by The Columns so
you have the two different numbers and
since we're doing just a single point of
data we have and you'll see with the
train y we don't need to have the extra
shape on here now this is going to run
into a problem uh and the reason is is
that we have what they call a Time
step and the time step is that long-term
short-term memory layer uh so we're
going to add another reshape on here let
me just go down here and put it into the
next cell and so we want to reshape the
input uh array in the form of sample
time step features we're only looking at
one
feature and I mean this is one of those
things when you're playing with this
you're like why am I getting an error in
the numpy array why is this giving me
something weird going on uh so we're
going to do is we're going to add one
more uh level on here instead of being
12991 we want to go one
more and when they put the code together
in the back you can see we kept the same
shape the
1299 uh we added the one dimension and
then we have our train X shape one um
and this could have depends again on how
far back in the long short-term memory
you want to go that is what that piece
of code is for and that reshape is and
you can see the new shape is now one uh$
12991 uh versus the
12991 and then the other part of the
shape
43211 again this is our TR our xn and of
course our test X and then our Y is just
a single column because we're just doing
one output that we're looking for so now
we've done our
80% um you know that's all the the
reading all the code reform in our data
um bringing it in now we want to go
ahead and do the fun part which is we're
going to go ahead and create and fit the
lstm neural network uh and if we're
going to do that the first thing we need
is we're going to need to go ahead and
create a model and we'll do the
sequential model and if you remember
sequential means it just goes in order
uh that means we have if you have two
layers the layers go from layer one to
Layer Two or layer zero to layer one
this is different than functional uh
functional allows you to split the data
and run two completely separate models
and then bring them back together we're
doing just sequential on here and then
we decided to do the long shortterm
memory uh and we have our input shape uh
which it comes in again this is what all
this switching was we could have easily
made this one two three or four going
back as far as the uh in number on there
we just stuck to going back
one and it's always a good idea when you
get to this point where the heck is this
model coming from um what kind of models
do we have available and uh there's Let
Me Go a and put the next model in there
uh cuz we're going to do two models and
the next model is going to go ahead and
we're going to do dent so we have model
equals sequential and then we're going
to add the lstm model and then we're
going to add a den model and if you
remember from the very top of our
code where we did the Imports oops here
we go our cross this is it right here
here's our importing a dense model and
here's our importing an
lstm now just about every tensor flow
model uses dents uh your D model is your
basic forward propagation uh reverse
propagation error or it does reverse
propagation to program the model uh so
any of your neural networks you've
already looked at that uh looks and says
here's the error and sends the error
backwards that's what this is the long
short-term memory is a little different
the real question that we want to look
at right now is where do you find these
models what what kind of models do you
have available and so for that let's go
to the carass website U which is the
cars.io if you go under API layers and I
always have to do a search just search
for carass uh API layers it'll open up
and you can see we have um your base
layers right here class trainable
weights all kinds of stuff like there
your activation uh so a lot of your
layers you can switch how it activates
uh railu which is like your smaller
arrays or if you're doing convolutional
neural networks the convolution usually
uses a Ru um your sigmoid um all the way
up to soft Max soft plus all these
different choices as far as how those
are set up and what we want to do is we
want to go ahead and if you scroll down
here you'll see your core layers and
here is your dense layer uh so you have
an input object your dense layer your
activation layer embedding layer this is
your your kind of your one setup on
there that's most common uh
convolutional neural networks or
convolutional layers these are like for
doing uh image categorizing uh so trying
to find objects in a picture that kind
of thing uh we have pooling layers so as
you have the layers come together um
usually bring them down into uh single
layer although you can still do like
Global Max pulling 3D and there is just
I mean this list just goes on and on uh
there's all kinds of different things
hidden in here as far as what you can do
and it changes you know go in here and
you just have to do a search for what
you're looking for uh and figure out
what's going to work best for you as far
as what project you're working on uh
long short-term memory is a big one
because this is when we start talking
about text uh what if someone says the
what comes after the uh The Cat in the
Hat little kid's book there um starts
programming it and so you really want to
know not only um what's going on but
it's going to be something that has a
history the history behind it tells you
what the next one coming up is now once
we've built all our different you know
we built our model we've added our
different layers we went in there um
play with it remember if you're in
functional you can actually link these
layers together and they Branch out and
come back together if you do a uh um the
sub setup then you can create your own
different model you can embed a model in
there that might be coming linear
regression you can embed a linear
regression model uh as part of your
functional split and then have that come
back together with other things so we're
going to go ahead and compile your model
this brings everything together we're
going to put in what the loss is which
we'll use the mean squared error uh and
we'll go and use the atom Optimizer
clearly there's a lot of choices on here
depending on what you're doing and just
like uh any of these uh different
prediction models if you've been doing
any uh um s kit from python uh you'll
recognize that we have to then fit the
model uh so what are we doing in here
we're going to T send in our train X our
train y y um we're going to decide how
many epics we're going to run it through
500 is probably a lot for this um I'm
guessing it' probably be about two or
300 probably do just fine our batch
size so how many different uh when you
process it this is a math behind it if
you're in data analytics um you might
try to know what this number is as a
data scientist where I haven't had the
PHD level math that says this is why you
want to use this particular batch size
you kind of play with this number a
little bit
um you can dig deeper into the math see
how it affects the results uh depending
on what you're doing and there's a
number of other settings on here uh we
did verbos 2 I'd have to actually look
that up to tell you what verbos means I
think that's actually the default on
there if I remember correctly uh there's
a lot of different settings when you go
to fit it the big ones are your epic and
your batch size those are what we're
looking for and so we're going to go
ahead and run
this and this is going to take a few
minutes to run because it's going
through
um 500 times through all the data so if
you have a huge data set this is the
point where you're kind of wondering oh
my gosh is this going to finish tomorrow
um if I'm running this on a single
machine and I have a terap terabyte of
data uh going into
it if this is my personal computer and
I'm running a terabyte of data into this
um you know this is running rather
quickly through all 500 iterations uh
but you know at a terabyte of data we're
talking something closer to days week um
you know even
with uh 3.5 gaher machine in in eight
cores it's still going to take a long
time to go through a full terabyte of
data and then we want to start looking
at putting it into some other framework
like spark or something that will P the
process on there more across multiple um
processors and multiple
computers and if we scroll all the way
down to the bottom you're going to see
here's our square mean error
0.0088 if we scroll way up you'll see it
kind of oscillates between 0888 and
0889 it's right around two 2 250 where
you start seeing that oscillation where
it's really not going anywhere so we
really didn't need to go through a full
500 epics uh you know if you're
retraining the stuff over and over again
it's kind of good to note where that
error zone is so you don't have to do
all the extra processing of course if
you're going to build a model
uh we want to go ahead and run a
prediction on it so let's go ahead and
make our prediction and remember we have
our training test set in our test set or
we have the train X and the train y for
training it or train predict and then we
have our test X and our test y going in
there uh so we can test to see how good
it did uh and when we come in here we
have um uh you'll see right here we go
ahead and do our train predict equals uh
model predict train X and Test predict
model predict Test X why would we want
to run the prediction on trainx well
it's not 100% on its prediction we know
it has a certain amount of error and we
want to compare the error we have on
what we programmed it with with the
error we get when we run it on new data
that's never seen the model's never seen
before and one of the things we can do
uh we go ahead and invert the
predictions uh this helps us level it
off a little bit more
um get rid of some of our bias we have
train predict equals an NP um
exponential M1 the train predict and
then train y equals the exponential M1
for train Y and then we do the again
that with train test predict and test
y um again reformatting the data so that
we can it all matches and then we want
to go ahead and calculate the root mean
square error so we have our train score
uh which is your math square root times
the mean square root error train uh Y
and train predict and again we're just
um uh this is just feeding the data
through so we can compare it and the
same thing with the test and let's take
a look at that because really the code
makes sense if you're going through it
line by line you can see what we're
doing but the answer really helps to
zoom in uh so we have a train score
which is
2.40 of our root mean square error and
we have a test score of 3.16 of the root
mean square error if these were reversed
if our test score is better than our
training score then we've overtrained
something's really wrong at that point
you got to go back and figure out what
you did wrong uh because you should
never have a better result on your test
data than you do on your training data
and that's why we put them both through
that's why we look at the error for both
the training and the testing when you're
going out and quoting your um U
publishing this you're saying hey how
good is my model it's the test score
that you're showing people this is what
it did on my test data that the model
had never seen before this is how good
my model is and a lot of times you
actually want to put together like a
little formal code um where we actually
want to print that out and if we print
that out you can see down here um test
prediction standard deviation of data
set 3.16 is less than 4 uh 40 I have to
go back and we're up here if you
remember we did the square means error
this is standard d deviation that's why
these numbers are different it's saying
the same thing that we just talked about
uh 3.16 is less than 4.40 model is good
enough we're saying hey this is this
model is valid we have a valid model
here so we can go ahead and go with that
uh and along with putting a formal print
out of there um we want to go ahead and
plot what's going
on uh and this we just want to pretty
graft here so that people can see what's
going on when I walk into a meeting and
I'm dealing with a number number of
people they really don't want these
numbers they don't want to say hey
what's I mean standard deviation unless
you know what statistics are um you
might be dealing with a number of
different departments head of celles
might not works with standard deviation
or have any idea what that really means
number-wise and so at this point we
really want to put it in a graph so we
have something to display and with
displaying you got to remember that
we're looking at uh the data today going
into it and what's going to happen
tomorrow so let's take a quick look at
this uh we're going going to go ahead
and shift the train predictions for
plotting uh we have our train predict
plot uh NP empty like data set uh train
predict
plot uh set it up with null
values you know it's just kind of it's
kind of a weird thing where we we're
creating the um the data groups as we
like them and then putting the data in
there is what's going on here uh so we
have our train predict plot uh which is
going to be our look back our link
plus look back we're just is going to
equal train uh train predict so we're
creating this basically we're taking
this and we're dumping the train predict
into it so now we have our nice train
predict
plot and then we have the shift test
predictions for the plotting uh we're
going to continue more of that oops
looks like I put it in here double no
it's just uh yeah they put it in here
double um didn't mean to do that we
really only need to do it once oh here
we go um this is where the problem was
is because this is the test predict so
we have our training prediction we're
doing the shift on here and then the
test predict we're going to look at that
same thing we're just creating those two
uh data sets uh test predict plot length
prediction set up on there and then
we're going to go through the plotting
the original data set and the
predictions uh so we have a Time axis
always nice to have your time set on
there um set that to the time array time
axes lap all this is setting up the time
variable for the bottom and then we have
a lot of stuff going on here as far as
setting up our
figure let's go ahead and run that and
then we'll break it
down we have on here uh our main plot we
have two different plots going on here
uh the ispu going up and the data and
the ispu here with all these different
settings on
it and so when we look at this we have
our um ax1 that's the main plot I mean
our ax that's the main plot and we have
our ax1 which is the secondary plot over
here so we're doing a figure PLT or PLT
do figure and we're going to dump those
two graphs on there um and so we take
and if you go through the code piece by
piece uh which we're not going to do
we're going to do the um the data set
here um exponential ential reverse
exponential so it looks correctly we're
going to label it the original data set
um we're going to plot the train predict
plot that's what we just created we're
going to make that orange and we'll
label it train prediction uh test
prediction plot we're going to make that
red and label it test prediction and so
forth um set our ticks up this's
actually just put tick um time axis gets
its ticks the little little marks they
going along the axes that kind of thing
and let's take a look can see what these
graphs look
like and these are just kind of fun you
know when you show up into a meeting and
this is the final output and you say hey
this is what we're looking at um here's
our original data in blue here's our
training prediction um you can see that
it trains pretty close to what the data
is up there I would also probably put a
um like a little little time stamp and
do just right before and right after
where we go from uh train to test
prediction and you can see with the test
prediction the data comes in in
red um and then you can also see what
the original data set looked like behind
it and how it differs and then we can
just isolate that here on the right
that's all this is um is just the test
prediction on the right uh and it's you
know there's you'll see with the
original data set there's a lot of Peaks
were missing and a lot of lows were
missing but as far as the actual test
prediction it's pretty does pretty good
it's pretty right on you can get a good
idea what to expect for your your ispu
and so from this we would probably
publish it and say hey this is um what
you expect and this is our area of this
is a range of error um that's the kind
of thing i' put out on a daily basis
maybe we predict the sales are going to
be this or maybe weekly so you kind of
get a nice you kind of flatten the um
data coming out and you say hey this is
what we're looking at the big takeaway
from this is that we're working with let
me go back up here oops oh too far there
we go uh
is this model here this is what this is
all about we worked through all of those
pieces um all the tensor flows and that
is to build this sequential model and
we're only putting in the two layers
this can get pretty complicated if you
get too complicated it never um it it
never verges into a usable model uh so
if you have like 30 different layers in
here there's a good chance you might
crash it kind of thing um so don't go
too haywire on that and that you kind of
learn as you go again it's domain
knowledge um and also starting to
understand all these different layers
and what they
mean the data analytics behind those
layers um is something that your data
analysis uh professional would come in
and say this is what we want to try but
I tell you as a data scientist um a lot
of these basic setups are common and I
don't know how many times uh working
with somebody and they're like oh my
gosh if I only did a tangent H instead
of a Ru activation I worked for two
weeks to figure that out well as a data
science I can uh run it through the
model in you know five minutes instead
of spending two weeks doing the the math
behind it um so that's one of the
advantages of data scientists is we do
it from programming side and a data
analytics is going to look for it how
does it work in math and this is really
the core right here of tensor flow and
coros is being able to build your data
model quickly and efficiently and of
course uh with any data sign science
putting out a pretty graph so that your
shareholders um again we want to take
and um reduce the information down to
something people can look at and say oh
that's what's going on they can see
stuff what's going on as as far as the
dates and the change in the ispu if you
are one of the aspiring data scientists
looking for online training and
graduating from the best universities or
a professional who elicits to switch
careers with data analytics by learning
from the experts then try giving a short
to Simply learn skel Tech post graduate
program in data science in collaboration
with IBM the link is mentioned in the
description box and you should navigate
to the homepage where you can find a
complete overview of the program being
offered hello and welcome to this
session on
tblo in this session we will start with
certain exercises which we will perform
in tblo in order to understand some
basic
concepts now in order to learn tblo the
basic first step is to import import a
sample data so in our case what we have
done is we have imported a sample super
store which is an Excel format a sample
superstore. excl which has three
worksheets in it orders people and
returns so by importing this data into
tblue first of all we will create
relationships between these sheets in
order to identify who all have placed or
ERS and how many people have returned
the orders we will do some analysis on
the orders placed by certain set of
people and Order returned by certain set
of people now as we have imported uh the
sheet we will make certain joins so the
first step is to drag the orders table
the order sheet on the relationship ship
canvas here okay and you can see the
data sample data the first 100 rows over
here okay then now we need to create an
inner join with people's table between
order and people okay so if you
see it has automatically detected the
field names on which the inner joint has
to be be created so on the order side
you
have region and on the right hand side
which is the people data you have also a
region okay so both these columns are
common and that's how we have made a
join between orders and people
data so if I close this box and go and
check the people's data
open so see the region and the person
these two columns from the people table
have now been joined with the orders
table right so it means that these are
the orders in a particular region which
has been placed by in this region
let me show you the sample Superstore
Excel file now this is the structure of
the file you have a sample list of
transactions basically the orders which
are placed by
customers across multiple regions south
west of USA South Region west region
then you have a list of order IDs which
have been returned so
basically the order ID in the returns
sheet matches with those orders in the
orders table and then you have the
people uh sheet in which you have
region and a person associated with that
region the salesperson associated with
that region okay so basically when we
are combining joining orders with people
we are joining
that which
orders belongs to which region and who's
the salesperson associated with it so
what we have done over here is we have
made a inner join means all the orders
should belong to particular region and
that region is in the people's sheet and
then in the second step now we will make
a left join between returns and orders
not in a join we'll make a left join
between returns and orders and we will
make a join using the order
ID okay so just edit
this click on left and select order ID
as the join column now what does left
join means left join means is that
consider all the orders from the orders
table and only consider the orders from
the returns table which have data means
which are returned otherwise show null
for the order IDs which are not returned
so if you see this is the these are the
two columns from the returns table and
these are null because this is relevant
to the order IDs which are not returned
okay which has been accepted by the
customer but these are the orders for
which you see data in the returned and
Order column it means that these have
been
return now with these joins in place
please save your book and now we have
our relations created in the
um in the table now we are ready to
create certain reports and extract
certain kpis using this relationship
model now we'll move to sheet
one
okay and first we will play
Place
State and person on the rose sheet
okay then I'll go to
my numbers
and put the profit or the so per state
per person how much profit I am making
as a company okay this is my goal to
check now sought by highest to lowest so
California is giving me the maximum
profit of 76,6 381 then New York then
Washington so this is the sorted order
in which I
have listed my profit in descending
order now I can also check
what are the number of
orders
placed and check the distinct
count so out of 120 out of the total
orders of 127 okay so this is the the
number of total number of orders which
have been returned for California is
127 it's 16 29 so the sorted order is as
per the revenue as per the profit and
this is the details of the orders which
have been returned per state so if you
see for Connecticut for a canas there
are zero returns
so you can also extract
data table to refresh his orders and
identify new rows using order date so as
in when new data is being added you
can refresh it now say
extract and now you can save this
information
profit by
state and click save so this is the
extraction of this particular report
which is possible in TBL Lo
so this is the first exercise which we
have completed
for reviewing and analyzing the profit
per state highest to lowest and within
that per state what are the number of
orders which have been returned by all
the customers the distinct count of
order IDs which have been returned
now let's start our second exercise on
creating calculated fields in tblo now
in this exercise we will be doing
certain activities like we will be
creating a set to show the states which
have more than 100 customers then we
will be creating a calculated field to
show an average sales per customer
okay then we will create a calculated
field to show the sales goals and then
show emerging and developing stage so
these are the four kpis which we have to
derive now the first thing we have our
sample super store data already imported
and the relationships created in a join
with people and left join with
returns now we have our sheet two in in
which we will create the uh States a
list of states which has more than 100
customers so what we have to do is we
have to click right click on the
customer name and click create
set
okay now we have to give the name as
states with 100 plus
customers and then go to the condition
tab
select by field and then apply condition
as count of customer
name greater than equal
to 100 and click
okay now we have this set created states
with 100 plus
customers now to determine average sales
by customer we have to now create a
calculated field so go to the analysis
and click on create calculate
field
okay now name it as
average sales per
customer and now we will say average
we will use
a okay so we are saying that per
customer we are using a level of
definition function include which means
that per customer what is my average
sales right we've already used a
function aggregated function called
average so we are saying per customer
give me the total and then give me the
average per customer so we're going to
click
okay now create another calculated field
you can also create from here and name
is as name it as sales goal
now in this we are going to type the
formula if
minimum
states with 100 plus customers equal to
true
then sum of
sales into
1.3 else
average sales per customer
into 100 so we we are saying
that if the customer belongs to the set
of states with 100 plus customers then
the sales Target should be 1.3 times the
actual sales as of today else it should
be 100% of the average sales per
customer now let's create another
calculated field which we call as
emerging or developing
state if distinct
count of customer
name is greater than equal to
100 then
the state is tagged as developing
State else it is called
as emerging
State okay so we have now three
calculated Fields average sales per
customer emerging or developing State
and sales goals now we will use this in
our
reporting so we will drag sales goal
Under The
Columns and then I'll drop my state so
now this is the statewise sales goal
depending whether the state has 100 plus
customers or
not then add your customer name
make the measure as count
distinct and make it as
distret okay so if you see
this we have the count of customers per
state and the the sales goal for that
particular state
and
now I'll put my sum of sales the total
sales which I want per which is there
per
state now go to show me and
select this particular chart
bullet
graph now to bring sales goals to column
right click on the sales AIS and select
swap reference line fields
now from the left hand panel drag and
drop emerging or developing State on the
color
panel okay so emerging state is the
orange one and the developing state is
the blue
one and save the sheet as developing and
the Emer
States so if you see this it's an
emerging State because its count is less
than the customer count is less than
100 its sales goal is
57384 but the actual sales is 1
19511
okay so now this is a developing State
its count is greater than equal to 100
and and its sales goal and its sales is
exactly the same it matches so that's
why you are saying the bar and the blue
bar is ending exactly where the vertical
bar
is so what we are trying to depict is
that whether the uh state is going
Beyond its Target sales goal or it's
behind it and you can see that using
this particular vertical bar like for
example Michigan its sales goal is 71
952 but its actual sales is
76270 average sales so that's why it is
be above its
Target and it's a developing
State because it has more than 100
customers so you can even
sort by the count of
the uh customers higher to lower so all
your developing state will group from at
the top and the emerging States Will
Group at the
bottom or you can sort
by the sales
go so the orange bar is the sales goal
or the blue bar so depending what sales
goal is
being derived for each state first let's
understand what is MySQL workbench or
MySQL
workbench so MySQL workbench is a
graphical tool for working with myql
servers and databases you may use the
MySQL workbench for Server
Administration for creating entity
relationship diagrams and for running
SQL
queries MySQL server is crossplatform
meaning that it can run on number of
different platforms such as Windows
Linux Mac OS and
others now we will see the MySQL
workbench functionalities so first we
have SQL development or SQL development
MySQL workbench enables you to create
and manage connections to database
servers along with enabling you to
configure connection parameters MySQL
workbench provides the capability to
execute SQL queries on the database
connections using the built-in SQL
editor next we have data modeling the
data modeling functionality of MySQL
allows you to create models of your
database schema graphically reverse and
forward engineer between a schema and a
live database and edit all aspects of
your database using the comprehensive
table editor the table editor provides
easy to ous functionalities for editing
tables columns indexes triggers
partitioning options inserts and
privileges routines as well as views
the third functionality is server
Administration it enables you to
administer MySQL server instances by
administering users performing backup
and Recovery inspecting audit data
viewing database health and monitoring
the MySQL server
performance fourth we have data
migration now this functionality of
MySQL workbench allows you to migrate
from Microsoft SQL Server Microsoft
Access DB cbase SQL SQL anywhere post
SQL and other relational database
management system tables objects and
data to mySQL migration also supports
migrating from earlier versions of MySQL
to the latest
releases and finally we have MySQL
Enterprise support the MySQL workbench
has support for Enterprise products such
as MySQL Enterprise backup MySQL
firewall and MySQL
audit
now let's have a look at the my SQ
workbench additions available to
us first we have the community addition
so this workbench Edition is open-
sourced and can be downloaded free of
cost it came under the GPL license and
is supported by a huge community of
developers then we have the standard
edition of MySQL
workbench the standard edition of MySQL
workbench is a commercial Edition that
provides the capability to deliver high
performance and scalable online
transaction processing that is popularly
known as oltp
applications so the last Edition that is
the Enterprise Edition we have is the
commercial Edition that includes a set
of advanced features management tools
and Technical Support to achieve the
highest scalability security reliability
and
uptime now this addition also reduces
the risk cost complexity in the
development deployment and managing
MySQL
applications now with this theoretical
knowledge let's go ahead and see how to
download and install MySQL workbench on
Windows
okay so I've searched for MySQL download
on Google and let me go ahead and click
on the first link which is from
www.myql.com
this will lead us to the downloads page
I've opened in a new tab and in the
download section if I scroll down you'll
find something called as MySQL Community
GPL downloads this GPL stands for
general public license which is for free
so we will install the community edition
of MySQL
workbench if I click on this it will
take you to another page where you have
all the MySQL Community downloads and
here you will find something called as
MySQL installer for Windows I'll click
on
this and here you can see we have the
MySQL installer 8.0 version please go
ahead and install from this link you can
just click on
download now we already have a video
published on how to install MySQL on
windows so please click on the above
link to know the steps for installation
let me show you the video so this is the
video I'm talking about which is already
published on our channel that is install
MySQL server and workbench on Windows
all right now once the installation is
complete go ahead and launch the MySQL
workbench so here in the search bar if I
just type MySQL you can see here I have
the MySQL workbench 8.0 version
installed I'll open
this so this is how the
interface looks like it says welcome to
mySQL workbench it tells you what MySQL
is and here I have already created a
MySQL connection now let me show you how
to create a new connection I'll just
click on this plus button and it will
open set up new
connection now here you can give a
connection name let me
type my name as MySQL unor
demo now the connection method you can
keep it as standard TCP
IP now the host name will be the default
port number should be
3306 the username I have kept it as root
because this is what I gave while
installing
MySQL if you want you can also set the
password if I click on this you can
enter the password I'll suggest you to
give a strong password I'll click on
okay and let's try to test the
connection you can see here it says
successfully made the MySQL connection
you have the host the port number user
and it says a successful MySQL
connection was made with the parameters
defined with this connection let's click
on okay and I'll click on okay again you
can see here we have made a new
connection which says MySQL demo I'll
click on this and it will open my query
editor so let me open my MySQL workbench
where we will perform some Hands-On
demonstrations and look at how to use
MySQL functions write different commands
and we'll see the
results so here on the search bar I'll
type
MySQL so it says MySQL workbench 8.2
version let me open
it okay you can see I have a local
instance connection I'll connect to this
and I'll enter the password so this is
how my MySQL workbench interface looks
like let me just drag this to the top
okay now the white workspace that you
see here is called the SQL visual editor
and
Below you will have the result pin and
the history output window I'll show you
once I run a query and on the right hand
side you can see we have a help pin or a
help window so if you want to check the
documentation or the usage of a certain
command you can just type here and it
will give you the documentation as you
can see I just choose one command that
is Select here and it has returned me
the syntax and what select statement
really does
all right and here on the left side you
can see we have something called
Administration and schemas these are the
different databases that I've already
created so for this demo we will start
by creating a new database so the
command I'll write is
create database if I hit tab it will
autoc complete and the database name
I'll give is SQL
workbench give a semicolon
I'll select this and you see this
thunder button if I click on this it
will execute my
query okay you can see here it has
returned me one R affected and my query
was successful all right now to move
inside this database I'll use the
command called use SQL workbench which
is my database
name let's run this
you can see we have successfully entered
into our database now we can create any
tables perform any calculations
operations data manipulations retrieve
data fetch data insert data and lot of
other
operations now under the schema let me
just refresh this so that you can see
our
new database this is our database you
can see SQL workbench currently we
haven't created any tables
if I create a table it will show here
you can see here views to procedures
functions and other things so first
let's get started by looking at some of
the numeric functions that are available
in
MySQL so I'm giving a comment by typing
shift and then my comment that is
numeric
functions first and foremost we'll see
how to use the absolute function in
MySQL
so for using the absolute function I'll
write select
abs and inside this ABS function I'll
pass in a value let's say
22 now if I run
this it gives me a result that is 22
since the absolute value of 22 is 22
because 22 we have passed in as a
positive value now let's say I'll pass
in minus 22 and let's run
it there you go you can see here my
absolute function has dropped the
negative sign and has return returned me
a positive value which is 22 you can try
for other values let's say I'll select
minus 50 if I run this it returns me 50
all
right
next we'll see how the mod function
works so the mod function Returns the
remainder of
division so if I write
select
mod and I want to divide 10 by
4 let's run this and see the result you
can see it has returned Me 2 because the
remainder for 10 ided 4 is 2 you can
also give an alas name to
this I'll write as
remainder the reason why I'm giving an
alas name is
because the result we have got if you
see the column this is not very readable
now if I run this you can see the
difference there you go now the output
is much readable than the previous
one okay so let's say I want to find the
mod of another division 15 comma
3 as remainder again
I'll run it again you can see 15 divided
by 3 the remainder is zero let's try for
one more let's say
50.6 divided by 2 you can already guess
the result it should be 6 there you go
you can see this is
my
remainder now as I
said the below window is called the
history output window or the Pan and
this is called the query result
pin which is part of the MySQL
workbench let me just drag this a bit
okay now moving on to the next numeric
function we will explore the power
function now suppose I write
select power 4 raised to 2 let's see the
result I'll run it it gives me
16 which is
correct let's
see I'll write select power 10 to
3 you can already think of the result it
should be
1,00
okay now like how
you use the power function similarly you
can use the square root function in
MySQL the square root function is
written as sqrt
so you can get the square root of a
number by using the sqrt function let's
say I want to find the square root of
4 if I run this it gives me two which is
the square root of 4 I Ed it here itself
let's say I want to find the square root
of
144 I run this it gives me 12 which is
the square root of
144 now moving
ahead let's say you want to find the
greatest number within a range of
numbers you can use the greatest
function that is available in
MySQL I'll write
select greatest I'll give a range of
numbers let's say 2 5
18
6 and let's say
12 we give a semicolon and let me run it
it returns me 18 because 18 is the
largest the greatest number in my range
of numbers similarly you can also find
the least number for that you can use
the least function this will return me
the minimum number present in my range
of numbers in this case it is two all
right there's another function that is
available in MySQL is truncate so the
truncate function truncates a number to
the specified number of decimal places
so suppose I write select
truncate
22.8
N7 comma
1 let's see the
output okay you can see here so the
truncate function has returned us 22.8
and it has truncated the rest of the
decimal
places I want to Tran it up to two
decimal places let's see the result
again you can see here it has given is
22.8
n next we have another function called
round so the round function in MySQL is
used to round a number to a specified
number of decimal places if no specified
number of decimal places is provided to
round of function the round of function
rounds the number to the nearest integer
so let's see a few
examples suppose I want to round the
value 22.8 N7
this time I'm not
passing the specified number of decimal
places we'll see the
result it has returned me 23 which is
the next highest integer suppose you
want
to round it to two decimal
places and give a comma and then say
two I'll run it it has given us
22.9 so this 89 it has converted to 9 0
similarly if you want
to round the number to one decimal place
it will give you
22.9 all
right
now let's create a students table and
store it in our MySQL
workbench so I'll just give
a comment called student table or
students table all right
so the way to create a table in a
database in MySQL workbench is you write
create table followed by the table name
that you want in this case I'm going to
create a students table then you start
with parenthesis
and give the columns that you want in
your table so first I want the student
ID you can think of this as the student
rule
number give a space and then give the
data type of the column now student ID
should ideally be of type integer I'm
assigning this as primary
key so the primary key will uniquely
identify each record in a
table and it cannot have any duplicate
values so ideally if you think all the
students in a school will have a unique
role
number no two students will have the
same role
number next column I want is the student
name now the name is of type
varar and I'll give the length as let's
say
25 then I want the age of the student
age is also of type
integer and I'll write in the same line
then I need the
gender gender of the student should be
of type character and the size I'll give
as
one then we have DOA which stands for
date of admission and it is of type
date then we have the city to which the
student belongs to then city is also of
type
Vare all right you close the bracket and
give a semicolon let's run
this there you go here you can see in
the history window it says
the table was created successfully now
let's just refresh this again so that
you can see the table here under MySQL
workbench database under tables you can
see we have a students table let me
expand this further so that you can see
the columns that we have there you go we
have the student ID the student name age
gender date of admission and the
city
okay now it's time to insert a few
records to this table for that you need
to write the insert statement now I'm
going to open a notepad where I have
already written the insert statement so
let me just copy paste it here okay so
here on my notepad I have my insert
statement already written I'll just copy
this and I'll paste it on my
workspace okay if I go to the top you
can see this is how my insert into
command look
like you have insert into followed by
the table name which is students then
you have the values keyword and we have
the set of Records so the
first value represents the student ID or
the role number then you have the
students's name you have the age you
have the gender then we have the date of
admission and then we have the city to
which the student belongs to all right
now let me go ahead and run my insert
query I'll just select everything and
I'll hit on execute you can see here we
have successfully inserted 20 rows of
information to our
table
now if you want
to check all the records in a
table the SQL command is Select star
from table name that is students
if I run
this this is my output you can see we
have the student ID the student name age
gender date of admission and City if I
scroll down there are total 20 rows of
information in my
table now the star represents you want
to select all the columns in the
students table suppose you want to
return specific columns from the table
you can mention that in the select
statement itself suppose I want to
select the student name the age of the
student and the city to which the
student belongs to so I will write
select store name which is my column
name comma the age comma the
city from my table name that is
students let's run
it you can see the output we have only
three columns display lead in the
result okay now let's see how to use the
wear clause in SQL or MySQL so the wear
Clause helps you filter specific records
based on certain condition or conditions
suppose I want to see the students who
are from Pune so I want to display only
the records of students who are
from Pune City
so the way to do is I'll write
select star
from
students
where
City equal to
Pune now this pun is
a character type so you need to mention
it within single codes let's run
it there you go in the output you can
see there are total four students who
are from from Pune you have Joseph
anubhab AKA and Anand all
right
now let's say you want
to filter only those students who are
from Miami and the gender is male again
you are going to use the wear clause and
along with the wear Clause we will use
the and
operator so I'll
write select star from
students where City equal
to I'll say
Miami this Miami should be within codes
I'll use my and operator and then write
the gender should be equal to M or
male
again M should be within single Cotes
I'll give a semicolon and let's run it
you can see here we have vipul ishan
tanishk and Amit who are from Miami city
and their gender is
male cool now moving ahead suppose you
want to know the students who are either
from City Boston or Chicago in that case
you are going to use an or operator I'll
just copy
this SQL command and I'll paste it here
and I'll Ed it in the same query so this
time we want the students who are from
City Boston or cicago so here instead of
Miami I'll write
Boston I'll use an or clause or an R
operator and instead of gender equal to
male I'll again write or city equal to
Chicago let us run the SQL query and see
what we have so there are total six
students who are either from
City Chicago or from Boston we also have
a female student
Alana all
right moving
further let's say you want to display
the records of the
students who belong to any city rather
than Chicago in that case you're going
to use the not operator so I'll write
select star
from students where not City equal to
Chicago let's run it and see the
result all right you can see total we
have returned 17 rows of information
which means there are total 17 students
who belong to other cities apart from
Chicago if you see the result you won't
find any student who is
from Chicago
City
okay now there's another
important feature of MySQL where you can
group certain rows of information based
on a particular column or
columns so let's see how to use Group by
in MySQL I'll give a
comment called Group
by now in this group by Clause we are
going to find the total number of
students present in each
City so the way to do is I'll write
select I want to select this city and
then I'm going to use the count function
this is an inbuilt MySQL
function I'm going to count the student
IDs I'll give an alas
as total
students from my table that is
students then I'm going to group my
result based on each
City let's run this and see the
output there you go you can see here so
our table has total six cities you have
Pune New York Miami Chicago Detroit and
Boston and here on the right hand side
you have the total students that we just
created using an alias name and this
total students column has the total
number of students from each City so if
you see
here New York has four students Pune has
four students in Miami Chicago we have
have four and three students and in
Detroit we have two students
respectively all
right the next Clause we are going to
see is
having now having and where are kind of
similar but there are certain
differences now for our example we are
going to find the cities where the total
number of students are more than three
if you see the results pin so we have
Pune we have New York and Miami as the
cities where there are more than three
students so let me show you how to
return these three records so I'll write
select
City
comma I'll count the students using the
count function and I'll pass my column
as student
ID and I'll give an alas as total
students from
my table that is
students I'll use
the group by Clause Group by City And
then I'm going to use
having the count of student
ID greater than
three let me run it and show you the
result there you go as a expected we
have Pune New York and Miami where the
total number of students are more than
three let's see how the order by Clause
works so the order by Clause is used
to filter the records based on a
particular order in ascending or
descending order so
suppose see I write select star
from
students and then I'll write order by
let's say
City let me show you the result first
okay you can see the city column has
been ordered
in alphabetical
order so first the city start with b
then we have c d m and then we have n
and p what if I want to order the result
in descending order
so again it will alphabetically sort my
result in descending order of City so on
the top we have Pune because
alphabetically in descending order Pune
starts with p so it appears on the top
then we have New York followed by Miami
Detroit Chicago and
Boston now it's time to explore some
string functions in MySQL
so I have given a comment string
functions first let's say you want to
convert a certain string into upper ke
so I can write
select the function I'll use is upper
and within this function you can pass in
the string let's say I'll write
India if you want you can give an alas
name as let's say uppercase
I'll give a
semicolon and let's run it there you go
so my input was in sentence case and
using the upper function we have
converted everything into upper case
similarly let me just copy
this and I'll show
you if you want to convert a string into
a lower case you can use the lower
function I'll run this you can see the
result everything is in lower case now
of course because I need to change the
alas name to lower
case instead of using lower as the
function there is another function that
MySQL provides which is called the L
case so I'll just edit this and write L
case and let's see I'll write India
in uppercase let's run it it returns me
the same
result
cool moving
on let's let's say you want to find the
length of a string you can use the
character length
function I'll write
select use the function character
length and I'm again going to pass in
my string as India as let's say total
length let's run it this time I'm going
to hit control enter to run my SQL L
command there you go it has given us the
right result which is five because India
has five characters in it now these
functions you can also apply on a table
now let me show you how to do it let's
say we already have the students table
and you want to find
the length
of each of the student names so here you
can pass sore
name and you can give the same alas name
let's say total
length and then you can write from table
name that is
students if I run
this you can see the output it has given
me total 20 rows of information is not
readable actually let me also display
these student names so that we can
compare
their length
all right I'll run this
again and now you can see the result so
Joseph has six characters NES has six
vipul has five anubhab has seven
similarly if you see AE has six Tanis
has seven ragav has six cins has seven
rabada has six so on and so
forth now instead of using this
character length you can also use the
function
car
length it will work the same way let's
see the
result there you go it has given us the
same result you can either use character
length or car
length there's another very interesting
function called concat so the concat
function adds two or more Expressions
together let's say I'll write
select use the function Conca
inate the function is actually concat
and I'm going to pass
in
my string values let's say
India
is in
Asia let's run this and see our result
you can see here it has concatenated
everything let us make it more readable
I'll give a space
in between so that you can read it
clearly now this is much more readable
India is in
Asia and if you want you can give an
alas name as well as let's say
merged there you go now the same concat
operation you can also perform on a
table I'm going to use the same students
table let's say I want to return the
student ID
followed by the student
name and then I am going to
merge the student
name followed by your
space followed by the age of the
student and I can give an alas as let's
say name _
age from my table that is students
let's see how this
works okay you see here the result is
very clear we have the student ID the
student name and the concatenated column
that we created which was name _ each
where we have the student name with a
space followed by the age of the
student if I scroll down you can see the
rest of the
results
cool now moving ahead
let's see how the reverse function Works
in
MySQL so the MySQL reverse function
returns a string with the characters
printed in reverse order so suppose I
write
select reverse I'll use the same string
again let's say I have
India let's run it you will see all the
characters printed in reverse order
again you can perform the same operation
on
a table as well let's say I'll write
select
reverse and I'll pass in the column as
student
name from my table that is
students let's run it it gives you 20
students and all the names have been
printed in reverse
order
okay now let's see what the replace
function does so the replace function
replaces all occur of a substring within
a string within a new substring so let
me show you what I mean I'll write
select
replace I'll pass in my input string
which
is let's say orange is
a
vegetable which is ideally incorrect I'm
purposely writing this so that I can
replace the
word
vegetable with
fruit okay so what this replace function
does is it is going to find where my
word vegetable is within the string my
input string and it is going to replace
my word vegetable with fruit let's run
it and see the output there you go now
this is correct which is Orange is a
fruit
all
right now MySQL also provides some trim
functions you can use the left trim
right trim and just the trim function so
let me show you how this left trim
Works left trim or L trim removes the
leading space characters from a string
passed as an
argument so see I write select I'll use
the left trim function which is L trim
and then I'm going to to purposely give
a few
Spees in the beginning of the string
I'll give a word let's say India and
then I'll give some space after the bird
India and see how the elri works if I
run this it gives me India which is fair
enough but before
that let's first find the length of my
string so I'll use my length function
here and within this function I'm going
to find the length of my string which
has India along with some leading and
trailing spaces I'll paste this here
give a semicolon and I'll run
it okay so the entire string is 17
characters long or the length of the
string is 17 now say I use lrim on my
same string
what it returns me is
India and if
I run length over it you can see the
difference as in you can see how many
spaces were deleted from the left of the
string you can see here now it says
17 and I'm going to use
lrim let's see the difference it gives
me 12 the reason being it has deleted
five spaces from the left you can count
it 1 2 3 4 and 5 so 17 - 5 is 12 which
is correct similarly you
can use the rrim function which removes
the trailing spaces from a string
trailing spaces are these spaces when
you use left Rim it deletes the leading
spaces which is this now let me just
replace L trim with r trim which stands
for right trim
and see the
result so the length is 10 now the
reason being it has
deleted seven spaces from the right of
the string if you can count it 1 2 3 4 5
6 and
7
cool you can also use the trim function
which will delete both the leading and
the trailing spaces so here if I just
write trim and I'll
run it it gives me five because India is
total five characters long and it has
deleted all the leading and the trailing
spaces all
right there's also a function called
position in MySQL the position function
Returns the position of the first
occurrence of a substring in a string so
if the substring is not found with the
original string the function will return
zero so let's say I'll write select
position I want to find
where fruit is in my string that
is
Orange is a
fruit I'll give an alas as
name there's some error
here this should be within double Cotes
now let's run it
and see the result okay it says at the
13th place or at the 13th position we
have the word fruit in our string which
is Orange is a fruit now the final
function we are going to see is called
asky so the asky function Returns the
asky value for a specific character
let's say I write
select ask key of the letter small
a if I run this it will give me the ask
ke value which is
97 let's say you want to find the ask
key
value
of
4 let's see the result it gives me
52 all
right so that brings us to the end of
the demo session on MySQL
workbench let me go to the top and show
you the things that we covered so first
of all we created a new database called
MySQL workbench which you could see it
here under schemas there was a database
created called MySQL workbench then we
entered into the database using the
command use MySQL workbench or SQL
workbench we explored a few numeric
functions where we saw how to use
absolute mod power square root least
greatest truncate round and then we
created a table called students we
inserted some 20 rows of information of
different students and
the age date of admission
City we saw how to use wear Clause how
to use the and and R
operator we used Group by to group our
records based on certain conditions we
also used having we used order by and
then we look at a
few string functions such as lower upper
concat character length reverse
replace asky and position all right with
that we have reached the end of this
data science tools full course if you
have any questions please feel free to
leave us a comment and we'll have it
answered for you at the earliest until
next time thank you for watching stay
safe keep learning and get ahead staying
ahead in your career requires continuous
learning and upskilling whether you're a
student aiming to learn today's top
skills or a working professional looking
to advance your career we've got you
covered explore our impressive catalog
of certification programs in cuttingedge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to no
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click here