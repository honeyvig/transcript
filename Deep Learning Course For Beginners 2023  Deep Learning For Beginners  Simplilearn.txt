hello everyone and welcome to this
amazing video on deep learning full
course for beginners by simply loan
before we begin with the video if you
enjoy watching these kind of videos and
find them interesting then subscribe to
our YouTube channel because we bring the
best videos for you daily also hit the
Bell icon to never miss any updates from
Simply loan so let's go through the
agenda for this video real quick we'll
brief you with a detail introduction to
deep learning covering the data source
and neural networks after that we'll see
the difference between AI machine
learning and deep learning after that
we'll take you through the road map of
deep learning engineering followed by
Deep learning with python we'll also
walk you through some amazing Concepts
like py versus tensorflow versus Kos the
difference between tensorflow 1.0 and
2.2 and many more such Concepts after
that we'll see what Gans are followed by
CNN and RNN in deep learning and
computational graphs in deep learning
finally we'll cover some essential deep
learning projects you can mention in
your CV to make it stand out from the
crowd in your upcoming interviews
speaking of interviews we have covered
you along with the most frequently Asked
deep learning interview questions to
help you crack the most challenging
interviews before we move on to what
deep learning is if you are an aspiring
Ai and machine learning engineer then
there's no better time to train yourself
in Ai and machine learning if you're
looking for a course that covers
everything from the fundamentals to
Advanced Techniques then accelerate your
career in Ai and ml with a comprehensive
postgraduate program in artificial
intelligence and machine learning boost
your career with this Ai and ml course
delivered in collaboration with the perd
University and IBM learn in demand
skills such as machine learning deep
learning NLP computer vision
reinforcement learning generative AI
prompt engineering chat GPD and many
more you will receive a prestigious
certificate and ask me anything sessions
by IBM with five capstones in different
domain using real data sets you'll also
gain practical experience master classes
by Peru faculty and IBM experts ensure
top-notch education simply learns job
assist helps you get noticed by Leading
companies this program covers statistics
python supervised and unsupervised
learning NLP neural networks computer
vision G caras tensorflow and many more
such skills admission to this
postgraduate program nii and machine
learning requires a bachelor's degree
with an average of 50% or higher marks
along with basic understanding of
programming Concepts and Mathematics and
candidates with two plus years of work
experience are preferred to enroll in
this program so enroll now and unlock
exciting Ai and machine learning
opportunities the link is mentioned in
the description box below so without any
further delay let's send over this
session to a training expert further Ado
let's get started so deep learning is
cons considered to be a part of machine
learning so this diagram very nicely
depicts what deep learning is at a very
high level you have the all-encompassing
artificial intelligence which is more a
concept rather than a technology or a
technical concept right so it is it's
more of a concept at a very high level
artificial intelligence under the herd
is actually machine learning and deep
learning and machine learning is a
broader concept you can say or a broader
technology and and deep learning is a
subset of machine learning the primary
difference between machine learning and
deep learning is that deep learning uses
neural networks and it is suitable for
handling large amounts of unstructured
data and the last but not least one of
the major differences between machine
learning and deep learning is that in
machine learning the feature extraction
or the feature engineering is done by
the data scientists manually but in deep
learning since we use neural networks
the feature engineering happens
automatically so that's a little bit of
a a quick difference between machine
learning and deep learning and this
diagram very nicely depicts the relation
between artificial intelligence machine
learning and deep learning now why do we
need deep learning machine learning was
there for quite some time and it can do
a lot of stuff that probably what deep
learning can do but it's not very good
at handling large amounts of
unstructured data like images Voice or
even text for that matter so traditional
machine learning is not that very good
at doing this it traditional machine
learning can handle large amounts of
structured data but when it comes to
unstructured data it's a big challenge
so that is one of the key
differentiators for deep learning so
that is number one and increasingly for
artificial intelligence we need image
recognition and we need to process
analyze images and voice that's the
reason deep learning is required
compared to let's say traditional
machine learning it can also perform
complex uh algorithms more complex than
let's say what machine learning can do
and it can achieve best performance with
large amounts of data so the more you
have the data let's say reference data
or labeled data the better the system
will do because the training process
will be that much better and last but
not least with deep learning you can
really avoid the manual process of
feature extraction those are some of the
reasons why we need deep learning some
of the applications of deep learning
deep learning has made major in s and it
is a major area in which deep learning
is applied is healthare and within
Health Care a particularly oncology
which is uh basically cancer related
stuff one of the issues with cancer is
that a lot of cancers today are curable
they can be cured they are detected
early on and the challenge with that is
when a Diagnostics is performed let's
say an image has been taken of a patient
to detect whether there is cancer can or
not you need a specialist to look at the
image and determine whether it is the
patient is fine or there is any onset of
cancer and the number of Specialists are
limited so if we use deep learning if we
use automation here or if we use
artificial intelligence here then the
system can with a certain amount of the
good amount of accuracy determine
whether a particular patient is having
cancer or not so the predi or the
detection process of a disease like
cancer can be expedited the detection
process can be expedited can be faster
without really waiting for a specialist
we can obviously then once the
application once the artificial
intelligence detects or predicts that
there is an onset of cancer this can be
crosschecked by a doctor but at least
the initial screening process can be
automated and that is where the current
focus is with respect to deep learning
in healthcare what else robotics is
another area deep learning is majorly
used in robotics and you must have seen
nowadays robots are everywhere humanoids
the industrial robots which are used for
manufacturing process you must have
heard about sopia who got citizenship
with Saudi Arabia and so on there are
multiple such robots which are knowledge
oriented but there are also industrial
robots are used in Industries in the
manufacturing process and increasingly
in security and also in defense for
example image processing video is fed to
them and they need to be able to detect
objects obstacles and so on and so forth
so that's where deep learning is used
they need to be able to hear and make
sense of the sounds that they are
hearing that needs deep learning as well
so robotics is a major area where deep
learning is applied then we have
self-driving cars or autonomous cars you
must have heard of Google's autonomous
car which has been tested for millions
of miles and pretty much incident-free
there were of course a couple of
incidents here and there but it is uh
considered to be fairly safe and there
are today a lot of Automotive companies
in fact pretty much every automotive
company worth its name is investing in
self-driving cars or autonomous cars and
it is predicted that in the next
probably 10 to 15 years these will be in
production and they will be used
extensively in real life right now they
are all in R&D and in test phases but
pretty soon these will be on the road so
this is another area where deep learning
is used and how is it used where is it
used within autonomous driving the car
actually is fed with video of
surroundings and it is supposed to
process that information process that
video and determine if there are any
obstacles it has to determine if there
are any cars in the side will detect
whether it is driving in the lane also
it has to determine whether the signal
is green or red so that accordingly it
can move forward or wait so for all
these video analysis deep learning is
used in addition to that the training
overall training to drive the car
happens in a deep learning environment
so again a lot of scope here to use deep
learning couple of other applications
are machine translation so today we have
a lot of information and very often this
information is in one particular
language and more specifically in
English and people need information in
in various parts of the world it is
pretty difficult for human beings to
translate each and every piece of
information or every document into all
possible languages there are probably at
least hundreds of languages or if not
more to translate each and every
document into every language is uh
pretty difficult therefore we can use
deep learning to do pretty much like a
real time time translation mechanism so
we don't have to translate everything
and keep it ready but we train
applications or artificial intelligence
systems that will do the translation on
the Fly for example you go to somewhere
like China and you want to know what is
written on a signboard now it is
impossible for somebody to translate
that and put it on the web or something
like that so you have an application
which is trained to translate stuff on
the fly so you probably this can running
on your mobile phone on your smartphone
you scan this the application will
instantly translate that from Chinese to
English that is one then there could be
web applications where there may be a
research document which is all in maybe
Chinese or Japanese and you want to
translate that to study that document or
in that case you need to translate that
so therefore deep learning is used in
such situations as well and that is
again on demand so it is not like you
have to translate all these documents
from other languages into English in one
sh and keep it somewhere that is again
an pretty much an impossible task but on
a need basis so you have systems that
are trained to translate on the flag so
Mission translation is another major
area where deep learning is used then
there are a few other upcoming areas
where synthesizing is done by neural
nets for example music composition and
generation of music so you can train a
neural net to produce music even to
compose music so this is a fun thing
this is still upcoming it it needs a lot
of effort to train such neural land it
has been proved that it is possible so
this is a relatively new area and on the
same lines colorization of images so
these two images on the left hand side
is a grayscale image or a black and
white image this was colored by a neural
net or a deep learning application as
you can see this has done a very good
job of applying the colors and obviously
this was trained to do this uh
colorization but yes this is one one
more application of deep learning now
one of the major secret source of deep
learning is neural network deep learning
works on neural network or consists of
neural network so let us see what is
neural network neural network or
artificial neural network is designed or
based on the human brain now human brain
consists of billions of small cells that
are known as neurons artificial neural
networks is in a way trying to simulate
the human brain so this is a quick
diagram of biological Neuron a
biological neuron consists of the major
part which is the cell nucleus and then
it has some tentacles kind of stuff on
the top called dendrite and then there
is like a long tail which is known as
the axon further again at the end of
this axon are what are known as synapses
these in turn are connected to the
dendroid of of the next neuron and all
these neurons are interconnected with
each other therefore there are like
billions of them sitting in our brain
and they're all active they're working
they based on the signals they receive
signals as inputs from other neurons or
maybe from other parts of the body and
based on certain criteria they send
signals to the neurons at the other end
so they they get either activated or
they don't get activated based on so it
is like a binary gate so they get
activated or not activated based on the
inputs that they receive and so on so we
will see a little bit of those details
as we move forward in our artificial
neuron but this is a biological neuron
this is the structure of a biological
neuron and artificial neural network is
based on the human brain the smallest
component of artificial neural network
is an artificial neuron as shown here
sometimes is also referred to as
perceptron now this is a very high level
diagram the artificial neuron has a
small central unit which will receive
the input if it is doing let's say image
processing the inputs could be pixel
values of the image which is represented
here as X1 X2 and so on each of the
inputs are multiplied by what is known
as weights which are represented as W1
W2 and so on there is in the central
unit basically there is a summation of
these weighted inputs which is like X1
into W1 + X2 into W2 and so on the
products are then added and then there
is a bias that is added to that in the
next uh slide we will see that passes
through an activation function and the
output comes as a y which is the output
and based on certain criteria the cell
gets either activated or not activated
so this output would be like a zero or a
one binary format okay so we will see
that in a little bit more detail but
let's do a quick comparison between
biological and artificial neuron just
like a biological neuron there are
dendrites and then there is a cell
nucleus and synapse and and an axon we
have in the artificial neuron as well
these inputs come like the dendrite if
you will act like the dendrites there is
a like a central unit which performs the
summation of these uh weighted inputs
which is basically W1 X1 W2 X2 and so on
and then a bias is added here and then
that passes through what is known as an
activation function okay so these are
known as the weights W1 W2 and then
there is a bias which will come out here
and that is added the bias is by the way
common for a particular neuron so there
won't be like B1 B2 B3 and so on only
weights will be one per input the bias
is common for the entire neuron it is
also common for or the value of the bias
Remains the Same for all the neurons in
a particular layer we will also see this
as we move move forward and we see deep
neural network where there are multiple
neurons so that's the output now the
whole exercise of training a neuron is
about changing these weights and biases
as I mentioned artificial neural network
will consist of several such neurons and
as a part of the training process these
weights keep changing initially they are
assigned some random values through the
training process the weights the whole
process of training is to come up with
the optimum values of W1 W2 and WN and
then the B for or the bias for this
particular neuron such that it gives an
accurate output as required so let's see
what exactly that means so the training
process this is how it happens it takes
the inputs each input is multiplied by a
weight and these weights during training
keep changing so initially they're
assigned some random values and based on
the output whether it is correct or
wrong there is a feedback coming back
and that will basically change these
weights until it starts giving the right
output that is represented in here as
Sigma I going from 1 to n if there are n
inputs wi into x i so this is the
product of W1 X1 W2 X2 and so on right
and there is a bias that gets added here
and that entire thing goes to what is
known as an activation function so
essentially this is Sigma of WI XI plus
a value of bias which is a b so that
entire thing goes as an input to an
activation function now this activation
function takes this as an input gives
the output as a binary output it could
be a zero or a one there are of course
to start with let's assume it's a binary
output later we will see that there are
different types of activation functions
so it need not always be binary output
but to start with let's keep Simple so
it decides whether the neuron should be
f ired or not so that is the output like
a binary output zero or one all right so
again let me summarize this so it takes
the inputs so if you're processing an
image for example the inputs are the
pixel values of the image X1 X2 up to xn
there could be hundreds of these so all
of those are fed as so these are some
values and these pixel values again can
be from 0 to to 56 each of those pixel
values are then multiplied with what is
known as a weight this is a numeric
value can be any value so this is a
number W1 similarly W2 is a number so
initially some random values will be
assigned and each of these weights are
multiplied with the input value and
their sum this is known as the weighted
sum so that is performed in this kind of
the central unit and then a bias is
added remember the bias is common for
each neuron so this is not the bias
value is not one bias value for per
input so just keep that in mind the bias
value there is one bias per neuron so it
is is like this summation plus bias is
the output from this section this is not
the complete output of the neuron but
this is the bias for output for step one
that goes as an input to what is known
as an activation function and that
activation function results in an output
usually a binary output like a zero or a
one which is known as the firing of the
neuron okay good so we talked about
activation function so what is an
activation function an activation
function basically takes the weighted
sum which is we saw W1 X1 W2 X2 the sum
of all that plus the bias so it takes
that as an input and it generates a
certain output now there are different
types of activation functions and the
output is different for different types
of activation functions moreover why is
an activation function required it is
basically required to bring in
nonlinearity that's the main reason why
an activation function is required so
what are the different types of
activation functions there are several
types of activation functions but these
are the most common ones these are the
ones that are currently in use sigmoid
function was one of the early activation
functions but today reu has kind of
taken over so reu is by far the most
popular activation function that is used
today but still sigmoid function is
still used in many situations these
different types of activation functions
are used in different situations based
on the kind of problem we are trying to
solve so what exactly is the difference
between these two sigmoid gives the
values of the output will be between 0
and one threshold function is the value
will be zero up to a certain value and
beyond that this is also known as a step
function and beyond that it will be one
in case of sigmoid there is a gradual
increase but in case of threshold it's
like also known as a step function
there's a rapid or instantaneous change
from 0 to 1 whereas in sigmoid we will
see in the next slide there is a gradual
increase but the value in this case is
between 0 and one as well Now reu
function on the other hand it is equal
to basically if the input is zero or
less than zero then the output is zero
whereas if the input is greater than
zero then the output is equal to the
input I know it's a little confusing but
in the next slides where we show the ru
function it will become clear similarly
hyperbolic tangent this is similar to
sigmoid in terms of the shape of the
function however while sigmoid goes from
0 to 1 hyperbolic tangent goes from
min-1 to 1 and here again the increase
or the change from minus1 to 1 is
gradual and not like thresold or step
function where it happens
instantaneously so let's take a little
detailed look at some of these functions
so let's start with the sigmoid function
so this is the equation of a sigmoid
function which is 1 by 1 + e^ - x so X
is the value that is the input it goes
from 0 to -1 so this is sigmoid function
the equation is 5X = 1 by 1 + e ^ - x
and as you can see here this is the
input on the x- axis as X is the value
is coming from in fact it can also go
negative this is negative actually so
this is the zero so this is the negative
value of x so as X is coming from
negative value towards zero the value
value of the output slowly as it is
approaching zero it it slowly and very
gently increases and actually at the
point let me just use a pen at the point
here it is it is 05 it is actually .5
okay and slowly gradually it increases
to one as the value of X increases but
then as the value of X increases it
tapers off it doesn't go beyond one so
that is the speciality of sigmoid
function so the output value will remain
between 0 and 1 it will never go below
zero or above one okay then so that is
sigmoid function now this is threshold
function or this is also referred to as
a step function and here we can also set
the threshold in this case it is that's
why it's called the threshold function
normally it is zero but you can also set
a different value for the threshold now
the difference between this and the
sigmoid is that here the change is Rapid
or instantaneous
as the x value comes from negative up to
0 it remains zero and at 0 it pretty
much immediately increases to one okay
so this is a mathematical representation
of threshold function 5x is equal to 1
if x is greater than equal to 0 and 0 if
x is less than 0 so for all negative
values it is zero since we have set the
threshold to be 0o so as soon as it
reaches zero it becomes one you see the
difference between this and the the
previous one which is basically the
sigmoid where the increase from 0 to one
is gradual and here it is instantaneous
and that's why this is also known as a
step function threshold function or step
function this is a reu reu is one of the
most popular activation functions today
this is the definition of reu 5x equal
to Max of X comma 0 what it says is if
the value of x is less than zero then 5x
is um zero the moment it increases goes
beyond zero the value of 5x is equal to
X so it doesn't stop at 1 actually it
goes all the way so as the value of X
increases the value of y will also
increase infinitely so there is no limit
here unlike your sigmoid or threshold or
the next one which is basically
hyperbolic tangent okay so in case of
reu remember there is no upper limit the
output is equal to either zero in in
case the value of x is negative or it is
equal to the value of x so for example
here if the value of x is 10 then the
value of y is also 10 right okay so that
is reu and there are several advantages
of reu and it is much more efficient and
uh provides much more accuracy compared
to other activation functions like
sigmoid and so on so that's the reason
it is very popular all right so this is
hyperbolic tangent activation function
the function looks similar to sigmoid
function the curve if you see the shape
it looks similar to sigmoid function but
the difference between hyperbolic
tangent and sigmoid function is that in
case of sigmoid the output goes from 0o
to 1 whereas in case of hyperbolic
tangent it goes from minus1 to 1 so that
is the difference between hyperbolic
tangent and sigmoid function otherwise
the shape looks very similar there is a
gradual increase unlike the step
function where there was an instant
increase or instant change here again
very similar to sigmoid function the
value changes gradually from minus1 to 1
so this is the equation of hyperbolic
tangent activation function yeah so then
let's move on this is a diagrammatic
representation of the activation
function and how the overall data or how
the overall progression happens from
input to the output so we get the input
from the input layer by the way the
neural network has three layers
typically there will be three layers
there is an input layer there is an
output layer and then you have the
hidden layer so the inputs come from the
input layer and they get processed in
the hidden layer and then you get the
output in the output layer so let's take
a little bit of a detailed look into the
working of a neural network so let's say
we want to classify some images between
dogs and cats how do we do this this is
known as a classification proc process
and we are trying to use neural networks
and deep learning to implement this
classification so how do we do that so
this is how it works so you have four
layer neural network there is an input
layer there is an output layer and then
there are two hidden layers and what we
do is we provide labeled training data
which means these images are fed to the
network with the label saying that okay
this is a cat the neural network is
allow to process it and come up with a
prediction saying whether it is a cat or
a dog and obviously in the beginning
there may be mistakes a cat may be
classified as a dog so we then say that
okay this is wrong this output is wrong
but every time it predicts correctly we
say yes this output is correct so that
learning process so it will go back make
some changes to its weights and biases
we again feed these inputs and it will
give us the output we will check whether
it is correct or not and so on so this
is a iterative process which is known as
the training process so we are training
the neural network and what happens in
the training process these weights and
biases you remember there were weights
like W1 W2 and so on so these weights
and biases keep changing every time you
feed these which is known as an Epoch so
there are multiple iterations every
iteration is known as an Epoch and each
time the weights are dated to make sure
that the maximum number of images are
classified correctly so once again what
is the input this input could be like
thousand images of cats and dogs and
they are labeled because we know which
is a cat and which is a dog and we feed
those thousand images the neural network
will initially assign some weights and
biases for each neuron and it will try
to process extract the features from the
images and it will try to come up with a
prediction for each image and that
prediction that is calculated by the
network is compared with the actual
value whether it is a cat or a dog and
that's how the error is calculated so
let's say there are th000 images and in
the first run only 500 of them have been
correctly classified that means we are
getting only 50% accuracy so we feed
that information back to the network
further update these weights and biases
for each of the neurons and we run this
these inputs once again it will try to
calculate extract the features and it
will try to predict which of these is
cats and dogs and this time let's say
out of, 700 of them have been predicted
correctly so that means in the second
iteration the accuracy has increased
from 50% to 70% all right then we go
back again we feed this maybe for a
third iteration fourth iteration and so
on and slowly and steadily the accuracy
of this network will keep increasing and
it may reach probably you never know 90%
95% and there are several parameters
that are known as Hyper parameters that
need to be changed and tweaked and that
is the overall training process and
ultimately at some point we say okay you
will probably never reach 100% accuracy
but then we set a limit saying that okay
if we receive 95% accuracy that is good
enough for our application and then we
say okay our training process is done so
that is the way training happens and
once the training is done now with the
training data set the system has has
let's say seen all these thousand images
therefore what we do is the next step
like in any normal machine learning
process we do the testing where we take
a fresh set of images and we feed it to
the network the fresh set which it has
not seen before as a part of the
training process and this is again
nothing new in deep learning this was
there in machine learning as well so you
feed the test images and then find out
whether we are getting a similar
accuracy or not so maybe that accuracy
May red reduce a little bit while
training you may get 98% and then for
test you may get 95% but there shouldn't
be a drastic drop like for example you
get 98% in training and then you get 50%
or 40% with the test that means your
network has not learned you may have to
retrain your network so that is the way
neural network training works and
remember the whole process is about
changing these weights and biases and
coming up with the optimal values of
these weights and biases so that the
accuracy is the maximum possible all
right so little bit more detail about
how this whole thing works so this is
known as forward propagation which is
the data or the information is going in
the forward Direction the inputs are
taken weighted summation is done bias is
added here and then that is fed to the
activation function and then that is
that comes out as an output so that is
forward propagation and the output is
compared with the actual value and that
will give us the error the difference
between them is the error and in
technical terms that is also known as
our cost function and this is what we
would like to minimize there are
different ways of defining the cost
function but one of the simplest ways is
mean square error so it is nothing but
the square of the difference of the
errors or the sum of the squares of the
difference of the errors and this is
also nothing new we have probably if
you're familiar with machine learning
you must have come across this mean
square error now there are different
ways of defining cost function it need
not always be the mean square error but
the most common one is this so you
define this cost function and you ask
the system to minimize this error so we
use what is known as an optimization
function to minimize this error and the
error itself sent back to the system as
feedback and that is known as back
propagation and so this is the cost
function and how do we optimize the cost
function we use what is known as
gradient descent so the gradient descent
mechanism identifies how to change the
weights and biases so that the cost
function is minimized and there is also
what is known as the rate or the
learning rate that is what is shown here
as slower and faster so you need to
specify what should be the learning rate
now if the learning rate rate is very
small then it will probably take very
long to train whereas if the learning
rate is very high then it will appear to
be faster but then it will probably
never what is known as converge now what
is Convergence now we are talking about
a few terms here convergence is like
this this is a representation of
convergence so the whole idea of
gradient descent is to optimize the cost
function or minimize the cost function
in order to do that we need to represent
the cost function as this curve we need
to come to this minimum value that is
what is known as the minimization of the
cost function now what happens if we
have the learning rate very small is
that it will take very long to come to
this point on the other hand if you have
large Higher Learning rate what will
happen is instead of stopping here it
will cross over because the learning
rate is high and then it has to come
back so it will result in what is known
as like an oscillation so it will never
come to this point which is known as
convergence instead it will go back and
forth so these are known as Hyper
parameters the learning rate and so on
and these have to be those numbers or
those values we can determine typically
using trial and error out of experience
we we try to find out these values so
that is the gradient descent mechanism
to optimize the cost function and that
is what is used to train our neural
network this is another representation
of how the training process works and
here in this example we are trying to
classify these images whether they are
cats or dogs and as you can see actually
each image is fed in each time one image
is fed rather and these values of X1 X2
up to xn are the pixel values within
this image okay so those values are then
taken and for each of those values a
weight is Multiplied and then it goes to
the next layer and then to the next
layer and so on ultimately it comes as
the output layer and it gives an output
as whether it is a dog or a cat remember
the output will never be a named output
so these would be like a zero or a one
and we say Okay zero corresponds to dogs
and one corresponds to cats so that is
the way it typically happens this is a
binary classification we have similar
situations where there can be multiple
classes which means that there will be
multiple more neurons in the output
layer okay so this is once again a quick
representation of how the forward
propagation and the backward propagation
works so the information is going in
this direction which is basically the
forward propagation and at the output
level we find out what is the cost
function the difference is basically
sent back as uh part of the backward
propagation and gradient descent then
adjust the weights and biases for the
next iteration this happens iteratively
till the cost function is minimized and
that is when we say the whole the
network has converged or the training
process has converged and there can be
situations where convergence may not
happen in rare cases but by and large
the network will converge and after
maybe a few iterations it could be tens
of iteration ation or hundreds of
iterations depending on what exactly the
number of iterations can vary and then
we say okay we are getting a certain
accuracy and we say that is our
threshold maybe 90% accuracy we we stop
at that and we say that the system is
trained the train model is then deployed
for production and so on so that is the
way the neural network training happens
okay so that is the way classification
Works in deep learning using neural
network and this slide is an an anation
of this whole process as you can see the
forward propagation the data is going
forward from the input layer to the
output layer and there is an output and
the error is calculated the cost
function is calculated and that is fed
back as a part of backward propagation
and that whole process repeats once
again okay so remember in neural
networks the training process is nothing
but the finding the best values of the
weights and biases for each and every
neuron in the network that's all
training of neural network consists of
finding the optimal values of the
weights and biases so that the accuracy
is
maximum human versus artificial
intelligence humans are amazing let's
just face it we're amazing creatures
we're all over the planet we're
exploring every nich and Nook we've gone
to the Moon
uh we've got into outer space we're just
amazing creatures we're able to use the
available information to make decisions
to communicate with other people
identify patterns and data remember what
people have said adapt to new situations
so let's take a look at this so so you
can get a picture you're a human being
so you know what it's like to be human
let's take a look at artificial
intelligence versus the human artificial
intelligence develops computer systems
that can accomplish tasks that require
human
intelligence so we're looking at this
one of the things that computers can do
is they can provide more accurate
results this is very important recently
I did a project on cancer where it's
identifying
markers and as a human being you look at
that and you might be uh looking at all
the different images and the data that
comes off of them and say I like this
person so I want to give them a very
good um Outlook and the next person you
might not like so you want to give him a
bad Outlook
well with artificial intelligence you're
going to get a consistent prediction of
what's going to come out interacts with
humans using their natural language
we've seen that as probably the biggest
development feature right now that's in
the commercial Market that everybody
gets to use as we saw with the example
of Alexa they learn from their mistakes
and adapt to new environments so we see
this slowly coming in more and more and
they learn from the data and automate
repetitive learning repetitive learning
has a lot to do with the neural networks
you have to program thousands upon
thousands of pictures in there and it's
all automated so as today's computers
evolved it's very quick and easy and
affordable to do this what is machine
learning and deep learning all about
imagine this say you had some time to
waste not that any of us really have a
lot of time anymore to just waste in
today's world and you're sitting by the
road and you have a whole lot of and a
whole lot of time passes by there's a
few hours and suddenly you wonder
how many cars buses trucks and so on
passed by in the six hours now chances
are you're not going to sit by the road
for six hours and count buses cars and
trucks unless you're working for the
city and you're trying to do City
Planning and you want to know hey do we
need to add a new truck route maybe we
need a Bicycle Link we have a lot of
bicyclists here that kind of thing so
maybe City Planning would be great for
this machine learning well the way
machine Learning Works is we have
labeled data with features okay so you
have a truck or a car a motorcycle a bus
or a bicycle and each one of those are
labeled it comes in and based on those
labels and comparing those features it
gives you an answer it's a bicycle it's
a truck it's a motorcycle this look a
little bit more in depth on this in the
model here it actually the features
we're looking at would be like the tires
someone sits there and figures out what
a tire looks like takes a lot of work if
you try to try to figure the difference
between a car to Tire a bicycle tire a
motorcycle tire uh so in the M machine
learning field this could take a long
time if you're going to do each
individual aspect of a car and try to
get a result on there and that's what
they did do that was a a very this is
still used on smaller amounts of data
where you figure out what those features
are and then you label them deep
learning so with deep learning one of
our Solutions is to take a very large
unlabeled data set and we we put that
into a training model using artificial
neural networks and then that goes into
the neural network itself and we create
a neural network and you'll see um the
arrows are actually kind of backward but
uh which actually is a nice point
because when we train the neural network
we put the bicycle in and then it comes
back and says if it said truck it comes
back and says well you need to change
that to bicycle and then it changes all
those weights going backward they call
it back propagation and let it know it's
a bicycle and that's how it learns once
you've trained the neural network you
then put the new data in and they call
this testing the model so you need to
have some data you've kept off to the
side where you know the answer to and
you take that and you provide the
required output and you say okay is this
is this neural network working correctly
did it identify a bike as a bike a truck
is a truck a motorcycle as a motorcycle
let's just take a little closer look at
that determining what objects are
present in the data so how does deep
learning do this and here we have the
image of the bike it's 28 by 28 pixels
that's a lot of information there um
could you imagine trying to guess that
this is a bicycle image by looking at
each one of those pixels and trying to
figure out what's around it uh and we
actually do that as human beings it's
pretty amazing we know what a bicycle is
and even though it comes in as all this
information and what this looks like is
the image comes in it converts it into a
bunch of different nodes in this case
there's a lot more than what they show
here and it goes through these different
layers and outcomes and says okay this
is a bicycle
a lot of times they call this the magic
Black Box why because as we watch it go
across here all these weights and all
the math behind this and it's not it's a
little complicated on the math side you
really don't need to know that when
you're programming or doing working with
the Deep learning but it's like magic
you you don't know you really can't
figure out what's going to come out by
looking what's in each one of those dots
and each one of those lines are firing
and what's going in between them so we
like to call it the magic box uh so
that's where deep learning comes in
and in the end it comes up and you have
this whole neural notwork it comes up
and it says okay we fire all these
different pixels and we connects all
these different dots and gives them
different weights and it says okay this
is a bicycle and that's how we determine
what the object is present in the data
with deep learning machine learning
we're going to take a step into machine
learning here and you'll see how these
fit together in a minute the system is
able to make predictions or take
decisions based on past data that's very
important for machine learning is that
we're looking at stuff and based on
what's been there before we're creating
a decision on there we're creating
something out of there we're coloring a
beach ball we're telling you what the
weather is in Chicago what's nice about
machine learning is a very powerful
processing capability it's quick and
accurate outcomes so you get results
right away once you program the system
the results are very fast and the
decisions and predictions are better
they're more accurate they're consistent
you can analyze very large amounts of
data some of these data things that
they're analyzing now are pedabytes and
terabytes of data it would take hundreds
of people hundreds of years to go
through some of this data and do the
same thing that the machine learning can
do in a very short period of time and
it's inexpensive compared to hiring
hundreds of people so it becomes a very
affordable way to move into the future
to apply the machine learning to
whatever businesses you're working on
and deep Learning Systems think and
learn like humans using artificial
neural networks again it's like a magic
box performance improves with more data
so the more data the Deep learning gets
the more it gives you better results
it's scalability so you can scale it up
you can scale it down you can increase
what you're looking at currently you
know we're limited by the amount of
computer processing power as to how big
that can get but that envelope
continually gets pushed every day on
what it can do problem problem solved in
an end to end method so instead of
having to break it apart and you have
the first piece coming in and you
identify tires and the second piece is
identifying uh labeling handlebars and
then you bring that together that if it
has handlebars and tires it's a bicycle
and if it has something that looks like
a large Square it's probably a truck the
neural networks does this all in one
network you don't really know what's
going on in all those weights and all
those little bubbles uh but it does it
pretty much in one package that's why
the neural network systems are so big
nowadays and coming into their own best
features are selected by the system and
it this is important they kind of put it
it's on a bullet on the side here it's a
subset of machine learning this is
important when we talk about deep
learning it is a form of machine
learning there's lots of other forms of
machine learning data analysis but this
is the newest and biggest thing that
they apply to a lot of different
packages and they use all the other
machine learning tools available ailable
to work with it and it's very fast to
test um you put in your information you
then have your group of uh tests and
then you held some aside you see how
does it do it's very quick to test it
and see what's going on with your deep
learning and your neural network are
they really all that
different AI versus machine learning
versus deep learning concepts of AI so
we have concepts of II you'll see
natural language processing uh machine
learning an approach to create
artificial intelligence so it's one of
the subsets of artificial intelligence
knowledge representation automated
reasoning computer vision robotics
machine learning versus AI versus deep
learning or Ai and machine learning and
deep
learning so when we look at this we have
ai with machine learning and deep
learning and so we're going to put them
all together we find out that AI is a
big picture we have a collection of
books it goes through some deep learning
the digital data is analyzed text mining
comes through the particular book you're
looking for maybe it's a genre books is
identified and in this case uh we have a
robot that goes and gives a book to the
patron I have yet to be at a library
that has a robot bring me a book but
that will be cool when it happens uh so
we look at some of the pieces here this
information goes into uh there as far as
this example the translation of the
handwritten printed data to digital form
that's pretty hard to do that's pretty
hard to go in there and translate
hundreds and hundreds of books and
understand what they're trying to say if
you've never read them so in this case
we use the Deep learning because you can
already use examples where they've
already classified a lot of books and
then they can compare those texts and
say oh okay this is a book on automotive
repair this is a book on robotic
building the Digital Data is in analyzed
then we have more text mining using
machine learning so maybe we'd use a
different program to do a basic classify
U what you're looking for and say oh
you're looking for auto repair and
computers so you're looking for
automated cars once it's identified then
of course it brings you the
book so here's a nice summation of what
we were just talking about AI with
machine learning and deep learning deep
learning is a subset of machine learning
which is a subset of artificial
intelligence so you can look at
artificial intelligence as the big
picture how does this compare to The
Human Experience in either uh doing the
same thing as a human we do or it it
does it better than us and machine
learning which has a lot of tools uh is
something that learns from data past
experiences it's programmed it's uh
comes in there and it says hey we
already had these five things happen the
six one should be about the same and
then uh then there's a lot of tools in
machine learning but deep learning then
is a very specific tool in machine
learning learning it's the artificial
neural network which handles large
amounts of data and is able to take huge
pools of experiences pictures and ideas
and bring them together real life
examples artificial intelligence news
generation very common nowadays as it
goes through there and finds the news
articles or generates the news based
upon the news feeds or the uh backend
coming in and says okay let's give you
the actual news based on this there's
all the different things Amazon Echo
they have a number of different Prime
music on there of course there's also
the Google command and there's also
Cortana there's tons of smart home
devices now where we can ask it to turn
the TV on or play music for us that's
all artificial intelligence from front
to back you're having a human experience
with these computers and these objects
that are connected to the processing
machine learning uh spam detection very
common machine learning doesn't really
have the human interaction part so this
is the part where it goes and says okay
that's a Spam that's not a Spam and it
puts it in your spam
folder search engine result refining uh
another example of machine learning
whereas it looks at your different
results and it Go and it uh is able to
categorize them as far as this had the
most hits this is the least viewed this
has five star s um you know however they
want to wait it uh all exam good
examples of machine learning and then
the Deep learning uh deep learning
another example is as you have like a
exit sign in this case is translating it
into French sorti I hope I said that
right um neural network has been
programmed with all these different
words and images and so it's able to
look at the exit in the middle and it
goes okay we want to know what that is
in French and it's able to push that out
in French French in learn how to do
that and then we have chatbots um I
remember when Microsoft first had their
little paperclip um boy that was like a
long time ago that came up and you would
type in there and chat with it these are
growing you know it's nice to just be
able to ask a question and it comes up
and gives you the answer and instead of
it being were you're just doing a search
on certain words it's now able to start
linking those words together and form a
sentence in that chat box types of a I
and machine
learning types of artificial
intelligence this in the next few slides
are really important so one of the types
of artificial intelligence is reactive
machines systems that only react they
don't form memories they don't have past
experiences they have something that
happens to them and they react to it my
washing machine is one of those if I put
a ton of clothes in it and they it all
clumped on one side it automatically
adds a weight to reciter it so that my
washing machine is actually a reactive
machine working with whatever the load
is and keeps it nice and so when it
spins it doesn't go thumping against the
side limited memory another form of
artificial intelligence systems look
into the past information is added over
a period of time and information is
shortlived when we're talking about this
and you look at like a neural network
that's been programmed to identify cars
it doesn't remember all those pictures
it has no memory as far far as the
hundreds of pictures you process through
it all it has is this is the pattern I
use to identify cars as the final output
for that neural network we looked at so
when they talk about limited memory this
is what they're talking about they're
talking about I've created this based on
all these things but I'm not going to
remember anyone specifically theory of
Mind systems being able to understand
human emotions and how they affect
decision-making to adjust their
behaviors according to their human
understanding this is important because
this is our page mark this is how we
know whether it is an artificial
intelligence or not is it interacting
with humans in a way that we can
understand uh without that interaction
is just an object uh so we talk about
theory of mind we really understand how
it interfaces that whole if you're in
web development user experience would be
the term I would put in there so the
theory of mind would be user experience
how's the whole UI connected together
and one of the final things is as we get
into artificial intelligence is systems
being Ware of themselves understanding
their internal States and predicting
other people's feelings and act
appropriately so as artificial
intelligence continues to progress uh we
see ones are trying to understand well
what makes people happy how would they
increase our happiness uh how would they
keep themselves from breaking down if
something's broken inside they have that
self-awareness to be able to fix it and
just based on all that information
predicting which action would work the
best what would help people uh if if I
know that you're having a cup of coffee
first thing in the morning is what makes
you happy as a robot I might make you a
cup of coffee every morning at the same
time uh to help your life and help you
grow that'd be the self-awareness is
being able to know all those different
things types of machine learning and
like I said on the last slide this is
very important this is very important if
you decide to go in and get certified in
machine learning or know more about it
these are the three primary types of
machine learning the first one is
supervised learning systems are able to
predict future outcome based on past
data requires both an input and an
output to be given to the model for it
to be trained so in this case we're
looking at anything where you have 100
images of a
bicycle and those 100 images you know
are bicycle so it's they're preset
someone already looked at all 100 images
and said these are pictures of bicycles
and so the computer learns from those
and then it's given another picture and
maybe the next picture is a bicycle and
it says oh that resembles all these
other bicycles so it's a bicycle and the
next one's a car and it says it's not a
bicycle that would be supervised
learning because we had to train it we
had to supervise it unsupervised
learning systems are able to identify
hidden patterns from the input data
provided by making the data more
readable and organized the patterns
similarities or anomalies become more
evident uh you'll heard the term cluster
how do you cluster things together some
of these things go together some of
these don't this is unsupervised where
can look at an image and start pulling
the different pieces of the image out
because they aren't the same the human
all the parts of the human are not the
same as a fuzzy tree behind them it's
slightly out of focus which is not the
same as the beach ball it's unsupervised
because we never told it what a beach
ball was we never told it what the human
was and we never told it that those were
trees all we told it was hey separate
this picture by things that don't match
and things that do match and come
together and finally there's
reinforcement learning systemss are
given no training it learns on the basis
of the reward punishment it received for
performing its Last Action it helps
increase the efficiency of a tool
function or a program reinforced
learning or reinforcement learning is
kind of you give it a yes or no yes you
gave me the right response no you didn't
and then it looks at that and says oh
okay so based on this data coming in uh
what I gave you was a wrong response so
next time I'll give you a different one
comparing machine learning and deep
learning so remember that deep learning
is a subcategory of machine learning so
it's one of the many tools and so they
we're grouping a ton of machine learning
tools all together linear regression K
means clustering there's all kinds of
cool tools out there you can use in
machine learning enables machines to
take decisions to make decisions on
their own based on past data enables
machines to make decisions with the help
of artificial neural networks so it's
doing the same thing but we're using an
artificial neural Network as opposed to
one of the more traditional machine
learning tools needs only a small amount
of training data this is very important
when you're talking about machine
learning they're usually not talking
about huge amounts of data we're talking
about maybe your spreadsheet from your
business and your totals for the end of
the year when you're talking about
neural networks you usually need a large
amount of data to train the data so
there's a lot of training involved if
you have under 500 points of data that's
probably not going to go into machine
learning or maybe have like the case of
one of the things 500 points of data and
30 different fields it starts getting
really confusing there in artificial
intelligence or machine learning and the
Deep learning aspect really shines when
you get to that larger data that's
really
complex works well on a low-end systems
so a lot of the machine learning tools
out there you can run on your laptop
with no problem and do the calculations
there where with the machine learning
usually needs a higher end system to
work it takes a lot more PR processing
power to build those neural networks and
to train them it goes through a lot of
data we're talking about the general
machine learning tools most features
need to be identified in advanced and
manually coded so there's a lot of human
work on here the machine learns the
features from the data it is provided so
again it's like a magic box you don't
have to know what a tire is it figures
it out for you the problem is divided
into parts and solved individually and
then combined so machine learning you
usually have all these different tools
and use different tools for different
parts and the problem is solved in an
end toin manner so you only have one
neural network or two neural networks
that is bringing the data in and putting
it out that's not going through a lot of
different processes to get there and
remember you can put machine learning
and deep learning together so you don't
always have just the Deep learning
solving the problem you might have
solving one piece of the puzzle with
regular machine learning and most of
machine learning tools out there they
take longer to test and understand how
they work and with the Deep learning
it's pretty quick once you build that
neural network you test it and you know
so we're dealing with very crisp rules
limited
resources you have to really explain how
the decision was made when you use most
machine learning tools but when you use
the Deep learning tool inside the
machine learning tools the system takes
care of it based on its own logic and
reasoning and again it's like a magic
Black Box you really don't know how it
came up with the answer you just know it
came up with the right answer like
glimpse into the future so a quick
glimpse into the future artificial
intelligence be using it to detecting
crimes before they happen humanoid AI
helpers which we already have a lot of
there'll be more and more maybe it'll
actually be Androids that'd be cool to
have an Android that comes and get stuff
out of my fridge for me machine learning
increasing efficiency in health care
that's really big in all the forms of
machine learning better marketing
techniques any of these things if we get
into the Sciences it's just off the
scale machine learning and artificial
intelligence go everywhere and then the
subcategory Deep learning increased
personalization so it's really nice
about the Deep learning is it's going to
start now catering to you that'll be one
of the things we see more and more of
and we'll have more of a hyper
intelligent personal assistant I'm
excited about that first we'll
understand the basics and starting with
the basics we'll start with what is deep
learning so deep learning often touted
as an attempt to replicate human brain
learning operates through intricate
mathematical functions enabling
computers to perform tasks similar to
humans for instance it underpins
groundbreaking Technologies like
driverless cars as well as voice
activated assistant such as Siri and
Amazon Alexa in practice deep learning
empowers computers to learn directly
from various data formats such as images
text or audio through this process
computer model achieve remarkable
accuracy occasionally surpassing human
level performance it's not just about
mimicking human cognition it's about
harnessing the immense computational
power to expect vast amounts of data and
extract meaningful patterns the backbone
of deep learning consist of neural
networks interconnected nodes that
process data through layers each layer
refining the representation of the data
training these networks involve
adjusting parameters to minimize errors
and improve accuracy a process
resembling the human learning experience
Albert on a vastly accelerated scale
while data and computational challenges
exist the field continues to evolve
research and Innovation are driving deep
learning forward opening doors to
improved image recognition natural
language understanding medical
Diagnostics and more the promise of
unsupervised learning and the
integration of deep learning into
diverse Industries paints an exciting
future for this transformative
technology impacting the way we interact
with machines and pushing the boundaries
of what's possible so this was all about
the basics and about the Deep learning
now we'll see why we should learn deep
learning so it's a game changer in
various aspects offering
state-of-the-art performance scalability
with data reduced feature engineering
and transferability across task firstly
deep learning shines where traditional
machine learning algorithm struggle
speech recognition image classification
and object detection are areas with deep
learning algorithms like CNN
convolutional neural networks and recant
neural networks RNN Excel due to their
specialized architectures pushing the
boundaries of what's achievable moreover
as data becomes abundant deep learning's
performance continues to improve unlike
classical machine learning algorithms
which plate to in performance with more
data deep learning algorithms thrive on
it offering a scalable solution for the
ever expanding data sets another
significant Advantage is the reduced
need for intricate feature engineering
traditional methods often require
complex manual feature extraction in
deep learning the emphasis is on
learning directly from the data reducing
the burden of feature engineering and
making it more accessible for beginners
additionally de planning models are
transferable pre-rain networks such as
vgg16 rest Nets and mobile net can be
leveraged as feature extraction tools
for related task this accelerates model
training and enhances performance
especially when faced with limited data
and computational resources in a sense
deep learning empowers you with cutting
it skills enabling you to Tech complex
problems scale with data and Achieve
impressive results without the need for
extensive feature engineering its
transferability across task provide a
strategic Advantage making it an
invaluable skill for modern Ai and
machine learning practitioners so these
were the points that show us why we
should learn deep learning and before
moving on to the ra and before moving on
to the road map to become a deep
learning engineer we have a course
offering for you that is if you are one
of the aspiring deep learning
enthusiasts looking for own online
training or a professional who at sits
to switch careers in deep learning by
learning from the experts then try
giving a short to ktech a ml boot camp
that is in collaboration with simply
learn and the link to this boot camp is
mentioned in the description box below
that will navigate you to the Boot Camp
Page where you can find a complete overw
of the program being offered now moving
on to the road map to become an deep
learning engineer so learning deep
learning as a beginner involves several
essential steps each building upon the
previous to develop a comprehensive
understanding and practical skills in
this field so the first is learn and
master python so python is the
foundation for many deep learning
libraries and Frameworks start by
mastering python syntax data structures
and basic programming Concepts it's
essential to be comfortable with python
as you will use it extensively for
coding deep learning models and moving
on to the next step that is mathematics
for deep learning develop a solid
understanding of the mathematical
Concepts behind deep learning this
includes linear algebra that includes
vectors matrices operations then we have
calculus in that we have differentiation
integration and then we have statistics
that includes probability and
distributions these concepts are vital
to grasp the inner workings of neural
networks and then we have neural
networks the third step that is dive
into the core of deep learning
understanding neural networks explore
topics like loss functions that is how
the mode model measures its performance
activation functions that is which add
nonlinearity to the network then we have
weight initialization that is critical
for model convergence and the vanishing
or exploding gradient problem that is a
challenge in deep networks then we have
the next step that is architectures so
familiarize yourself with various neural
network architectures start with feed
forward neural networks which form the
basis for many models progress to auto
encoders convolutional neural networks
CNN for image task recent neural
networks RNN for sequential data
Transformers for NLP task SES networks
for similarity comparison generative and
veral networks GN for generating data
and explore evolving architectures like
neat neuro evolution of augmenting
topologies and now moving on to the next
step that is learning tools and
Frameworks so learn to work with popular
deep learning tools and Frameworks
familiarize yourself with tensor flow P
torch caras and ml flow these tools
simplify model development and
management so after this step you should
learn the model optimization so
understand techniques for model
optimization this includes distillation
that is knowledge transfer from large
models to smaller ones quantization
reducing model size without significant
loss of performance and neural
architecture search that is automated
methods to find Optimal Network
architectures then you should have
Hands-On projects to practice Z that is
apply what you have learned by working
on Hands-On projects start with simple
task and gradually tackle more complex
problems implement the different
architectures you have learned use the
Frameworks and experiment with model
optimization techniques by following
this structured approach you will build
a strong foundation in deep learning
enabling you to create train and
optimize neural networks for various
tasks the combination of theoretical
understanding and practi IC experience
gain from Hands-On projects will equip
you to explore Advanced topics and
contribute to the exciting field of deep
learning and the field of deep learning
is rapidly evolving so stay updated with
the latest research papers new
techniques and emerging Technologies
engage with the AI community Through
forums blogs and conferences and
consider joining AI focused online
communities and attend conferences like
neural PS cvpr or icml and there you
have it the Deep learning Engineer Road
mapap remember this Journey requires
dedication continuous learning and a
passion for solving complex problems
whether you are aiming to work in
Industry research or both this road map
will set you on the right path then
accelerate your career in Ai and ml with
a comprehensive post-graduate program in
artificial intelligence and machine
learning so enroll now and unlock
exciting Ai and machine learning
opportunities the link is mentioned in
the description box below deeper into
looking at the theory behind the neural
network and we kind of flip back and
forth between these because there's two
huge aspects of it one is from the
outside what are you seeing and what's
going on from the inside so you can find
to do what you need to do and give the
best results you can and we start off
with uh what do we feed we feed an
unlabeled image to a machine which
identifies it without any human
intervention and so you can see here we
have a circle that comes in at 784
pixels and it comes in by 28x 28 and you
can see how it colors in the um the
circle on there and we put a triangle in
the Triangle in also comes in as 28x 28
and it has 784 pixels so you'll see
between these two both of them are
784 pixels this machine is intelligent
enough to differentiate between the
various shapes so that's what we want to
use our neural network to do is to say
hey this is a circle this is a triangle
that's more of a categorical you can
also do a regression model where you're
actually putting out a float value or a
numerical value we'll be looking at the
true false or the categorical model
mostly because that's where you usually
start at the different there is no real
difference when you as far as the way
the internal functioning goes when you
start flipping between them other than
well we'll talk about that in just a
minute so you can actually go between
the two quite easily and the neural
network provides this capability so
we're going to use this capability to
look between those two one of the things
I want you to note in here is that we're
looking at 784 pixels we're looking at
784 inputs that's very different than
stock with a high low or last year's
cells based on date we're looking at
just a couple of numbers and they're
very clear their numbers they're very
clear what they are which is something
you'd put into a machine learning linear
regression model this is a step up from
that in that we're looking at complex
patterns and how do you figure those
complex patterns out so a neural network
is a system modeled on the human brain
and we looked at that comparing the two
let's go ahead and look deeper into the
neural network itself we have our inputs
coming in so the inputs are fed to a
neuron that processes the data and gives
us an output input and output this is
the most basic structure of a neural
network known as a perceptron so if you
see the term perceptron that's what
we're talking about we're talking about
this single node that has inputs and an
output however neural networks are
usually much more complex let's start
with visualizing a neural network as a
black box and I always love that symbol
it's a black box it's kind of magical we
have our inputs coming in and we want
certain outputs the Box takes inputs
processes them and gives an output let's
have a look at what happens within this
box and you can see me there in my uh
Secret agent get up I got my hidden hood
and everything I guess I'm part of the
black skull or something like that group
uh so let's take a look at what happens
within this magic box and remember we're
skipping back and forth between the
theory of what's going on in the box
which you have to know how to find tune
and how to build versus looking at it
from the outside we're programming this
box and we have an input and an output
to the box as a whole within the Box
exists a network that is a core of deep
learning and you can see here we're
showing uh one layer and we have our
grid coming in in the network consists
of layers of neurons each neuron is
associated with a number called the bias
and you can think of the bias uh if you
overly simplify this and we're doing a
linear regression model this is your Y
intercept in your ukian geometry you
have to have something that offsets it
and so you always have a bias in these
cells neurons of each layer transmit
information to neurons of the next layer
over channels and so you can see each of
our layers going through from left to
right these channels are associated with
numbers called called weights these
weights along with the biases determine
the information that is passed over from
the neuron to neuron so just like the
bias is your Y intercept in ukian
Geometry you could look at the an one
weight remember this is very complicated
so we're not looking at just one weight
you could look at the weight as your
slope of the line or if you're doing xal
uh m y + C it would be the M value
neurons of each layer transmit
information to neurons of the next layer
and you can see here as they light up
going across into the final layer and
then to the output and in this case the
output is going to be either uh a square
in this one or it might light up the
other one which is a circle the output
layer emits a predicted output so in
this case we're looking at a
classification uh true false is it a
circle is it a triangle is it a square
let's now go deeper what happens within
the neuron so we're going to dig deeper
and start getting a little bit closer to
some of the math don't worry you don't
have to be a calculus expert or know
your differential equations even though
this is one giant differen itial
equation you don't need to understand
those to understand what's going on
within each neuron the following
operations are performed the product of
each input and the weight of the channel
it's passed over is found this is simply
addition we're going to sum up the
weight times the output from the
previous channel and plus a bias some of
the weighted products is computed this
is called the weighted sum bias unique
to the neuron is added to the weighted
sum the final sum is then subjected to
the particular function and we'll
discuss those that particular function
that part is really important because
those functions uh have a huge impact on
how well your model performs under
different conditions the final sum is
then subjected to a particular function
this is the activation function so if
you ever hear the term activation
function that's what we're talking about
what activates this cell and what
doesn't as we dig deeper into activation
function an activation function takes
the weighted sum of the input as its
input adds a bias and provides an output
and a lot of times you'll actually see
one formula for the sum of the we the
weighted sum and the bias you'll just
see that as a single line of everything
added together and here we've broken it
apart because it makes it clear that
this bias is not computed the same as
the weighted sums here are the most
popular types of activation function and
I always find these interesting because
at one point I was sitting at a table
with a gentleman who was finishing his
PhD he was in his last year and he said
he went through all this stuff and he
ended up just trying the four different
activation functions on this particular
problem he was working on so knowing the
math behind it doesn't necessarily mean
you're going to know it right away uh so
even somebody who might have a PhD and
be doing the calculations on this comes
back out of it and ends up just trying
the different uh um activation functions
to see what's going to make a difference
and a lot of times that's a final step
that's the kind of thing where you built
your whole model you've come back and
you're like wait a minute can I do a
better deal with a sigmoid function or
the threshold or the rectifier knowing
what they're doing is important so you
can explain it to somebody else and
again again you probably do this on a
small set of data if you're working with
big data uh you don't want to take down
the full server Farm just to test out
your three different series you take a
small portion of that data test it and
then you put it through to the big data
so let's take a look at this we have the
sigmoid function and it's used for
models where we have to predict the
probability as an output it exists
between zero and one and you'll see
that's true of all of our um activation
functions we're working with either the
cell's on or off it's true or false and
there might be a little variation in
there which as an output could be used
to compute uncertainty in your solution
so if you're getting a 7 with this
activation function it might be well I'm
not sure if that's really a square or
I'm not sure that's really a triangle
and that might be a flag for it to be
looked at by a human Observer at least
in today's models where we're at right
now and you can see here we have the
formula is simply equals 1 over 1 + eus
x where X is your value coming in and
it's going to give you a result that
looks very similar to the graph on there
which is somewhere between zero and one
um and right in the middle you can see
that there's a huge uh kind of you can
go through all the different values and
uncertainties involved so the sigmoid
function is probably the default of most
of them uh the next one is the threshold
function it is a threshold based
activation function if x value is
greater than a certain value the
function is activated and fired else not
pretty straightforward yes no true false
um I don't want to test for
improbabilities I just want a straight
answer I don't want to know if there's a
partial value on there it either is true
or it's false and the rectifier function
it is the most widely used activation
function I would debate that um
rectifier is pretty common one although
I see the the sigmoid function is used
to be the basic one but it's up there
the rectifier function is very commonly
used because the output of x if x is
positive and zero otherwise and you can
see here again just like um uh it's
either kind of get a value going up
there so max of X of zero so it's it's
again it's like the threshold function
yes no true false uh it's either zero or
it's uh some kind of progressive value
and then we have the rectifier function
I would argue with this because the
sigmoid function used to be the most
common one but with the rectifier
function uh it now says it is the most
commonly used or widely used activation
function and gives an output of x if x
is positive and zero otherwise this is
kind of nice because it now says
absolutely not or it gives you a value
of probability now when I say a value of
probability be very careful there I'm
not saying that it's it's going to tell
you this is 75% chance of being a circle
I'm going to tell you that it says hey
if this says 0.1 it probably needs to be
looked at or 02 or 3 it's going to
depend on your data as to what that
value means in general that just means
it's flagging it that if it's not a one
then Chances Are needs to be looked at
by a person and re-evaluated and there's
a hyperbolic tangent function this
function is similar to sigmoid function
is bound to a range of minus one to one
so you can see there's our 1 - eus 2x
and 1 plus plus over 1 + eus 2x again
it's very similar to the sigmoid
function the bonus of the hyperbolic
function is you have that variable
coming through the middle so again you
can look at it and you have a little bit
more weight as far as you can process
that down the line that's a little bit
more advanced in than what we're looking
at right now and a lot of times it's not
even necessary in a lot of our different
uh uses for these activation functions
now we looked at activation functions
and I kind of said those are a little
bit like a black box because even if you
know the math a lot of times you end up
just playing with them to find out what
works and it also depends on what model
you're working with whether you need a
flat yes no true false or you need to
have something in the middle that says
hey this isn't quite a one you might
need to process this with the human
intervention and you can look at that
one example would be self-driving cars
you don't want a car to be yes no I'm
going to go through the the light you
want it to be like okay if it's uh
almost yes maybe we stop and have human
intervention so we don't get an accident
cost function is something you can
really see and Ma meure and is very
important the cost value is the
difference between the neural Net's
predicted output and the actual output
from a set of labeled training data so
we have our group of data that's a
square circle and since we're looking at
geometrical shapes we've had somebody
already labeled that data they've
already said this is a triangle this is
a square and so if this is coming up and
it's giving us and it's saying a square
is a triangle and it's saying a triangle
is a circle the output is wrong and so
that output can then be measured and the
versus the actual output and that's the
cost uh you might also hear this as
error because that's the error value
being returned how far off is it and
what we're looking for is the least cost
or the least error value and it's
obtained by making adjustments to the
weights and biases iteratively
throughout the training process and this
this is called back propagation and
we're going to look in that a little
deeper as we look into an example it's
really hard to see when you're just
looking at arrows without actual numbers
and where that flow is coming from but
you can look at this is here's our input
puts they put out a prediction the
prediction comes out and says hey we've
already labeled this data cuz we're in
training mode and the training data is
off this is the cost can we send that
error or that cost back and adjust those
weights and we do it in very small
increments across large amounts of data
so that those weights minimize that cost
or that error but what happens Within
These neurons so let's look at a little
example of this kind of helps if you
have some kind of visual let's build a
neural network that predict bike prices
is based on a few of its features and
we'll see here we have our CC our
mileage and our abs and these are our
three input layers and then we have the
bike price and the output layer now it
doesn't do us very good to just uh pump
it in from the beginning and pump it out
and to be honest I would use a machine
learning linear regression model on this
since these are just straight numbers
but because we want a simple example
we're going to put this through and show
you as a neural network what that looks
like and we got to put a hidden layer in
there the hidden layer helps in
improving the output accuracy and you
could look at this as a bunch of ores so
it might say hey when we compare these
three values on the first hidden layer
neuron we're looking at one set of
features and then we might weigh them in
the second one so these are a bunch of
different ores kind of how the math
comes out in behind the scenes and then
they go out of course to the bike prer
or the output layer and each of the
connections have a weight assigned with
it and you'll see here we have mileage
CC with the weight one and weight two
going into our first neuron and you
could also have your abs going in there
and so X1 * weight 1 + X2 * weight 2
plus the bias of one and step two is our
activation the activation function
coming in there when does this fire and
the neuron takes a subset of the inputs
and processes it and then we go through
we do that with the um second hidden
layer neuron and the third one and so on
so you process each layer in order going
forward now when I told you this is in
its infant stage they now have neurons
that fire into the same layer or back a
layer so that you now have a Time series
and there's all kinds of wild things
that they're experimenting with on these
layers this basic setup has been around
since the mid 90s it's only now because
of our technology that it's open to
almost everybody to play with it and
that's why I say this is in an infant
stage in development is this basic math
is here but what we can do with it is
amazing and what they're actually doing
with all these different things is
amazing and so we're just at the
beginning of how to use all these
different Tools in our deep learning in
our neural networks and so once we have
our hidden layer computed the
information reaching the neurons in the
hidden layer is subjected to the
respective activation function and so
each one of these fires an activation
output uh and then those are each
weighted to the final output layer so
the processed information is now sent to
the output layer once again overweighted
channels and you could look at this as
each one of these is um I always like at
this as like a group of people they're
all looking at the bulletin board and
the first person says this is what I
project sales for the company the second
person and the third and and so on and
then their perspectives are weighted
based on their expertise so your
accountant might have a very high weight
where the um maybe your janitor has a
very low weight because their expertise
is not in accounting and then that goes
into the output layer and once in the
output layer it goes uh the output which
is the predicted value is compared
against the original value so now we
have our output layer and since we have
like already a list of uh bikes with
their the different setups and what
their value is we can now generate an
error from this the cost function
determines the error and prediction and
reports it back to the neural network so
this is the cost this is how far off it
is this is your error coming back and as
you can see this is a back propagation
going on so now our error is going in
reverse because we know we're not
completely correct on this particular
channel the weights are adjusted in
order to reduce the error so each time
we go back we are changing those weights
to reduce that error and we change them
in small increments you don't want to
fit one input remember you might have a
data pool with a terabyte of data you
don't want to solve for the first set of
data that comes in and that be the main
solution because everything else will be
off this is going to confuse you that's
also called a bias so we have the bias
in the cell where we're adding a value
the kind of like the Y intercept and we
have a bias of the whole neural network
which means it is weighted towards one
set of answers so we want to make small
changes in these weights so we don't
create a bias and the weights are
adjusted in order to reduce the error or
the cost the network is now trained
using the new weights once again the
cost is determined and back propagation
is continued until the cost cannot be
reduced any further so let's go ahead
and plug in values and see how our
neural network works so here we come in
here and initially our channels are
assigned with random weights this is
important because if you assign them all
with the same weight you might be able
to reproduce it but it turns out that if
I put all my weights as one or all my
weights as zero it takes longer to train
where if you have random weights they
already have like a little bit of
adjustment and ores built in and that
will give us a better answer and train
faster our first neuron takes a value of
mileage and CC as inputs so here comes
our computation whatever those inputs
are and we do that again with the second
neuron with those values coming in you
can see here we have weight three and so
on and then our third neuron coming down
and of course our fourth neuron so we're
adding all these different values coming
in here in our hidden layer the process
value from each neuron is sent to the
output layer over weighted channels so
again here's our weights coming in and
we have N1 N2 N3 and N4 once again the
values are subjected to the activation
function and a single value is emitted
as the output on comparing the predicted
value to the acual value we clearly see
that our Network requires training so
here we have it that our bike price uh
we put out we thought it was worth 2,000
on our random weights and the bike
actually was 4,000 on there guessing
that's not US dollar cuz that'd be a
very expensive bikee but maybe it is
there's some $2,000 $4,000 bikes out
there the cost function is calculated
and back propagation takes place and
this is pretty simple you can look at
that as our um we're subtracting one
value from the other we Square it and
then we take half of that and that is
propagated back up and each layer
generates its own errors let's go back
one cuz you have your predicted Y and
your actual y that goes back to the
first layer and then based on the value
of the cost function certain weights are
changed so we look at the next layer
that error is not the original 4,000 -
2000 squar / 2 this error is based on
the error of each cell generated how far
off is that cell as far as its weights
we're not going to show you it's
actually a very complicated differential
equation and you can probably write it
out if you wanted to you just write out
each formula that goes into the next
level and you add them all together and
you can write it out all the way through
computers make it so you don't have to
and our neural network is considered
trained when the value for the cost
function is minimum so when we get our
error way down as low as we can that's
when our neural network is trained and
there I mean just recently they've come
up with all kinds of different means for
measuring that particular value a little
bit beyond the scope of today's neural
network but you can actually you can
actually see it you know how far do you
do this until the um neural network
doesn't need to be trained anymore and
you can overtrain a neural network now
the tools that we're looking at
automatically let you know when to stop
which is really nice and that is just
like I said we're at the beginning
stages in neural networks and it's just
really cool what they can do now and how
much of it's automated and how much of
it is experimental right now let's take
a look at gradient descent but what
approach do we take to minimize the cost
function so here we have a nice error
thing coming in this is our cost or our
error uh let's start with plotting the
cost function against the predicted
value and so you can see they fed in
multiple y's and these are the errors
coming in and the cost of each of these
inputs and changes going on Note we
start at a random point on the curve so
usually you put in you know you pick up
your data and you randomly pick where to
start in your data a lot of times you
just run it from the beginning because
you're going through so much data is not
that big of a deal but you start with
one point going in so your forward
propagation goes through you're going to
go ahead and find your cost or your
error it points that on the curve and
you can see how we're plotting it right
here since the gradient at this point is
positive we may move right so we're
going to move a little bit to the right
on here and this time the gradient is
negative we move a little bit to left
eventually we try out the point where
the gradient is zero this is the least
value of cost function you have to be a
little careful with this because this
particular I mean they make it look nice
and simple in this graph sometimes these
curves look like stair steps and so
there is global minimums and then there
is local there might be a local point
where the gradient is zero but it's not
the global one uh so it might be way off
to the left where it just happens to
step down a little bit and you think
you're in the right gradient and with
that we have all the right weights and
we can say our network is trained so
here we have um just some major these
are some of the big names out there
right now in development for deep
learning platforms tensorflow which
we'll actually do an example in in a
minute deep learning for J which is in
the Java platform uh so if you're a Java
programmer uh by the way tensorflow is
accessed most people are using python to
access it but it is a system that's kind
of separate from a lot of the
programming languages which makes it a
lot more um flexible as far as used deep
learning for J is Java based and then
cross is just exploding right now and
this is interesting cross is uh working
with tensor flow it actually can sit on
top of tensor flow and it can also do
its own thing uh so if you're studying
deep learning you're getting into it you
want to know the basics of tensor flow
but you also are going to want to know
the upper level of carass sitting on top
of tensor flow we're just looking at
tensor flow today though in our example
and there's also torch on there there's
a bunch more that we didn't list on here
um even sklearn or the Side Package in
Python has a neural network you can
program a very basic one and it is the
same basic one that you could do in
tensorflow if you stripped everything
out of it and then tensorflow has a lot
of tools they've added in and so has
carass but we're going to be looking
specifically at tensor flow in our
example and tensor flow is an open
source tool used to define and run
computations on what they call tensors
very common language now so more and
more we see the term tensor as being a
standard in the uh deep learning
language and this was originally
developed by Google so let's dig a
little bit big in there what are tensors
tensors are just another name for arrays
so a tensor of Dimension five you can
see here we have a b k m q whatever so
it's an array coming in and the tensor
of Dimension 54 more like a picture very
common to see that in a picture you can
also see a tensor even more detailed in
a picture as we go to the next one
tensor of Dimension 333 this is 3D space
you might have a picture that also has
colors that might be the third Dimension
you might have four dimensions because
you have both your grid and your
different color channels and your zplot
you can see where you can now process a
very high level set of data coming in
whether as an image or features they
could be features that have nothing to
do with images so there's a lot of stuff
you can do now with the tensors coming
in thus is where the term tensor flow
comes from so we have um right now that
tensorflow is the most popular library
in deep learning and I did mention
carass now works with tensorflow there's
a lot of stuff you can do between the
two uh it's an open-source software
Library developed by Google uh so they
hit a roadblock and they realized hey
this is an infant stage technology you
know we thought it was going to be the
next greatest thing and we were going to
have a hold on it but it's really infant
as far as how it's applied and what we
can do with it let's open source it so
everybody can work on it uh let's take
it to the next level and that's really
what open source does to a lot of these
uh packages when they release them and
you can run on either a CPU or a GPU so
when we look at the details if you have
your graphic processing units um what's
nice about those is they run a lot
faster the downside is you have to play
with them a little bit to get them up
and running and it's a hardware upgrade
when we run it I'll be running it in the
CPU mode I have played with it in my GPU
on my personal computer you know it does
increase the processing uh but I did run
into some version problems with my
Python and stuff like that and when I
did finally work it out I went back to
the CPU cuz it didn't increase my speed
enough for what I was working on but in
a larger group you might be able to put
that on if you're working with a larger
stack of computers you might want to run
it in the GPU you can create a data flow
graphs that have nodes and edges so
there's our edges coming in we didn't
talk about edges but this very upand
cominging way of looking at your
analytical data as how do different
nodes connect what do those edges look
like in between them and it's used for
machine learning applications such as
neural networks it is mostly a neural
network but they have all kinds of tools
which sit on top of our basic neural
network they have have new stuff
evolving into the tensor flow Library so
it's very much uh just exploding great
time to jump into tensorflow there's all
kinds of cool things are doing with it
and all kinds of cool applications you
can now use a tensor flow for so let's
take a look at implementation in
tensorflow and we're going to build a
neural network to identify handwritten
digits using the uh mnist database or
the MN database and that stands for
modified National Institute of Standards
and Technology database it is a
collection of 70,000 handwritten digits
and the digit labels identify each of
the digits from 0 to 9 this is a cool
example because it's simple enough that
you could actually run this through some
basic machine learning categorizing
algorithms and train them and you'll get
about the same answer because again it's
it's simple grid the digits on the grid
don't have a huge amount of variation
like you would say an automated driving
car looking at the environment so you
can still do this with a lot of your um
different linear models and stuff like
that you can solve this and you'll get
about the same answer when I run a
comparison between tensor flow and
between some basic uh regression models
or category models uh in machine
learning they came up pretty even as far
as their output uh so this is kind of
where we start to see the complexity of
something coming in uh in this case a
tensor you know or a grid of uh
information where the Deep learning
model does as good as the regular models
and when you get past this kind of
complexity and features suddenly the
neural networks come up with better
answers better Solutions and a better
build and that's why there's such a move
into neural networks is we live in a
complicated world and it's just really
cool we can do with this so the
handwritten digits from the um nist
database they come in the data set is
used to train the machine a new image of
a digit is fed and the digit is
identified um and if you've looked at
any of our other machine learning tools
where we're doing training uh where we
train our uh model to fit and then you
test it out this will look pretty
familiar uh and there is some tools out
there for say untrained categorizing uh
where it's just looking for features
that fit together so there are tools
that don't need that training but this
is where uh when we talk about neural
networks we do need to train them and
this is what we're looking at so for
this I'm going to use the Anaconda
Navigator just because it's a very nice
visual tool you might be in py charm or
one of your other IDE for editing python
because we're are looking at Python
tensorflow and under Anaconda we have
The Notebook which is something we use
pretty regularly and they have the
Jupiter lab the Jupiter lab is the
Jupiter notebook but with tabs and a few
new features so we'll be using the
Jupiter lab today and under the
environment you'll want to go ahead and
and uh if you haven't yet uh you'll see
that I have a number of different setups
in here right now I have the python
version 36 and the tensor flow and this
case I have tensorflow 1.12 if we scroll
down you can see that uh here we go
tensor flow and it's version 1.12 and in
here if you haven't yet you'll need to
install those go in and just open our
terminal and um if you've never used the
anaconda or if you're in your other
thing you might have something simple
like pipa is what I use for my install
and you can simply do cond install
tensorflow that should bring in the most
current version now when I installed
this a few months ago python version I'm
not going to run this because I already
have installed on here python version
3.7 the newest one out still had a
couple glitches with the tensor flow I
Believe they' fixed it as a writing of
this but um I'm going to stick with 36
just so I don't get any surprises on
there so this is python version 36 with
in tensor flow 1.12 on here and if you
haven't installed it yet you also want
to install numpy for this example that's
numbers python or uh Nu
mpy you can just simply run install on
there keep in mind if you're in Anaconda
uh and you've created one of these
environments specifically to this keep
with cond if you're going to use cond if
you're going to use pip keep with Pip
don't install one package with Pip and
one under cond because that's how they
track those version numbers and how they
fit together and you can end up with a
problem they don't pip doesn't see cond
and vice versa uh so just keep that in
mind when you're running your installs
we'll go ahead and open up Jupiter lab
and we're going to launch that so here's
my Jupiter lab one of the really cool
features of Jupiter lab is you have tabs
now so you can open up multiple
notebooks this is nice cuz I have my
notes I'm working on and then our actual
window we're looking in and we'll go
ahead and zoom in a little bit here
there we go so you have a nice U
hopefully easy to see fonts and then
we'll go ahead and do a simple our get
our Imports out of the way um and so
we're going to import our tensor flow as
TF uh that's pretty much a standard for
tensor flow numpy our numbers python as
py and we'll import our Matt plot
Library as PLT again these are very
common so if you see TF or py or PLT
this is is a standard that most people
use do you have to no you could just do
import numpy instead of doing as py and
then from tensor flow.
examples. tutorials this is always nice
because they actually include data set
we're going to play with so we're going
to import input data so there's our data
coming in that's all we're doing is um
telling it this is where it's coming
from and if we're going to tell where
it's coming from we need to go ahead and
create a variable with that information
in it and we'll just call this uh MN n
or minced you know I don't really know
how they pronounce that I should
probably look that up it's a very common
data set to use and there's our input
data and we're going to read data sets
and this is um if you look at this we
imported input data from our tensor flow
and so this is a tensorflow read
statement for their tutorials so this
isn't like some special python setup
this is just their setup makes it easy
to pull it in so once we get into their
data sets we need to go and tell out
what kind of data set and again this is
what we brought in but it's going to be
the nince data and this part is very
important one hot equals true this means
that instead of importing a value from 0
to 9 we evaluate the data set it's going
to bring it in as one hot when you ever
see one hot encoder we're flattening
that out and we have true false for zero
true false for One True Falls for two so
our output if you remember from our
output from the slide we did earlier uh
in this case grab the one for bike price
doesn't really matter which one we use
this has one output so we have our bike
price on this we're going to have
instead of one output we're going to
have 10 outputs representing each of the
digits in there and this code really
isn't going to show us anything it's
good to see what we're actually looking
at so um let's go ahead and do a figure
ax equals go into our plot Library
subplots 10 comma 10 and that is if you
remember we talked about tensor tensor
being data coming in this is a 10x1 grid
or 100 pixels on there and if we're
going to display it uh let's go do k0
for I in range 10 just a simple Loop
through on the data let's do what is
itth for J in range 10 so I actually
misquoted that 10 uh 10 x 10 is not the
actual size of the pixels uh the actual
pixels are going to be um we look at the
shapes we'll get into that in just a
second here we'll take a quick look at
the shape on there uh turns out they're
uh what are they are I believe 28x 28 uh
so let's take a look at that and we're
just going to plot these what are we
looking at what are we working with as a
data scientist you should always be
looking back at your data and seeing
what it looks like and get that human
perspective because you just never know
you know the the computer might put
something out that looks makes no sense
and uh at that point you want to go back
and reevaluate what you did uh so we're
going to go ahead and plot we're going
to plot 10 digit you know 10 of the
digits by 10 of the digits and here's
our ax we'll create the JJ on our
subplots and we're going to do an image
show we're going to look at the training
image for images of K and then we want
to reshape this we're going to reshape
this and we're going to reshape this 28
by 28 that's how I knew I had it wrong
it's cuz I looked down my notes I was
like oh no that says 28 it's not 10 x 10
and I should know that already cuz I've
done enough messing with this data set
that I should remembered uh but it's 28
by 28 and the aspect we're going to do
is Auto and this is all if you look at
this here's our variable nist the nist
is coming from data set uh so this is
all part of the TF tensor flow learning
or examples tutorial in there and then
we'll go ahead and do uh K plus equal 1
so we just keep paging through our
different um images and let's see what
that looks like let's go ahead and do a
plot show uh and we'll go ahead and run
this so we can take a look and see what
we have here and so we have a nice plot
here and you can just see that we have
uh some random numbers showing up in
each one of these little subplots if
you're wanting a copy of this code put a
note down in the YouTube video and let
us know or come visit us at www.s
simplylearn
docomo of what we're working on you get
a copy of that for your own setup uh so
now we taken a look we can just see we
have here's our pictures that are coming
on WE plotted them so we have an idea of
what we're looking at let's go ahead and
uh print let's look at the shape of the
features uh so when we have this we have
our nist train images and we'll do the
shape on there let's take a look and
just see what we're looking at uh as far
as uh our account and everything and so
you can see here we have
55,000 that's basically how many images
we have and this by 784 and in this data
set there's also our labels so let's
take a look at that we have our nit
train labels shape let's take a look and
see what that looks like uh and there we
have 10 because there's 10 digits so we
brought in that's our output we're
looking at and so we have there we go
55,000 they match they should match
because you should have equal numbers in
both of those you know here's our data
in and here's our answer if you remember
this is a bunch of zeros and with one
each each one will be 00001 would be
what letter four or something like that
so let's take a look at what our one hot
encoding did for the first observation
and this is when we're exploring data
you really want to dig in there and just
see what the heck am I looking at so
we're going to look at the labels and
this would be the first label that comes
up and we'll go ahead and run this and
we look at that you can see this is what
I'm talking about 0 0 or one is zero two
is 0 3 is 0 four is0 5 is 0 6 is0 7
equals 1 so our very first label is a
seven but a very first label comes up
that it's a seven and so we don't have
like 0 through n we have a bunch of
zeros and just the one to mark it as a
seven on here so now we' kind of looked
a quick look at the data and in here you
might ask some questions like what is
784 24 * 24 remember that's the size of
our grid on there or our tens are coming
in so 784 is a set up on there and we've
gone through all this viewing the data
we'll go ahead and start looking at our
tensor flow so let's take our X variable
this is going to be our training set
we'll do a placeholder and then we're
going to have these come in as float now
if I remember correctly they're actually
you know zero or one for the values
because they're either but we have them
coming in as a float value and we have a
little bit of a shape coming in here and
there's our 784 uh so we let it know
that this is what's what our input is
for our tensor flow and this is our
training set so we'll just put a label
on there to help us uh track that train
set and then w and with W we'll go ahead
and do TF variables and we'll do this as
uh zeros variables TF zeros and we'll
set this as as 784 by 10 10 being the
output 784 being our number of variables
in and this is our weights remember we
have a bias in there too and I'll go
back over this in just a second as we
see how that fits together in our tensor
flow and we'll do this one um with our
variables again we have 10 so we're
going to do the bias we do it the same
kind of format and setup on here and so
we'll do that as as tf0 of 10 so we just
create an array of 10 there and this is
our bias so with these three lines um
and there's actually they're coming out
with the eager execution which would
bypass some of what we're doing but this
is important to understand is the first
thing you have to do with tensor flow is
we have to allocate a space for the
variables in our TF placeholder and our
TF variable with our weights and our
biases this actually hasn't done
anything yet so all it is is
placeholders that's why it's okay to use
zeros um you could have just as easily
used ones or anything else and it
wouldn't matter the next stage is to go
ahead and set up some of the functions
going on but before we do that just note
that this hasn't done anything even if I
execute it all it's done is created
placeholders until we do the final
initialization and so we need to go
ahead and set up let do y tf. nn.
softmax and the code for this is tf.
mapm x w + B and this is our uh sum
let's just put a note here so we can
keep track of what's going on we're
finding weighted sum of inputs plus the
bias uh so there's our plus b the bias
and then we need to go ahead keep um
let's do ycore and again another
placeholder and this one will set um it
actually will put in as TF placeholder
on here TF placeholder float none 10
there's our one hot encoder going on
there so our 10 values coming out and
we'll do a cross entropy on here and
this is going to be minus TF reduce sum
and we'll do y here's our y underscore
which is remember we have your y output
and your actual output uh so this will
be our ycore times the TF log of Y and
then finally um before we do the actual
initialization of all our variables
we'll set up our train step this equals
our gradient descent Optimizer very
important remember we looked at that
chart on our um slides and so we've set
up all these formulas and here's our
gradient descent Optimizer and as it
keeps looking it keeps looking for that
zero value that's what we're doing with
that particular formula so let's take a
look and see what we're doing here we
just put together all of our pieces for
tensor flow and you know the devil's in
the um details we have here our training
set coming in we have to put a
placeholder on there we have our
variables with their weights we have our
biases coming out and then we put in our
uh the weighted sum so here's H summize
our summation here then we have our y
variable output so there's our Y how it
works and then of course the actual
output on there and then we have our
cross entropy coming in and that's our
minus tf. ruce sum the y * the TF log of
Y and then the training step gradient
descent Optimizer and we're using a 0.01
in this and we're going to minimize
cross entropy so we're going to let it
do all the work so once we've set up all
of these different layers we've
allocated for them we need to go ahead
and initialize them so we're going to do
an init TF initialize and it's going to
be all variables uh one of the cool
things is they're in the process of
doing away with this so all these steps
would be bundled into one instead of
having to had placeholders you
initialize them in the same process
going on and then finally everything in
tensorflow is based on your session now
this is changing that there's other
options to be able to run this but we
want to go ahead and do uh session
there's our TF which is a TF session and
then we want to go ahead and do session
run and what are we going to run well we
did initialization of all our variables
uh so this is what we're running and
this is where actually once we do this
we actually create our tensorflow object
so this whole piece of code right here
is our tensor flow object we have our
input coming in with our weighted
variables coming in our soft Max for our
metal going out how does it added
together for our y value uh then we have
the actual uh float value coming out
checking on our all the way down so you
can see all the different stages going
through that we're setting up um and
this is one of the reasons that a lot of
people like tensor flow is because you
can designate all these different pieces
one step at a time this is also one of
the reasons people don't like tensor
flow is because you have to designate
all the different layers coming down and
there's a lot of steps being made right
now to minimize this to make it either
easier to automate it or to allow you to
do more complicated things and all those
steps are still at play so it's worth
looking into the more advanced version
of what's going on with carass on top of
tensorflow it's also important to
understand what's going on in these
individual levels if you're going to
play with them it's important to
understand hey what's going on with the
uh finding the weighted sum of the
inputs plus the bias because there's
other ways to do that there's all kinds
of other tools in there now but this is
the basic setup that you want to do on a
tensor flow coming in and we want to go
ahead and just run and knit our tensor
flow so let's go ahead and do that let's
run this we do get a warning here
because uh there's a move to use Global
variables this is one of the changes
they're making but it as far as this
example it's not going to make a
difference because we're doing once we
initialize it this is initializing our
variables and again these are only
placeholders up here until we initialize
them and I would highly suggest put a
note down there or or go over to Simply
learn.com and let them know and have
them email you a copy of the code so you
can actually play with this code right
here because this is the body of what's
going on in tensorflow this is the build
in neural networks and then once we've
done that now comes kind of the fun part
is we need to go ahead and train it uh
so we've created our tensor flow we've
created our Network and now we need to
go ahead and train it uh so let's put
together that training code and let's
just do uh for ION range uh 1,000 0 to
1,000 so we're just going to look at uh
the first 1,000 in our training and the
way we pull that data from our mints
train next batch of 100 uh so you look
at this we're going to be doing groups
of 100 and then there's going to go
through a thousand of them this is very
important that tensor flow builds this
in this is one of the downsides of doing
sklearn or one of the older packages is
they don't let you batch groups in uh
they wanted to have it all up front and
then you have to build your own batch
programs right now uh this lets us go
ahead and do that you can see here we
have batch X of s batch y of s so
there's our X and our y you can look at
this as our training of X and our train
of Y or the data n and the answer n uh
and then we simply do our session run uh
so here's our session that we've created
we're going to run it and we want to do
the train step we initialized our train
step up here and our TF and so there's
our train step feed it's a dictionary
dictionary coming in which we're going
to create right here uh is X is our
batch X of our sample comma and our y
underscore is going to be our batch of
our y sample uh and so this goes through
and we've now hit the Run button and
we've trained trained our session we've
trained this setup on here and once
we've trained it then we need to go
ahead and find out how good our accuracy
was and actually start running some
predictions through there uh so we'll go
ahead and create a a correct prediction
and this is where our tf. equal we'll
use our argmax y of one and TF argmax of
Y of underscore of one to help us get
the correct predictions on there and
then we want to use that to feed into an
accuracy and so our accuracy is going to
be TF reduce uh mean and we'll take that
and we'll do um cast and this is the
correct prediction that we're sending in
there and it is a float value keep it
simple and let's go ahead and print this
out so we can see where we're looking at
uh so what are we printing out uh we
need to do a session run this session
run is going to be on the accuracy
worded accuracy comes from this is our
we're casting our TF on there with the
correct predictions on that so here's
our accuracy feed in so it needs a
dictionary for the data coming in we're
going to create our dictionary and X
just going to be our nist test images
and ycore there is going to be our nest.
test labels let me just double check and
make sure I have that typed in there
correctly there we go and let's go ahead
and run that and see what comes up and
we end up with a
9165 for our accuracy which means our
trained neural network does a pretty
good job letting us know what these
different symbols are and guessing that
a seven and a three and a four uh
something that is humans we kind of take
for granted I even have trouble reading
this so I don't know if I would be able
like that first one I would sit there
for a long time figuring out that's a
seven versus a two that could have
easily been a two to me seeing see we do
a pretty good job analyzing this data
and this is used to analyze something
very complicated on these images very
different than uh just a straight value
of uh cost of sales and here's our
return and our marketing uh we can now
create this nice neural network that
does all kinds of cool things let's move
on to the differences between tensorflow
Kass and P touch the first difference
that we'll be looking at is called level
of API there are two main types of apis
a lowlevel API and a high level API API
stands for application programming
interface a lowlevel application
programming interface is generally more
detailed and allows you to have more
detailed control to manipulate functions
within them on how to use and Implement
them while a high level API is more
generic and simple and provides more
functionality with one command
statements than a lower level API high
level interfaces are comparatively
easier to learn and to implement the
models using them they allow you to
write code in a shorter amount of time
and to be less involved with the details
in this case tensor flow is a high and
lowlevel API pure tensor flow is a
lowlevel API while tensorflow wrapped in
kasas is a high level API
Keras in itself is a high level API
which uses multiple low-level apis as a
backend and simplifies the operation of
these low-level apis py torch is a
low-level API the next criteria that
we'll be looking at is speed tensor flow
is very fast and is used for high
performances caras is slower as it works
on top of tensor flow not only does it
have to wait for tens of FL to finish
implementation it then starts its own
implementation
meanwhile pytorch works at the same
speed as tensorflow as both of them are
both low-level apis now kasas is a rer
class for tensor flow and has added
abstraction functionalities on top of
tensor flow which make it slower than
tensor flow and py toor in computation
speed both tensor flow and py toch are
almost equal and in development speed
kasas is faster as it has built-in
functionalities which can significantly
reduce your development time the next
difference is on the
architecture tensor flow is not very
easy to use and even though it provides
caras as a framework that makes it work
easier tensorflow still has a very
complex architecture which is hard to
use meanwhile kasas has a simpler
architecture and is easier to use it
provides a high level of abstraction
which makes implementation of programs
in kasas significantly easier pych on
the other hand also has a complex
architecture and the readability is less
when compared to kasas tensorflow uses
computational graphs which makes it very
complex and hard to interpret but it has
amazing computational ability across
platforms py is a little hard for
beginners but is really good for
computer vision and deep learning
purposes data sets and debugging tensor
flow works with large data sets due to
its high execution speed and debugging
is really hard intensive flow due to its
complex nature meanwhile while kasas
only works with very small data sets as
its speed of execution is low programs
do not require frequent debugging in
kasas as they are relatively simpler and
P toch can manage high level tasks in
higher Dimension data sets and is easier
to debug than both Keras and
tensorflow next we'll be looking at ease
of development as we said before
tensorflow works with many hard Concepts
such as comput graphs and tensors which
means that writing code in tensor flow
is very hard it is generally used by
people when they are doing research work
and really need very specific
functionalities kasas on the other hand
provides a high level of abstraction
which makes it very easy to use it is
best for people who are just starting
out with python and machine learning py
toch is easier than tens oflow but is
still comparatively hard than Keras it
is not very easy to learn for beginners
but is significantly more powerful than
just plain caras ease of deployment tens
of flow is very easy to deploy as it
uses tensorflow serving tensorflow
serving is a flexible high performance
serving system for machine learning
models designed for production
environments tensorflow serving makes it
easy to deploy new algorithms and
experiments while keeping the same
server architecture and apis tensorflow
serving provides out of the box
integration with tensorflow models but
can be easily extended to serve other
types of models and data in kasas model
deployment can be done with either
tensorflow serving or flask which makes
it relatively easy but not as easy as
you as it would be with tensorflow and
py to py uses py mobile which makes
deployment easy but again for tensorflow
deployment is way easier as tensorflow
serving can update your machine learning
backend on the flat
without the user even realizing there's
a growing need to execute ml models on
edge devices to reduce latency preserve
privacy and enable new interactive use
cases in the past Engineers used to
train models separately they would then
go through a multi-step error prone and
often complex process to train the
models for execution on a mobile device
the mobile runtime was often
significantly different from the
operations available during training
leading to inconsistent developer and
eventually user experience all of these
frictions have been removed by pyo
Mobile by allowing a seamless process to
go from training to deployment by
staying entirely within the pyo
ecosystem it provides an endtoend
workflow that simplifies the research to
production environment for mobile
devices in addition it paves the way for
privacy preserving features via
Federated learning techniques
at the end of the day the question that
really matters is which framework should
you use kasas tensorflow or P
Touch now tensor flow has implemented
various levels of abstraction to make
implementation of deep learning and
neural networks easy this has also made
debugging easier kasas is simple and
easy but not as fast stens of flow it is
more user friendly than any other deep
learning API however and is easier to
learn for beginners py on the other hand
is the preferred deep learning API for
teachers but it is not as widely used in
production as tensor flow is it is
faster but it has lower GPU
utilization at the end of the day the
framework that we would suggest that you
use is tensor
flow why while py torch may have been
the preferred deep learning library for
researchers tsor flow is much more
widely used in day-to-day production
pych is ease of use combined with the
default eager execution mode for easier
debugging predestines it to be used for
fast hacky Solutions and smaller scale
models but tensor flows extensions for
deployment on both servers and mobile
devices combined with the lack of python
overhead makes it the preferred option
for companies that work with deep
learning models in addition the tens
oflow board visualization features
offers a nice way of showing the inner
workings of your model to say your
customers meanwhile between tensive flow
and caras the main difference isn't in
performance tensorflow is a bit faster
due to less overhead but also the level
of control you would like kasas is much
easier to start with then plain tens of
flow but if you want to do something
with kasas that doesn't come out of the
box it'll be harder to implement that
tens of flow on the other hand allows
you to create any arbitrary
computational graph providing much more
flexibility so so if you're doing more
research type of work tensor flow is the
sure route to go due to the flexibility
that it provides then accelerate your
career in Ai and ml with a comprehensive
post-graduate program in artificial
intelligence and machine learning so
enroll now and unlock exciting Ai and
machine learning opportunities the link
is mentioned in the description box
below so many things out there that the
1.0 really needed so when we start
talking about tensor flow 1.0 versus 2.0
um I guess you would need to know this
for uh a legacy programming job if
you're pulling apart somebody else's
code the first thing is that tensorflow
2.0 supports eager execution by default
it allows you to build your models and
run them instantly and you can see here
from tensorflow one to tensorflow 2 uh
we have um almost double the code to do
the same thing so if I want to do um
with tf. session or tensorflow session
um as a session the session run you have
your variables your session run you have
your tables initializer and then you do
your model fit um X train y train and
then your validation data your x value
yv value and your epics and your batch
size all that goes into the fit and you
can see here where that was all just
compressed to make it run easier you can
just create a model and do a fit on it
uh and you only have like that last set
of code on there so it's automatic
that's what they mean by the eager so if
you see the first part and you're like
what the heck is all this session thing
going on that's tensor flow 1.0 and then
when you get into 2.0 it's just nice and
clean if you remember from the beginning
I said coros on our list up there and
cross is the highlevel API in tensorflow
2.0 cross is the official highle API of
tensorflow 2.0 it has incorporated cross
as tf. cross cross provides a number of
model building apis such as sequential
functional and subclassing so you can
choose the right level of abstraction
for your project and uh we'll hopefully
touch base a a little bit more on this
sequential being the most common uh form
that is your your layers are going from
one side to the other so everything's
going in a sequential order
functional is where you can split the
layers so you might have your input
coming on one side it splits into two
completely Mo different models and then
they come back together U and one of
them might be doing classification the
other one might be doing just linear
regression kind of stuff or neural basic
uh reverse propagation neural network
and then those all come together into
another layer which is your uh neural
network reverse propagation setup
subclassing is the most complicated as
you're building your own models and you
can subclass your own models into carass
so very powerful tools here this is all
the stuff that's been coming out
currently in the tensorflow carass setup
a third big change we're going to look
at is it in tensor flow
1.0 uh in order to use TF layers as
variables uh you would have to write TF
variable block so you'd have to
predefine that in tensor flow 2 you just
add your layers in under the sequential
and it automatically defines them as
long as they're flat layers of course
this changes a little bit as some more
complicated um tensor you have coming in
but all of it's very easy to do and
that's what 2.0 does a really good job
of and here we have um a little bit more
on the scope of this and you can see how
tensorflow one asks you to do um these
different layers and values if you look
at the scope and the default name you
start looking at all the different code
in there to create the variable scope
that's not even necessary in t or 2.0 so
you'd have to do one before you do do
what you see the code in 2.0 in 2.0 you
just create your model it's a sequential
model and then you can add all your
layers in you don't have to pre-create
the um uh variable scope so if you ever
see the variable scope you know that
came from an older version and then we
have the last two which which is our API
cleanup and the autograph uh in the API
cleanup tensor flow one you could build
models using TF Gans TF app TF contri TF
Flags Etc in tensorflow 2 uh a lot of
apis have been removed and this is just
they just cleaned them up CU people
weren't using them and they've
simplified them and that's your TF app
your TF flags and your TF logging are
all gone uh so there's those are three
Legacy features that are not in 2.0
and then we have our TF function and
autograph feature in the old version uh
tensorflow 10 the python functions were
limited and could not be compiled or
exported
reimported so you were continually
having to redo your code and you
couldn't very easily just um put a
pointer to it and say hey let's reuse
this in tensorflow 2 you can write a
python function using the TF function to
mark it for the jit compilation for the
python jit so that tensorflow runs it as
a single graph autograph feature of TF
function helps to write graph code using
natural python
syntax uh now we just threw in a new
word in you graph uh graph is not a
picture of a person uh you'll hear graph
x and some other things graph is what
are all those lines that are connecting
different objects so if you remember
from before where we had uh the
different layers going through
sequentially each one of those wh lined
arrows would be a graph FX that's where
that computation is taken care of and
that's what they're talking about and so
if you had your own special code or
python way that you're sending that
information forward you can now put your
own function in there instead of using
whatever function they're using in
neural networks this would be your
activation function although it could be
almost anything out there depending on
what you're doing next let's go for
hierarchy and architecture and then
we'll cover three basic Tools in
tensorflow before we roll up our sleeves
and dive into the example so let's just
take a quick look at tensor flow tool
kits in their hierarchy at the high
level we have our object-oriented API so
this is what you're working with you
have your TF carass you have your
estimators this sits on top of your TF
layers TF losses TF metrics so you have
your reusable libraries for model
building this is really where tins or
flow shines is between the carass uh
running your estimators and then being
able to swap in different layers you can
your losses your metrics all of that is
so built into tensorflow makes it really
easy to use and then you can get down to
your lowlevel
TFI um you have extensive control over
this you can put your own formulas in
there your own procedures or models in
there uh you can have it split we talked
about that earlier so with the 2.0 you
can now have it split One Direction
where you do a linear regression model
and then go to the other where it does a
uh neural network and maybe each neural
network has a different activation set
on it and then it it comes together into
another layer which is another neural
network so you can build these really
complicated models and at the low level
you can put in your own apis you can
move that stuff around and most recently
we have the TF code can run on multiple
platforms and so you have your CPU which
is uh basically like on the computer I'm
running on I have uh eight cores and 16
dedicated threads I hear they now have
one out there that has over a 100 cores
uh so you have your TPU running and then
you have your GPU which is your graphics
card and most recently they also include
the TPU setup which is specifically for
tensor flow models uh neural network
kind of set up so now you can export the
TF code and it can run on all kinds of
different platforms for the most um
diverse setup out there and moving on
from the hierarchy to the architecture
in the tensorflow 2.0 architecture uh we
have uh you can see on the left this is
usually where you you start out with and
80% of your time in data science is
spent pre-processing data making sure
it's loaded correctly and everything
looks right uh so the first level in
tensor flow is going to be your read and
pre-processed data your TF data feature
columns this is going to feed into your
TF Cross or your pre-made
estimators and kind of you have your
tensor flow Hub that sits on top of
there so you can see what's going on uh
once you have all that set up you have
your distribution strategy where are you
going to run it are you going to be
running it on just your regular CPU are
you going to be running it uh with a GPU
added in um like I have a pretty highend
graphics card so it actually grabs that
GPU processor and uses it or do you have
a specialized TPU setup in there that
you paid extra money for uh it could be
if you're and later on when you're
Distributing the package you might need
to run this on some really high
processors because you're processing at
a server level for uh let's say you
might be processing this at a um a
distribute you're Distributing it not
the distribution strategy but you're
Distributing it into a server where that
server might be analyzing thousands and
thousands of purchases done every minute
um and so you need that higher speed to
give them a um to give them a
recommendation or suggestion so they can
buy more stuff off your website or maybe
you're looking for uh data fraud
analysis working with the banks you want
to be able to run this at a high speed
so that when you have hundreds of people
sending their transactions in it says
hey this doesn't look right someone's
scamming this person and probably has
their credit card so when we're talking
about all those fun things we're talking
about saved model this is we were
talking about that earlier where it used
to be when you did one of these models
it wouldn't truncate the float numbers
the same and so a model going from one
you build the model on your com machine
in the office and then you need to
distribute it and so we have our tensor
flow serving Cloud on premium that's
what I was talking about if you're like
a banking or something like that now
they have tensorflow light so you can
actually run a tensorflow on an Android
or an iOS a Raspberry Pi little breakout
board there in fact they just came out
with a new one that has a built-in this
just a little mini TPU with the camera
on it so it can pre-process a video so
you can load your tensor flow model onto
that um talking about an affordable way
to beta test uh a new product uh you
have the tensorflow JS which is for
browser and noes server so you can get
that out on the browser for some simple
computations that don't require a lot of
heavy lifting but you want to distribute
to a lot of end points and now they also
have other language bindings so you can
now create your uh tensorflow backend
save it and have it accessed from C Java
go C rust r or from whatever package
you're working on so we kind of have an
overview of the architecture and what's
going on behind the scenes and in this
case what's going on as far as
Distributing it let's go ahead and take
a look at uh three three specific pieces
of tensor flow and those are going to be
constants variables and sessions uh so
very basic things you need to know and
understand when you're working with the
tensor flow uh setup so constants in
tensor flow in tensorflow constants are
created using the function constant in
other words they're going to stay static
the whole time whatever you're working
with the Syntax for constant uh value
dtype 9 shape equals none name constant
verify shape equals false that's kind of
the syntax you're looking at and we'll
explore this with our hands on a little
more in depth uh and you can see here we
do zal tf. constant 5.2 name equals x uh
dtype is a float that means that we're
never going to change that 5.2 it's
going to be a constant value and then we
have our variables in tensor flow uh
variables in tensor flow are in memory
buffers that store tensors and so we can
declare a 2x3 tensor populated by ones
you could also do constants this way by
the way so you can create a um an array
of ones for your constants I'm not sure
why you do that but you know you might
need that for some reason um in here we
have V equals tf.
variables and then in tensorflow you
have tf. On's and you have the shape
which is 23 which is then going to
create a nice uh 2x3 um array that's
filled with ones and then of course you
can go in there and their variables so
you can change them it's a tensor so you
have full control over that and then you
of course have a s in tensor flow a
session in tensorflow is used to run a
computational graph to evaluate the
nodes and remember when we're talking
about graph or graph x we're talking
about all that information then goes
through all those arrows and whatever
computations they have that take it to
the next node and you can see down here
uh where we have import tensor flow as
TF if we do x equals a tf. constant of
10 we do yal a TF constant of 2.0 of
20.0 and then you can do zal tf.
variable and it's a tf. addx comma y uh
and then once you have that set up in
there you go ahead and init your TF
Global variables initializer with TF
session as session you can do a session
run nit and then you print the session
run
y uh and so when you run this you're
going to end up with of course the uh 10
+ 20 is 30 and we'll be looking at this
a lot more closely as we actually roll
up our sleeves and put some code
together so let's go ahead and take a
look at that and for my coding today I'm
going to go ahead and go through
anaconda and then I'll use specifically
the Jupiter Notebook on there and of
course this code is going to work uh
whatever platform you choose whether
you're in a notebook um the Jupiter lab
which is just a Jupiter notebook but
with tabs for larger projects we're
going to stick with Jupiter notebook pie
charm uh whatever it is you're going to
use in here uh you know you have your
spider and your QT console for different
programming environments the thing to
note um it's kind of hard to see but I
have my main Pi
36 right now when I was writing this
tensor flow Works in Python version 36
if you have python version 37 or 38
you're probably going to get some errors
in there uh might be that they've
already updated and I don't know it and
I have an older version but you want to
make sure you're in Python version 36 in
your environment and of course in
Anaconda I can easily set that
environment up make sure you go ahead
and and um pip in your tensor flow or if
you're in Anaconda you can do AA install
tensor flow to make sure it's in your
package so let's just go ahead and dive
in and bring that up this will open up a
nice browser window I just love the fact
I can zoom in and zoom out depending on
what I'm working on making it really
easy to adjust um a demo for the right
size go under new and let's go ahead and
create a new Python and once we're in
our new python window this is is going
to leave it Untitled uh let's go ahead
and import import tensor flow as TF uh
at this point we'll go a and just run it
real quick no errors yay no
errors I do that whenever I do my
imports because I I unbearably will have
opened up a new environment and
forgotten to install tensor flow into
that environment uh or something along
those lines so it's always good to
double
check uh and if we're going to double
check that we also it's also good to
know uh what version we're working with
and we can do that simply by um using
the version command in
tensorflow which uh you should know is
is probably intuitively the TF _ uncore
verore
uncore and you know it always confuses
me because sometimes you do tf. version
for one thing you do TF F doore version
underscore for another thing uh this is
a double underscore in tensor flow for
pulling your version out and it's good
to know what you're working with we're
going to be working in tensor flow
version
2.1.0 and I did tell you that the um we
were going to dig a little deeper into
our constants and you can do an array of
constants and we'll just create this
nice array um AAL tf. constant and we're
just going to put the array right in
there
4361 uh we can run this and now that is
what a is equal to and if we want to
just double check that uh remember we're
in Jupiter notebook where we can just
put the letter a and it knows that
that's going to be print um otherwise
you you surround it in print and you can
see it's a TF tensor it has the shape
the type and the and the array on here
it's a 2X two array and just like we can
create a constant we can go and create a
variable and this is also going to be a
2X two array and if we go ahead and
print the V out we'll run that uh and
sure enough there's our TF variable in
here uh then we can also let's just go
back up here and add this in here um I
could create another tensor and we'll
make it a constant this
time and we'll go and put that in over
here uh we'll have B TF constant and if
we go and print out uh V and B let go
and run that and this is an interesting
thing that always that happens in here
uh you'll see right here when I print
them both out what happens is only
prints the last one unless you use print
commands uh so important to remember
that in Jupiter notebooks but we can
easily fix that by go ahead and print
and Surround V with brackets and now we
can see with the two different variables
we have uh we have the 3152 which is a
variable and this is just a flat a
constant so it comes up as a TF tensor
shape two kind of two and that's
interesting to note that this label is a
tf. tensor and this is a TF variable so
that's how it's looking in the back end
when you're talking about the difference
between a variable and a constant the
other thing I want you to notice is that
in variable we capitalize the V and with
the constant we have a lowercase C
little things like that can lose you
when you're programming and you're
trying to find out hey why doesn't this
work uh so those are a couple little
things to note in here and just like any
other array in math uh we can do like a
concatenate or concatenate the different
values here uh and you can see we can
take um AB concatenated you just do a
tf. concat values and there's our AB
axis on one hopefully you're familiar
with axes and how that works when you're
dealing with matrixes and if we go ahead
and print this out uh you'll see right
here we end up with a tensor so let's
put it in as a constant not as a
variable and you have your array 4378
and 6145 it's concatenated the two
together and again I want want to
highlight a couple things on this our
axis equals 1 this means we're doing the
columns um so if you had a longer array
like right now we have an array that is
like you know has a shape one Whatever
It Is 2 comma 2 um axis zero is going to
be your first one and axis one is going
to be your second one and it translates
as columns and rows if we had a shape
let me just put the word shape here um
so you know what I'm talking about and
it's very and this is I'll tell you what
I spent a lot of time looking at these
shapes and trying to figure out which
direction I'm going in and whether to
flip it or whatever um so you can get
lost in which way your Matrix is going
and which is column which is rows are
you dealing with the third Axis or the
second axis um axis one you know 0 1 2
that's going to be our columns uh and if
you can do columns then we also can do
rows and that is simply just changing
the concatenate uh we'll just grab this
one here and copy it we'll do the whole
thing over um contrl
copy crl V and changes from axis one to
axis zero and if we run that uh you'll
see that now we can catenate by row as
opposed to column and you have 43 61
7847 so it just brings it right down
turns it into rows versus columns you
can see the difference there your output
this really you want to look at the
output sometimes just to make sure your
eyes are looking at it correctly and
it's in the format um I find visually
looking at it is almost more important
than understanding what's going on uh
because conceptually your mind just just
too many dimensions sometimes the second
thing I want you to notice is this says
a numpy array uh so tensor flow is
utilizing numpy as part of their format
as far as Python's concerned and so you
can treat you can treat this output like
a numpy array because it is just that
it's going to be a numpy array other
thing that comes up uh more than you
would think is filling U one of these
with zeros or ones and so you can see
here we just create a tensor tf. Zer and
we give it a shape we tell it what kind
of data type it is in this case we're
doing an integer and then if we um print
out our tensor again we're in Jupiter so
I can just type out tensor and I run
this you can see I have a nice array of
um with shape 3 comma 4 of zeros one of
the things I want to highlight here is
integer 32 if I go to the um tensor flow
data types I want you to notice how we
have float 16 float 32 float 64 uh
complex if we scroll down you'll see the
integer down here 32 the reason for this
is that we want to control how many bits
are used in the
Precision this is for exporting it to
another platform uh so what would happen
is I might run it on this computer where
python goes does a float to indefinite
however long it wants to um and then we
can take it but we want to actually say
hey we don't want that high Precision we
want to be able to run this on any
computer and so we need to control
whether it's a TF float 16 in this case
we did an integer
32 we could also do this as a float so
if I run this as a float 32 uh that
means this has a 32-bit Precision you'll
see 0 point whatever and then to go with
uh
zeros we have ones if we're going from
the opposite side and so we can easily
just create a tensor flow with
ones you might ask yourself why would I
want zeros and ones and your first
thought might be to initiate a new
tensor usually we initiate a lot of this
stuff with random numbers because it
does a better job solving it if you
start with a uniform uh set of ones or
zeros you're dealing with a lot of bias
so be very careful about starting a
neural network uh for one of your rows
or something like that with ones and
zeros on the other hand uh I use this
for masking you can do a lot of work
with masking you can also have uh it
might be that one tense a row is masked
um you know zero is is false one is true
or whatever you want to do it um and so
in that case you do want to use the
zeros and ones and there are cases where
you do want to initialize it with all
zeros or all ones and then swap in
different numbers as the as the um
tensor learns so it's another form of
control but in general
you see zeros and ones you usually are
talking about a mask over another array
and just like in uh numpy you can also
uh do reshapes so if we take our um
remember this is shaped 3 comma 4 maybe
we want to swap that to 4 comma 3 and if
we print this out you will see let me
just go and do that contrl V let me run
that and you'll see that the the order
of these is now switched instead of uh
four across now we have three across and
four down
down and just for fun let's go back up
here where we did the ones and I'm going
to change the ones to um tf. random
uniform uh and we'll go and just take
off well we'll go and leave that we'll
go and run this and you'll see now we
have uh
0441 and this way you can actually see
how the reshape looks a lot different uh
0411 5.71 and then instead of having
this one it rolls down here to the
0.14
and this is uh what I was talking about
sometimes you fill a lot of times you
fill these with random numbers and so
this is the random. uniform is one of
the ways to do that now I just talked a
little bit about this float 32 and all
these data types uh one of the things
that comes up of course is recasting
your
data um so if we have a dtype float 32
we might want to convert these two
integers because of the project we're
working on um I know one of the projects
I've worked on ended up wanting to do a
lot of roundoff so that it would take a
dollar amount or a float value and then
have to round it off to a dollar amount
so we only wanted two decimal points um
and in which case you have a lot of
different options you can multiply by
100 and then round it off or whatever
you want to do there's a lot of or then
convert it to an integer was one way to
round it off uh kind
of cheap and dirty
trick uh so we can take this and we can
take the same tensor and we go ahead and
create a um as an integer and so we're
going to take this tensor we're going to
tf. cast it and if we
print tensor uh and then we're going to
go ahead and
print our tensor let me just do a quick
copy and paste and when I'm actually
programming I usually type out a lot of
my stuff just to double check it uh in
doing a demo copy and paste works fine
but sometimes be aware that uh copy and
paste can copy the wrong code over
personal choice depends on what I'm
working on and you can see here we took
um a float 32 4.6 4.2 and so on and it
just converts it right down to a integer
value uh it's our integer 32 setup and
uh remember we talked about um a little
bit about
reshape um as far as flipping it and I
just did uh 4 comma 3 on the reshape up
here and we talked about axis zero axis
one uh one of the things that is
important to be able to do is to take
one of these variables we'll just take
this last one tensor as
integer and I want to go ahead and
transpose it and so I can do um we'll do
a equals tf.
transpose and we'll do our tensor
integer in there and then if I print the
A out and we run this you'll see it's
the same array but we've flipped it so
that our columns and rows are flipped
this is the same as reshaping uh so when
you transpose you're just doing a
reshape what's nice about this is that
if you look at the numbers The Columns
when when we went up here and we did the
reshape they kind of roll down to the
next row so you're not maintaining the
structure of your Matrix so when we do a
reshape up here they're similar but
they're not quite the same and you can
actually go in here and there's settings
in the reshape that would allow you to
turn it into a transform
uh so when we come down here it's all
done for you and so there are so many
times you have to transpose your digits
that this is important to know that you
can just do that you can flip your rows
and columns rather quickly here and just
like numpy you can also do multip your
different math functions we'll look at
multiplication and so we're going to
take matrix multiplication of tensors uh
we'll go and create a as a constant
5839 and we'll put in a vector v 4 comma
2 and we could have done this where they
matched where this was a 2X two array um
but instead we're going to do just a 2x1
array and the code for that is your tf.
matat mole uh so Matrix multiplier and
we have a * V and if we go ahead and run
this oh
let's make sure we print out our av on
there and if we go ahead and run this uh
you'll see that we end up with 36 by 30
and if it's been a while since you've
seen The Matrix math uh this is 5 * 4 +
8 * 2 um 3 * 4 + 9 *
2 and that's where we get the 36 and 30
now I know we're covering a lot really
quickly as far as the basic
functionality uh so the Matrix or your
Matrix multiplier is a very commonly
used backend tool as far as computing
um uh different models or linear
regression stuff like that one of the
things is to note is that just like in
um numpy you have all of your different
math so we have our TF math and if we go
in here we have um functions we have our
cosiness absolute angle all of that's in
here so all of these are available for
you to use in the tensorflow model and
if we go back to our example let's go
ahead and pull um oh let's do some
multiplication that's always good we'll
stick with our um AV our um constant a
and our vector
v and we'll go ahead and do some bitwise
multiplication and we'll create an AV
which is a * V let's go and print that
out and you can see coming across here
uh we have the 42 and the
5839 and it produces uh 20 32 618
and that's pretty straightforward if you
look at it you have 4 * uh 5 is 20 4 * 8
is uh 32 that's where those numbers come
from uh we can also quickly create an
identity
Matrix which is basically um your main
values on the diagonal being ones and
zeros across the other side let's go
ahead and take a look and see what that
uh
uh looks like and we can do let's do
this uh so we're going to get the shape
um this is a simple way very similar to
your numpy you can do a. shape and it's
going to return a tupal in this case our
rows and columns and so we can do a
quick uh print we'll do rows
oops and we'll do
columns
and if we run this uh you can see we
have three rows uh two
columns and then if we go ahead and
create an identity
Matrix the
Scoops the script for that hit a wrong
button there the script for that looks
like
this where we have the number of rows
equals rows the number of Colum colums
equals columns and dtype is a 32 and
then if we go ahead and just print out
our
identity you can see we have a nice
identity column with our ones going
across here now clearly we're not going
to go through every math module um
available but we do want to start
looking at this as a prediction model
and seeing how it functions so we're
going to move on to a more of a a um
direct setup where you can actually see
the full tensor flow in use for that
let's go back and create a uh new
setup and we'll go in here new Python 3
module there we go bring this out so it
takes up the whole window because I like
to do that hopefully you made it through
that first part and you have a basic
understanding of tensor flow as far as
being uh a series of numpy arrays you
got your math equations and different
things that go into them we're going to
start building a full um setup as far as
the numpy so you can see how uh K sits
on top of it and the different aspects
of how it works the first thing we want
to do is we're going to go and do a lot
of imports uh date times warning scipi
scipi is your um uh math so the backend
scientific math uh warnings because
whenever we do a lot of this you have
old older versions newer versions um and
so sometimes when you get warnings you
want to go ahead and just suppress them
we'll talk about that if it comes up on
this particular setup and of course date
time pandas again is your data frame
think rows and columns we import it as
PD numpy is your uh numbers array which
of course tensor flow is integrated
heavily with caborn for our graphics and
the caborn as SNS is going to be set on
top of our mat plot Library which we
import as MPL and then of course we're
going to import our matap plot Library
pip plot as PLT and right off the bat
we're going to set some graphic colors
um patch Force Edge color equals true
the style we're going to use the 538
style you can look this all up there's
when you get into map plot Library into
Seaborn there are so many options in
here it's just kind of nice to make it
look pretty when we start the um when we
start up that way we don't have to think
about it later on on uh and then we're
going to take we have our uh mlrc we're
going to put a patch Ed color dim gray
linewidth again this is all part of our
Graphics here in our setup uh we'll go
ahead and do an interactive shell uh
node interactivity equals last
expression uh here we are PD for pandas
options display Max columns so we don't
want to display more than 50 um and then
our map plot library is going to be in
line This is a Jupiter notebook thing
the map plot library in line uh then
warnings we're going to filter our
warnings and we're just going to ignore
warnings that way when they come up we
don't have to worry about them not
really what you want to do when you're
working on a major project you want to
make sure you know those warnings and
then uh filter them out and ignore them
later on and if we run this it's just
going to be loading all that into the
background uh so that's a little backend
kind of stuff then what we want to go
ahead and do is we want to go ahead and
import our specific packages U that
we're going to be working with with
which is under carass now remember
carass kind of sits on tensorflow so
when we're importing carass and the
sequential model we are in effect
importing um tensor flow underneath of
it uh we just brought in the math
probably should have put that up above
and then we have our carass models we're
going to import sequential now if you
remember from our uh slide there was
three different options let me just flip
back over there so we can have a quick
uh recall on that and so in carass uh we
have sequential functional and
subclassing so remember those three
different setups in here we talked about
earlier and if you remember from here we
have uh sequential where it's going one
tensor flow layer at a time you go kind
of look think of it as going from left
to right or top to bottom or whatever
Direction it's going in but it goes in
One Direction all the time where
functional can have a very complicated
graph of directions you can have the
data split into two separate um t s and
then it comes back together into another
tensor um all those kinds of things and
then subclassing is really the really
complicated one where now you're adding
your own sub classes into the tensor to
do external computations right in the
middle of like a huge flow of data uh
but we're going to stick with sequential
it's not a big jump to go from
sequential to functional uh but we're
running a sequential tensor flow and
that's what this first import is here we
want to bring in our sequential and then
we have our layers and let's talk a
little bit about these layers this is
where cross and tensor flow really are
happening this is what makes them so
nice to work with is all these layers
are pre-built uh so from carass we have
layers import dents from carass uh
layers import
lstm when we talk about these layers uh
coros has so many built-in layers you
can do your own layers the dense layer
is your standard neural network U by
default it uses Ru for its
activation and then the lstm is a long
shortterm memory layer since we're going
to be looking probably at sequential
data uh we want to go ahead and do the
lstm and if we go
into um carass and we look at their
layers this is the carass website you
can see as we scroll down for the cross
layers that are built in we can get down
here and we can look at see here we have
our layer activation our base layers um
activation layer weight layer weight
just a lot of stuff in here we had the
ru which is the basic activation that
was listed up here for layer activations
you can change those and here we have
our core layers in our dense layers you
have an input layer a dense layer um and
then we've added more customized one
with the long-term short-term memory
layer and of course you can even do your
own custom layers in carass there's a
whole functionality in there if you're
doing your own thing
what's really nice about this is it's
all built in uh even the convolutional
layers this is for processing Graphics
there's a lot of cool things in here you
can do um this is why cross is so
popular it's open source and you have
all these tools right at your fingertips
so from Cross we're just going to import
a couple layers the dense layer um and
the long short-term memory layer and
then of course from uh sklearn our s kit
we want to go ahead and do our minmax
scaler standard scaler for pre-editing
our data and then metrics just so we can
take a look at the errors and compute
those let me go ahead and run this and
that just loads it up we're not
expecting anything from the output and
our file coming in is going to be air
quality. CSV let's go ahead and take a
quick look at that this is in Open
Office it's just a standard you know you
do excel whatever you're using for your
spreadsheet and you can see here we have
uh a number of columns uh number of rows
it actually goes down to like
8,000 the first thing we want to notice
is that the first row is kind of just a
random number put and going down
probably not something we're going to
work with the second row um is Bandung
I'm guessing that's a reference for the
profile if we scroll to the bottom which
I'm not going to do because it takes
forever to get back up they're all the
same uh the same thing with the status
the status is the same we have a date so
we have a sequen IAL order here um here
is the gam which I'm going to guess is
the time stamp on there so we have a
date and time we have our O3 Co NO2
reading s SO2 no CO2 vo um and then some
other numbers here pm1 pm2.5 pm4 pm1 10
uh without actually looking through the
data um I mean some of this I can guess
is like temperature humidity I'm not
sure what the PMS are um but we we have
a whole slew of data here uh so we're
looking at air quality as far as an area
in a region and what's going on with our
date time stamps on there and so
codewise we're going to read this into a
pandas data frame so our data frame uh
DF is a nice abbreviation commonly Ed
for data frames equals pd. read CSV and
then our the path to it just happens to
be on my D drive uh separated by spaces
and so if we go ahead and run this we'll
print out the head of our data and again
this looks very similar to what we were
just looking at um being in Jupiter I
can take this and go the other way uh
make it real small so you can see all
the columns going across and we get a
full view of it um or we can bring it
back up in size that's pretty small on
there overshot um but you can see it's
the same data we were just looking at uh
we're looking at the number we're
looking at the profile which is the
Bandung the um date we have a Tim stamp
our 03 count Co and so forth on here uh
and this is just your basic pandas
printing out the top five rows we could
easily have done uh three rows uh five
rows 10 whatever you want to put in
there by default that's five for pandas
now I talk about this all the time so I
know I've already said it at least once
or twice during this video most of our
work is in preform formatting data what
are we looking at how do we bring it
together uh so we want to go ahead and
start with our date time uh it's come in
in two columns we have our date here and
we have our time and we want to go ahead
and combine that and then we have uh
this is just a simple script in there
that says combined date time that's our
formula we're
building our we're going to submit our
um Panda's data frame and the tab name
when we go ahead and do this uh that's
all of our information that we want to
go ahead and create and then goes for
ION range DF uh shape zero so we're
going to go through um the whole setup
and we're going to list tab a pin DF
location I and here is our date going in
there and then return the numpy array
list tab D types date time 64 that's all
we're doing we're just switching this to
a date time stamp and if we go ahead and
U do DF date time equals combined date
time and then I always like to uh print
we'll do DF head just so we can see what
that looks like and so when we come out
of this uh we now have our setup on here
and of course just added it on to the
far right here's our date time you can
see the format's changed uh so there's
our we've added in the date time column
and we brought the date over and we've
taken this format here and it's an
actual variable with a 0000 on here well
that doesn't look good so we need to
also include the time part of this we
want to convert it into hourly data uh
so let's go ahead and do that uh to do
that uh to finish combining our date
time let's go ahead and create a uh a
little script here to combine the time
in there same thing we just did we're
just creating a numpy array returning a
numpy array and forcing this into a
datetime format and we can actually
spend hours just going through uh these
conversions how do you pull it from the
Panda's data frame how do you set it up
um so I'm kind of skipping through it a
little fast because I want to stay
focused on tensor flow and carass keep
in mind this is like 80% of your coding
when you're doing a lot of this stuff is
going to be reformatting these things
resetting them back up uh so that it
looks right on here and uh you know it
just takes time to to get through all
that but that is usually what the
companies are paying you for that's what
the big bucks are
for and we want to go ahead and uh a
couple things going on here is we're
going to go ahead and do our date time
we're going to reorganize some of our
setup in here convert into hourly data
me just put a pause in there um now
remember we can select from DF our
different columns we're going to be
working with and you're going to see
that we actually dropped a couple of the
columns those ones I showed you earlier
they're just repetitive data uh so
there's nothing in there that exciting
and then we want to go ahead and we'll
create a uh second data frame here let
me just get rid of the DF head and df2
was we're going to group by date time
and we're looking at the mean value and
then we'll print that so you can see
what we're talking about uh we have now
reorganized this so we put in date time
03 Co so now this is in the same order
um as it was before and you'll see the
date time now has our 0 z uh same date 1
2 3 and so on so it's group the data
together so it's a lot more manageable
and in the format we want and in the
right sequential order and if we go back
to um there we go our air quality uh you
can see right here we're looking at um
these columns going across we really
don't need since we're going to create
our own date time column we can get rid
of those these are the different Columns
of information we want and that should
reflect right here in the columns we
picked coming across so this is all the
same columns on there that's all we've
done is reformatted our data grouped it
together by date and then you can see
the different data coming out uh set up
on there uh and then as a data scientist
first thing I want to do is get a
description what am I looking at uh and
so we can go ahead and do the df2
describe and this gives us our um you
know describe gives us our basic uh data
analytics information we might be
looking for like what is the mean
standard
deviation uh minimum amount maximum
amount we have our first quarter second
quarter and third quarter um numbers
also in there uh so you can get a quick
look at a glance describe desing the
data or descriptive analysis and even
though we have our quantile information
in here we're going to dig a little
deeper into that uh we're going to
calculate the quantile for each variable
uh we're going to look at a number of
things for each variable we'll see right
here q1 uh we can simply do the quantile
.25% which should match um our 25% up
here and we're going to be looking at
the Min the max um and we're just going
to do this is basically we're breaking
this down for for each uh different
variable in there one of the things is
kind of fun to do uh we're going to look
at that in just a second let me get put
the next piece of code in here um we got
clean out some of our um we're going to
drop a couple thing our um last rows and
first row because those have usually
have a lot of null values and the first
row is just are titles uh so that's
important it's important to drop those
rows in here and so this right here as
we look at our different
quantiles again it's it's the the same
you we're still looking at the 25
quantile here we're going to do a little
bit more with this um so now that we've
cleared off our first and last rows
we're going to go ahead and go through
all of our columns and this way we can
look at each uh column individually and
so we'll just create a q1 Q3 min max uh
Min IQR Max IQR and calculate the
quantile of I of
df2 we're basically doing uh the math
that they did up here but we're
splitting it apart um that's all this is
and this happens a lot because you might
want to look at individual uh if this
was my own project I would probably
spend days and days going through what
these different values mean one of the
biggest data science uh things we can
look at that's
important is uh use your use your common
sense you know if you're looking at this
data and it doesn't make sense and you
go back in there and you're like wait a
minute what the heck did I just do at
that point you probably should go back
and double check what you have going on
now uh we're looking at this and you can
see right here here's our attribute for
our 03 so we've broken it down uh we
have our q1 5.88 Q3 10.37 if we go back
up here here's our 58 we've uh rounded
it off 10.37 in
there so we've basically done the same
math uh just split it up we have our
minimum and our Max IQR and that's
computed uh let's see where is it here
we go uh q1 - 1.5 * IQR and the IQR is
your Q3 minus q1 so that's the
difference between our two different
quarters and this is all uh data science
um as far as a hard math we're really
not we're actually trying to focus on
carass and tensor flow he's still got to
go through all this stuff I told you 80%
of your programming is going through and
understanding what what the heck uh
happened here what's going on what does
this data mean and so when we looking in
that we're going to go ahead and say hey
um we've computed these numbers and the
reason we've computed these numbers is
if you take the minimum value and it's
less than your Min minimum
IQR uh that means something's going
wrong there and they usually in this
case is going to show us an outlier uh
so we want to go ahead and find the
minimum value if it's less than the
minimum minimum IQR it's an outlier and
if the max value is greater than the uh
Max IQR uh we have an outlier and that's
all this is doing low outlier is found
uh minimum value High outlier is found
uh really important actually outliers
are almost everything in data sometimes
sometimes you do this project just to
find the outliers because you want to
know uh crime detection what are we
looking for we're looking for the
outliers what doesn't fit a normal
business deal and then we'll go ahead
and throw in um just threw in a lot of
code oh my goodness uh so we have if
your max is greater than IQR print
outlier is found what we want to do is
we want to start cleaning up these
outliers and so we want to convert uh
we'll do create a convert Nan x max IQR
equals Max _ IQR Min IQR equals Min IQR
so this is just saying this is the data
we're going to send that's all that is
in Python and if x is greater than the
max IQR and X is less than the Min IQR x
equals null we're going to set it to
null why because we want to clear these
outliers out of the data now again if
you're doing fraud detection you would
do the opposite you would be cleaning
everything else that's not in that
Series so that you can look at just the
outlier uh and then we're going to
convert the Nan hum again we have x uh
Max IQR is 100% Min IQR is min IQR if x
is greater than Max IQ q r and X is less
than Min IQR again we're going to return
a null value otherwise it's going to
remain the same value x x equals X and
you can see um as we go through the code
if I equals um our huum uh then we go
ahead and do that's the that's a column
specific to humidity that's your hm
column uh then we're going to go ahead
and convert do the run a map on there
and convert the
nonm you can see here it's this is just
cleanup uh we run we found out that
humidity probably has some weird values
in it uh we have our outliers um that's
all this is and so when we go ahead and
finish this and we take a look at our
outliers and we run this code
here uh we have a low outlier 2.04 we
have a high outlier
99.06 outliers have been
interpolated
that means we've given them a new value
uh chances are these days when you're
looking at something like um these
sensors coming in they probably have a
failed sensor in there something went
wrong um that's a kind of thing that you
really don't want to do your data
analysis on uh so that's what we're
doing is we're pulling that out and then
uh converting it over and setting it up
method linear so we interpolate method
linear is it's going to fill that data
in based on a linear regression model of
similar data uh same thing with this up
here with the um df2 I interpolate
that's what we're doing again this is
all data prep uh we're not actually
talking about tensor flow we're just
trying to get all our data set up
correctly so that when we run it it's
not going to cause problems or have a
huge
bias so we've dealt with outliers
specifically in
humidity and again this is one of these
things where when we start running
um we run through this you can see down
here that we have our um outliers found
high low outliers um migrated them in we
also know there's other issues going on
with this data uh how do we know that
some of it's just looking at the data
playing with it until you start
understanding what's going on let's take
the temp value and we're going to go
ahead and and use a logarithmic function
on the temp value and uh it's
interesting because it's like how do you
how do you heck do you even know to use
logarithmic on the temp value that's
domain specific we're talking about
being an expert in air care I'm not an
expert in Air Care um you know it's not
what I go look at I don't look at Air
Care data in fact this is probably the
first Air Care data setup I've looked at
but the experts come in there and they
come to you and say hey in data science
um this is a um exponentially VAR
variable on here so we need to go ahead
and do to um transform it and use a
logarithmic scale on
that so at that point that would be
coming from your um uh data here we go
data science programmer overview does a
lot of stuff connecting the database and
connecting in with the experts data
analytics a lot of times you're talking
about somebody who is a data analysis
might be all the way us a PhD level data
science programming level interfaces
database manager that's going to be the
person who's your admin working on it so
when we're looking at this we're looking
at uh something they've sent to me and
they said hey domain Air Care this needs
to be this is a skew because the data
just goes up exponentially and affects
everything else and we'll go ahead and
take that data um let me just go ahead
and run this just for another quick look
at it um we have our uh uh we'll do a
distribution DF we'll create another
data frame from the temp values and then
from a data set from the log temp so we
can put them side by side and we'll just
go ahead and do a quick histogram and
this is kind of nice plot of figure
figure size here's our PLT from matplot
library uh and then we'll just do a
distribution umore DF there's our data
frames this is nice because it just
integrates the histogram right into
pandas love pandas and this is a chart
you would send back to your data
analysist and say hey is this what you
wanted this is how the data is
converting on here as a data science
scientist the first thing I note is
we've gone from a
10230 scales at 2.5 3.0 3.5 scale um and
the data itself has kind of been uh uh
adjusted a little bit based on some kind
of a skew on there so let's jump into uh
we're getting a little closer to
actually doing our uh um carass on here
we'll go ahead and split our data up
um and this of course is any good data
scientists you want to have a training
set and a test Set uh and we'll go ahead
and do the train
size we're going to use 75% of the data
make sure it's an integer don't want to
take a slice as a float value give you a
nice error uh and we'll have our train
size is 75% and the test size is going
to be um of course the uh train size
minus the length of the data set and
then we can simply do train comma test
here's our data set which is going to be
the train size the test size uh and then
if we go and print this let me just go
and run this we can see how these values
um split it's a nice split of 1298 and
then 433 points of value that are going
to be for our um setup on here and if
you remember we're specifically looking
at the data set where did we create that
data set from um that was from up here
that what we called the uh logarithmic
um value of the temp uh that's where the
data set came from so we're looking at
just that column with this train size
and the test with the train and test
data set here and let's go ahead and do
a converted an array of values into a
data set Matrix we're going to create a
little um setup in here we create our
data set our data set's going to come in
we're going to do a look back of one so
we're going to look back one piece of
data going backward and we have our data
X and our data y for ION range length of
data set look back minus one uh this is
creating let me just go ahead and run
this actually the best way to do this is
to go ahead and create this data and
take a look at the shape of it uh let me
go ahead and just put that code in here
so we're going to do a look back one
here's our train X our train Y and it's
going to be adding the data on there and
then when we come up here and we take a
look at the shape there we go um and we
run this piece of code here we look at
the shape on this and we have uh a new
slightly different change on here but we
have a shape of X 1296 comma 1 shape of
Y train y Test X test Y and so what
we're looking at is that um the X comes
in and we're only having a single value
out uh we want to predict what the next
one is that's what this little piece of
code is here for what are we looking for
well we want to look back one that's the
um then what we're going to train the
data with is yesterday's data yesterday
says Hey the humidity was at 97% what
should today's humidity be at if it's
97% yesterday is it going to go up or is
it going to go down today if 97 does it
go up to 100 what's going on there uh
and so our we're looking forward to the
next piece of data which says Hey
tomorrow's is going to you know today's
humidity is this this is what tomorrow's
humidity is going to be that's all that
is all that is is stacking our data so
that U our Y is basically x + one or X
could be y minus
one and then a couple things to note is
our X data um we're only dealing with
the one column but you need to have it
in a shape that has it by The Columns so
you have the two different numbers and
since we're doing just a single point of
data we have and you'll see with the
train y we don't need to have the extra
shape on here now this is going to run
into a problem uh and the reason is is
that
we have what they call a Time
step and the time step is that long-term
short-term memory layer uh so we're
going to add another reshape on here let
me just go down here and put it into the
next cell and so we want to reshape the
input uh array in the form of sample Tim
step features we're only looking at one
feature and I mean this is one of those
things when you're playing with this
you're like why am I getting an error in
the numpy array why is this giving me
something weird going on uh so we're
going to do is we're going to add one
more uh level on here instead of being
12991 we want to go one
more and when they put the code together
in the back you can see we kept the same
shape the
1299 uh we added the one dimension and
then we have our train X shape one um
and this could have depends again on how
far back in the long shortterm memory
you want to go that is what that piece
of code is for and that reshape is and
you can see the new shape is now one uh
$ 12299 1 1 uh versus the
12991 and then the other part of the
shape
43211 again this is our TR our xn and of
course our test X and then our Y is just
a single column because we're just doing
one output that we're looking for so now
we've done our
80% um you know that's all the the
writing all the code reformatting our
data um bringing it in now we want to go
ahead and do the fun part which is we're
going to go ahead and create and fit the
the lstm neural network uh and if we're
going to do that the first thing we need
is we're going to need to go ahead and
create a model and we'll do the
sequential model and if you remember
sequential means it just goes in order
uh that means we have if you have two
layers the layers go from layer one to
Layer Two or layer zero to layer one
this is different than functional uh
functional allows you to split the data
and run two completely separate models
and then bring them back together we're
doing just sequential on here and then
we we decided to do the long shortterm
memory uh and we have our input shape uh
which it comes in again this is what all
this switching was we could have easily
made this uh one two three or four going
back as far as the uh in number on there
we just stuck to going back one and it's
always a good idea when you get to this
point where the heck is this model
coming from um what kind of models do we
have available and uh there's let me go
and put the next model in there uh cuz
we're going to do two models and the
next model is going to go ahead and
we're going to do dent so we have model
equals sequential and then we're going
to add the lstm model and then we're
going to add a DSE model and if you
remember from the very top of our
code where we did the Imports oops here
we go our cross this is it right here
here's our importing a dense model and
here's our importing an
lstm now just about every tense or flow
model uses dents uh your D model is your
basic forward propagation uh reverse
propagation error or it does reverse
propagation to program the model uh so
any of your neural networks you've
already looked at that uh looks and says
here's the error and sends the error
backwards that's what this is the long
short-term memory is a little different
the real question that we want to look
at right now is where do you find these
models what what kind of models do you
have available and so for that let's go
to the carass website uh which is the
cars.io if you go under API layers and I
always have to do a search just search
for carass uh API layers it'll open up
and you can see we have um your base
layers right here class trainable
weights all kinds of stuff like there
your activation uh so a lot of your
layers you can switch how it activates
uh railu which is like your smaller
arrays or if you're doing convolutional
neural networks the convolution usually
uses a Ru um your sigmoid um all the way
to soft Max soft plus all these
different choices as far as how those
are set up and what we want to do is we
want to go ahead and if you scroll down
here you'll see your core layers and
here is your dense layer uh so you have
an input object your dense layer your
activation layer embedding layer this is
your your kind of your one setup on
there that's most common uh
convolutional neural networks or
convolutional layers these are like for
doing uh image categorizing uh so trying
to find objects in a picture that kind
of thing uh we have pooling layers so as
you have the layers come together um
usually bring them down into uh single
layer although you can still do like
Global Max pulling 3D and there is just
I mean this list just goes on and on uh
there's all kinds of different things
hidden in here as far as what you can do
and it changes you know go in here and
you just have to do a search for what
you're looking for uh and figure out
what's going to work best for you as far
as what project you're working on uh
long shortterm memory is a big one cuz
this is when we start talking about text
uh what if someone saysthe what comes
after the uh The Cat in the Hat little
kid's book there um starts programming
it and so you really want to know not
only um what's going on but it's going
to be something that has a history the
history behind it tells you what the
next one coming up is now once we've
built all our different you know we
built our model we've added our
different layers we went in there um
play with it remember if you're in
functional you can actually link these
layers together and they Branch out and
come back together if you do a uh um the
sub setup then you can create your own
different model you can embed a model in
there that might be coming linear
regression you can embed a linear
regression model uh as part of your
functional split and then have that come
back together with other things so we're
going to go ahead and compile your model
this brings everything together we're
going to put in what the loss is which
we'll use the mean squared error uh and
we'll go ahe and use the atom Optimizer
clearly there's a lot of choices on here
depending on what you're doing and just
like uh any of these uh different
prediction models if you've been doing
any uh um s kit from python uh you'll
recognize that we have to then fit the
model uh so what are we doing in here
we're going to T send in our train X our
train y um we're going to decide how
many epics we're going to run it through
500 is probably a lot for this um I'm
guessing it' probably be about two or
300 probably do just fine our batch
size so how many different uh when you
process it this is a math behind it if
you're in data analytics um you might
try to know what this number is as a
data scientist where I haven't had the
PHD level math that says this is why you
want to use this particular batch size
you kind of play with this number a
little bit um you can dig deeper into
the math see how it affects the results
uh depending on what you're doing and
there's a number of other settings on
here uh we did verbos 2 I'd have to
actually look that up to tell you what
verbos means I think that's actually the
default on there if I remember correctly
uh there's a lot of different settings
when you go to fit it the big ones are
your epic and your batch size those are
what we're looking for and so we're
going to go ahead and run
this and this is going to take a few
minutes to run because it's going
through um 500 times through all the
data so if you have a huge data set this
is the point where you're kind of
wondering oh my gosh is this going to
finish tomorrow um if I'm running this
on a single machine and I have a terapy
terabyte of data uh going into
it if this is my personal computer and
I'm running a terabyte of data into this
um you know this is running rather
quickly through all 500 iterations uh
but you know at a terabyte of data we're
talking something closer to days week um
you know even with a uh um
uh 3.5 gaher machine in in eight cores
it's still going to take a long time to
go through a full terabyte of data and
then we want to start looking at putting
it into some other framework like spark
or something that will P the process on
there more across multiple um processors
and multiple
computers and if we scroll all the way
down to the bottom you're going to see
here's our square mean error
0.088 if we scroll way up you'll see it
kind of oscillates between 088 88 and
0889 it's right around two 2 250 where
you start seeing that oscillation which
it's really not going anywhere so we
really didn't need to go through a full
500 epics uh you know if you're
retraining the stuff over and over again
it's kind of good to note where that
error zone is so you don't have to do
all the extra processing of course if
you're going to build a model uh we want
to go ahead and run a prediction on it
so let's go ahead and make our
prediction and remember we have our
training test set
in our test set or we have the train X
and the train y for training it or train
predict and then we have our test X and
our test y going in there uh so we can
test to see how good it did uh and when
we come in here we have um uh you'll see
right here we go ahead and do our train
predict equals uh model predict train X
and Test predict model predict test X
why would we want to run the prediction
on trainex well it's not 100% on its
prediction we know it has a certain
amount of error and we want to compare
the error we have on what we programmed
it with with the error we get when we
run it on new data that's never seen the
model's never seen before and one of the
things we can do uh we go ahead and
invert the predictions uh this helps us
level it off a little bit more um get
rid of some of our bias we have train
predict equals an NP um exponential M1
the train predict and then train y
equals the exponential M1 for train Y
and then we do the again that with train
test predict and test
y um again reformatting the data so that
we can it all matches and then we want
to go ahead and calculate the root mean
square error so we have our train score
uh which is your math square root times
the mean square root error train uh Y
and train predict and again we're just
um uh this is just feeding the data
through so we can can compare it and the
same thing with the test and let's take
a look at that cuz really the code makes
sense if you're going through it line by
line you can see what we're doing but
the answer really helps to zoom in uh so
we have a train score which is
2.40 of our root mean square error and
we have a test score of 3.16 of the root
mean square error if these were reversed
if our test score is better than our
training score we've overtrained
something's really wrong at that point
you got to go back and figure out what
you did wrong uh cuz you should never
have a better result on your test data
than you do on your training data and
that's how we put them both through
that's why we look at the error for both
the training and the testing when you're
going out and quoting your um U
publishing this you're saying hey how
good is my model it's the test score
that you're showing people this is what
it did on my test data that the model
had never seen before this is how good
my model is and a lot of times you
actually want to put together like a
little formal code um where we actually
want to print that out and if we print
that out you can see down here um test
prediction standard deviation of data
set 3.16 is less than 4 uh 40 i' have to
go back and we're up here if you
remember we did the square means error
this is standard deviation that's why
these numbers are different it's saying
the same thing that we just talked about
uh 3.16 is less 4.40 model is good
enough we're saying hey this is this
model is valid we have a valid model
here so we can go ahead and go with that
uh and along with putting a formal print
out of there um we want to go ahead and
plot what's going
on uh and this we just want a pretty
graph here so that people can see what's
going on when I walk into a meeting and
I'm dealing with a number of people they
really don't want these numbers they
don't want to say hey what's I mean
standard deviation unless you know what
statistics are um you might be dealing
with a number of different departments
head of cells might not works with
standard deviation or have any idea what
that really means number-wise and so at
this point we really want to put it in a
graph so we have something to display
and with displaying you got to remember
that we're looking at uh the data today
going into it and what's going to happen
tomorrow so let's take a quick look at
this uh we're going to go ahead and
shift the train predictions for plotting
uh we have our train predict plot uh NP
mty like data Set uh train predict
plot uh set it up with uh null
values you know it's just kind of it's
kind of a weird thing where we we're
creating the um the data groups as we
like them and then putting the data in
there is what's going on here uh so we
have our train predict plot uh which is
going to be our look back or length plus
look back we're just is going to equal
train uh train predict so we're creating
this basically we're taking this and
we're dumping the train predict into it
so now we have our nice train predict
plot and then we have the shift test
predictions for the plotting uh we're
going to continue more of that oops
looks like I put it in here double no
it's just a yeah they put it in here
double um didn't mean to do that we
really only need to do it once oh here
we go um this is where the problem was
is because this is the test predict so
we have our training prediction we're
doing the shift on here and then the
test predict we're going to look at that
same thing we're just creating those two
uh data sets uh test predict plot length
prediction set up on there and then
we're going to go through the plotting
the original data set and the
predictions uh so we have a Time axis
always nice to have your time set on
there um set that to the time array time
axis lap all this is setting up the time
variable for the bottom and then we have
a lot lot of stuff going on here as far
as setting up our
figure let's go ahead and run that and
then we'll break it
down we have on here uh our main plot we
have two different plots going on here
uh the ispu going up and the data and
the ispu here with all these different
settings on
it and so when we look at this we have
our um ax1 that's the main plot I mean
our ax that's the main plot and we have
our ax one which is the secondary plot
over here so we're doing a figure PLT or
PLT do fig and we're going to dump those
two graphs on there um and so we take
and if you go through the code piece by
piece uh which we're not going to do
we're going to do the um the data set
here um exponential reverse exponential
so it looks correctly we're going to
label it the original data set um we're
going to plot the train predict plot
that's what we just created we're going
to make that orange and we'll label it
train prediction uh test predic plot
we're going to make that red and label
it test prediction and so forth um set
our ticks up let's actually just put
tick um time axis gets its ticks the
little little marks they going along the
axes that kind of thing and let's take a
look and see what these graphs look
like and these are just kind of fun you
know when you show up into a meeting and
this is the final output and you say hey
this is what we're looking at um here's
our original data in blue here's our
training prediction um you can see that
it trains pretty close to what the data
is up there I would also probably put a
um like a little little time stamp and
do just right before and right after
where we go from uh train to test
prediction and you can see with the test
prediction the data comes in in red um
and then you can also see what the
original data set looked like behind it
and how it differs and then we can just
isolate that here on the right that's
all this is um is just the test
prediction on the right uh and it's you
know there's you'll see with the
original data set there's a lot of Peaks
were missing and a lot of lows were
missing but as far as the actual test
prediction it's pretty does pretty good
it's pretty right on you can get a good
idea what to expect for your ispu and so
from this we would probably publish it
and say hey this is um what you expect
and this is our area of this is a range
of error um that's the kind of thing I'd
put out on a daily basis maybe we
predict the sales are going to be this
or maybe weekly so you kind of get a
nice you kind of flatten the um data
coming out and you say hey this is what
we're looking at the big takeaway from
this is that we're working with let me
go back up here oops oh too far there we
go um is this model here this is what
this is all about we worked through all
of those pieces um all the tensor flows
and that is to build this sequential
model and we're only putting in the two
layers this can get pretty complicated
if you get too complicated it never um
it never verges into a usable model uh
so if you have like 30 different layers
in here there's a good chance you might
crash it kind of thing um so don't go
too haywire on that and that you kind of
learn as you go again it's domain
knowledge um and also starting to
understand all these different layers
and what they
mean the data analytics behind those
layers um is something that your data
analysis uh professional would come in
and say this is what we want to try but
I tell you as a data scientist um a lot
of these basic setups are common and I
don't know how many times uh working
with somebody and they're like oh my
gosh if I only did a tangent H instead
of a railu activation I worked for two
weeks to figure that out well as a data
science I can uh run it through the
model in you know know five minutes
instead of spending two weeks doing the
the math behind it um so that's one of
the advantages of data scientists is we
do it from programming side and a data
analytics is going to look for it how
does it work in math and this is really
the core right here of tensor flow and
carass is being able to build your data
model quickly and efficiently and of
course uh with any data science putting
out a pretty graph so that your
shareholders um again we want to take
and um reduce the information down to
something people can look at and say oh
that's what's going on they can see
stuff what's going on as as far as the
dates and the change in the ispu now
let's talk a little bit about recurrent
neural networks so neural networks are
of different types we have CNN we have
RNN Ann so on now RNN is one type of
neural network RNN stands for recurrent
neural network the networks like CNN
andn are feed forward Network which
means that the information pretty much
only goes from left to right or from
front to back whichever way you call it
whereas in case of recurrent neural
network there will be some information
traveling backwards as well so that is
why it is known as recurrent neural
network and each of these types have a
specific application so for example
convolutional neural networks or CNN are
very good for doing image processing and
uh object detection using for video and
images whereas recurent neural networks
are pretty good for or doing NLP or
speech recognition and so on okay so for
the next few minutes we will kind of
focus on recurrent neural networks and
we will see an example of how we can use
RNN to do a Time series analysis so in a
typical neural network this is how it
looks right so where you have a neuron
and then the inputs are coming to the
neuron and uh then you have an output
which goes to other neurons in the other
layers in case of recurrent neural
network what happens is you have the the
inputs let's say at a given point in
time but then a part of the previous
output also gets fed in along with the
inputs for the given time now this can
be a little confusing so let's see if we
can take a little expanded view of this
so this is another view of one single
neuron which is what is known as in an
unfolded manner okay so if we are
getting inputs or data over a period of
time then this is how the neuron will
look remember these are not three
neurons this is one neuron and it is
what is known as it is shown in a
unfolded way okay we are we have
unfolded this single neuron over a
period of time so at a given time let's
say T minus one an input of XT minus1 is
fed to the neuron and it gets a output
of YT minus one then the next instant
which is XT at a time T right there is
an input of XT and then there is an
output of YT so this is a single neuron
but is displayed in an unfolded way so
this is like expanding it over a period
of time so let's start with this part
here this particular neuron gets an
input at at an instant T minus one let's
say time is equal to T minus1 it gets an
input of XT minus1 and it gives an
output of YT minus then when we go to
the instant T here it gets an input of
XT and it also additionally gets an
input from this previous time frame T
minus one so this YT minus1 gets fed
here and that results in YT all right
then when we go to the time t + 1 there
is an input of XT minus 1 at that given
time plus the input from the previous
time frame which is a time frame of T
that also gets fed in here and then we
get an output of YT + 1 okay so let me
explain once again this is a single
neuron this these are not three
different neurons a single neuron seen
over a period of time from T minus1 to t
+ 1 and unlike a regular feedforward
neuron which only gets X in this case
the input is X and also another input
which is coming from the previous time
frame and that is what this Loop is in
this on the left hand side diagram this
Loop is indicating that okay so on the
right hand side it is represented in an
unfolded way okay so the input if we
take a time frame T for example is not
only XT which is is the input which is
the normal input at the time frame T but
it is also getting an input from the
previous time frame which is this YT
minus one is also being fed as an input
that is the key differentiator here okay
similarly at the instant t + 1 it is
getting a normal input of XT + 1 plus
this YT is actually being fed here and
then the output comes as YT + 1 okay so
this is the construct of a recurrent
neural network now quick and neural
networks again can be of different
subtypes it can be one to one one to
many many to one and many to many
depending on what kind of application we
want to use one of the examples is like
the stock price so you're feeding so
there is only one input only thing is it
is spread over a period of time so
you're feeding the stock price input
that is what comes here as an input and
you get an output which is again the
stock price which is probably predicted
over the next uh 2 days or 3 days or 10
days or whatever so the number of
outputs and inputs is the same there is
only one input there is only one output
only thing is it is spread over time so
variables are not many then you have one
to many so there is one input but you're
expecting multiple outputs what is an
example of this let's say you want to
caption an image so how do you want to
do that what is the input that will go
here is just an image which is one input
but what is output you're looking for
you're looking for a phrase maybe right
not just a word but a phrase so it's
like cat is sitting on a matap right
right so the images there is only one
image which consists of the cat sitting
on a mat but the output are like maybe
three or four words which is saying the
cat is sitting on a mat so this is one
to many right then you can have many to
one examples of this are like you're
feeding some text and you want to know
the output whether the text is what kind
of sentiment is expressed by the text it
could be positive it could be negative
so that's the output you're looking for
so you feed a large number of words
maybe the text May consist of words or
lines or whatever so that is what is the
multiple inputs but the output all it
says is the sentiment is positive so it
is just one output or the sentiment is
negative just one output right so this
is many to one then we have many to many
so what is an example of this let's say
you want to do some translation so how
do you do a translation you feed in a
sentence maybe in a particular language
English and then you want another
sentence actually in another language so
that is like there are multiple inputs
one sentence can consist of multiple
words and the output also is a sentence
consisting of multiple words all right
so how do we Implement RNN this is an
example of implementing an RNN for a
particular use case so in our particular
example we have the data about milk
production over a few months and using
RNN we want to predict because this is
time series analysis so RNN is good to
do time series analysis so using RNN we
want to predict what will be the milk
production in the future so let us see
how we can do that I will first take you
through quickly through the slides and
then I will actually run this in in a
Jupiter notebook the code I will run it
in the Jupiter notebook so this is how
it looks the code looks so while this is
tensive flow you still use the standard
python libraries like numpy and pandas
and even matplot lib to do some initial
processing getting the data processing
it cleaning it whatever all that can be
still done in within the same program so
that's what we're doing here we're
importing some libraries like numai and
pandas and then we read the data file
and if we plot the data we see here it
is the data for for the years 1962 to 75
and we can see that there is a certain
Trend right so this is how a typical
time series data would look again some
of you if you're not familiar with time
series and uh time series analysis and
so on I would recommend you to go
through some videos around that which
will make it easy to understand this so
this is our typical time series data
would look in this case it is nothing
but there is only one variable which is
milk production and it is spread across
several years so it's going from 1962 to
75 and that value has been plotted so if
you see there is a certain kind of a
trend here which is basically going
upwards and there will be some
seasonality so time series data has
these three components right so it will
have a trend it will have a seasonality
it will have seasonality and then it
will have some Randomness so that's what
this graph is uh showing and um now if
we want to perform analysis on this time
series analysis on this first thing we
need to do is split the data into train
and uh test and in this case we will
just use a straightforward method which
is uh taking the data for the first 13
years so the idea here is we need to
train our model right this is time
series data so what and we want to
predict for the future so the way we
need to use this is we have to take the
data for a certain period of time and
train our model
and then we use a part of the known data
so that we can then ask our model to
predict for that duration and compare it
with the known information so what do we
mean by that so here if you see this
data I will show you in the notebook as
well jupter notebook as well this has 13
years of data so what we are doing is we
are taking the first 12 years and using
that as our training set right so 12
years we doing and this has about I
think 14 years of data so we are taking
1 years of data for training purpose and
then we are using the last one year
remaining one year of data for testing
purpose because here what we can do is
this onee data we know the values right
because if we want to compare the
accuracy or anything like that we need
to know the values so in this case we
know the values of this one particular
year so we will use that but at the same
time we train the model for 13 years of
data and for the 14th Year we will ask
our model to predict and then compare it
with this known value so that we know
how accurate our model is I hope that
makes sense okay so that's what we are
doing here 156 is nothing but 12 into 13
on a monthly basis we get the data the
next step is to scale the data this is
all regular data manipulation data
monging activity and then you split it
you are basically assigning the train
and test data to the to the scaled
variables and uh then you need to read
the data in batches so it is very
important as I mentioned earlier also
that instead of reading the entire data
in one shot you feed the data in batches
so in order to do that we are writing a
a function for that that's all we are
doing here so that is still here what we
have done is regular Python Programming
there is no tensor flow as of now here
okay so so far what we have done is
preparation data preparation data Ming
now what we are doing is actually
training our model so this is where
tensorflow now comes into picture so we
are importing first step is to import
the tensorflow library so this is how
you do it import tensorflow SDF and then
you can Define some variables or
constants whichever way now here of
course there are a couple of ways of
doing it you can create them as
tensorflow nodes but that is a little
bit of an overhead we will just use the
regular python variables or constants so
here I'm creating regular python
variables and I'm saying number of
inputs is one so instead of hard coding
I'm just storing them as variables so
this is number of inputs is one number
of time steps is 12 number of neurons is
100 and so on right and then learning
rate is 03 and number of item iterations
we are seeing is 4,000 then we create
placeholders now we will be storing the
independent variables in X and the
dependent variables in y and this we
will read from an external file remember
I told you placeholders are used for
getting data from outside and then
feeding it to our model so that's the
reason we have two placeholders one for
reading the X values and another for
reading the Y values in this step we are
only just creating the nodes right so
this is just creating the graph and
similarly we are mentioning what is the
loss function and what is the optimizer
and how to run the optimizer and once
that is done you are initializing the
variables and then you're creating a sa
variable or a saver instance so sa is
basically nothing but in machine
learning you can train your model and
you can save it for later use so that's
where the saving comes into play we will
see how to use that as well and then the
last step is to create your session and
run this graph right so we are
initializing the variables remember this
we run in it so which is nothing but
this one so we are initializing the
variables whatever are there instead of
hard coding remember in earlier case we
were hardcoding how many iterations so
here we are saying for the given number
of iterations which are stored in this
maybe how many iterations We Said is
What is the value of iterations this is
training iterations is 4,000 so we
specify that based on the number of
training iterations next batch will
basically fetch the data and then we run
the session we are basically saying
train is the node which we will run in
the session and uh this will basically
train our model and this is more for
printing every 100 times you print it
that's all this there's nothing more to
it so for example the output would look
somewhat like this this is for zero this
for 100 next for 200 and so on okay and
then you save the model you remember sa
we created so you save the model for
later use so this is our test data
remember we are doing this on training
data right so once the training data
training is done we then try to create
the inference on the test data so that
we can compare how accurate this is
right so this is how the test set would
look and uh then we will basically
restore this model okay because in the
previous slide we created the model and
we stored it so now we have to restore
the model and then run our test data
against it and see what are the values
that are predicted what are the Y values
and then compare with X to see the
accuracy so train seed is what we are
seeing here so that is what we have the
the predicted values and uh these
predicted values what we want to do is
we want to create an additional column
because remember we need to compare
because we want to find out the accuracy
of this model so we need to compare the
predicted values with the actual value
so what we are doing here is adding
another column called generated and uh
assign a value to that all right so this
is the result of the prediction and then
if we need to reshape because we have to
show this in the form of monthly results
so that's what we doing here and um once
we generate the results and then display
it we can actually see it month-wise
here and uh the actual and the generated
values okay so we create a data frame
which is a combination of the actual
values and the predicted values from the
test set and then we can plot to see how
the trend is as you can see pretty much
the actual value is in blue color and
the generated or predicted value the
curve is in yellow color the trend is
maintained so it will probably it's not
100% accurate but the trend is
maintained all right so let's do one
thing let's go into the Jupiter notebook
and take a look at how this works in
tensorflow environment actually the code
I will walk you through the code this is
my Jupiter notebook and uh the data is
taken from this particular Link in case
you're interested you can uh download it
from there and this is the data for the
years 1962 to 75 the first thing thing
we do is import these libraries because
before we start with the actual
tensorflow activity we need to prepare
the data and so on so for that you can
use your regular python libraries which
are like numpy pandas and so on so
that's what we doing here and then we
read the data using pandas into a data
frame so this is a data frame milk is a
data frame and we will do some quick
exploratory analysis just to see how our
data is looking initial five records
that we'll get one 2 3 4 5 that is head
so it goes from 1962 January to 1962 May
once we rechange or basically we need to
split this into month specifically
separately so that's what we are doing
here date to time so we kind of do a
little bit of a reindexing and then if
we plot this this is how the data look
as you can see there is a clear upward
Trend and U there is some seasonality
and so on but anyway we will not break
that up into those components we will
just use RNN to predict and test okay so
the next thing is to check some if we
can do run like a info it will tell us
what is the information about this data
set so let us just run that okay so it
is just telling us how many total
columns and what is the size of the file
and so on and so forth again in case
you're interested in doing some
exploratory analysis so what we'll do
next is we will take the 13 years of
data for our training data set so what
are we going to do now let me step back
so what do you we seen here we have seen
that there are 168 records so which is
nothing but 14 years of data we now have
to split this into our training and test
data set so how do we do that I think in
case of Time series we cannot do it like
8020 like we do in normal splitting in
normal machine learning process here it
is time series data so what we are going
to do is we will take out of this 14
years we will take the first 13 years of
data and we will use that for training
and then we will test it with the last
which is the 14th year data we will use
it for testing okay so that's what we
are going to do here so let's uh split
that so training set will consist of my
156 records which is nothing but 12 13
into 12 right my 13 years of data I'm
taking for training and then my test set
will consist of the bottom 12
observations which is last one year of
data which is the 14th year okay so
that's now done training and test
splitting is done the next step is to do
some normalizing which is basically
we'll use or scaling rather we will use
minmax scaler and uh we will just scale
the data and now we do it for both test
as well as train now we are ready to
create our RNN model but before that one
quick thing in order to fetch the data
we have to create a function so that's
what we are doing here we create a
function called Next batch and uh how
you want to fetch the steps they are all
defined as our constants if you remember
and uh that's what this uh function is
all about so we Define that function we
will be calling that in our Training
Method All right so from here onwards up
to there we are done with uh the
preparation of the data and whatever
functions or whatever are required from
here onwards is where the tensorflow
part starts so the first thing we do is
import the tensor flow Library typically
this is a very standard way of importing
the library we say import tens oflow as
TF now TF is nothing but a name so you
can change it to tf1 or ABC or XY Z
whatever so this part you can change but
by and log everybody uses this so we I
would rather recommend you also use the
same syntax so you say importance of low
stf so it's very easy for others to
understand as well and then you declare
or Define a bunch of constants uh that's
what we are doing here like for example
the number of uh time steps in in this
case it is 12 number of neurons there
are 100 number of outputs it is only one
and so on right and then the learning
rate and all that we are declaring or
defining those variables in this
particular block next is to create
placeholders you remember the
placeholders are used for feeding the
data so we have X and Y X is for the
input which is the independent variable
and Y is the output which is the
dependent variable and in our case these
are not different characteristics or
features but it is the same one feature
or one variable but it is over a period
of time so that's the only difference so
that's what we are doing here and now we
need to create our Network right the
neural network so in our case we said we
will create a RNN layer now there are
different ways or different formats of
RNN which is probably not in scope of
this module so for now we will just
assume we are creating one RNN layer and
one type of RNN is Gru cell so we will
use Gru cell and we use this apis for
creating our layer so each cell will be
with an output projection wrapper there
is a need for doing that and again the
details of this we will probably do in
another video where we talk in detail
about RNN so for now we are creating a
GRU cell with the wrapper and the the
Syntax for creating the gru cell is uh
like this the number of units which we
we said there should be 100 neurons what
will be the activation function in this
case it is relu and then number of
outputs in our case it is only one okay
so we create the cell here okay then we
got the gru cell and then we say what is
our output which is nothing but we get
the outputs and the states and their
states and which is uh the way we get
that is tf. nnn do dynamicore RNN so if
we call this and we pass the cell and
the data which is basically in
placeholder again remember all we are
doing here is we are creating just the
nodes for the graph so nothing is
actually getting executed from a
tensorflow perspective okay all right so
once that is done now we need to pass
this calculate the outputs and the
states using the dynamicore arant method
and we pass the cell as a parameter and
then the X values as a parameter and if
we run that we will have the outputs and
the states and this is where we actually
run the Training Method so or create not
really run the Training Method but we
create the nodes for the training and
Optimizer then we have the
initialization of the variables and we
save this model we create a save object
just to save the model because we will
then restore it and run it to do our
predictions so this is what we will do
here so that is another node and this is
where we actually create a session and
run the training okay so let's go ahead
and do that that will take a little
while because we said 4,000 iterations
so we will allow it to finish we will
probably come back once the training is
completed all right so the training is
done now let's go and U run this on the
test data so let's just quickly take a
look at the test data set so this is our
one-ear data for the year of 1975 so if
we see this this from January February
and so on and we will pass this to our
model so what are we doing here we are
restoring the model here for example
right this is the path that we have
given when we were saving the model
let's go back once and show you let me
show you where we did the saving of the
model yes so we saved the model here so
that again we will restore that and we
will use that to run it on our test data
and see how well it predicts so let's go
ahead and run this and this is just 12
record so this will not take time
remember I told you training is what
takes a lot of time the regular
inference this is called inference
doesn't take much time right so it just
depends on how much data there were only
12 records here so it was very quick but
in training what we do we pass this
multiple times there are iterations
4,000 iterations we did for example so
that takes longer and in general in
machine learning deep learning training
is what takes the maximum amount of time
all right so let's go ahead and see the
results we will in order to plot it we
will have to kind of adjust the format
of the results otherwise we will not be
able to see it in a proper way and then
we will so this is our so what we have
done is we created a data frame which
consists of the predicted value and the
and the actual value so this is the data
frame so this is the actual value this
834 782 and so on this is what our model
has predicted so it may not be so easy
to see in a table format so let's go
ahead and plot it so that we can compare
these two so when we plot it you will
see that the trend is more or less
maintained right so we go from the blue
color is the actual values and the
yellow color or the orange color
whatever is it is the one one which is
generated by our model so it pretty much
is following the actual Trend so not bad
for such a quick iteration and uh
training this tutorial is about object
detection we will walk you through a
tensorflow code using which we will do
object detection in images we will tell
you what are the libraries that are
required a little bit about the Coco
data set and then we will show you the
implementation code itself a demo of the
code all right so let's get started so
what is the tensorflow object detection
API this is an open source framework
which is actually provided by the
tensorflow team and there are train
models available and the sample code is
also available which we can use in order
to easily detect objects in images and
videos this is pretty robust and can
detect objects fairly quickly and this
is very easy for people to use as well
well people with very less or no
knowledge of machine learning or deep
learning can also with a little bit of
Python Programming knowledge can
actually use this API this library to
build object detection applications this
is a list of libraries that are required
and they have been shown in the code as
well the exact purpose of each of this
Library why it is required is out of the
scope of this tutorial but we will see
in the code as we walk through some of
these libraries how and why they are
used the Coco data set Coco stands for
common objects in context so this data
set consists of 300,000 images of uh 90
most commonly found objects like chairs
and tables and so on and so forth so
this model has been trained or in fact a
set of models have been trained on this
data data set and this is pretty good to
detect the most common objects in the
images and videos so with that let's get
into the code all right so the first
part is to import all these libraries
and this we have shown you in uh the
slides as well again large part of this
will be for doing some helper functions
and maybe for visualizing the images and
so on and so forth so that's the reason
they are required as a said the exact
details of each and every Library
probably is out of scope but these are
needed so as a first step maybe you just
go ahead and include these libraries and
run the code and maybe at later point we
can discuss what each of these libraries
does now this will work with tensorflow
version higher than 1.4 so if you are
having tens flow version below 1.4 you
may have to upgrade to a higher version
so let me go ahead and execute this cell
and um we also need this line of code to
make sure that once we run this object
detection the labeled images are
displayed within this uh notebook and
many of you by now must be familiar with
this and we will import a few utility
libraries and you will see that we will
be using some of these for visualization
purpose so once the objects are Det Ed
we need to display the information what
that object is and then what percentage
of confidence the model has so all these
details that's the utility functions
that stored here and then the next part
is to prepare the model as I mentioned
we will be using an existing trained
model the tensorflow team has actually
provided these models the one that we
will be using is SSD with the mobile net
but you can actually use any one of the
ones that are listed in this URL let me
just quickly take you through this URL
these are a bunch of models trained
models that are readily available for
anybody to use it is open source and let
me scroll down the only thing is that if
there are some of them with where the
accuracy is much higher but they take
longer and there are some where the
accuracy is not so high and they are
much faster so they are faster but
accuracy may not be that very high so
you can play around with some of these
and we in this particular exercise or in
this particular tutorial we are using
this SSD model which is SSD mobilet
version one so that's the model that
we'll be using so in this cell we are
primarily creating a bunch of variables
with the various for example the name of
the model the path and so on and so
forth so that we will be using these
names in the Next Step which is to
download this model and install it
locally these are also referred to as a
frozen model so once they are trained
and then you kind of extract or you you
freeze the model so that's the reason
they are called Frozen model so that
others can just use this without any
further training so this is where we
download and extract our model locally
so this will take a little while let me
see if I can wait or maybe pause the
video and come back once it is done
might take a little while let's see if
it uh completes I have a pretty high
speed network but even then it takes
some time so that's good but this part
is over now let us see this part and yes
both are done so once that is done we
need to load some label mapping uh
basically what this will do is your
model as you may be aware by now if you
do some classification the model will
actually not give any output as a text
it will give some numberers so if there
are five classes it will say okay this
belongs to mod class one or two or three
or five and so on the numbers now each
of these will obviously the numbers will
not make any sense to the outside world
so we need to do some small mapping so
in this case let's say one may be a
chair two may be a table three may be a
balloon and so on so that kind of number
to text mapping we need to do and that
is what is being done in this particular
cell and then we have a helper code
which will load the image and convert it
into a numpy array so that the number
array is what gets processed and used by
the model to do the detection part of it
so that is what this uh method is all
about so we will be that later on we
will be calling that function and next
is preparation for detection so here we
are basically telling where the images
are stored and how many images or what
is the naming convention or format of
the images now if you want you can
modify this code for example currently I
have testore images as my folder so let
me go and show this to you so this is
under my object detection folder I have
another subfolder where I'm storing my
images which is textor images now you
can rename this folder and give some
other name and then in your code you can
probably give that particular name for
the sub Sim L the format of these files
what is the name and format of this
files here it is in a very simple format
which is the names of the files are like
beach one beach 2 Beach 3 and so on so I
have taken Beach as the theme so I have
images which are related to beaches so
this is beach one beach 2 and then Beach
3 a few others but we will use these
three for our demo and so that's what
I'm saying here the name of the images
will be Beach something. jpeg which is U
jpeg format and in this curly braces
basically we will will be filled with
either one 2 or three based on in this
particular for Loop okay so that is what
this is doing all right so the next step
is to run inference on these images in a
loop and what we have basically doing
here is um getting these images one by
one and then running through the model
to find find out what are the objects
that can be detected and then against
each of the object a box will be drawn
and it will be labeled with the name and
the percentage of accuracy or confidence
that the model detects these objects
okay so that is um the function here and
so let me just run that piece of code
and here is basically where we are
calling this function so we are loading
this images and then we are calling this
function for each image and then we are
displaying this using the Mac plot liet
Library so let's run this it will take
one image at a time and then detect the
images now the beauty is that the same
format of the code can be used for doing
object detection in a video so we have
another video for doing object detection
in a video so most of the code out there
will be reused from here and only thing
is that in instead of reading the images
from the local storage we read the
frames from the video and there is a
neat little video reader that is
available and it will be shown in the
other video and frame by frame we read
the video and then pass on to this
function and it will act as if each of
these frames is an image and then it
will do the same object detection on the
entire video so that's in a separate
video just uh look out for that and
actually the information about that is
uh provided uh in the cards the I symbol
so that's the the video object detection
in video that's the separate uh tutorial
all right so now that we have all the
pieces together this the lastar cell is
where the whole action takes place so
let's run this and see how it looks so
it will take probably a little while and
there about three images let's see what
it detects there we go so good so the
first one it has detected a person and
that to with 97% accuracy which is uh I
think pretty good okay and then the next
image it detects umbrella and chair
there are a few other objects but it's
not able to detect it has detected
umbrella with 63% accuracy or confidence
rather and uh the chair with 58% again
not bad
then let's see the next image so here
these are actually balloons hot air
balloons but the model thinks it is a
kite which is uh probably not that bad
it sees there's something in the sky and
therefore probably it thinks it is a
kite and it detects that with 65%
confidence okay so that was pretty much
all I wanted to show you in this
particular tutorial about object
detection in images and in this video I
will walk you through the tensorflow
code to perform object detection in a
video so let's get started this part is
basically you're importing all the
libraries we need a lot of these
libraries for example numpy we need
image IO datetime and pill and so on and
so forth and of course mat plot lib so
we import all these libraries and then
there are a bunch of VAR Ables which
have some parts for the files and
folders so this is regular stuff let's
keep moving then we import the M plot
lib and make it in line and uh a few
more Imports all right and then these
are some warnings we can just ignore
them so if I run this code once again it
will go away all right and then here
onwards we do the model preparation and
what we're going to do is we're going to
use an existing neural network model so
we are not going to train a new one
because that really will take a long
time and uh it needs a lot of
computation resources and so on and it
is really not required there already
models that have been trained and in
this case it is the SSD with mobile net
that's the model that we are going to
use and uh this model is trained to
detect objects and uh it is readily
available as open source so we can
actually use this and if you want to use
other models there are a few more models
a available so you can click on this
link here and uh let me just take you
there there are a few more models but we
have chosen this particular one because
this is faster it may not be very
accurate but that is one of the faster
models but on this link you will see a
lot of other models that are readily
available these are trained models some
of them would take a little longer but
they may be more accurate and so on so
you can probably play around with these
other models okay so we will be using
that model so this piece of code this
line is basically importing that model
and this is also known as uh Frozen
model the term we use is frozen model so
we import download and import that and
then we will actually use that model in
our code all right so these two cells we
have downloaded and import the model and
then once it is available locally we
will then load this into our program all
right so we are loading this into memory
and uh you need to perform a couple of
additional steps which is basically we
need to to map the numbers to text as
you may be aware when we actually build
the model and when we run predictions
the model will not give a text the
output of the model is usually a number
so we need to map that to a text so for
example if the network predicts that the
output is five we know that five means
it is an aeroplane things like that so
this mapping is done in this next cell
all right so let's keep moving and then
we have a helper code which will
basically load the data or load the
images and transform into numpy arrays
this is also used in doing object
detection in images so we are actually
going to reuse because video is nothing
but it consists of frames which in turn
our images so we are going to pretty
much use reuse the same code which we
used for doing object detection in
images so this is where the actual
detection starts so here this is the
path for where the images are stored so
this is here once again we are reusing
the code which we wrote for detecting
objects in an image so this is the path
where the images were stored and this is
the extension and this was done for
about 2s three images so we will
continue to use this and uh we go down
I'll skip this section so this is the
cell where we are actually loading the
video and converting it into frames and
then using frame by frame we are
detecting the objects in the image so in
this code what we are doing basically is
there a few lines of code what they do
is basically once they find an object a
box will be drawn around those uh each
of those objects and the input file the
name of the input video file is uh
traffic it is the extension is MP4 and
uh we have this video reader so
excellent object which is basically part
of this class called image iio so we can
read and write videos using that and uh
the video that we are going to use is
traffic. MP4 you can use any mp4 file
but in our case I picked up video which
has uh like car so let me just show you
so this is in this object detection
folder I have this mp4 file I'll just
quickly prate this video it's a little
slow yeah okay so here we go this is the
video it's a short one relatively small
video so that for this particular demo
and what it will do is once we run our
code it will detect each of these cars
and it will annotate them as cars so in
this particular video we only have cars
we can later on see with another video I
think I have cat here so we can also Al
try with that but let's first check with
this uh traffic video so let me go back
so we will be reading this frame by
frame and um no actually we will be
reading the video file but then we will
be analyzing it frame by frame and we
will be reading them at 10 frames per
second that is the rate we are
mentioning here and analyzing it and
then annotating and then writing it back
so you will see that we will have a
video file named something like this
traffic under _ annotated and um we will
see the annotated video so let's go back
and run through this piece of code and
then we will come back and see the
annotated uh video this might take a
little while so I will pause the video
after running this particular cell and
then come back to show you the results
all right so let's go ahead and run it
so it is running now and it is also
important that at the end you close the
video writer so so that it is similar to
a file pointer when you open a file you
should also make sure you close it so
that it doesn't hog the resources so
it's very similar at the end of it the
last piece or last line of code should
be video writer. close all right so I'll
pause and then I'll come back okay so I
will see you in a little bit all right
so now as you can see here the procing
is done the r Glass has disappeared that
means the video has been processed so
let's go back and check the annot video
we go back to my file manager so this
was the original traffic. MP4 and now
you have here traffic annotated MP4 so
let's go and run this and see how it
looks you see here it just got each of
these cars are getting detected let me
pause and show you so we pause here it
says car 70% let us allow it to go a
little further it detects something on
top what is that that truck okay so I
think because of the board on top it
somehow thinks there is a truck let's
play some more and see if it detects
anything else so this is again a car
looks like so let us yeah so this is a
car and it has confidence level of 69%
okay this is again a car all right so
basically till the end it goes and
detects each and every car that is
passing by now we can quickly repeat
this process process for another video
let me just show you the other video
which is a cat again there is uh this
cat is not really moving or anything but
it is just standing there staring and
moving a little slowly and uh our
application will our network will detect
that this is a cat and uh even when the
cat moves a little bit in the other
direction it'll continue to detect and
show that it is a cat Okay so yeah so
this is how the original video is let's
go ahead and change our code to analyze
this one and see if it detects our
Network detects this cat close this here
we go and I'll go back to my code all we
need to do is change this traffic to cat
the extension it will automatically pick
up because it is given here and then it
will run through so very quickly once
again what it is doing is this video
reader video reader has a a neat little
feature or interface whereby you can say
or frame in video reader so it will
basically provide frame by frame so
you're in a loop frame by frame and then
you take that each frame that is given
to you you take it and analyze it as if
it is an image individual image so
that's the way it works so it is very
easy to handle this all right so now
let's once again run just this cell rest
of the stuff Remains the Same so I will
run this cell again it will take a
little while so the our classes come
back I will pause and then come back
back in a little while all right so the
processing is done let's go and check
the annotated video go here so we have
cat annotated MP4 let's play this all
right so you can see here it is
detecting the cat and in the beginning
you also saw it detected something else
here there it looks like it detected one
more object so let's just go back and
see what it has detected here let's see
yes so what is it trying to show here
it's too small small not able to see but
uh it is trying to detect something I
think it is saying it is a car I don't
know all right okay so in this video
there's only pretty much only one object
which is a cat and uh let's wait for
some time and see if it continues to
detect it when the cat turns around and
moves as well just in a little bit
that's going to happen and we will see
there we go and in spite of turning the
other way I think our network is able to
Det detect that it is a cat so let me
freeze and then show here it is actually
still continues to detect it as a cat
all right so that's pretty much it I
think that's the only object that it
detects in this particular video okay so
close this so that's pretty much it
thank you very much for watching this
video hey there learner simply learn
brings you master's program in
artificial intelligence created in
collaboration with IBM to learn more
about this course you can find the
course Link in the description box below
in this tutorial we will take the use
case of recognizing hand written digits
this is like a hollow world of deep
learning and this is a nice little Ms
database is a nice little database that
has images of handwritten digits nicely
formatted because very often in deep
learning and neural networks we end up
spending a lot of time in preparing the
data for training and with amness
database we can avoid that you already
have the data in the right format which
can be directly used for training and
amnest also offers a bunch of built-in
utility functions that we can straight
away use and call those functions
without worrying about writing our own
functions and that's one of the reasons
why amness database is very popular for
training purposes initially when people
want to learn about deep learning and
tensor flow this is the database that is
used and it has a collection of 70,000
handwritten digits and a large part of
them are for training then you have test
just like in any machine learning
process and then you have validation and
all of them are labeled so you have the
images and their label and these images
they look somewhat like this so they are
handwritten images collected from a lot
of individuals people have these are
samples written by human beings they
have handwritten these numbers these
numbers going from 0 to 9 so people have
written these numbers and then the
images of those have been taken and
formatted in such a way that it is very
easy to handle so that is amness
database and the way we are going to
implement this in our tens oflow is we
will feed this data especially the train
in data along with the label information
and uh the data is basically these
images are stored in the form of the
pixel information as we have seen in one
of the previous slides all the images
are nothing but these are piels so an
image is nothing but an arrangement of
pixels and the value of the pixel either
it is lit up or it is not or in
somewhere in between that's how the
images are stored and that is how they
are fed into the neural network and for
training once the network is trained
when you provide a new image it will be
able to identify within a certain error
of course and for this we will use one
of the simpler neural network
configurations called softmax and for
Simplicity what we will do is we will
flatten these pixels so instead of
taking them in a two-dimensional
arrangement we just flatten them out so
for example it starts from here it is a
28x 28 so there are
7484 pixels so pixel number one starts
here it goes all the way up to 28 then
29 starts here and goes up to 56 and so
on and the pixel number 784 is here so
we take all these pixels flatten them
out and feed them like one single line
into our neural network and this is a
what is known as a softmax layer what it
does is once it is trained it will be
able to identify what digit this is so
there are in this output layer there are
10 neurons each signifying a digit and
at any given point of time when you feed
an image only one of these 10 neurons
gets activated so for example if this is
strained properly and if you feed a
number nine like this then this
particular neuron gets activated so you
get an output from this neuron let me
just use uh a pen or a laser to show you
here okay so you're feeding a number
nine let's say this has been trained and
now if you're feeding a number nine this
will get activated now let's say you
feed one to the trained Network then
this neuron will get activated if you
feed two this neuron will get activated
and so on I hope you get the idea so
this is one type of a neural network or
an activation function known as softmax
layer so that's what we will be using
here there's one of the simpler ones for
quick and easy understanding so this is
how the code would look we will go into
our lab environment in the cloud and uh
we will show you there directly but very
quickly this is how the code looks and
uh let me run you through briefly here
and then we will go into the Jupiter
notebook where the actual code is and we
will run that as well so as a first step
first of all we are using python here
and that's why the syntax of the
language is Python and the first step is
to import the tensor flow Library so and
we do this by using this line of code
saying import tensor flow as DF DF is
just for convenience so you can name
give any name and once you do this TF is
tens flow is available as an object in
the name of TF and then you can run its
uh methods and accesses its attributes
and so on and so forth and M database is
actually an integral part of tensor flow
and that's again another reason why we
as a first step we always use this
example Mist database example so you
just simply import mnist database as
well using this line of code and you
slightly modify this so that the labels
are in this format what is known as one
hot true which means that the label
information is stored like an array and
uh let me just uh use the pen to show
what exactly it is so when you do this
one hot true what happens is each label
is stored in the form of an array of 10
digits and let's say the number is uh 8
okay so in this case all the remaining
values there will be a bunch of zeros so
this is like AR at position zero this is
at position one position two and so on
and so forth let's say this is position
7 then this is position 8 that will be
one because our input is eight and again
position 9 will be zero okay so one hot
encoding this one hot encoding true will
kind of load the data in such a way that
the labels are in such a way that only
one of the digits has a value of one and
that indicat So based on which digit is
one we know what is the label so in this
case the eighth position is one
therefore we know this sample data the
value is eight similarly if you have a
two here let's say then the labeled
information will be somewhat like this
so you have your labels so you have this
as zero the zeroth position the first
position is also zero the second
position is one because this indicates
number two and then you have third as
zero and so on okay so that is the
significance of this one hot true all
right and then we can check how the data
is uh looking by displaying the the data
and as I mentioned earlier this is
pretty much in the form of digital form
like numbers so all these are like pixel
values so you will not really see an
image in this format but there is a way
to visualize that image I will show you
in a bit and uh this tells you how many
images are there in each set so the
training there are 55,000 images in
training and in the test set there are
10,000 and then validation there are
5,000 so altogether there are 70,000
images all right so let's uh move on and
we can view the actual image by uh using
the matplot clip library and this is how
you can view this is the code for
viewing the images and you can view them
in color or you can view them in Gray
scale so the cmap is what tells in what
way we want to view it and what are the
maximum values and the minimum values of
the pixel values so these are the Max
and minimum values so of the pixel
values so maximum is one because this is
a scaled value so one means it is uh
White and zero means it is black and in
between is it can be anywhere in between
black and white and the way to train the
model there is a certain way in which
you write your tensorflow code and um
the first step is to create some
placeholders and then you create a model
in this case we will use the softmax
model one of the simplest ones and um
placeholders are primarily to get the
data from outside into the neural
network so this is a very common
mechanism that is used and uh then of
course you will have variables which are
your you remember the these are your
weights and biases so for in our case
there are 10 neurons and each neuron
actually has
784 because each neuron takes all the
inputs if we go back to our slide here
actually every neuron takes all the 784
inputs right this is the first neuron it
has it receives all the 784 this is a
second neuron this also receives all the
78 so each of these inputs needs to be
multiplied with the weight and that's
what we are talking about here so these
are this is a a matrix of
784 values for each of the neurons and
uh so it is like a 10 by 784 Matrix
because there are 10 neurons and uh
similarly there are biases now remember
I mentioned bias is only one per neuron
so it is not one per input unlike the
weights so therefore for there are only
10 biases because there are only 10
neurons in this case so that is what we
are creating a variable for biases so
this is uh something little new in
tensorflow you will see unlike our
regular programming languages where
everything is a variable here the
variables can be of three different
types you have placeholders which are
primarily used for feeding data you have
variables which can change during the
course of computation and then a third
type which is not shown here are
constants so these are like fixed
numbers all right so in a regular
programming language you may have
everything as variables are at the most
variables and constants but in tens
oflow you have three different types
placeholders variables and constants and
then you create what is known as a graph
so tensorflow programming consists of
graphs and tensors as I mentioned
earlier so this can be considered
ultimately as a tensor and then the
graph tells how to execute the whole
implementation so that the execution is
stored in the form of a graph and in
this case what we are doing is we are
doing a multiplication TF you remember
this TF was created as a tensorflow
object here one more level one more so
TF is available here now tensor flow has
what is known as a matrix multiplication
or mmal function so that is what is
being used here in this case so we are
using the matrix multiplication of 10 a
flow so that you multiply your input
values x with W right this is what we
were doing x w + B you're just adding B
and this is in very similar to one of
the earlier slides where we saw Sigma XI
wi so that's what we are doing here
matrix multiplication is multiplying all
the input values with the corresponding
weights and then adding the bias so that
is the graph we created and then we need
to Define what is our loss function and
what is our Optimizer so in this case we
again use the tensor flows apis so tf.
NN softmax cross entropy with logits is
the uh API that we will use and redu
mean is what is like the mechanism
whereby which says that you reduce the
error and Optimizer for doing deduction
of the error what Optimizer are we using
so we we using gradient descent
Optimizer we discussed about this in
couple of slides uh earlier and for that
you need to specify the learning rate
you remember we saw that there was a a
slide somewhat like this and then you
define what should be the learning rate
how fast you need to come down that is
the learning rate and this again needs
to be tested and tried and to find out
the optimum level of this learning rate
it shouldn't be very high in which case
it will not converge or shouldn't be
very low because it will in that case it
will take very long so you define the
optimizer and then you call the method
minimize for that Optimizer and that
will Kickstart the training process and
so far we've been creating the graph and
in order to actually execute that graph
we create what is known as a session and
then we run that session and once the
training is completed we specify how
many times how many iterations we want
it to run so for example in this case we
are saying Thousand Steps so that is a
exit strategy in a way so you specify
the exit condition so it training will
run for thousand iterations and once
that is done we can then evaluate the
model using some of the techniques shown
here so let us get into the code quickly
and see how it works so this is our
Cloud environment now you can install
tensor flow on your local machine as
well and showing this demo on our
existing Cloud but you can also install
denlow on your local machine and uh
there is a separate video on how to set
up your tsor flow environment you can
watch that if you want to install your
local environment or you can go for
other any cloud service like for example
Google Cloud Amazon or Cloud Labs any of
these you can use and U run and try the
code okay so it has got
started we will log
in all right so this is our deep
learning tutorial uh code and uh this is
our tensorflow
environment and uh so let's get started
the first we have seen a little bit of a
code walk through uh in the slides as
well now you will see the actual code in
action so the first thing we need to do
is import tensor flow and then we will
import the data and we need to adjust
the data in such a way that the one hot
is encoding is set to True one hot
encoding right as I explained earlier so
in this case the label values will be
shown appropriately and if we just check
what is the type of the data so you can
see that this is a uh data sets python
data sets and if we check the number of
images the way it looks so this is how
it looks it is an array of type float 32
similarly the number if you want to see
what is the number
of training images there are 55,000 then
there are test images 10,000 and then
validation images 5,000 now let's take a
quick look at the data itself
visualization so we will use um mat plot
clip for this and and um if we take a
look at the shape now shape gives us
like the dimension of the tensors or or
or the arrays if you will so in this
case the training data set if we see the
size of the training data set using the
method shape it says there are 55,000
and 55,000 by 784 so remember this 784
is nothing but the 28 by 28 28 into 28
so that is equal to 784 so that's what
it is uh showing now we can take just uh
one image and just see what is the the
first image and see what is the shape so
again size obviously it is only 784
similarly you can look at the image
itself the data of the first image
itself so this is how it it shows so
large part of it will probably be zeros
because as you can imagine in the image
only certain areas are written rest is U
blank so that's why you will mostly see
Z Z either it is black or white but then
there are these values are so the values
are actually they are scaled so the
values are between Z and one okay so
this is what you're seeing so certain
locations there are some values and then
other locations there are zeros so that
is how the data is stored and loaded if
we want to actually see what is the
value of the handwritten image if you
want to view it this is how you view it
so you create like do this reshape and
um matplot lib has this um feature to
show you these images so we will
actually use the function called um I am
show and then if you pass this
parameters appropriately you will be
able to see the different images now I
can change the values in this position
so which image we are looking at right
so we can say if I want to see what is
the there in maybe
5,000 right
so 5,000 has three similarly you can
just say five what is in five five as
eight what is in
50 again H so basically by the way if
you're wondering uh how I'm executing
this code shift enter in case you're not
familiar with jupyter notebooks shift
enter is how you execute each cell
individually will sell and if you want
to execute the entire program you can go
here and say run all so that is
how this code gets executed and um here
again we can check what is the maximum
value and what is the minimum value of
this pixel values as I mentioned this is
it is scaled so therefore it is between
the values lie between 1 and zero now
this is where we create our
model the first thing is to create the
requir IDE placeholders and variables
and that's what we are doing here as we
have seen in the slides so we create one
placeholder and we create two variables
which is for the weights and biases
these two variables are actually
matrices so each variable has 784 by
10al values okay so one for this 10 is
for each neuron there are 10 neurons and
784 is for the pixel values inputs that
are given which is 28 into 28 and the
biases as I mentioned one for each
neuron so there will be 10 biases they
are stored in a variable by the name b
and this is the graph which is basically
the multiplication of these matrix
multiplication of X into W and then the
bias is added for each of the neurons
and the whole idea is to minimize the
error so let me just execute I think
this code is executed then we Define
what is our the Y value is basically the
label value so this is another
placeholder we had X as one placeholder
and Yore true as a second placeholder
and this will have values in the form of
uh 10 digigit 10 digigit uh arrays and
uh since we said one hot encoded the
position which has a one value indicates
what is the label for that particular
number all right then we have cross
entropy which is nothing but the loss
loss function and we have the optimizer
we have chosen gradient descent as our
Optimizer then the training process
itself so the training process is
nothing but to minimize the cross
entropy which is again nothing but the
loss function so we Define all of this
in the form of a graph so up to here
remember what we have done is we have
not exactly executed any tensorflow code
till now we are just preparing the graph
the execution plan that's how the
tensorflow code works so the whole
structure and format of this code will
be completely different from how we
normally do programming so even with
people with programming experience may
find this a little difficult to
understand it and it needs quite a bit
of practice so you may want to view this
uh video also maybe a couple of times to
understand this flow because the way
tensor flow programming is done is
slightly different from the normal
programming some of you who let's say
have done uh maybe spark programming to
some extent will be able to easily
understand this uh but even in spark the
the programming the code itself is
pretty straightforward behind the scenes
the execution happens slightly
differently but in tens oflow even the
code has to be written in a completely
different way so the code doesn't get
executed uh in the same way as you have
written so that that's something you
need to understand and little bit of
practice is needed for this so so far
what we have done up to here is creating
the variables and feeding the variables
and um or rather not feeding but setting
up the variables and uh the graph that's
all defining maybe the uh what kind of a
network you want to use for example we
want to use softmax and so on so you
have created the variables how to load
the data loaded the data viewed the data
and prepared everything but you have not
yet executed anything in tens of flow
now the next step is the execution in
tens of flow so the first step for doing
any execution in tensor flow is to
initialize the variables so anytime you
have any variables defined in your code
you have to run this piece of code
always so you need to basically create
what is known as a a node for
initializing so this is a node you still
are not yet executing anything here you
just created a node for the
initialization so let us go ahead and
create that and here onwards is where
you will actually execute your code uh
intensive flow and in order to ex the
code what you will need is a session
tensor flow session so tf. session will
give you a session and there are a
couple of different ways in which you
can do this but one of the most common
methods of doing this is with what is
known as a withd loop so you have a
withd tf. session as says and with a uh
colon here and this is like a block
starting of the block and these these
indentations tell how far this block
goes and this session is valid till this
block gets executed so that is the
purpose of creating this width block
this is known as a width block so with
tf. session as cess you say cs. run in
it now cs. run will execute a node that
is specified here so for example here we
are saying SS do run sess is basically
an instance of the session right so here
we are saying tf. session so an instance
of the session gets created and we are
calling that cess and then we run a node
within that one of the nodes in the
graph so one of the nodes here is in it
so we say run that particular node and
that is when the initialization of the
variables happens now what this does is
if you have any variables in your code
in our case we have W is a variable and
B is a variable so any variables that we
created you have to run this code you
have to run the initialization of these
variables otherwise you will get an
error okay so that is the that's what
this is doing then we within this width
block we specify a for Loop and we are
saying we want the system to iterate for
thousand steps and perform
the
training that's what this for Loop does
run training for thousand
iterations and what it is doing
basically is it is fetching the data or
these images remember there are about
50,000 images but it cannot get all the
images in one shot because it will take
up a lot of memory and performance
issues will be there so this is a very
common way of Performing deep learning
training you always do in batches so we
have maybe 50,000 images but you always
do it in batches of 100 or maybe 500
depending on the size of your system and
so on and so forth so in this case we
are saying okay get me 100 uh images at
a time and get me only the training
images remember we use only the training
data for training purpose and then we
use test data for test purpose you must
be familiar with machine learning so you
must be aware of this but in case you
are not not in machine learning also not
this is not specific to deep learning
but in machine learning in general you
have what is known as training data set
and test data set your available data
typically you will be splitting into two
parts and using the training data set
for training purpose and then to see how
well the model has been trained you use
the test data set to check or test the
validity or the accuracy of the model so
that's what we are doing here and and
You observe here that we are actually
calling an mnist function here so we are
saying mnist train. nextt batch right so
this is the advantage of using mes
database because they have provided some
very nice helper functions which are
readily available otherwise this
activity itself we would have had to
write a piece of code to fetch this data
in batches that itself is a a lengthy
exercise so we can avoid all that if we
are using amness database and that's why
we use the for the initial learning
phase okay so when we say fetch what it
will do is it will fetch the images into
X and the labels into Y and then you use
this batch of 100 images and you run the
training so cs. run basically what we
are doing here is we are running the
training mechanism which is nothing but
it passes this through the neural
network passes the images through the
neural network finds out what what is
the output and if the output obviously
the initially it will be wrong so all
that feedback is given back to the
neural network and thereby all the W's
and Bs get updated till it reaches th000
iterations in this case the exit
criteria is th000 but you can also
specify probably accuracy rate or
something like that for the as an exit
criteria so here it is it just says that
okay this particular image was wrongly
predicted so you need to update your
weights and biases that's the feedback
given to each neuron and that is run for
thousand iterations and typically by the
end of this thousand iterations the
model would have learned to recognize
these handwritten images obviously it
will not be 100% accurate okay so once
that is
done after so this happens for thousand
iterations once that is done you then
test the accuracy of these models by
using the test data set right so this is
what we are trying to do here the code
may appear a little complicated because
if you're seeing this for the first time
you need to understand uh the various
methods of tensor flow and so on but it
is basically comparing the output with
the what has been what is actually there
that's all it is doing so you have your
test data and uh you're trying to find
out what is the actual value and what is
the predicted value and seeing whether
they are equal or not TF do equal right
and how many of them are correct and so
on and so forth and based on that the
accuracy is uh calculated as well so
this is the accuracy and uh that is what
we are trying to see how accurate the
model is in predicting these uh numbers
or these digits okay so let us run this
this entire thing is in one cell so we
will have to just run it in one shot it
may take a little little while let us
see and uh not bad so it has finished
the thousand iterations and what we see
here as an output is the accuracy so we
see that the accuracy of this model is
around
91% okay now which is pretty good for
such a short exercise within such a
short time we got 90% accuracy however
in real life this is probably not
sufficient so there are other ways in to
increase the accuracy we will see
probably in some of the later tutorials
how to improve this accuracy how to
change maybe the hyper parameters like
number of neurons or number of layers
and so on and so forth and uh so that
this accuracy can be increased Beyond
90% hey there learner simply learn
brings you master's program in
artificial intelligence created in
collaboration with IBM to learn more
about this course you can find the
course Link in the descript description
box below we're going to dive right into
what is carass we'll also go all the way
through this into a couple tutorials
because that's where you really learn a
lot is when you roll up your sleeves so
we talk about what is carass carass is a
highlevel deep learning API written in
Python for easy imple implementation of
neural networks uses deep learning
Frameworks such as tensor flow pytorch
Etc is back in to make computation
faster and this is really nice because
as a programmer there is so much stuff
out there and it's evolving so fast it
can get confusing and having some kind
of high level order in there we can
actually view it and easily program
these different neural networks uh is
really powerful it's really powerful to
to um uh have something out really quick
and also be able to start testing your
models and seeing where you're going so
cross works by using complex deep
learning Frameworks such as tensorflow
pytorch um ml play Etc as a backend for
fast computation while providing a
userfriendly and easy tolearn
frontend and you can see here we have
the carass API uh specifications and
under that you'd have like TF carass for
tensor flow thano carass and so on and
then you have your tensorflow workflow
that this is all sitting on top
of and this is like I said it organizes
everything the heavy lifting is still
done by tensor flow or whatever you know
underlying package you put in there and
this is really nice because you don't
have to um dig as deeply into the heavy
end stuff while still having a very
robust package you can get up and
running rather quickly and it doesn't
distract from the processing time
because all the heavy lifting is done by
packages like tensor flow this is the
organization on top of it so the working
principle of
carass uh the working principle of
carass is carass uses computational
graphs to express and evaluate
mathematical
Expressions you can see here we put them
in blue they have the expression um
expressing complex problems as a
combination of simple mathematical
operators uh where we have like the
percentage or in this case in Python
that's usually your uh left your um
remainder or multiplication uh you might
have the operator of x uh to the power
of3 and it us is useful for calculating
derivatives by using uh back propagation
so if we're doing with neural networks
when we send the error back up to figure
out how to change it uh this makes it
really easy to do that without really
having not banging your head and having
to hand write everything it's easier to
implement distributed computation and
for solving complex problems uh specify
input and outputs and make sure all
nodes are connected and so this is
really nice as you come in through is
that um as your layers are going in
there you can get some very complicated
uh different setups nowadays which which
we'll look at in just a second and this
just makes it really easy to start
spinning this stuff up and trying out
the different models so when we look at
caros models uh caros model we have a
sequential model sequential model is a
linear stack of layers where the
previous layer leads into the next
layer and this if you've done anything
else even like the sklearn with their
neural networks and propagation and any
of these setups this should look
familiar you should have your input
layer it goes into your layer one layer
two and then to the output layer and
it's useful for simple classifier
decoder models and you can see down here
we have the model equals a coros
sequential and this is the actual code
you can see how easy it is uh we have a
layer that's dense your layer one as an
activation now they're using the Rao in
this particular example and then you
have your name layer one layer Den Rilo
name Layer Two and so forth uh and they
just feed right into each other so it's
really easy just to stack them as as you
can see here and it automatically takes
care of everything else for you and then
there's a functional model and this is
really where things are at this is new
make sure you update your coros or
you'll find yourself running this um
doing the functional model you'll run
into an error code because this is a
fairly new release and he uses
multi-input and multi-output model the
complex model which Forks into two or
more branches and you can see here we
have our image inputs equals your carass
input shape = 32x 32x 3 you have your
dense layers dense 64 activation railu
this should look similar to what you
already saw before uh but if you look at
the graph on the right it's going to be
a lot easier to see what's going on you
have two different
inputs uh and one way you could think of
this is maybe one of those is a small
image and one of those is a full-sized
image and that feedback goes into you
might feed both of them into one note
because it's looking for one thing and
then only into one node for the other
one and so you can start to get kind of
an idea that there's a lot of use for
this kind of split and this kind of
setup uh where we have multiple
information coming in but the
information's very different even though
it overlaps and you don't want it to
send it through the same neural network
um and they're finding that this trains
faster and is also has a better result
depending on how you split the data up
and you and how you Fork the models
coming
down and so here we do have the two
complex uh models coming in uh we have
our image inputs which is a 32x 32 by3
or three channels or four if you're
having an alpha channel uh you have your
dense your layers dense is 64 activation
using the railu very common uh x equals
dense inputs X layers dense x64
activation equals Rao X outputs equals
layers dense 10 X model equals coros
model inputs equals inputs outputs
equals output name equals ninc
model uh so we add a little name on
there and again this is this kind of
split here this is setting us up to um
have the input go into different areas
so if you're already looking at corus
you probably already have this answer
what are neural networks uh but it's
always good to get on the same page and
for those people who don't fully
understand neural networks to dive into
them a little bit we're do a quick
overview neural networks are deep
learning algorithms modeled after the
human brain they use multiple neurons
which are mathematical operations to
break down and solve complex maical
problems and so just like the neuron one
neuron fires in and it fires out to all
these other neurons or nodes as we call
them and eventually they all come down
to your output layer and you can see
here we have the really standard graph
input layer a hidden layer and an output
layer one of the biggest parts of any
data processing is your data
pre-processing
uh so we always have to touch base on
that with a neural network like many of
these models they're kind of uh when you
first start using them they're like a
black box you put your data in you train
it and you test it and see how good it
was and you have to pre-process that
data because bad data in is uh bad
outputs so in data pre-processing we
will create our own data examples set
with carass the data consists of a
clinical trial conducted on 2100
patients ranging from ages 13 to 100
with a the patients under 65 and the
other half over 65 years of age we want
to find the possibility of a patient
experiencing side effects due to their
age and you can think of this in today's
world with uh coid uh what's going to
happen on there and we're going to go
ahead and do an example of that in our
uh life Hands-On like I said most of
this you really need to have handson to
understand so let's go ahead and bring
up our anaconda and uh open that up and
open up a Jupiter notebook for doing the
python code in now if you're not
familiar with those you can use pretty
much any of your uh setups I just like
those for doing demos and uh showing
people especially shareholders it really
helps CU it's a nice visual so let me go
and flip over to our anaconda and the
Anaconda has a lot of cool to tools they
just added datal lore and IBM Watson
Studio clad into the Anaconda framework
but we'll be in the Jupiter lab or
Jupiter notebook um I'm going to do
jupyter notebook for this because I use
the lab for like large projects with
multiple pieces because it has a
multiple tabs where the notebook will
work fine for what we're doing and this
opens up in our browser window because
that's how Jupiter doeb sorry Jupiter
notebook is set to run and we'll go
under new create a new Python 3 and uh
it creates an Untitled python we'll go
ahead and give this a title and we'll
just call this uh carass
tutorial and let's change that to
Capital there we go we go and just
rename that and the first thing we want
to go ahead and do is uh get some
pre-processing tools involved and so we
need to go ahead and import some stuff
for that like our numpy do some random
number
Generation Um I mentioned sklearn or
your s kit if you're installing sklearn
the sklearn stuff it's a site kit you
want to look
up that should be a tool of anybody who
is um doing data science if if you're
not if you're not familiar with the
sklearn
toolkit it's huge uh but there's so many
things in there that we always go back
to and we want to go ahead and create
some train labels and train samples uh
for training our
data and then just a note of what we're
we're actually doing in here uh let me
go ahead and change this this is kind of
a fun thing you can do we can change the
code to
markdown and then markdown code is nice
for doing examples once you've already
built this uh our example data we're
going to do
experimental there we go experimental
drug was tested on 2100 individuals
between 13 to 100 years of age half the
participants are under 65 and 95% of
participants are under 65 experien no
side effects well 95% of participants
over 65 um experience side effects so
that's kind of where we're starting at
um and this is just a real quick example
because we're going to do another one
with a little bit more uh complicated
information uh and so we want to go
ahead and
generate our setup uh so we want to do
for I in range and we want to go ahead
and create if you look here we have
random
integers train the labels of pin so
we're just creating some random
data uh let me go ahead and just run
that and so once we've created our
random data and if you if I mean you can
certainly ask for a copy of the code
from Simply learn they'll send you a
copy of this or you can zoom in on the
video and see how we went ahead and did
our train samples a pin um and we're
just using this I do this kind of stuff
all the time I was running a thing on uh
that had to do with errors following a
bell-shaped curve on uh a standard
distribution error and so what do I do I
generate the data on a standard
distribution error to see what it looks
like and how my code processes it since
that was the Baseline I was looking for
in this we're just doing uh uh
generating random data for our setup on
here and uh we could actually go in um
print some of the data up let's just do
this
print um we'll do
train samples
and we'll just gen do the first um five
pieces of data in there to see what that
looks like and you can see the first
five pieces of data in our train samples
is 49 85 41 68 19 just random numbers
generated in there that's all that is uh
and we generated significantly more than
that um let's see 50 up here 1,000 yeah
so there's 1,00 here 1,000 numbers we
generated and we could also if we wanted
to find that out we could do a quick uh
print the length of it
and so or you could do a shape kind of
thing and if you're using
numpy although the link for this is just
fine and there we go it's actually 2100
like we said in the demo setup in
there and then we want to go ahead and
take our labels oh that was our train
labels we also did samples didn't
we uh so we could also print do the same
thing
oh
labels um and let's change this
to
labels and
[Music]
labels and run that just a double check
and sure enough we have 2100 and they're
labeled one Z one0 one0 I guess that's
if they have symptoms or not one
symptoms uh zero none and so we want to
go ahead and take our train label and
we'll convert it into a numpy array and
the same thing with our samples and
let's go ahead and run that and we also
Shuffle uh this is just a neat feature
you can do in uh numpy right here put my
drawing thing on which I didn't have on
earlier um I can take the data and I can
Shuffle it uh so we have our so it's it
just randomizes it that's all that's
doing um we've already randomized it so
it's kind of an Overkill it's not really
necessary
but if you're doing uh a larger package
where the data is coming in and a lot of
times it's organized somehow and you
want to randomize it just to make sure
that that you know the input doesn't
follow a certain pattern um that might
create a bias in your model and we go
ahead and create a scaler uh the scaler
range uh minimum Max scaler feature
range 0 to
one uh then we go ahead and scale the uh
scaled train samples we're going to go
ahead and fit and trans form the data uh
so it's nice and scaled and that is the
age uh so you can see up here we have 49
85 41 we're just moving that so it's
going to be uh between zero and one and
so this is true with any of your neural
networks you really want to convert the
data uh to zero and one otherwise you
create a bias uh so if you have like a
100 creates a bias
versus the math behind it gets really
complicated um if you actually start
start multiplying stuff CU a lot of
multiplication addition going on in
there that higher end value will
eventually multiply down and it will
have a huge bias as to how the model
fits it and then it will not fit as well
and then one of the fun things we can do
in Jupiter notebook is that if you have
a variable and you're not doing anything
with it it's the last one on the line it
will automatically
print um and we're just going to look at
the first five samples on here and so
just going to print the first five
samples
and you can see here we go
995 791 so everything's between 0 and
one and that just shows us that we
scaled it properly and it looks good uh
it really helps a lot to do these kind
of print UPS halfway through uh you
never know what's going to go on
there I don't know how many times I've
gotten down and found out that the data
sent to me that I thought was scaled was
not and then I have to go back and track
it down and figure it out on
there uh so let's go ahead and create
our artificial neural
network and for doing that this is where
we start diving into tensor flow and
carass uh tensor
flow if you don't know the history of
tensor
flow it helps to uh jump into we'll just
use
Wikipedia careful don't quote Wikipedia
on these things because you get in
trouble uh but it's a good place to
start uh back in 2011 Google brain built
disbelief as a proprietary machine
learning setup tensor flow became the
open source for it uh so tensorflow was
a Google product and then it became uh
open sourced and now it's just become
probably one of the defacto when it
comes for neural networks as far as
where we're at uh so when you see the
tensor flow
setup it it's got like a huge following
there are some other setups like um the
S kit under the sklearn has our own
little neural network uh but the
tensorflow is the most robust one out
there right now and carass sitting on
top of it makes it a very powerful tool
so we can leverage both the carass uh
easiness in which we can build a
sequential setup on top of tensor
flow and so in here we're going to go
ahead and do our input of tensor flow uh
and then we have the rest of this is all
carass here from number two down uh
we're going to import from tensor flow
the cross uh connection and then you
have your tensorflow across models
import sequential it's a specific kind
of model we'll look at that in just a
second if you remember from the files
that means it goes from one layer to the
next layer to the next layer there's no
funky splits or anything like
that uh and then we have from tensorflow
Cross layers we're going to import our
activation and our dense
layer and we we have our Optimizer atom
um this is a big thing to be aware of
how you optimize uh your data when you
first do it atom's as good as any atom
is usually uh there's a number of
Optimizer out there there's about uh
there's a couple main ons but atom is
usually assigned to bigger data uh it
works fine usually the lower data does
it just fine but atom is probably the
mostly used but there are some more out
there and depending on what you're doing
with your layers your different layers
might have different activations on them
and then finally down here you'll see um
our setup where we want to go ahead and
use the
metrics and we're going to use the
tensorflow cross metrics um for
categorical cross entropy uh so we can
see how everything performs when we're
done that's all that is um a lot of
times you'll see us go back and forth
between tensor flow and then pyit has a
lot of really good metrics also for
measuring these things um again it's the
end of the you know at the end of the
story how good does your model do and
we'll go ahead and load all that and
then comes the fun part um I actually
like to spend hours messing with these
things and uh four lines of code you're
like ah you're G to spend hours on four
lines of code um no we don't spend hours
on four lines of code that's not what
we're talking about when I say spend
hours on four lines of code uh what we
have here I'm going to explain that in
just a second we have a model it's a
sequential model if you remember
correctly we mentioned the sequential up
here where it goes from one layer to the
next and our first layer is going to be
your
input it's going to be uh what they call
DSE which is um usually it's just D and
then you have your input and your
activation um how many units are coming
in we have 16 uh what's the shape What's
the activation and this is where it gets
interesting um because we have in here
uh
railu on two of these and soft Max
activation on one of these there are so
many different options for what these
mean um and how they function how does
the ru how does the softmax
function and they do a lot of different
things um we're not going to go into the
activations in here that is what really
you spend hours doing is looking at
these different activations
um and just some of it is just uh um
almost like you're playing with it like
an artist you start getting a fill for
like a uh inverse tangent activation or
the tan
activation takes up a huge processing
amount uh so you don't see it a lot yet
it comes up with a better solution
especially when you're doing uh when
you're analyzing Word documents and
you're tokenizing the words and so
you'll see this shift from one to the
other because you're both trying to
build a better model and if you're
working on a huge data set um it'll
crash the system it'll just take too
long to process um and then you see
things like soft Max uh soft Max
generates an interesting um
setup where a lot of these when you talk
about Ru oops let me do this uh Ru there
we go rayu has um a setup where if it's
less and zero it's zero and then it goes
up um and then you might have what they
call lazy uh setup where it has a slight
negative to it so that the errors can
translate better same thing with softmax
it has a slight laziness to it so that
errors translate better all these little
details make a huge different on your
model um so one of the really cool
things about data science that I like is
you build your uh what they call you
build defil and it's an interesting uh
design setup oops I forgot the end of my
code
here the concept to build a fail is you
want the model as a whole to work so you
can test your model
out so what you can do um you can get to
the end and you can do your let's see
where was it overshot down here you can
test your test out the quality of your
setup on there and see where did I do my
tensor FL oh here we go I it was right
above me here we go we start doing your
cross entropy and stuff like that is you
need a full functional set of code so
that when you run
it you can then test your model out and
say hey it's either this model works
better than this model and this is why
um and then you can start swapping in
these models and so when I say I spend a
huge amount of time on pre-processing
data is probably 80% of your programming
time um well between those two it's like
8020 you'll spend a lot of time on the
models once you get the model down once
you get the whole code and the flow down
uh set depending on your data your
models get more and more robust as you
start experimenting with different
inputs different data streams and all
kinds of things and we can do a simple
model summary
here uh here's our sequential here's our
layer our output our
parameter this is one of the nice things
about carass is you just you can see
right here here's or sequential one
model boom boom boom boom everything's
set and clear and easy to read so once
we have our model built uh the next
thing we're going to want to do is we're
want to go ahead and train that
model and so the next step is of course
model
training and when we come in here this a
lot of times is just paired with the
model because it's so straightforward
it's nice to print out the model setup
so you can have a tracking
but here's our model uh the keyw in
Cross is
compile Optimizer atom learning rate
another term right there that we're just
skipping right over that really becomes
the meat of um the setup is your
learning rate uh so whoops I forgot that
I had an arrow but I'll just underline
it a lot of times the learning rate set
to 0.0 uh set to 0.01 uh depending on
what you're doing this learning rate um
can overfit and underfit uh so you'd
want to look up I know we have a number
of tutorials out on overfitting and
underfitting that are really worth
reading once you get to that point in
understanding and we have our loss um
sparse categorical cross entropy so this
is going to tell carass how far to go
until it stops and then we're looking
for metrics of accuracy so we'll go
ahead and run that and now that we've
compiled our model
we want to go ahead and um run it fit it
so here's our model fit um we have our
scaled train
samples our train labels our validation
split um in this case we're going to use
10% of the data for
validation uh batch size another number
you kind of play with not a huge
difference as far as how it works but it
does affect how long it takes to run and
it can also affect the bias a little bit
uh most of the time though a batch size
is between 10 to 100 um depending on
just how much data you're processing in
there we want to go ahead and Shuffle it
uh we're going to go through 30 epics
and uh put a verbose of two let me just
go and run this and you can see right
here here's our epic here's our training
um here's our loss now if you remember
correctly up here we set the loss's see
where was it um compiled our
data there we go loss uh so it's looking
at the sparse categorical cross entropy
this tells us that as it goes how how
how much um how how much does the um
error go down uh is the best way to look
at that and you can see here the lower
the number the better it just keeps
going down and vice versa accuracy we
want let's see where's my
accuracy value accuracy at the end uh
and you can see 61 9. 69. 74 it's going
up we want the accuracy would be ideal
if it made it all the way to one but we
also the loss is more important because
it's a balance um you can have 100%
accuracy in your model doesn't work
because it's overfitted uh again you w't
look up overfitting and underfitting
models we went ahead and went through 30
epics it's always fun to kind of watch
your code going um to be honest I
usually uh um the first time I run it
I'm like oh that's cool I get to see
what it does and after the second time
of running it I'm like I like to just
not see that and you can repress those
of course in your code uh repress the
warnings in the
printing and so the next step is going
to be building a test set and predicting
it now uh so here we go we want to go
ahead and build our test set and we have
uh just like we did our training set a
lot of times you just split your your
initial set up uh but we'll go ahead and
do a separate set on here and this is
just what we did above uh there's no
difference as far as
um the randomness that we're using to
build this set on here uh the only
difference is
that we already um did our scaler up
here well it doesn't matter because that
the data is going to be across the same
thing but this should just be just
transform down here instead of fit
transform uh CU you don't want to refit
your data um on your testing
data there we go now we're just
transforming it because you never want
to transform the test data um easy
mistake to make especially on an example
like this where we're not
doing um you know we're randomizing the
data anyway so it doesn't matter too
much because we're not expecting
something
weird and then we went ahead do our
predictions the whole reason we built
the model is we take our model we
predict and we're going to do here's our
xcal data batch size 10 verbose and now
we have our predictions in here and we
could go ahead and do a um oh we'll
print
predictions and then I guess I could
just put down predictions and fives we
can look at the first five of the
predictions and what we have here is we
we have our age and uh the prediction on
this age versus on what what we think
it's going to be what what we think is
going to they going to have uh symptoms
or not and the first thing we notice is
that's hard to read because we really
want a yes no answer uh so we'll go
ahead and just uh round off the
predictions using the argmax um the
numpy argmax uh for prediction so it
just goes to a
01 and if you remember this is a Jupiter
notebook so I don't have to put the
print I can just put in uh rounded
predictions and we'll just do the first
five and you can see here 0 one 0 0 0 so
that's what the predictions are that we
have coming out of this um is no
symptoms symptoms no symptoms symptoms
no symptoms and just as uh we were
talking about at the beginning we want
to go ahead and um take a look at this
there we go confusion matrixes for
accuracy check um most important part
part when you get down to the end of the
story how accurate is your model before
you go and play with the model and see
if you can get a better accuracy out of
it and for this we'll go ahead and use
the S kit um the SK learn metrics uh s
kit being where that comes from import
confusion Matrix uh some iteration tools
and of course a nice map plot library
that makes a big difference so it's
always nice to
um have a nice graph to look at um
pictures worth a a thousand
words um and then we'll go ahead call it
CM for confusion Matrix y true equals
test labels why predict rounded
predictions and we'll go ahead and load
in our
cm and I'm not going to spend too much
time on the plotting um going over the
different plotting
code um you can spend uh like whole we
have whole tutorials on how to do your
different
plotting on there uh but we do have here
is we're going to do a plot confusion
Matrix there's our CM our classes
normalized false title confusion Matrix
cmap is going to be in
blues and you can see here we have uh to
the nearest cmap titles all the
different pieces whether you put tick
marks or not the marks the classes the
color bar um so a lot of different
information on here as far as how we're
doing the printing of the of the
confusion Matrix you can also just dump
the confusion Matrix um into a caborn
and real quick get an output it's worth
knowing how to do all this uh when
you're doing a presentation to the
shareholders you don't want to do this
on the Fly you want to take the time to
make it look really nice uh like our
guys in the back did and uh let's go
ahead and do this forgot to put together
our CM plot labels we'll go and run
that and then we'll go ahead and call
the little the definition
for our
mapping and you can see here plot
confusion Matrix that's our the the
little script we just wrote and we're
going to dump our data into it um so our
confusion Matrix our classes um title
confusion Matrix and let's just go ahead
and run
that and you can see here we have our
basic setup uh no side effects
195 had side effects uh 200 no side
effects that had side effects so so we
predicted the 10 of them who actually
had side effects and that's pretty good
I mean I I don't know about you but you
know that's 5% error on this and this is
because there's 200 here that's where I
get 5% is uh divide these both by by two
and you get five out of a 100 uh you can
do the same kind of math up here not as
quick on the flight it's 15 and 195 not
an easily rounded number but you can see
here where they have 15
people who predicted to have no uh with
the no side effects but had side effects
kind of setup on there and these
confusion Matrix are so important at the
end of the day this is really where
where you show uh whatever you're
working on comes up and you can actually
show them hey this is how good we are or
not how messed up it
is so this was a uh I spent a lot of
time on some of the parts uh but you can
see here is really simple uh we did the
random generation of data but when we
actually built the model coming up here
uh here's our model
summary and we just have the layers on
here that we built with our model on
this and then we went ahead and trained
it and ran the prediction now we can get
a lot more complicated uh let me flip
back on over here because we're going to
do another uh demo so that was our basic
introduction to it we talked about the
uh oops here we go okay so implementing
a neural network with carass after
creating our samples and labels we need
to create our carass neural network
model we will be working with a
sequential model which has three layers
and this is what we did we had our input
layer our hidden layers and our output
layers and you can see the input layer
uh coming in uh was the age Factor we
had our hidden layer and then we had the
output are you going to have symptoms or
not so we're going to go ahead and go
with something a little bit more
complicated um training our model is a
two-step process we first compile our
model and then we train it in our
training data set uh so we have compile
comp iling compiling converts the code
into a form of understandable by
Machine we use the atom in the last
example a gradient descent algorithm to
optimize a model and then we trained our
model which means it let it uh learn on
training data uh and I actually had a
little backwards there but this is what
we just did is we if you remember from
our code we just had o let me go back
here
um here's our model that we
created summarize
uh we come down here and we compile it
so it tells it hey we're ready to build
this model and use it uh and then we
train it this is the part where we go
ahead and fit our model and and put that
information in here and it goes through
the training on there and of course we
scaled the data which was really
important to do and then you saw we did
the creating a confusion Matrix with
carass um as we are performing
classifications on our data we need a
confusion Matrix to check the results a
confusion Matrix breaks down the various
misclassifications as well as correct
classifications to get the
accuracy um and so you can see here this
is what we did with the true positive
false positive true negative false
negative and that is what we went over
let me just scroll down
here on the end we printed it out and
you can see we have a nice print out of
our confusion Matrix uh with the true
positive false positive false negative
true negative and so the blue ones uh we
want those to be the biggest numbers
because those are the better side and
then uh we have our false predictions on
here uh as far as this one so it had no
side effects but we predicted let's see
no side effects predicting side effects
and vice versa if getting your learning
started is half the battle what if you
could do that for free visit scaleup by
simply learn click on the link in the
description to no more now uh saving and
loading models with carass we're going
to dive into a more complicated
demo um and you're going to say oh that
was a lot of complication before well if
you broke it down we randomized some
data we created the um carass setup we
compiled it we trained it we predicted
and we ran our Matrix uh so we're going
to dive into something a lot little bit
more fun is we're going to do a face
mask detection with carass uh so we're
going to build a cross model to check if
a person is wearing a mask or not in
real time and this might be important if
you're here at the front of a store this
is something today which is um might be
very useful as far as some of our you
know making sure people are
safe uh and so we're going to look at
mask and no mask and let's start with a
little bit on the
data and so in my data I have with a
mask you can see they just have a number
of images showing the people in masks
and again if you want some of this
information uh contact simply learn and
they can send you some of the
information as far as people with
without masks so you can try it on your
own and this is just such a wonderful
example of this setup on here so before
I dive into the mask detection uh
talking about being in the current with
h coid and seeing that people are
wearing masks this particular example I
had to go ahead and update to a python
3.8 version uh it might run in a 37 I'm
not sure I I kind of skipped 37 and
installed
38 uh so I'll be running in a three
python 38 um and then you also want to
make sure your tensor flow is up to dat
because the um they call
functional uh layers with that's where
they split if you remember correctly
from back uh oh let's take a look at
this remember from here the functional
model and a functional layer allows us
to feed in the different layers into
different you know different nodes into
different layers and split them uh very
powerful tool very popular right now in
the edge of where things are with neural
networks and creating a better model so
I've upgraded to python 3.8 and let's go
ahead and open that up and go through uh
our next example which includes uh
multiple layers um programming it to
recognize whether someone wears a mask
or not and then uh saving that model so
we can use it in real time so we're
actually almost a full um end to end
development of a product here of course
this is a very simplified version and
there'd be a lot more to it you'd also
have to do like recognizing whether it's
someone's face or not all kinds of other
things go into this so let's go ahead
and jump into that code and we'll open
up a new Python 3 oops Python 3 it's
working on it there we
go um and then we want to go ahead and
train our mask we'll just call this
train
mask and we want to go ahead and train
mask and save it uh so it's it's uh save
mask train mask detection not to be
confused with masking data a little bit
different we're actually talking about a
physical mask on your
face and then from the cross standpoint
we got a lot of imports to do here and
I'm not going to dig too deep on the
Imports uh we're just going to go ahead
and notice a few of them uh so we have
in here oops let me go alt D there we go
have something to draw with a little bit
here we have our uh image
processing and the image processing
right here let me underline that uh
deals with how do we bring images in
because most images are like a a square
grid and then each value in there has
three values for the three different
colors uh carass and tensorflow do a
really good job of uh working with that
so you don't have to do all the heavy
lising and figuring out what's going to
go on uh and we have the mobile net
average pooling 2D um this again is how
do we deal with the images and pulling
them uh dropout's a cool thing worth
looking up if you have't when as you get
more and more into carass and tensor
flow uh it'll Auto drop out certain
notes that way you'll get a better um
the notes just kind of die uh they find
that they actually create more of a bias
and a help and they also add processing
time so they remove them um and then we
have our flatten that's where you take
that huge array with the three different
colors and you find a way to flatten it
so it's just a one-dimensional array
instead of a 2X two
by3 uh dense input we did that in the
other one so that should look a little
familiar oops there we go our input um
our model again these are things we had
on the last one here's our Optimizer
with our atom
um we have some pre-processing on the
input that goes along with bringing in
the data in uh more pre-processing with
image to array loading the image um this
stuff is so nice it looks like a lot of
work you have to import all these
different modules in here but the truth
is is it does everything for you you're
not doing a lot of pre-processing you're
letting the software do the
pre-processing um and we're going to be
working with the setting something to
categorical again that's just a
conversion from a number to a category
uh 01 doesn't really mean anything it's
like true false um label bizer the same
thing uh we're changing our labels
around and then there's our train test
split classification report um our IM
utilities let me just go ahead and
scroll down here Notch for these this is
something a little different going on
down here this is not part of the U
tensor flow or the SK learn this is s
kit setup and tensor flow above uh the
path this is part of um open CV and
we'll actually have another tutorial
going out with the open CV so if you
want to know more about Open CV you'll
get a glance on it in uh this software
especially the ne the second piece when
we reload up the data and hook it up to
a video camera we're going to do that on
this round um but this is part of the
open CV thing and you'll see CV2 is
usually how that's referenced um but the
IM utilities has to do with how do you
rotate pictures around and stuff like
that uh and resize them and then the map
plot library for plotting because it's
nice to have a graph tells us how good
we're doing and then of course our numpy
numbers array and just a straight OS
access wow so that was a lot of imports
uh like I said I'm not going to spend I
spend a little time going through them
uh but we didn't want to go too much
into
them and then I'm going to create um
some variables that we need to go ahead
and initialize we have the learning rate
number of epics to train for and the
batch size and if you remember correctly
we talked about the learning rate uh to
the4
.001 um a lot of times it's 0.001 or
0.001 usually it's in that uh variation
depending on what you're doing and how
many epics and they kind of play with
the epics the epics is how many times
are we going to go through all the
data now I have it as two um the actual
setup is for 20 20 and 20 works great
the reason I have it for two is it takes
a long time to process one of the
downsides of
Jupiter is that Jupiter isolates it to a
single kernel so even though I'm on an8
core processor uh with 16 dedicated
threads only one thread is running on
this no matter what so it doesn't matter
uh so it takes a lot longer to run even
though um tensor flow really scales up
nicely and the batch size is how many
pictures we load it once in process
again those are numbers you have to
learn to play with depending on your
data and what's coming in and the last
thing we want to go ahead and do is
there's a directory with a data set
we're going to
run uh and this just has images of masks
and not
masks and if we go in here you'll see
data
set um and you have pictures with mass
they're just images of people with mass
on their face uh and then we have the
opposite let me go back up here
without masks so it's pretty
straightforward they look kind of a skew
because they tried to format them into
very similar uh setup on there so
they're they're mostly squares you'll
see some that are slightly different on
here and that's kind of important thing
to do on a lot of these data sets get
them as close as you can to each other
and we'll we actually will run in the in
this processing of images up here and
the cross uh layers and importing and
and dealing with images it does such a
wonderful job of converting these it lot
of it we don't have to do a whole lot
with uh so you have a couple things
going on there and so uh we're now going
to be this is now loading the um images
and let me see and we'll go ahead and uh
create data and labels here's our um uh
here's the features going in which is
going to be our pictures and our labels
going out and then for categories in our
list directory directory and if you
remember I just flashed that at you it
had uh uh face face mask or or no face
mask those are the two options and we're
just going to load into that we're going
to pin the image itself and the labels
so we're just create a huge array uh and
you can see right now this could be an
issue if you had more data at some point
um thankfully I have a a 32 gig hard
drive or
Ram even that does you could do with a
lot less of that probably under 16 or
even eight gigs would easily load all
this stuff um and there's a conversion
going on in here I told you about how we
are going to convert the size of the
image so it resizes all the images and
that way our data is all identical the
way it comes
in and you can see here with our labels
we have without mask without mask
without mask uh the other one would be
with mask those are the two that we have
going in
there uh and then we need to change it
to the one not hot
encoding and this is going to take our
um um up here we had what was it labels
and data uh we want the labels uh to be
categorical so we're going to take
labels and change it to categorical and
our labels then equal a categorical list
uh we'll run that and again if we do uh
labels and we just do the last or the
first 10 let's do the last 10 just
because um minus 10 to the end there we
go just so we can see what the other
side looks like we now have one that
means they have a m one Z one 0er so
on uh one being they have a mask and
zero no mask and if we did this in
Reverse I just realized that this might
not make sense if you've never done this
before let me run this
01 so zero is uh do they have a mask on
zero do they not have a mask on one so
this is the same as what we saw up here
without mask mask 1 equals um the second
value is without mask so with mask
without mask uh and that's just a with
any of your data
processing we can't really a zero if you
have a 01
output uh it causes issues as far as
training and setting it up so we always
want to use a one hot encoder if the
values are not actual uh linear value or
regression values they're not actual
numbers if they represent
thing and so now we need to go ahead and
do our train X test X train y test y um
train split test data and we'll go ahead
and make sure it's going to be uh random
and we'll take 20% of it for testing and
the rest for um setting it up as far as
training their model this is something
that's become so cool when they're
training these Set uh they realize we
can augment the data what does augment
mean well if I wrote rotate the data
around and I zoom in I zoom out I rotate
it um share it a little bit flip it
horizontally um fil mode as I do all
these different things to the data it um
is able to it's kind of like increasing
the number of samples I have uh so if I
have all these perfect samples what
happens when we only have part of the
face or the faces tilted sideways or all
those little shifts cause a problem if
you're doing just a standard set of data
so we're going to create an augment and
our image data generator um which is
going to rotate zoom and do all kinds of
cool thing and this is worth looking up
this image data generator and all the
different features it has um a lot of
times I'll the first time through my
models I'll leave that out so I want to
make sure there's a thing we call build
the fail which is just cool to know you
build a whole process and then you start
adding these different things in uh so
you can better train your model and so
we go and run this and then we're going
to load um and then we need to go ahead
and you probably would have gotten an
error if you hadn't put this piece in
right here um I haven't run it myself
cuz the guys in the back did this uh we
take our base model and one of the
things we want to do is we want to do a
mobile net
V2 um and this what we this is a big
thing right here include the top equals
false a lot of data comes in with a
label on the top row uh so we want to
make sure that that is not the case and
then the construction of the head of the
model that will be placed on the top of
the base model uh we want to go ahead
and set that
up and you'll see a warning here I'm
kind of ignoring the warning because it
has to do with the uh size of the
pictures and the weights for input shape
um so they'll it'll switch things to
defaults just saying hey we're going to
Auto shape some of this stuff for you
you should be aware of that with this
kind of imagery we're already augmenting
it by moving it around and flipping it
and doing all kinds of things to it uh
so that's not a bad thing in this but
another data it might be if you're
working in a different
domain and so we're going to go back
here and we're going to have we have our
base model we're going to do our head
model equals our base model
output um and what we got here is we
have an average pooling 2D pool size 77
head model um head model flatten so
we're flattening the data uh so this is
all
processing and flattening the images and
the pooling has to do with some of the
ways it can process some of the data
we'll look at that a little bit when we
get down to the lower level on this
processing it um and then we have our
dents we've already talked a little bit
about a d just what you think about and
then the head model has a Dropout of
05 uh what we can do with a
Dropout the Dropout says that we're
going to drop out a certain amount of
not nodes while training uh so when you
actually use the model you will use all
the nodes but this drops certain ones
out and it helps stop biases from up
forming uh so it's really a cool feature
on here they discovered this a while
back uh we have another dense mode and
this time we're using soft Max
activation lots of different activation
options here softmax is a real popular
one for a lot of things U so is the
ru and you know there we could do a
whole talk on activation formulas uh and
why what their different uses are and
how they work when you first start out
you'll you'll use mostly the ru and the
softmax for a lot of them uh just
because they're they're some of the
basic setups it's a good place to
start uh and then we have our model
equals model inputs equals base model.
input outputs equals head model so again
we're still building our model here
we'll go ahead and run
that and then we're going to Loop over
all the layers in the base model and
freeze them so they will not be updated
during the first training
process uh so for layer and base model
layers layers. trainable equals False A
lot of times when you go through your
data um you want to kind of jump in
partway through um I I'm not sure why in
the back they did this for this
particular example um but I do this a
lot when I'm working with series and and
specifically in stock data I wanted to
iterate through the first set of 30 Data
before it does
anything um I would have to look deeper
to see why they froze it on this
particular one and then we're going to
compile our model uh so compiling the
model
atom andit layer Decay um initial
learning rate over epics and we go ahead
and compile our loss is going to be the
binary cross entropy which we'll have
that print out Optimizer for opt metric
is accuracy same thing we had before not
a huge jump as far as um the previous
code and then we go ahead and we've gone
through all this and now we need to go
ahead and fit our model uh so train the
head of the network print info
trainining head
run now I skipped a little time because
it you'll see the run time here is um at
80 seconds per epic takes a couple
minutes for it to get through on a
single kernel
one of the things I want you to notice
on here while we're while it's finishing
the
processing is that we have up here our
augment going on so anytime the train X
and train y go in there's some
Randomness going on there and is
jiggling it around what's going into our
setup uh of course we're batch sizing it
uh so it's going through whatever we set
for the batch values how many we process
at a time and then we have the steps per
epic uh the train X the batch size
validation data here's our test X and
Test Y where we're sending that
in uh and this again it's validation one
of the important things to know about
validation is our um when both our
training data and our test data have
about the same accuracy that's when you
want to stop that means that our model
isn't biased if you have a higher
accuracy on your uh testing you know
you've trained it and your accuracy is
higher on your actual test data then
something in there is probably uh has a
bias and it's overfitted uh so that's
what this is really about right here
with the validation data and validation
steps so looks like it's let me go ahead
and see if it's done processing looks
like we've gone ahead and gone through
two epics again you could run this
through about 20 with this amount of
data and it would give you a nice
refined uh model at the end we're going
stop at 2: cuz I really don't want to
sit around all afternoon and I'm running
this on a single thread so now that
we've done this we're going to need to
evaluate our model and see how good it
is and to do that we need to go ahead
and make our
predictions um these are predictions on
our test X to see what it thinks are
going to be uh so now it's going to be
evaluating the network and then we'll go
ahead and go down here and we want to
need to uh turn the index in CU remember
it's it's either zero or one it's uh 0
one0 one so you have two outputs uh not
wearing uh wearing a mask not wearing a
mask and so we need to go ahead and take
that argument at the end and change
those predictions to a zero or one
coming out uh and then to finish that
off we want to go ahead and let me just
put this right in here and do it all in
one shot we want to show a nicely
formatted classification report so we
can see what that looks like on here and
there we have have it we have our
Precision uh it's 97% with a mask
there's our F1 score support without a
mask
97% um so that's pretty high high uh
setup on there you know three people are
going to sneak into the store who are
without a mask and that thinks they have
a mask and there's going to be three
people with a mask that's going to flag
the person at the front to go oh hey
look at this person you might not have a
mask um if I guess it's a setup in front
of a store um so there you have it and
of course one of the other cool things
about this is if someone's walking into
the store and you take multiple pictures
of them um you know this is just an it
it would be a way of flagging and then
you can take that average of those
pictures and make sure they match or
don't match if you're on the back end
and this is an important step because
we're going to this is just cool I love
doing this stuff uh so we're going to go
ahead and take our model and we're going
to save it uh so model save Mass
detector. model we're going to give it a
name uh we're going to save the format
um in this case we're going to use the
H5
format and so this model we just
programmed has just been saved uh so now
I can load it up into say another
program what's cool about this is let's
say I want to have somebody work on the
other part of the program well I just
Sav the model they upload the model now
they can use it for whatever and then if
I get more information uh and we start
working with that at some point I might
want to update this model um make a
better model and this is true of so many
things where I take this model and maybe
I'm uh running a prediction on uh making
money for a company and as my model gets
better I want to keep updating it and
then it's really easy just to push that
out to the actual end user uh and here
we have a nice graph you can see the
training loss and accuracy as we go
through the epics uh we only did the you
know only shows just the one Epic coming
in here but you can see right here as
the uh um value loss train accuracy and
value
accuracy starts switching and they start
converging and you'll hear converging
this is the convergence they're talking
about when they say you're you're um I
know when I work in the S kit with
sklearn neural networks this is what
they're talking about a convergence is
our loss and our accuracy come together
and also up here and this is why I'd run
it more than just to epics as you can
see they still haven't converged all the
way uh so that would be a cue for me to
keep
going but what we want to do is we want
to go ahead and create a new Python
3
program and we just did our train mask
so now we're going to go ahead and
import that and use it and show you in a
live action um get a view of uh both
myself in the afternoon along with my
background of an off office which is in
the middle still of reconstruction for
another
month and we'll call this uh
mask
detector and then we're going to grab a
bunch of um a few items coming
in uh we have our um mobet V2 import
pre-processing input so we're still
going to need that um we still have our
tensor floral image to array we have our
load model that's where most of the
stuff's going on this is our CV2 or open
CV again I'm not going to dig too deep
into that we're going to flash a little
open CV code at you uh and we actually
have a tutorial on that coming out um
our numpy array our IM utilities which
is part of the open CV or CV2
setup uh and then we have of course time
and just our operating system so those
are the things we're going to go ahead
and set up on here and then we're going
to create this takes just a
moment our module here which is going to
do all the heavy
lifting uh so we're going to detect and
predict a mask we have frame face net
Mass net these are going to be generated
by our open CV we have our frame coming
in and then we want to go ahead and and
create a mask around the face it's going
to try to detect the face and then set
that up so we know what we're going to
be processing through our
model um and then there's a frame shape
here this is just our height versus
width that's all HW stands for um
they've called it blob which is a CV2
DNN blob form image frame so this is
reformatting this Frame that's going to
be coming in literally from my camera
and we'll show you that in a minute that
little piece of code that shoots that in
here uh and we're going to pass the blob
through the network and obtain the face
detections uh so fa net. set inport blob
detections face net forward print
detections
shape uh so these is this is what's
going on here this is that model we just
created we're going to send that in
there and I'll show you in a second
where that is but it's going to be under
face
net uh and then we go ahead and
initialize our list of faces their
corresponding locations and the list of
predictions from our face mask
Network we're going to Loop over the
detections and this is a little bit more
work than you think um as far as looking
for different faces what happens if you
have a fa a crowd of faces um so We're
looping through the detections and the
shapes going through here and
probability associated with the
detection uh here's our confidence of
detections we're going to filter out
weak detection by ensuring the
confidence is greater than the minimum
confidence
uh so we've set it remember 0o to one so
0 five would be our minimal confidence
probably is pretty good um and then
we're going to put in compute bounding
boxes for the object if I'm zipping
through this it's because we're going to
do an open CV and I really want to stick
to just the carass
part and so I'm I'm just kind of jumping
through all this code you can get a copy
of this code from Simply learn and take
it apart or look for the open CV coming
out
and we'll create a box uh the box sets
it around the
image ensure the bounding boxes fall
within dimensions of the
frame uh so we create a box around
what's going what we hope is going to be
the face extract the face Roi convert it
from BGR to RGB
Channel again this is an open CV issue
not really an issue but it has to do
with the order um I don't know how many
times I've forgotten to check the order
colors when working with open CV because
it's all kinds of fun things when red
becomes blue and blue becomes red uh
then we're going to go ahead and resize
it process it frame it uh face frame
setup again the face the CBT color we're
going to convert it uh we're going to
resize it image to array pre-process the
input uh pin the face locate face. x.y
and x boy that was just a huge amount
and I skipped over a ton of it but the
bottom line is we're building a box
around the face and that box because the
open CV does a decent job of finding the
face and that box is going to go in
there and see hey does this person have
a mask on
it uh and so that's what that's what all
this is doing on here and then finally
we get down to this where it says
predictions equals mass net. predict
faces batch size
32 uh so these different images of where
we're guessing where the face is are
then going to go through an generate an
array of faces if you will and we're
going to look through and say does this
face have a mask on it and that's what's
going right here is our prediction
that's the big thing that we're working
for and then we return the locations and
the predictions the locations just tells
where on the picture it is and then the
um prediction tells us what it is is it
a mask or is it not a mask all right so
we've loaded that all up so we're going
to load our serialized face detector
model from dis um and we have our the
path that it was saved in obviously
you're going to put it in a different
path depending on where you have it or
however you want to do it and how you
saved it on the last one where we
trained it uh and then we have our
weights path um and so finally our face
net here it is equals CB2 dn. read net
uh protot text path weights path and
we're going to load that up on here so
let me go ahead and run
that and then we also need to I'll just
put it right down here I always hate
separating these things in there um and
then we're going to load the actual mass
detector model from disk this is the the
model that we saved so let's go ahead
and run that on there also so this is
pulling in all the different pieces we
need for our model and then the next
part is we're going to create open up
our video uh and this is just kind of
fun because it's all part of the open
CV video
setup and me just put this all in as one
there we go
uh so we're going to go ahead and open
up our video we're going to start it and
we're going to run it until we're
done and this is where we get some real
like kind of liveaction stuff which is
fun this is what I like working about
with images and videos is that when you
start working with images and videos
it's all like right there in front of
you it's Visual and you can see what's
going on uh so we're going to start our
video streaming this is grabbing our
video stream Source zero
start uh that means it's c grabbing my
main camera I have hooked up um and then
you know starting video You're GNA print
it out here's our video Source equals
zero start Loop over the frames from the
video
stream oops a little redundancy there um
let me
close I'll just leave it that's how they
had it in the code so uh so while true
we're going to grab the frame from the
threaded video stream and resize it to
have the maximum width of 400 pixels so
here's our frame we're going to read it
uh from our visual uh stream we're going
to resize
it and then we have a returning remember
we returned from the our procedure the
location and the prediction so detect
and predict mask we're sending it the
frame we're sending it the face net and
the mass net so we're sending all the
different pieces that say this is what's
going through on
here and then it returns our location
and predictions and then for our box and
predictions in the location and
predictions um and the box is is again
this is an open CV set that says hey
this is a box coming in from the
location um because you have the two
different points on there and then we're
going to unpack the box and predictions
and we're going to go ahead and do mask
without a mask equals
prediction we're going to create our
label no mask we create color if the
label equals mask l00 225 and you know
this is GNA make a lot more sense when I
hit the Run button here uh but we have
the probability of the label we're going
to display the label and bounding box
rectangle on the output
frame uh and then we're going to go
ahead and show the output from the frame
CV2 IM show frame frame and then the key
equals CV2 wait key1 we're just going to
wait till the next one comes through
from our
feed and we're going to do this until we
hit the stop button pretty much so are
you ready for this let's see if it works
we've distributed our um our model we've
loaded it up into our distributed uh
code here we've got it hooked into our
camera and we're going to go ahead and
run
it and there it goes it's going to be
running and we can see the data coming
down here and we're waiting for the
popup and there I am in my office with
my funky headset
on uh and you can see in the background
my unfinished wall and it says up here
no mask oh no I don't have a mask on uh
I wonder if I cover my mouth what would
happen uh you can see my no
mask goes down a little bit I wish I'd
brought a mask into my office it's up at
the house but you can see here that this
says you know there's a 95 98% chance
that I don't have a mask on and it's
true I don't have a mask on right now
and this could be distributed this is
actually an excellent little piece of
script that you could start you know you
install somewhere on a a video feed on a
on a security camera or something and
then you'd have this really neat uh
setup saying heyy do you have a mask on
when you enter a store or a public
transportation or whatever it is where
they're required to wear a mask uh let
me go aad and stop
that now if you want a copy of this uh
code definitely give us a hauler we will
be going into open CV in another one so
I skipped a lot of the open CV um code
in here as far as going into detail
really focusing on the carass uh saving
the model uploading the model and then
processing a streaming video through it
so you can see that the model works we
actually have this working model that
hooks into the video
camera which is just pretty cool and a
lot of
fun so I told you we're going to dive in
and really Roll Up Our Sleeve and do a
lot of coating today uh we did the basic
uh demo up above for just pulling in a
carass and then we went into a cross
model uh where we pulled in data to see
whether someone was wearing a mask or
not so very useful in today's world as
far as a fully running application then
accelerate your career in Ai and ml with
a comprehensive post-graduate program in
artificial intelligence and machine
learning so enroll now and unlock
exciting Ai and machine learning
opportunities the link is mentioned in
the description box below so what are
generative adversarial
networks generative adversarial networks
or Gans introduced in 2014 by yanj good
fellow and co-authors became very
popular in the field of machine learning
Gan is an unsupervised learning task in
machine learning it consists of two
models that automatically discover and
learn the patterns in input data the two
models called generator and
discriminator compete with each other to
analyze capture and copy the variations
within a data set Gans can be used to
generate new examples that possibly
could have been drawn from the original
data
set in the image below you can see that
there is a database that has real 100
rupe notes
the generator which is basically a
neural network generates fake 100 rupees
notes the discriminator network will
identify if the notes are real or fake
let us now understand in brief about
what is a generator a generator in Gans
is a neural network that creates fake
data to be trained on the discriminator
it learns to generate plausible data the
generated instances become negative
training examples for the discriminator
it takes a fixed length random Vector
carrying noise as input and generates a
sample now the main aim of the generator
is to make the discriminator classify
its output as real the portion of the
Gan that trains a generator
includes a noisy input Vector the
generator Network which transforms the
random input into a data instance a
discriminator network which classifies
the generator data and a generator loss
which penalizes the generator for
failing to do the discriminator the back
propagation method is used to adjust
each weight in the right direction by
calculating the weight's impact on the
output the back propagation method is
used to obtain gradients and these
gradients can help change the generator
weights now let us understand in brief
what a discriminator is a discriminator
is a neural network model that
identifies real data from the fake data
generated by the generator the
discriminator training data comes from
two
sources the real data instances such as
real pictures of birds humans currency
noes Etc are used by the discriminator
is positive samples during the training
the fake data instances created by the
generator are used as negative examples
during the training process while
training the discriminator it connects
with two loss functions during
discriminator training the discriminator
ignores the generator loss and just uses
the discriminator LW in the process of
training the discriminator the
discriminator classifies both real data
and fake data from the generator the
discriminator law penalizes the
discriminator from misclassifying in a
real data instance as fake or a fake
data instance as real now moving ahead
let's understand how Gans work now Gans
consists of two networks a generator
which is represented as G of X and A
discriminator which is represented as D
ofx they both play an adversarial game
where the generator tries to fool the
discriminator by generating data similar
to those in the training set the
discriminator tries not to be fooled by
identifying fake data from the real data
they both work simultaneously to learn
and train complex data like audio video
or image files now you are aware that
Gans consists of two networks a
generator G ofx and discriminator D ofx
now the generator Network takes a sample
and generates a fake sample of data the
generator is trained to increase the
probability of the discriminator network
to make mistakes on the other hand the
discriminator network decides whether
the data is generated or taken from the
real sample using a binary
classification problem with the help of
a sigmoid function function that gives
the output in the range 0o and 1 here is
an example of a generative adversarial
Network trying to identify if the 100
rupe notes are real or fake so first a
noise vector or the input Vector is fed
to the generator Network the generator
creates fake 100 rupe notes the real
images of 100 rupe notes stored in a
database are passed to the discriminator
along with the fake noes the
discriminator then identifies the notes
and classifies them as real or fake we
train the model calculate the loss
function at the end of the discriminator
network and back propagate the loss into
both discriminator and Generator now the
mathematical equation of training again
can be represented as you can see here
now this is the
equation and these are the parameters
here G represents generator D represents
the discriminator now P dat of X is the
probability distribution of real data P
of Z is the distribution of Generator X
is the sample of probability data of X
Zed is the sample size from P of z d of
X is the discriminator network and G of
Z is the generator Network now the
discriminator focuses to maximize the
objective function such that D of X is
close to 1 and Z of Z is close to zero
it simply means that the discriminator
should identify all the images from the
training set as real that is one and all
the generated images as fake that is
zero the generator wants to minimize the
objective function such that D of Z of Z
is 1 this means that the generator tries
to generate images that are classified
as real that is one by the discriminator
network next let's see the steps for
training a neural network so we have to
first Define the problem and collect the
data then we'll choose the architecture
of Gan now depending on your problem
choose how your Gan should look like
then we need to train the discriminator
in real data that will help us predict
them as real for n number of times next
you need to generate fake inputs for the
generator after that you need to train
the discriminator on fake data to
predict the generator data is fake
finally train the generator on the
output of
discriminator with the discriminator
predictions available train the
generator to fool the discriminator let
us now look at the different types of
Gans so first we have vanilla Gans now
vanilla Gans have minmax op ization
formula that we saw earlier where the
discriminator is a binary classifier and
is using sigmoid cross entropy loss
during
optimization in vanilla Gans the
generator and the discriminator are
simple multi-layer percep troms the
algorithm tries to optimize the
mathematical equation using stochastic
gradient descent up next we have deep
convolutional Gans or DC Gans now DC
Gans support convolutional neural
networks instead of vanilla neural
networks at both discriminator and
Generator they are more stable and
generate higher quality images the
generator is a set of convolutional
layers with fractional strided
convolutions or transpose convolutions
so it unsampled the input image at every
convolutional layer the discriminator is
a set of convolutional layers with
strided convolutions so it down samples
the input image at every convolutional
layer moving ahead the third type you
have is conditional Gans or C Gans
vanilla Gans can be extended into
conditional models by using an extra
label information to generate better
results in cgan an additional parameter
called Y is added to the generator for
generating the corresponding data labels
of fed as input to the discriminator to
help distinguish the real data from fake
data generated finally we have super
resolution Gans now Sr Gans use deep
neural networks along with adversarial
neural network to produce higher
resolution images super resolution Gans
generate a photo realistic high
resolution image when given a low
resolution image let's look at some of
the important applications of
Gans so with the help of DC Gans you can
train images of cartoon characters for
generating faces of Anime characters and
Pokmon characters as well next Gans can
be used on the images of humans to
generate realistic faces the faces that
you see on your screens have been
generated using Gans and do not exist in
reality third application we have is
Gans can be used to build realistic
images from textual descriptions of
objects like birds humans and other
animals we input a sentence and generate
multiple images fitting the description
here is an example of a text to image
translation using Gans for a bird with a
black head yellow body and a s beak the
final application we have is creating 3D
objects so Gans can generate 3D models
using 2D pictures of objects from
multiple perspectives Gans are very
popular in the gaming industry
Gans can help automate the task of
creating 3D characters and backgrounds
to give them a realistic feel do you
know how deep learning recognizes the
objects in an image and really this
particular neural network is how image
recognition works it's very Central one
of the biggest building blocks for image
recognition it does it using convolution
neural network and we over here we have
the basic picture of a uh hummingbird
pixels of an image fed as input you have
your input layer coming in so it takes
that graphic and put it into the input
layer you have all your hidden layers
and then you have your output layer and
your output layer one of those is going
to light up and say oh it's a bird we're
going to go into depth we're going to
actually go back and forth on this a
number of times today so if you're not
catching all the image um don't worry
we're going to get into the details so
we have our input layer accepts the
pixels of the image as input in the form
of arrays and you can see up here where
they've actually um labeled each block
of the bird in different arrays so we'll
dive into deep as to how that looks like
and how those matrixes are set up your
hidden layer carry out feature
extraction by performing certain
calculations and manipulation so this is
the part that kind of reorganizes that
picture multiple ways until we get some
data that's easy to read for the neural
network this layer uses a matrix filter
and performs convolution operation to
detect patterns in the image and if you
remember that convolution means to coil
or to twist so we're going to twist the
data around and alter it and use that
operation to to dedi a new pattern there
are multiple hidden layers like
convolution layer real U is how that is
pronounced and that's the rectified
linear unit that has to do with the
activation function that's used pooling
layer also uses multiple filters to
detect edges corners eyes feathers beak
Etc and just like the term says pulling
is pulling information together and
we'll look into that a lot closer here
so if you're if it's a little confusing
now we'll dig in deep and try to get to
uh Square weared away with that and then
finally there is a fully connected layer
that identifies the object in the image
so we have these different layers coming
through in the hidden layers and they
come into the final area and that's
where we have say one node or one neural
network entity that lights up that says
it's a bird what's in it for you we're
going to cover an introduction to the
CNN what is convolution neural network
how CNN recognizes images we're going to
dig deeper into that and really look at
the individual layers in the
convolutional neural network and finally
we do a use case implementation using
the CNN we'll begin our introduction to
the CNN by introducing the pioneer of
convolutional neural network Yan leun he
was the director of Facebook AI research
group built the first convolutional
neural network called lenette in
1988 so these have been around for a
while and have had a chance to mature
over the years it was used for character
recognition tasks like reading zip code
digits imagine processing mail and
automating that process CN N is a feed
forward neural network that is generally
used to analyze visual images by
producing data with a grid-like topology
a CNN is also known as a convet and very
key to this is we are looking at images
that was what this was designed for and
you'll see the different layers as we
dig in Mirror some of the other some of
them are actually now used since we're
using uh tensorflow and carass in our
code later on you'll see that some of
those layers appear in a lot of your
other neural network Frameworks uh but
in this case this is very Central to
processing images and doing so in a
variety that captures multiple images
and really drills down into their
different features in this example here
you see flowers of two varieties Orchid
and a rose I think the Orchid is much
more dainty and beautiful and the rose
smells quite beautiful I have a couple
rose bushes in my yard uh they go into
the input layer that data is then sent
to all the different nodes and the next
layer one of the Hidden layers based on
its different weights and its setup it
then comes out and gives those a new
value those values then are multiplied
by their weights and go to the next
hidden layer and so on and then you have
the output layer and one of those notes
comes out and says it's an orchid and
the other one comes out and says it's a
rose depending on how was well it was
trained what separates the CNN or the
convolutional neural network from other
neural networks is a convolutional
operation forms the basis of any
convolutional neural network in a CNN
every image is represented in the form
of arrays of pixel values so here we
have a real image of the digit 8 uh that
then gets put onto its pixel values
represented in the form of an array in
this case you have a two-dimensional
array and then you can see in the final
in form we transform the digit 8 into
its representational form of pixels of
zeros and one where the ones represent
in this case the black part of the eight
and the zeros represent the white
background to understand the convolution
neural network or how that convolutional
operation Works we're going to take a
side step and look at matrixes in this
case we're going to simplify it we're
going to take two matrices A and B of
one dimension now kind of separate this
from your thinking as we learn that you
want to focus just on the Matrix aspect
of this and then we'll bring that back
together and see what that looks like
when we put the pieces for the
convolutional operation here we've set
up two arrays we have uh in this case
are a single Dimension Matrix and we
have a equals 5
37597 and we have b equal 23 so in the
convolution as it comes in there it's
going to look at these two and we're
going to start by doing multiplying them
a * B and so we multiply the arrays
elementwise and we get 5 6 6 where five
is the 5 * 1 6 is 3 * 2 and then the
other six is 2 * 3 and since the two
arrays aren't the same size they're not
the same setup we're going to just
truncate the first one and we're going
to look at the second array multiplied
just by the first three elements of the
first array now that's going to be a
little confusing remember a computer
gets to repeat these processes hundreds
of times so we're not going to just
forget those other numbers later on
we'll see we'll bring those back in and
then we have the sum of the product in
this case 5 + 6 + 6 = 17 so in our a * B
our very first digit in that Matrix of a
* B is 17 and if you remember I said
we're not going to forget the other
digits so we now have 325 we move one
set over and we take 325 and we multiply
that times B and you'll see that 3 * 1
is 3 2 * 2 is 4 and so on and so on WE
sum it up so now we have the second
digit of our a * B product in The Matrix
and we continue on with that same thing
so on and so on so then we would go from
uh 375 to 759 to 597 this short Matrix
that we have for a we've now covered all
the different entities in a that match
three different levels of B now in a
little bit we're going to cover where we
use this math at this multiplying of
matrixes and how that works uh but it's
important to understand that we're going
through the Matrix of multiplying the
different parts to it to match the
smaller Matrix with the larger Matrix I
know a lot of people get lost at is you
know what's going on here with these
matrixes uh H scary math not really that
scary when you break it down we're
looking at a section of a and we're
comparing it to B so when you break that
down your mind like that you realize
okay so I'm I'm just taking these two
matrixes and comparing them and I'm
bringing the value down into one Matrix
a * B we're producing that information
in a way that will help the computer see
different aspects let's go ahead and
flip over again back to our images here
we are back to our images talking about
going to the most basic two-dimensional
image you can get to consider the
following two images the image for the
symbol backslash when you press the
backslash the above image is processed
and you can see there for the image for
the forward slash is the opposite so
when we click the the forward slash
button that flips uh very basic we have
four pixels going in can't get any more
basic than that here we have a little
bit more complicated picture we take a
real image of a smiley face um then we
represent that in the form of black and
white pixels so if this was an image in
the computer it's black and white and
like we saw before we convert this into
the zeros and on so where the other one
would have just been a matrix of just
four dots now we have a significantly
larger image coming in so don't worry
we're going to bring this all the
together here in just a little bit
layers in convolutional neural network
when we're looking at this we have our
convolution layer and that really is the
central aspect of processing images in
the convolutional neural network that's
why we have it and then that's going to
be feeding in and you have your reu
layer which is you know as we talked
about the rectified linear unit we'll
talk about that a little bit layer the
reu isn't how it Act is how that layer
is activated is the math behind it what
makes the neuron fire you'll see that in
a lot of other neural networks when
you're using it just by itself it's for
processing smaller amounts of data where
you use the atom activation feature for
large data coming in now because we're
processing small amounts of data in each
image the reu layer works great you have
your pooling layer that's where you're
pulling the data together pooling is a
neural network term it's very commonly
used I like to use the term reduce so if
you're coming from the map and reduce
side you'll see that we're mapping all
this data through all these networks and
then we're going to reduce it we're
going to pull it together and then
finally we have the fully connected
layer that's where our output's going to
come out so we have started to look at
matrixes we've started to look at the
convolutional layer and where it fits in
and everything we've taken a look at
images so we're going to focus more on
the convolution layer since this is a
convolutional neural network a
convolution layer has a number of
filters and perform convolution
operation every image is considered as a
matrix of pixel values consider the
following 5x5 image whose pixel values
are only zero and one now obviously when
we're dealing with color there's all
kinds of things that come in on color
processing but we want to keep it simple
and just keep it black and white and so
we have our image pixels uh so we're
sliding the filter Matrix over the image
and Computing the dot product to detect
the patterns and right here you're going
to ask where does this filter come from
this is a bit confusing because the
filter is going to be derived uh later
on we build the filters when we program
or train our model so you don't need to
worry what the filter actually is but
you do need to understand how a
convolution layer works is what is a
filter doing filter and you'll have many
filters you don't have just one filter
you'll have lots of filters that are
going to look for different aspects and
so the filter might be looking for just
edges it might be looking for different
parts we'll cover that a little bit more
detail in a minute right now we're just
focusing on how the filter works as a
matrix remember earlier we talked about
multiplying matrixes together and here
we have our two-dimensional Matrix and
you can see we take the filter and we
multiply it in the upper left image and
you can see right here 1 * 1 1 * 0 1 * 1
we multiply those all together then sum
them and we end up with a convolved
feature of four we're going to take that
and sliding the filter Matrix over the
image and Computing the dot product to
detect patterns so we're just going to
slide this over we're going to predict
the first one and slide it over one
notch predict the second one and so on
and so on all the way through until we
have a new Matrix and this Matrix which
is the same size as a filter has reduced
the image and whatever filter whatever
that's filtering out it's going to be
looking at just those features reduced
down to a smaller uh Matrix so once the
feature maps are extracted the next step
is to move them to the reu layer so the
ra you layer The Next Step first is
going to perform an element wise
operation so each of those Maps coming
in if there's negative pixels so it sets
all the negative pixels to zero um and
you can see this nice graph where it
just zeros out the negatives and then
you have a value that goes from zero up
to whatever value is um coming out of
the Matrix this introduces nonlinearity
to the network uh so up until now we
have a we say linearity we're talking
about the fact that the feature has a
value so it's a linear feature this
feature um came up and has let's say the
feature is the edge of the beak you know
it's like or the backslash that we saw
um you'll look at that and say okay this
feature has a value from -10 to 10 in
this case um if it was one and say yeah
this might be a beak it might not might
be an edge right there a minus five
means no we're not even going to look at
it a zero and so we end up with an
output and the output takes all these
feature all these filtered features
remember we're not just running one
filter on this we're running a number of
filters on this image and so we end up
with an rectified feature map that is
looking at just the features coming
through and how they weigh in from our
filters so here we have an input of a
looks like a twocan
bird very exotic looking real image is
scanned in multiple convolution and the
relu layers for locating features and
you can see up here is turned it into a
black and white image and in this case
we're looking in the upper right hand
corner for a feature and that box scans
over a lot of times it doesn't scan one
pixel at a time a lot of times it will
Skip by two or three or four pixels uh
to speed up the process that's one of
the ways you can compensate if you don't
have enough resources on your
computation for large images and it's
not just one filter slowly goes across
the image uh you have multiple filters
that been programmed in there so you're
looking at a lot of different filters
going over the different aspects of the
image and just sliding across there and
forming a new Matrix one more aspect to
note about the reu layer is we're not
just having one reu coming in uh so not
not only do we have multiple features
going through but we're generating
multiple relu layers for locating the
features that's very important to note
you know so we have a quite a bundle we
have multiple filters multiple railu uh
which brings us to the next step forward
propagation now we're going to look at
the pooling layer the rectified feature
map now goes through a pooling layer
pooling is a down sampling operation
that reduces the dimensionality of the
feature map that's all we're trying to
do we're trying to take a huge amount of
information and reduce down to a single
answer this is a specific kind of bird
this is an iris this is a rose so you
have a rectified feature map and you see
here we have a rectified feature map
coming in um we set the max pooling with
a 2X two filters and a stride of Two And
if you remember correctly I talked about
not going one pixel at a time uh well
that's where the stride comes in we end
up with a 2X two pulled feature map but
instead of moving one over each time and
looking at every possible combination we
skip a we skip a few there we go by two
we skip every other pixel and we just do
every other one um and this reduces our
rectified feature map which as you can
see over here 16x 16 to a 4x4 so we're
continually trying to filter and reduce
our data so that we can get to something
we can manage and over here you see that
we have the Max uh 3 4 1 and two and in
the max pooling we're looking for the
max value a little bit different than
what we were looking at before so coming
from the rectified feature we're now
finding the max value and then we're
pulling those features together so
instead of think of this as image of the
map think of this as how valuable is a
feature in that area how much of a
feature value do we have and we just
want to find the best or the maximum
feature for that area they might have
that one piece of the filter of the beak
said oh I see a one in this beak in this
image and then it skips over and says I
see a three in this image and says oh
this one is rated as a four we don't
want to sum it together because then you
know you might have like five ones and
I'll say ah five but you might have uh
four zeros and one 10 and that 10 says
well this is definitely a beak where the
ones will say probably not a beak a
little strange analogy since we're
looking at a bird but you can see how
that pulled feature map comes down and
we're just looking for the max value in
each one of those matrixes pooling layer
uses different filters to identify
different parts of the image like edges
corners body feathers eyes beak Etc um I
know I focus mainly on the beak but
obviously each feature could be each a
different part of the bird coming in so
let's take a look at what that looks
like structure of a convolution neural
network so far this is where we're at
right now we have our input image coming
in and then we use our filters and
there's multiple filters on there that
are being developed to kind of twist and
change that data and so we multiply the
matrixes we take that little filter
maybe it's a 2 by two we multiply it by
each piece of the image and if we step
two then it's every other piece of the
image that generates multiple
convolution layers so we have a number
of convolution layers we have um set up
in there just looking at that data we
then take those convolution layers we
run them through the reu setup and then
once we've done through the reu setup
and we have multiple reu going on
multiple layers that are relu then we're
going to take those multiple layers and
we're going to be pooling them so now we
have the pooling layers or multiple
poolings going on up until this point
we're dealing with uh sometimes it's
multiple Dimensions you can have three
dimensions some strange data setups that
aren't doing images but looking at other
things they can have four five six seven
dimensions uh so right now we're looking
at 2D image Dimensions coming in into
the pooling layer so the next step is we
want to reduce those Dimensions or
flatten them so flattening flattening is
a process of converting all of the
resultant two-dimensional arrays from
pulled feature map into a single long
continuous linear Vector so over here
you see where we have a pulled feature
map maybe that's the Birdwing and it has
value
6847 and we want to just flatten this
out and turn it into 6847 or a single
linear vector and we find out that not
only do we do each of the pulled feature
Maps we do all of them into one long
linear Vector so now we've gone through
our convolutional neural network part
and we have the input layer into the
next setup all we've done is taken all
those different pooling layers and we
flatten them out and combine them into a
single linear Vector going in so after
we've done the flattening we have H just
a quick recap because we've covered so
much so it's important to go back and
take a look at each of the steps we've
gone through the structure of the
network so far is we have our
convolution where we twist it and we
filter it and multiply the matrixes we
end up with our convolutional layer
which uses the relu to figure out the
values going out into the pooling and so
you have numerous convolution layers
that then create numerous pooling layers
pulling that data together which is the
max value which one we want to send
forward we want to send the best value
value and then we're going to take all
of that from each of the pooling layers
and we're going to flatten it and we're
going to combine them into a single
input going into the final layer once
you get to that step you might be
looking at that going boy that looks
like the normal inut to most neural
network and you're correct it is so once
we have the flattened Matrix from the
pooling layer that becomes our input so
the pooling layer is fed as an input to
the fully connected layer to classify
the image and so you can see as our
flattened Matrix comes in in this case
we have the pixels from the flattened
Matrix fed as an input back to our
twocan or whatever that kind of bird
that is um I need one of these to
identify what kind of bird that is it
comes into our forward propagation
network uh and that will then have the
different weights coming down across and
then finally it selects that that's a
bird and that it's not a dog or a cat in
this case even though it's not labeled
the final layer there in red is our
output layer our final output layer that
says bird cat or dog so so quick recap
of everything we've covered so far we
have our input image which is twisted
and multiply the filters are multiplied
times the uh matri the two matrixes
multiplied all the filters to create our
convolution layer our convolution layers
there's multiple layers in there because
it's all building multiple layers off
the different filters then goes through
the reu as it say activation and that
creates our pooling and so once we get
into the pooling layer we then in the
pooling look for who's the best what's
the max value coming in for from our
convolution and then we take that layer
and we flatten it and then it goes into
a fully connected layer our fully
connected neural network and then to the
output and here we can see the entire
process how the CNN recognizes a bird
this is kind of nice cuz it's showing
the little pixels and where they're
going you can see the filter is
generating this convolution network and
that filter shows up in the bottom part
of the convolution network and then
based on that it uses the relo for the
pooling the pooling then find out which
one's the best and so on all the way to
the fully connected layer at the end or
the classification in the output layer
so that'd be a classification neural
network at the end so we covered a lot
of theory up till now and you can
imagine each one of these steps has to
be broken down in code so putting that
together can be a little complicated not
that each step of the process is overly
complicated but because we have so many
steps uh we have 1 2 3 4 five different
steps going on here with substeps in
there we're going to break that down and
walk through that in code so in our use
case implementation using the CNN we'll
be using the cfr1 data set from Canadian
Institute for advanced research for
classifying images across 10 categories
Unfortunately they don't let me know
whether there going to be a twocan or
some other kind of bird but we do get to
find out whether it can categorize
between a ship a frog deer bird airplane
automobile cat dog horse truck so that's
a lot of fun and if you're looking
anything in the news at all of our
automated cars and everything else you
can see where this kind of processing is
so important in today's world and very
Cutting Edge as far as what's coming out
in the commercial deployment I mean this
is really cool stuff we're starting to
see this just about everywhere in
Industry uh so great time to be playing
with this and figuring it all out let's
go ahead and dive into the code and see
what that looks like when we're actually
writing our script before we go on let's
do uh one more quick look at what we
have here let's just take a look at data
batch one keys and remember in Jupiter
notebook I can get by with not doing the
print statement if I put a variable down
there it'll just display the variable
and you can see under data batch one for
the keys since this is a dictionary we
have the batch one label data and file
names uh so you can actually see how
it's broken up in our data set so for
the next step or step four as we're
calling it uh we want to display the
image using Matt plot Library there's
many ways to display the images you
could even uh well there's other ways to
drill into it but map plot library is
really good for this and we'll also look
at our first reshape uh setup or shaping
the data so you can have a little
glimpse into what that means uh so we're
going to start by importing our map plot
and of course since I am doing jupyter
notebook I need to do the map plot
inline command so it shows up on my page
so here we go we're going to import
matplot library. pyplot is PLT and if
you remember map plot Library the P plot
is like a canvas that we paint stuff
onto and there's my percentage sign map
plot library in line so it's going to
show up in my notebook and then of
course we're going to import numpy as NP
for our numbers python array setup and
let's go ahead and set um x equals to
data batch one so this will pull in all
the data going into the x value and then
because this is just a long stream of
binary data uh we need to go a little
bit of reshaping so in here we have to
go ahead and reshape the data we have
10,000 images okay that looks correct
and this is kind of an interesting thing
it took me a little bit to I had to go
research this myself to figure out
what's going on with this data and what
it is is it's a 32x 32 picture and let
me do this let me go ahead and do a
drawing pad on here uh so we have 32
bits by 32 bits and it's in color so
there's three bits of color now I don't
know why the data is particularly like
this it probably has to do with how they
originally encoded it but most pictures
put the three afterward so what we're
doing here is we're going to take uh the
shape we're going to take the data which
is just a long stream of information and
we're going to break it up into 10,000
pieces and those 10,000 pieces then are
broken into three pieces each and those
three pieces then are 32x 32 you could
look at this like an oldfashioned
projector where they have the red screen
or the red projector the blue projector
and the green projector and they add
them all together and each one of those
is a 32x32 bit so that's probably how
this was originally formatted within
that kind of Ideal things have changed
so we're going to transpose it and we're
going to take the three which was here
and we're going to put it at the end so
the first part is reshaping the data
from a single line of bit data or
whatever format it is into 10,000 by 3x
32x 32 and then we're going to transpose
the color factor to the last place so
it's the image then the 32x 32 in the
middle that's this part right here and
then find finally we're going to take
this uh which is three bits of data and
put it at the end so it's more like we
do we process images now and then as
type this is really important that we're
going to use an integer 8 you can come
in here and you'll see a lot of these
they'll try to do this with a float or
float 64 what you got to remember though
is a float uses a lot of memory so once
you switch this into uh something that's
not integer 8 which is goes up to 128
you are just going to the the amount of
ram let just put that in here is going
to go way up the amount of ram that it
loads uh so you want to go ahead and use
this you can try the other ones and see
what happens if you have a lot of RAM on
your computer but for this exercise this
will work just fine and let's go ahead
and take that and run this so now our X
variable is all loaded and it has all
the images in it from the batch one data
batch one and just to show what we were
talking about with the as type on there
if we go ahead and take x0 and just look
for its max value let me go ahead and
run that uh you'll see it doesn't oops I
said 128 it's 255 uh you'll see it
doesn't go over 255 because it's an
basically an asky character is what
we're keeping that down to we're keeping
those values down so they're only 255 0
to 255 versus a float value which would
bring this up um exponentially in size
and since we're using the map plot
Library we can do um oops that's not
what I wanted since we're using the map
plot Library we can take our canvas and
just do a PLT do IM for image show and
let's just take a look at what x0 looks
like and it comes in I'm not sure what
that is but you can see it's a very low
grade image uh broken down to the
minimal pixels on there and if we did
the same thing oh let's do uh let's see
what one looks like hopefully it's a
little easier to see run on there not
enter let's hit the run on that uh and
we can see this is probably a semi
that's a good guess on there and I just
go back up here instead of typing the
same line in over and over and we'll
look at three uh that looks like a dump
truck unloading uh and so on you can do
any of the 10,000 images we can just
jump to 55 uh looks like some kind of
animal looking at us there probably a
dog and just for fun let's do just one
more uh uh run on there and we can see a
nice car for image number four uh so you
can see we past through all the
different images it's very easy to look
at them and they've been reshaped to fit
our view and what the uh matap plot
Library uses for its format so the next
step is we're going to start creating
some helper functions we'll start by a
one hot incoder to help us we're
processing the data remember that your
labels they can't just be words they
have to switch it and we use the one hot
incoder to do that and then we'll also
create a uh class uh CFR helper so it's
going to have a knit and a setup for the
images and then finally we'll go ahead
and run that code so you can see what
that looks like and then we get into the
fun part where we're actually going to
start creating our model our actual
neural network model so let's start by
creating our one hot encoder we're going
to create our own here uh and it's going
to return and out and we'll have our
Vector coming in and our values equal 10
what this means is that we have the 10
values the 10 possible labels and
remember we don't look at the labels as
a number cuz a car isn't one more than a
horse that'd be just kind of bizarre to
have horse equals z car equals 1 plane
equals 2 cat equals three so a cat plus
a C equals what uh so instead we create
a numpy array of zeros and there's going
to be 10 values so we have 10 different
values in there so you have uh zero or
one one means it's a cat zero means it's
not a cat um in the next line it might
be that uh one means it's a car zero
means it's not a car so instead of
having one output with a value of 0 to
10 you have 10 outputs with the values
of 0 to one that's what the one hot
encoder is doing here and we're going to
utilize this en code in just a minute so
let's go ahead and take a look at the
next helpers we have a few of these
helper functions we're going to build
and when you're working with a very
complicated python project dividing it
up into separate definitions and classes
is very important otherwise it just
becomes really ungainly to work with so
let's go ahead and put in our next
helper uh which is a class and this is's
a lot in this class so we we'll break it
down here let's just start UHS we put a
space right in there there we go that
this a little bit more readable add a
second space so we're going to create
our class the cipher Helper and we'll
start by initializing it now there's a
lot going on in here so let's start with
the uh nit part uh self. I equals z
that'll come in in a little bit we'll
come back to that in the lower part we
want to initialize our training batches
so when we went through this there was
like a meta batch we don't need the
metab batch but we do need the data
batch 1 2 3 4 5 and we do not want the
testing batch in here this is just the
self all train batches so we're going to
come make an array of of all those
different images and then of course we
left the test batch out so we have our
self. test batch uh we're going to
initialize the training images and the
training labels and also the test images
and the test labels so these are just
this is just to initialize these
variables in here then we create another
definition down here and this is going
to set up the images let's just take a
look and see what's going going on in
there now we could have all just put
this as part of the uh init part uh
since this is all just helper stuff but
breaking it up again makes it easier to
read it also makes it easier when we
start executing the different pieces to
see what's going on so that way we have
a nice print statement to say hey we're
now running this and this is what's
going on in here we're going to set up
these self-training images at this point
and that's going to go to a numpy array
vstack and in there we're going to load
up uh in this case the data for D and s
all trained batches again that points
right up to here so we're going to go
through each one of these uh files or
each one of these data sets they're not
a file anymore we've brought them in
data batch one points to the actual data
and so our self-training images is going
to stack them all into our into a numpy
array and then it's always nice to uh
get the training length and that's just
a total number of uh self-training
images in there and then we're going to
take the self trining images and let me
switch marker colors cuz I am getting a
little too much on the markers up here
oops there we go bring down our marker
change so we can see it a little better
and at this point this should look
familiar where did we see this well when
we wanted to uh uh look at this above
and we want to look at the images in the
matplot library we had to reshape it so
we're doing the same thing here we're
taking our self-training images and uh
based on the training length total
number of images because we stacked them
all together so now it's just one large
file of images we're going to take and
look at it as our our three video
cameras that are each displaying uh 32x
32 we're going to switch that around so
that now we have um each of our images
that stays the same place and then we
have our 32 by 32 and then by our three
our last our three different values for
the color and of course we want to go
ahead and uh they run this where you say
divide by 255 that was from earlier it
just brings all the data into 0 to one
that's what this is doing so we're
turning this into a
0:1 array which is uh all the pictures
32 by 32 by3 and then we're going to
take the self-training labels and we're
going to pump those through our one hot
encoder we just made and we're going to
stack them together and uh again we're
converting this into an array that goes
from uh instead of having horse equals 1
dog equals 2 and then horse plus dog
would equal three which would be cat no
it's going to be you know an array of 10
where each one is zero to one then we
want to go ahead and set up our test
images and labels and uh when we're
doing this you're going to see it's the
same thing we just did with the rest of
let me just change colors right here
this is no different than what we were
doing up here with our training Set uh
we're going to stack the different uh
images uh we're going to get the length
of them so we know how many images are
in there uh you certainly could add them
by hand but it's nice to let the
computer do it especially if it ever
changes on the other end and you're
using other data and again we reshape
them and transpose them and we also do
the one hot encoder same thing we just
did on our training images so now our
test images are in the same format so
now we have a definition which sets up
all our images in there and then the
next step is to go ahead and batch them
or next batch and let's do another
breakout here for batches because this
is really important to understand tends
to throw me for a little Loop when I'm
working with tensor flow or carass or a
lot of these we have our data coming and
if you remember we had like 10,000
photos let me just put 10,000 down here
we don't want to run all 10,000 at once
so we want to break this up into batch
sizes and you also remember that we had
the number of photos in this case uh
length of test or whatever numberers in
there uh we also have 32 by 32 by 3 so
when we're looking at the batch size we
want to change this from 10,000 to um a
batch of in this case I think we're
going to do batches of 100 so we want to
look at just 100 the first 100 of the
photos and if you remember we set selfi
equal to zero uh so what we're looking
at here is we're going to create X we're
going to get the next batch from the
very initialized we've already
initialized it for zero so we're going
to look at X from zero to batch size
which we've set to 100 so just the first
100 images and then we're going to
reshape that into uh and this is
important to let the data know that
we're looking at 100x 32x 32x 3 now
we've already formatted it to the 32x
32x 3 this just sets everything up
correctly so that X has the data in
there in the correct order and the
correct shape and then the Y just like
the X uh is our labels so our training
labels again they go from zero to batch
size in this case they do selfi plus
batch size because the selfi is going to
keep changing and then finally we
increment the selfi because we have zero
so we so the next time we call it we're
going to get the next batch size and so
basically we have X and Y X being the
photograph data coming in and Y being
the label and that of course is labeled
through one hot encoder so if you
remember correctly if it was say horse
is equal to zero it would be um one for
the zero position since this is the
horse and then everything else would be
zero in here me just put lines through
there there we go there's our array hard
to see that array so let's go ahead and
take that and uh we're going to finish
loading it since this is our class and
now we're armed with all this um uh our
setup over here let's go ahead and load
that up and so we're going to create a
variable CH with the cfar helper in it
and then we're going to do ch. setup
images uh now we could have just put all
the setup images under the init but by
breaking this up into two parts it makes
it much more readable and um also if
you're doing other work there's reasons
to do that as far as the setup let's go
ahead and run that and you can see where
it says uh setting up training images
and labels setting up test images and
that's one of the reasons we broke it up
is so that if you're testing this out
you can actually have print statements
in there telling you what's going on
which is really nice uh they did a good
job with this setup I like the way that
it was broken up in the back and then
one quick note you want to remember that
batch to set up the next batch say we
have to run uh batch equals CH next
batch of 100 because we're going to use
the 100 size uh but we'll come back to
that we're going to use that just
remember that that's part of our code
we're going to be using in a minute from
the definition we just made so now we're
ready to create our model first thing we
want to do is we want to import our
tensor flow as TF I'll just go ahead and
run that so it's loaded up and you can
see we got a a warning here uh that's
because they're making some changes it's
always growing and they're going to be
depreciating one of the uh values from
float 64 to float type or it's treated
as an NP float 64 uh nothing to really
worry about CU this doesn't even affect
what we're working on because we've set
all of our stuff to a 255 value or 0o to
one and do keep in mind that 0 to one
value that we converted to 255 is still
a float value uh but it'll easily work
with either the uh numpy float 64 or the
numpy dtype float it doesn't matter
which one it goes through so the
depreciation would not affect our code
as we have it and in our tensor flow uh
we'll go ahead let me just increase the
size in there just a moment so you can
get better view of the um what we're
typing in uh we're going to set a couple
placeholders here and so we have we're
going to set x equals TF placeholder TF
float 32 we just talked about the float
64 versus the numpy float we're actually
just going to keep this at float 32 more
than uh significant number of decimals
for what we're working with and since
it's a placeholder we're going to set
the shape equal to and we've set it
equal to none because at this point
we're just hold holding the place on
there we'll be setting up as we run the
batches that's what the first value is
and then 32x 32x 3 that's what we
reshaped our data to fit in and then we
have our one y true equals placeholder T
of float 32 and the shape equals none
comma 10 10 is the 10 different labels
we have so it's an array of 10 and then
let's create one more placeholder we'll
call this a hold prob or hold
probability and we're going to use this
we don't have to have a shape or
anything for this this placeholder is
for what we call Dropout if you remember
from our Theory before we drop out so
many nodes is looking at or the
different values going through which
helps decrease us so we need to go ahead
and put a a placeholder for that also
and we'll run this so it's all loaded up
in there so we have our three different
placeholders and since we're in tensor
flow when you use carass it does some of
this automatically but we're in tensor
flow direct carass sits on tensor flow
we're going to go ahead and create some
more helper functions we're going to
create something to help us initialize
the weights initialize our bias if you
remember that each uh layer has to have
a bias going in we're going to go ahead
and work on our our conversional 2D our
Mac pool so we have our pooling layer
our convolutional layer and then our
normal full layer so we're going to go
ahead and put those all into definitions
and let's see what that looks like in
code and you can also grab some of these
helper functions from the MN the uh nist
setup let me just put that in there if
you're under the tensor flow so a lot of
these are already in there but we're
going to go ahead and do our own and
we're going to create our uh a knit
weights and one of the reasons we're
doing this is so that you can actually
start thinking about what's going on in
the back end so even though there's ways
to do this with an auto ation sometimes
these have to be tweaked and you have to
put in your own setup in here uh now
we're not going to be doing that we're
just going to recreate them for our code
and let's take a look at this we have
our weights and so what comes in is
going to be the shape and what comes out
is going to be uh random numbers so
we're going to go ahead and just knit
some random numbers based on the shape
with a standard deviation of 0.1 kind of
a fun way to do that and then the TF
variable uh init random distribution so
we're just creating a random
distribution on there that's all that is
for the weights now you might change
that you might have a higher standard
deviation in some cases you actually
load preset weights that's pretty rare
usually you're testing that against
another model or something like that and
you want to see how those weights
configure with each other uh now
remember we have our bias so we need to
go ahead and initialize the bias with a
constant in this case we're using 0.1 a
lot of times the bias is just put in as
one and then you have your weights to
add on to that uh but we're going to set
this as 0.1 uh so we want to return a
convolutional 2d in this case a neural
network this is uh would be a layer on
here what's going on with the con 2D is
we're taking our data coming in uh we're
going to filter it strides if you
remember correctly strides came from
here's our image and then we only look
at this picture here and then maybe we
have a stride of one so we look at this
picture here we continue to look at the
different filters going on there the
other thing this does is that we have
our data coming in as
32 by 32 by 3 and we want to change this
so that it's just this is three
dimensions and it's going to reformat
this as just two Dimensions so it's
going to take this number here and
combine it with the 32x 32 so this is a
very important layer here CU it's
reducing our data down using different
means and it connects down I'm just
going to jump down one here uh it goes
with the convolutional layer so you have
your your kind of your preform
formatting and the setup and then you
have your actual convolution layer that
goes through on there and you can see
here we have a knit weights by the shape
a knit bias shape of three because we
have the three different uh here's our
three again and then we return the tfnn
reu with the convention 2D so this
convolutional uh has this feeding into
it right there it's using that as part
of it and of course the input is the XY
plus b the bias so that's quite a
mouthful but these two are the are the
keys here to creating the convolutional
layer is there the convolutional 2D
coming in and then the convolutional
layer which then steps through and
creates all those filters we saw then of
course we have our pooling uh so after
each time we run it through the
convectional layer we want to pull the
data uh if you remember correctly on the
on the pool side and let me just get rid
of all my marks it's getting a little
crazy there and in fact let's go ahead
and jump back to that slide let's just
take a look at that slide over here uh
so we have our image coming in we create
our convolutional layer with all the
filters remember the filters go you know
the filter is coming in here and it
looks at these four boxes and then if
it's a step let's say step two it then
goes to these four boxes and then the
next step and so on uh so we have our
convolutional layer that we generate or
convolutional layers they use the uh
relu function um there's other functions
out there for this though the reu is the
uh most the one that works the best at
least so far I'm sure that will change
then we have our pooling now if you
remember correctly the pooling was Max
uh so if we had the filter coming in and
they did the multiplication on there and
we have a one and maybe a two here and
another one here and a three here three
is the max and so out of all of these
you then create an array that would be
three and if the max is over here two or
whatever it is that's what goes into the
pooling of what's going on in our
pooling uh so again we're reducing that
data down we're reducing it down as
small as we can and then finally we're
going to flatten it out into a single
array and that goes into into our fully
connected layer and you can see that
here in the code right here we're going
to create our normal full layer um so at
some point we're going to take from our
pooling layer this will go into some
kind of flattening process and then that
will be fed into the full the different
layers going in down here um and so we
have our input size you'll see our input
layer get shape which is just going to
get the shape for whatever is coming in
uh and then input size initial weights
is also based on uh the input layer
coming in and the input size down here
is based on the input layer shape so
we're just going to already use the
shape and already have our size coming
in and of course uh you have to make
sure you init the bias always put your
bias on there and we'll do that based on
the size so this will return tf. matat
moo input layer w+b this is just a
normal full layer that's what this means
right down here that's what we're going
to return so that was a lot of steps we
went through let's go ahead and run that
so those are all loaded in there and
let's go ahead and and uh create the
layers let's see what that looks like
now that we've done all the heavy
lifting and everything uh we get to do
all the easy part let's go ahead and
create our layers we'll create a
convolution layer one and two two
different convolutional layers and then
we'll take that and we'll flatten that
out create a a reshape pooling in there
for our reshape and then we'll have our
full uh layer at the end so let's start
by creating our first uh convolutional
layer then we come in here and let me
just run that real quick and I want you
to notice on here the three and the 32
this is important because coming into
this convolutional layer we have three
different channels and 32 pixels each uh
so that has to be in there the four and
four you can play with this is your
filter size so if you remember you have
a filter and you have your image and the
filter slowly steps over and filters out
this image depending on what your step
is for this particular setup for four is
just fine that should work pretty good
for what we're doing and for the size of
the image and then of course at the end
once you have your convolutional layer
set up you also need to pull it and
you'll see that the pooling is
automatically set up so that it would
see the different shape based on what's
coming in so here we have Max Two by 2
by two and we put in the convolutional
one that we just created the
convolutional layer we just created goes
right back into it and that right up
here as you can see is the X it's coming
in from here so it knows to look at the
first first model and set the the data
accordingly set that up so it matches
and we went ahead and ran this already I
think I read let me go and run it again
and if we're going to do one layer let's
go ahead and do a second layer down here
and it's uh we'll call it convo 2 it's
also convolutional layer on this and
you'll see that we're feeding
convolutional one in the pooling so it
goes from convolutional one into
convolutional one pooling from
convolutional one pooling into
convolutional two and then from
convolutional two into convolutional 2
pooling and we'll go ahead and take this
and run this so these variables are all
loaded into memory and for our flatten
layer uh let's go ahead and we'll do uh
since we have 64 coming out of here and
we have a 4x4 going in let's do 8X 8 by
64 so let's do
4,096 this is going to be the flat layer
so that's how many bits are coming
through on the flat layer and we'll
reshape this so we'll reshape our uh
convo two pooling and that will feed
into here the convo two pooling and then
we're going to set it up as a single
layer that's 4,096 in size that's what
that means there we'll go ahead and run
this so we've now created this variable
the convo 2 flat and then we have our
first full layer this is the final uh
neural network where we have the flat
layer going in and we're going to again
use the uh reu for our uh setup on there
on a neural network for evaluation and
you'll notice that we're going to create
our first full layer our normal full
layer that's our definition so we
created that that's creating the normal
full layer and our input for the data
comes right here from the this goes
right into it uh the convo to flat so
this tells it how big the data is and
we're going to have it come out it's
going to have uh 1024 that's how big the
layer is coming out we'll go ahead and
run this so now we have our full layer
one and with the full layer one we want
to also Define the full one Dropout to
go with that so so our full layer one
comes in uh keep probability equals hold
probability remember we created that
earlier and the full layer one is what's
coming into it and this is going
backwards and training the data we're
not training every weight we're only
training a percentage of them each time
which helps get rid of the bias so let
me go ahead and run that and uh finally
we'll go ahead and create a y predict
which is going to equal the normal full
one Dropout and 10 cuz we have 10 labels
in there now in this neural network we
could have added additional layers that
would be another option to play with you
can also play with instead of 1024 you
can use other numbers for the way that
sets up and what's coming out and going
into the next one we're only going to do
just the one layer and the one layer
Dropout and you can see if we did
another layer it'd be really easy just
to feed in the full one Dropout into
full layer two and then full Layer Two
Dropout would have full Layer Two feed
into it and then you'd switch that here
for the Y prediction for right now this
is great this particular data set is
tried and true and we know know that
this will work on it and if we just type
in y predict and we run that uh we'll
see that this is a tensor object uh
shape question mark 10 dtype 32 a quick
way to double check what we're working
on so now we've got all of our uh we've
done a setup all the way to the Y
predict which we just did uh we want to
go ahead and apply the loss function and
make sure that's set up in there uh
create the optimizer and then uh train
our Optimizer and create a variable to
initialize all the global TF variables
so before we dive into the um loss
function let me point out one quick
thing or just kind of a rehap over a
couple things and that is when we're
playing with this these setups um we
pointed out up here we can change the 44
and use different numbers there they
change your outcome so depending on what
numbers you use here will have a huge
impact on how well your model fits and
that's the same here of the 1024 also
this is also another number that if you
continue to raise that number you'll get
um possibly a better fit you might
overfit and if you lower that number
you'll use less resources and generally
you want to use this in um the
exponential growth an exponential being
two 4 8 16 and in this case the next one
down would be 512 you can use any number
there but those would be the ideal
numbers uh when you look at this data so
the next step in all this is we need to
also create uh way of tracking how good
our model is and we're going to call
this a loss function and so we're going
to create a cross entropy loss function
and so before we discuss exactly what
that is let's take a look and see what
we're feeding it uh we're going to feed
it our labels and we have our true
labels and our prediction labels uh so
coming in here is where're the two
different uh variables we're sending in
or the two different probability
distributions is one that we know is
true and what we think it's going to be
now this function right here when they
talk about cross entropy uh in
information Theory the cross entropy
between two probability distributions
over the same underlying set of events
measures the average number of bits
needed to identify an event drawn from
the set that's a mouthful uh really
we're just looking at the amount of
error in here how many of these are
correct and how many of these um are
incorrect so how much of it matches and
we're going to look at that we're just
going to look at the average that's what
the mean the reduced to the mean means
here so we're looking at the average
error on this and so the next step is
we're going to take the error we want to
know our cross entropy or our loss
function how much loss we have that's
going to be part of how we train the
model so when you know what the loss is
and we're training it you feed that back
into the back propagation setup and so
we want to go ahead and optimize that
here's our Optimizer we're going to
create the optimizer using an atom
Optimizer remember there's a lot of
different ways of optimizing the data
atoms the most popular used uh so our
Optimizer is going to equal the TF TR TR
atom Optimizer if you don't remember
what the learning rate is let me just
pop this back into here here's our
learning rate when you have your weights
you have all your weights and your
different nodes that are coming out
here's our node coming out um and it has
all its weights and then the error is
being prop sent back through in reverse
on our neural network so we take this
error and we adjust these weights based
on the different formulas in this case
the atom formulas what we're using we
don't want to just adjust them
completely we don't want to change this
weight so it exactly fits the data
coming through because if we made that
kind of adjustment it's going to be
biased to whatever the last data we sent
through is instead we're going to
multiply that by 0.001 and make a very
small shift in this weight so our Delta
W is only 01 of the actual Delta W of
the full change we're going to compute
from the atom and then we want to go
ahead and train it so our training or
set up a training uh uh variable or
function and this is going to equal our
Optimizer minimize cross entropy and we
make sure we go ahead and run this so
it's loaded in there and then we're
almost ready to train our model but
before we do that we need to create one
more um variable in here and we're going
to create a variable to initialize all
the global TF variables and when we look
at this um the TF Global variable
initializer this is a tensor flow um
object it goes through there and it
looks at all our different setup that we
have going under our tensor flow and
then initializes those variables uh so
it's kind of like a magic wand because
it's all hidden in the back end of
tensor flow all you need to know about
this is that you have to have the
initialization on there which is an
operation um and you have to run that
once you have your setup going so we'll
go ahead and run this piece of code and
then we're going to go ahead and train
our data so let me run this so it's
loaded up there and so now we're going
to go ahead and run the model by
creating a graph session graph session
is a tensorflow term so you'll see that
coming up it's one of the things that
throws me because I always think of
graphic and Spark and graph as just
general graphing uh but they talk about
a graph session so we're going to goe
and run the model and let's go ahead and
walk through this uh what's going on
here and let's paste this data in here
and here we go so we're going to start
off with the with the TF session as cess
so that's our actual TF session we've
created uh so we're right here with the
TF uh session our session we're creating
we're going to run TF Global variable
initial ier so right off the bat we're
initializing our variables here uh and
then we have for I in range 500 so
what's going on here remember 500 we're
going to break the data up and we're
going to batch it in at 500 points each
we've created our session run so we're
going to do with TF session as session
right here we've created our variable
session uh and then we're going to run
and we're going to go ahead and
initialize it so we have our TF Global
variables initializer that we created um
that initializes our our session in here
the next thing we're going to do is
we're going to go for I in range of 500
batch equals ch. nextt batch so if you
remember correctly this is loading up um
100 pictures at a time and uh this is
going to Loop through that 500 times so
we are literally doing uh what is that
uh 500 time 100 is uh 50,000 so that's
50,000 pictures we're going to process
right there in the first process is
we're going to do a session run we're
going to take our train we created our
train variable or Optimizer in there
we're going to feed it the dictionary uh
we had our feed dictionary that created
and we have x equals batch zero coming
in y true batch one hold the probability
0.5 and then just so that we can keep
track of what's going on we're going to
every uh 100 steps we're going to run a
print So currently on step format C
accuracy is um and we're going to look
at matches equals tf. equal TF ARG
argument y prediction one tf. AR Max y
true comma 1 so we're going to look at
this as how many matches it has and here
our ACC uh all we're doing here is we're
going to take the matches how many
matches they have it creates it
generates a chart we're going to convert
that to float that's what the TF cast
does and then we just want to know the
average we just want to know the average
of the um accuracy and then we'll go
ahead and print that out uh print
session run accuracy feed dictionary so
it takes all this and prints out our
accuracy on there so let's go ahead and
take this oops FP screens there let's go
ahead and take this and let's run it and
this is going to take a little bit to
run uh so let's see what happens on my
old laptop and we'll see here that we
have our current uh we're currently on
Step Zero it takes a little bit to get
through the accuracy and this will take
just a moment to run we can see that on
our Step Zero it has an accuracy of1 or
0128 um and as it's running we'll go
ahead you don't need to watch it run all
the way but uh this accuracy is going to
change a little bit up and down so we've
actually lost some accuracy during our
step two um but we'll see how that comes
out let's come back after we run it all
the way through and see how the
different steps come out I was actually
reading that backwards uh the way this
works is the closer we get to one the
more accuracy we have uh so you can see
here we've gone from a 0.1 to a 39 um
and we'll go ahead and pause this and
come back and see what happens when
we're done with the full run all right
now that we've uh prepared the meal got
it in the oven and pulled out my
finished dish here if you've ever
watched uh any of the old cooking shows
let's discuss a little bit about this
accuracy going on here and how do you
interpret that we've done a couple
things first we've defined accuracy um
the reason I got it backwards before is
you have uh loss or accuracy and with
loss you'll get a graph that looks like
this it goes oops that's an S by the way
there we go you get a graph that curves
down like this and and with accuracy you
get a graph that curves up this is how
good it's doing now in this case uh one
is supposed to be really good accuracy
that mean it gets close to one but it
never crosses one so if you have an
accuracy of one that is phenomenal um in
fact that's pretty much un you know
unheard of and the same thing with loss
if you have a loss of zero that's also
unheard of the zero is actually on this
this axis right here as we go in there
so how do we interpret that because you
know if I was looking at this and I go
oh 0. 51 that's uh 51% you're doing
50/50 no this is not percentage let me
just put that in there it is not
percentage uh this is log rithmic what
that means is that 0. 2 is twice as good
as 0.1 and uh when we see 04 that's
twice as good as 0. 2 real way to
convert this into a percentage you
really can't say this is is a direct
percentage conversion what you can do
though is in your head if we work were
to give this a percentage uh we might
look at this as uh 50% we're just
guessing equals 0.1 and if 50% roughly
equals 0.1 that's where we started up
here at the top remember at the top here
here's our 0128 the accuracy of 50% then
75% is about 0.2 and so on and so on
don't quote those numbers because it
doesn't work that way they say that if
you have
.95 that's pretty much saying 100% And
if you have uh anywhere between you'd
have to go look this up let me go and
remove all my drawings there uh so the
magic number is 0. five we really want
to be over a 0. five in this whole thing
and we have uh both 0504 remember this
is accuracy if we were looking at loss
then we'd be looking the other way but
0.0 you know instead of how high it is
we want how low it is uh but when
accuracy being over a 0 five is pretty
valid that means this is pretty solid
and if you get to a point 95 then it's a
direct correlation that's what we're
looking for here in these numbers you
can see we finished with this model at.
5135 so still good um and if we look at
uh when they ran this in the other end
remember there's a lot of Randomness
that goes into it when we see the
weights uh they got
05251 so a little better than ours but
that's fine you'll find your own uh
comes up a little bit better or worse
depending on uh just that Randomness and
so we've gone through the whole model
we've created we trained the model and
we've also gone through on every 100th
run to test the model to see how
accurate it
is what's in it for you we will start
with of course of fundamentals what is a
neural network and popular neural
networks it's important to know the
framework we're in and what we're going
to be looking at specifically then we'll
touch on why a recurrent neural network
what is a recurrent neural network and
how does an RNN work uh one of the big
things about rnns is what they call the
vanish Ing and exploding gradient
problem so we'll look at that and then
we're going to be using a use case uh
study it's going to be in carass on
tensor flow carass is a python module
for doing neural networks in deep
learning and uh in there there's the
what they call long shortterm memory
lstm and then we'll use the use case to
implement our lstm on the carass so when
you see that lstm that is basically the
RNN Network and we'll get into that the
use case is always my favorite part
before we dive into any of this we're
going to take a look at what is an RNN
or an introduction to the RNN do you
know how Google's autocomplete feature
predicts the rest of the words a user is
typing I love that autocomplete feature
as I'm typing away saves me a lot of
time I can just kind of Hit the enter
key and it autofills everything and I
don't have to type as much well first
there's a collection of large volumes of
most frequently occurring consecutive
words uh this is fed into a recurrent
neural network analysis the data by
finding the sequence of words occurring
frequently and builds a model to predict
the next word in the sentence and then
Google what is the best food to eat in
Las I'm guessing you're going to say Las
Mexico no it's going to be Las Vegas uh
so the Google search will take a look at
that and say hey the most common auto
complete is going to be Vegas in there
and usually gives you three or four
different choices so it's a very
powerful tool it saves us a lot of time
especially when we're doing a Google
search or even in Microsoft Words has a
some people get very mad at it it
autofills with the wrong stuff uh but
you know you're typing away and it helps
you autofill I have that in a lot of my
different packages it's just a standard
feature that we are all used to now so
before we dive into the RNN and getting
Into the Depths let's go ahead and talk
about what is a neural network neural
networks used in deep learning consist
of different layers connected to each
other and work on the structure and
functions of a human brain you're going
to see that thread human in human brain
and human thinking through throughout
deep learning the only way we can
evaluate an artificial intelligence or
anything like that is to compare it to
human function very important note on
there and it learns from a huge volumes
of data and it uses complex algorithm to
train a neural net so in here we have
image pixels of two different breeds of
dog uh one looks like a nice floppy
eared lab and one a German Shepherd you
know both wonderful breeds of animals
that image then goes into an input layer
uh that input layer might be formatted
at some point because you have to let it
know like you know different pictures
are going to be different sizes and
different color content then it'll feed
into hidden layers so each of those
pixels or each point of data goes in and
then um splits into the hidden layer
which then goes into another hidden
layer which then goes to an output layer
RNN there's some changes in there which
we're going to get into so it's not just
a straightforward propagation of data
like we've covered in many other
tutorials and finally you have an output
layer and the output layer has two
outputs it has one that lights up if
it's a German shepherd and another that
lights up is if it's a labador so
identifies a dog's breed such networks
do not require memorizing the past
output so our forward propagation is
just that it goes forward it doesn't
have to rememorize stuff and you can see
there that's not actually me in the
picture uh dressed up in my
suit I haven't worn a suit in years so
as we're looking at this we're going to
change it up a little bit before we
cover that let's talk about popular
neural networks first there's the feed
forward neural network used in general
regression and classification problems
and we have the convolution neural
network used for image recognition deep
neural network used for acoustic
modeling deep belief Network used for
cancer detection and recurrent neural
network used for speech recognition now
taken a lot of these and mixed them
around a little bit so just because it's
used for one thing doesn't mean it can't
be used for other modeling but generally
this is where the field is and this is
how those models are generally being
being used right now so we talk about a
feed forward neural network in a feed
forward neural network information flows
only in the forward direction from the
input nodes through the hidden layers if
any and the output nodes there are no
Cycles or Loops in the network and so
you can see here we have our input layer
I was talking about how it just goes
straight forward into the hidden layers
so each one of those connects and then
connects to the next hidden layer
connects to the output layer and of
course we have a nice simplified version
where it has a predicted output and they
refer to the input as x a lot of times
and the output as y why decisions are
based on current input no memory about
the past no future scope why recurrent
neural network issues in feed forward
neural network so one of the biggest
issues is because it doesn't have a
scope of memory or time a feed forward
neural network doesn't know how to
handle sequential data uh it only
considers only the current input so if
you have a series of things and because
three points back affects what's
happening now and what your output
affects what's happening that's very
important so whatever I put as an output
is going to affect the next one um a
feed forward doesn't look at any of that
it just looks at this is what's coming
in and it cannot memorize previous
inputs so it doesn't have that list of
inputs coming in solution to feed
forward neural network you'll see here
where it says recurrent neural network
and we have our X on the bottom going to
H going to Y that's your feed forward uh
but right in the middle it has a value C
so it's a whole another process so it's
memorizing what's going on in the hidden
layers and the hidden layers as they
produce data feed into the next one so
your hidden layer might have an output
that goes off to Y uh but that output
goes back into the next prediction
coming in what this does is this allows
it to handle sequential data it
considers the current input and also the
previously received inputs and if we're
going to look at General drawings and um
Solutions we should also look at
applications of the RNN image captioning
RNN is used to caption an image by
analyzing the activities present in it a
dog catching a ball in midair uh that's
very tough I mean you know we have a lot
of stuff that analyzes images of a dog
and the image of a ball but it's able to
add one more feature in there that's
actually catching the ball in midair
time series prediction any time series
problem like predicting the prices of
stocks in a particular month can be
solved using RNN and we'll dive into
that in our use case and actually take a
look at some stock one of the things you
should know about analyzing stock today
is that it is very difficult and if
you're analyzing the whole stock the
stock Market at the New York Stock
Exchange in the US produces somewhere in
the neighborhood if you count all the
individual trades and fluctuations by
the second um it's like three terabytes
a day of data so we're only to look at
one stock just analyzing One stock is
really tricky in here we'll give you a
little jump on that so that's exciting
but don't expect to get rich off of it
immediately another application of the
RNN is natural language processing text
Mining and sentiment analysis can be
carried out using RNN for natural
language processing and you can see
right here the term natural language
processing when you stream those three
words together is very different than I
if I said processing language natural Le
so the time series is very important
when we're analyzing sentiments it can
change the whole value of a sentence
just by switching the words around or if
you're just counting the words you might
get one sentiment where if you actually
look at the order they're in you get a
completely different sentiment when it
rains look for rainbows when it's dark
look for stars both of these are
positive sentiments and they're based
upon on the order of which the sentence
is going in machine translation given an
input in one language RNN can be used to
translate the input into a different
languages as output I myself very
linguistically challenged but if you
study languages and you're good with
languages you know right away that if
you're speaking English you would say
big cat and if you're speaking Spanish
you would say cat big so that
translation is really important to get
the right order to get uh there's all
kinds of parts of speech that are
important to know by the order of the
words here this person is speaking in
English and getting translated and you
can see here a person is speaking in
English in this little diagram I guess
that's denoted by the flags I have a
flag I own it no um but they're speaking
in English and it's getting translated
into Chinese Italian French German and
Spanish languages some of the tools
coming out are just so cool so somebody
like myself who's very linguistically
challenged I can now travel into Worlds
I would never think of because I can
have something translate my English back
and forth readily and I'm not stuck with
a communication gap so let's dive into
what is a recurrent neural network
recurrent neural network works on the
principle of saving the output of a
layer and feeding this back to the input
in order to predict the output of the
layer sounds a little confusing when we
start breaking it down it'll make more
sense and usually we have a propagation
forward neural network with the input
layers the hidden layers the output
layer with the recurrent neural network
we turn that on its side side so here it
is and now our X comes up from the
bottom into the hidden layers into Y and
they usually draw very simplified X to H
with C is a loop a to Y where a B and C
are the perimeters a lot of times you'll
see this kind of drawing in here digging
closer and closer into the H and how it
works going from left to right you'll
see that the C goes in and then the X
goes in so the x is going Upward Bound
and C is going to the right a is going
out and C is also going out that's where
it gets a little confusing so here we
have xn uh CN and then we have y out and
C out and C is based on HT minus one so
our value is based on the Y and the H
value are connected to each other
they're not necessarily the same value
because H can be its own thing and
usually we draw this or we represent it
as a function h of T equals a function
of C where H of T minus one that's the
last H output and X of T going in so
it's a the last output of H combined
with the new input of x uh where HT is
the new state FC is a function with the
parameter C that's a common way of
denoting it uh HT minus one is the Old
State coming out and then X of T is an
input Vector at time of Step T well we
need to cover types of recurrent neural
networks and so the first one is the
most common one which is a one to one
single output one: one neural network is
usually known as a vanilla neural
network us for regular machine learning
problems why because vanilla is usually
considered kind of a just a real basic
flavor but because it's a very basic a
lot of times they'll call it the vanilla
neural network uh which is not the
common term but it is you know like kind
of a slang term people will know what
you're talking about usually if you say
that then we run one to Min so you have
a single input and you might have a
multiple outputs in this case uh image
captioning as we looked at earlier where
we have not just looking at it as a dog
but a dog catching a ball in the air and
then you have have many to One Network
takes in a sequence of inputs examples
sentiment analysis where a given
sentence can be classified as expressing
positive or negative sentiments and we
looked at that as we were discussing if
it rains look for a rainbow so positive
sentiment where rain might be a negative
sentiment if you were just adding up the
words in there and then of course if
you're going to do a one: one many to
one one to Min there's many to many
networks takes in a sequence of inputs
and generates a sequence of outputs
example machine translation so we have a
lengthy sentence coming in in English
and then going out in all the different
languages uh you know just a wonderful
tool very complicated set of
computations you know if you're a
translator you realize just how
difficult it is to translate into
different languages one of the biggest
things you need to understand when we're
working with this neural network is
what's called The Vanishing gradient
problem while training an RNN your slope
can be either too small or very large
and this makes training difficult when
the slope is too small the problem
problem is known as Vanishing gradient
and you'll see here they have a nice U
image loss of information through time
so if you're pushing not enough
information forward that information is
lost and then when you go to train it
you start losing the third word in the
sentence or something like that or it
doesn't quite follow the full logic of
what you're working on exploding
gradient problem Oh this is one that
runs into everybody when you're working
with this particular neural network when
the slope tends to grow exponentially
instead of decaying this problem is
called exploding gradient issues in
gradient problem long tring time poor
performance bad accuracy and I'll add
one more in there uh your computer if
you're on a lower-end computer testing
out a model will lock up and give you
the memory error explaining gradient
problem consider the following two
examples to understand what should be
the next word in the sequence the person
who took my bike and blank a thief the
students who got into engineering with
blank from Asia and you can see in here
we have our x value going in we have the
previous value going forward and then
you back propagate the error like you do
with any neural network and as we're
looking for that missing word maybe
we'll have the person took my bike and
blank was a thief and the student who
got into engineering with a blank were
from Asia consider the following example
the person who took the bike so we'll go
back to the person who took the bike was
blank a thief in order to understand
what would be the next word in the
sequence the RNN must memorize is the
previous context whether the subject was
singular noun or a plural noun so was a
thief is singular the student who got
into engineering well in order to
understand what would be the next word
in the sequence the RNN must memorize
the previous context whether the subject
was singular noun or a plural noun and
so you can see here the students who got
into engineering with blank were from
Asia it might be sometimes difficult for
the eror to back propagate to the
beginning of the sequence to predict
what should be the output so when you
run into the gradient problem we need a
solution the solution to the gradient
problem first we're going to look at
exploding gradient where we have three
different solutions depending on what's
going on one is identity initialization
so the first thing we want to do is see
if we can find a way to minimize the
identities coming in instead of having
it identify everything just the
important information we're looking at
next is to truncate the back propagation
so instead of having whatever
information it's sending to the next
series we can truncate what it's sending
we can lower that particular uh set of
layers make those smaller and finally is
a gradient clipping so when we're
training it we can clip what that
gradient looks like and narrow the
training model that we're using when you
have a Vanishing gradient the option
problem uh we can take a look at weight
initialization very similar to the
identity but we're going to add more
weights in there so it can identify
different aspects of what's coming in
better choosing the right activation
function that's huge so we might be
activating based on one thing and we
need to limit that we haven't talked too
much about activation functions so we'll
look at that just minimally uh there's a
lot of choices out there and then
finally there's long short-term memory
networks the
lstms and we can make adjustments to
that so just like we can clip the
gradient as it comes out we can also um
expand on that we can increase the
memory Network the size of it so it
handles more information and one of the
most common problems in today's uh setup
is what they call longterm dependencies
suppose we try to predict the last word
in the text the clouds are in the and
you probably said sky here we do not
need any further context it's pretty
clear that the last word is going to be
Sky suppose we try to predict the last
word in the text I have been staying in
Spain for the last 10 years I can speak
fluent maybe you said Portuguese or
French no you probably said Spanish the
word we predict will depend on the
previous few words in context here we
need the the context of Spain to predict
the last word in the text it's possible
that the gap between the relevant
information and the point where it is
needed to become very large
lstms help us solve this problem so the
lstms are a special kind of recurrent
neural network capable of learning
long-term dependencies remembering
information for long periods of time is
their default Behavior All recurrent
neural networks have the form of a chain
of repeating modules of neural network
Connections in standard rnns this
repeating module will have a very simple
structure such as a single tangent H
layer lstm s's also have a chainlike
structure but the repeating module has a
different structure instead of having a
single neural network layer there are
four interacting layers communicating in
a very special way lstms are a special
kind of recurrent neural network capable
of learning long-term dependencies
remembering information for long periods
of time is their default Behavior LST
tms's also have a chain-like structure
but the repeating module has a different
structure instead of having a single
neural network layer there are four
interacting layers communicating in a
very special way as you can see the
deeper we dig into this the more
complicated the graphs get in here I
want you to note that you have x a t
minus one coming in you have x a t
coming in and you have x a t + one and
you have H of T minus one and H of T
coming in and H of t + one going out and
of course uh on the other side is the
output a um in the middle we have our
tangent H but it occurs in two different
places so not only when we're Computing
the x of t + one are we getting the
tangent H from X of T but we're also
getting that value coming in from the X
of T minus one so the short of it is as
you look at these layers not only does
it does the propagate through the first
layer goes into the second layer back
into itself but it's also going into the
third layer so now we're kind of
stacking them those up and this can get
very complicated as you grow that in
size it also grows in memory too and in
the amount of resources it takes uh but
it's a very powerful tool to help us
address the problem of complicated long
sequential information coming in like we
were just looking at in the sentence and
when we're looking at our long shortterm
memory network uh there's three steps of
processing assessing in the lstms that
we look at the first one is we want to
forget irrelevant parts of the previous
date you know a lot of times like you
know is as in a unless we're trying to
look at whether it's a plural noun or
not they don't really play a huge part
in the language so we want to get rid of
them then selectively update cell State
values so we only want to update the
cell State values that reflect what
we're working on and finally we want to
put only output certain parts of the
cell state so whatever is coming out we
want to limit what's going out too and
let's dig a little deeper into this
let's just see what this really looks
like uh so step one decides how much of
the past it should remember first step
in the lstm is to decide which
information to be omitted in from the
cell in that particular time step it is
decided by the sigmoid function it looks
at the previous state h of T minus one
and the current input X of T and
computes the function so you can see
over here we have a function of T equals
the sigmoid function of the weight of f
the H at T minus one and then X at t
plus of course you have a bias in there
with any vural netw work so we have a
bias function so f equals forget gate
decides which information to delete that
is not important from the previous time
step considering an L STM is fed with
the following inputs from the previous
and present time step Alice is good in
physics John on the other hand is good
in chemistry so previous output John
plays football well he told me yesterday
over the phone that he had served as a
captain of his college football team
that's our current input so as we look
at this the first step is the forget
gate realiz is there might be a change
in context after en counting the First
full stop Compares with the current
input sentence of xit so we're looking
at that full stop and then Compares it
with the input of the new sentence the
next sentence talks about John so the
information on Alice is deleted okay
that's important to know so we have this
input coming in and if we're going to
continue on with John then that's going
to be the primary information we're
looking at the position of the subject
is vacated and is assigned to John and
so in this one we've seen that we've
weeded out a whole bunch of information
and we're only passing information on
John since that's now the new topic so
step two is then to decide how much
should this unit add to the current
state in the second layer there are two
parts one is a sigmoid function and the
other is a tangent H in the sigmoid
function it decides which values to let
through zero or one tangent H function
gives the weightage to the values which
are passed deci setting their level of
importance minus one to 1 and you can
see the two formulas that come come up
uh the I of T equals the sigmoid of the
weight of i h of T minus one X of t plus
the bias of I and the C of T equals the
tangent of H of the weight of C of H of
T minus one X of t plus the bias of C so
our I of T equals the input gate
determines which information to let
through based on its significance in the
current time step if this seems a little
complicated don't worry because a lot of
the programming is already done when we
get to the case study understanding
though that this is part of the program
is important when you're trying to
figure out these what to set your
settings at you should also note when
you're looking at this it should have
some semblance to your forward
propagation neural networks where we
have a value assigned to a weight plus a
bias very important steps than any of
the neural network layers whether we're
propagating into them the information
from one to the next or we're just doing
a straightforward neural network
propagation let's take a quick look at
this what it looks like from the human
standpoint um as I step out in my suit
again consider the current input at xft
John plays football well he told me
yesterday over the phone that he had
served as a captain of his college
football team that's our input input
gate analysis the important information
John plays football and he was a captain
of his college team is important he told
me over the phone yesterday is less
important hence it is forgotten this
process of adding some new information
can be done via the input gate now this
example is as a human form and we'll
look at training this stuff in just a
minute uh but as a human being if I
wanted to get this information from a
conversation maybe it's a Google Voice
listening in on you or something like
that um how do we weed out the
information that he was talking to me on
the phone yesterday well I don't want to
memorize that he talked to me on the
phone yesterday or maybe that is
important but in this case it's not I
want to know that he was the captain of
the football team I want to know that he
sered I want to know that John plays
football and he was the captain of the
college football team those are the two
things that I want to take away as a
human being again we measure a lot of
this from the human Viewpoint and that's
also how we try to train them so we can
understand these neural networks finally
we get to step three decides what part
of the current cell State makes it to
the output the third step is to decide
what will be our output first we run a
sigmoid layer which decides what parts
of the cell State make it to the output
then we put the cell State through the
tangent H to push the values to be
between minus1 and one and multiply it
by the output of the sigmoid gate so
when we talk about the output of T we
set that equal to the sigmoid of the
weight of zero of the H of T minus one
back One Step in Time by the x of t plus
of course the bias the H of T equals the
out of T times the tangent of the
tangent h of c a so our o equals the
output gate allows the past in
information to impact the output in the
current time step let's consider the
example to predicting the next word in
the sentence John played tremendously
well against the opponent and one for
his team for his contributions Brave
blank was awarded player of the match
there could be a lot of choices for the
empty space current input Brave is an
adjective adjectives describe a noun
John could be the best output after
Brave thumbs up for John awarded player
of the match and if you were to pull
just the nouns out of the sentence team
doesn't look right because that's not
really the subject we're talking about
contributions you know Brave
contributions or Brave team Brave player
Brave match um so you look at this and
you can start to train this these this
neural network so starts looking at and
goes oh no JN is what we're talking
about so brave is an adjective Jon's
going to be the best output and we give
JN a big thumbs up and then of course we
jump into my favorite part the case
study use case implementation of lstm
let's predict the prices of stocks using
the lstm network based on the stock
price data between 20 12 2016 we're
going to try to predict the stock prices
of 2017 and this will be a narrow set of
data we're not going to do the whole
stock market it turns out that the New
York Stock Exchange generates roughly
three terabytes of data per day that's
all the different trades up and down of
all the different stocks going on and
each individual one uh second to second
or nanc to nanc uh but we're going to
limit that to just some very basic
fundamental information so don't think
you're going to get rich off this today
but at least you can give an you can
give a step forward in how to start
processing something like stock prices a
very valid use for machine learning in
today's markets use case implementation
of
lstm let's dive in we're going to import
our libraries we're going to import the
training set and uh get the scaling
going um now if you watch any of our
other tutorials a lot of these pieces
just start to look very familiar CU it's
very similar setup uh but let's take a
look at that and um just a reminder
we're going to be using Anaconda the
Jupiter notebook so here I have my
anaconda Navigator when we go under
environments I've actually set up a
caros python 36 I'm in Python 36 and U
nice thing about Anaconda especially the
newer version remember a year ago
messing with Anaconda different versions
of python and different environments um
Anaconda now has a nice interface um and
I have this installed both on a Ubuntu
Linux machine and on windows so it works
fine on there go in here and open a
terminal window and then in here once
you're in the terminal window this is
where you're going to start uh
installing using pip to install your
different modules and everything now
we've already pre-installed them so we
don't need to do that in here uh but if
you don't have them installed in your
particular environment you'll need to do
that and of course you don't need to use
the anaconda or the Jupiter you can use
whatever favorite python ID you like I'm
just a big fan of this cuz it keeps all
my stuff separate you can see on this
machine I have specifically installed
one for carass since we're going to be
working with carass under tensorflow
when we go back to home I've gone up
here to application and that's the
environment I've loaded on here and then
we'll click on the launch Jupiter
notebook now I've already in my Jupiter
notebook um have set up a lot of stuff
so that we're ready to go kind of like
uh Martha Stewarts in the old cooking
shows we want to make sure we have all
our tools for you so you're not waiting
for them to load and uh if we go up here
to where it says new you can see where
you can um create a new Python 3 that's
what did here underneath the setup so it
already has all the modules installed on
it and I'm actually renamed this so if
you go under file you can rename it
we've I'm calling it RNN stock and let's
just take a look at start diving into
the code let's get into the exciting
part now we've looked at the tool and of
course you might be using a different
tool which is fine uh let's start
putting that code in there and seeing
what those Imports and uploading
everything looks like now first half is
kind of boring when we hit the rum
button because we're going to be
importing numpy as NP that's uh uh the
number python which is your numpy array
and the matap plot library because we're
going to do some plotting at the end and
our pandas for our data set our pandas
is PD and when I hit run uh it really
doesn't do anything except for load
those modules just a quick note let me
just do a quick uh draw here oops shift
all there we go you'll notice when we're
doing this setup if I was to divide this
up oops I'm going to actually um let
overlap these here we
go uh this first part that we're going
to do
is our
data prep a lot of prepping
involved um in fact depending on what
your system is since we're using carass
I put an overlap here uh but you'll find
that almost maybe even half of the code
we do is all about the data prep and the
reason I overlap this with uh coras let
me just put that down because that's
what we're working in uh is because
carass has like their own preset set
stuff so it's already pre-built in which
is really nice so there's a couple Steps
A lot of times that are in the carass
setup uh we'll take a look at that to
see what comes up in our code as we go
through and look at stock and then the
last part is to evaluate and if you're
working with um shareholders or uh you
know classroom whatever it is you're
working with uh the evaluate is the next
biggest piece um so the actual code here
crossed is a little bit more but when
you're working with uh some of the other
packages you might have like three lines
that might be it all your stuff is in
your pre-processing and your data since
carass has is is Cutting Edge and you
load the individual layers you'll see
that there's a few more lines here and
cross is a little bit more robust and
then you spend a lot of times uh like I
said with the evaluate you want to have
something you present to everybody else
to say hey this is what I did this is
what it looks like so let's go through
those steps this is like a kind of just
general overview and let's just take a
look and see what the next set of code
looks like and in here we have a data
set train and and it's going to be read
using the PD or pandas read CSV and it's
the Google stock pric train.csv and so
under this we have training set equals
data set train. iocation and we've kind
of sorted out part of that so what's
going on here let's just take a look at
that let's let's look at the actual file
and see what's going on there now if we
look at this ignore all the extra files
on this um I already have a train and a
test set where it's sorted out this is
important to notice because a lot of
times we do that as part of the
pre-processing of the data we take 20%
of the data out so we can test it and
then we train the rest of it that's what
we use to create our neural network that
way we can find out how good it is uh
but let's go ahead and just take a look
and see what that looks like as far as
the file itself and I went ahead and
just opened this up in a basic word pad
text editor just so we can take a look
at it certainly you can open up in Excel
or any other kind of spreadsheet um and
we note that this is a comma separated
variables we have a date uh open high
low close volume this is the standard
stuff that we import into our stock one
the most basic set of information you
can look at in stock it's all free to
download um in this case we downloaded
it from uh Google that's why we call it
the Google stock price um and it
specifically is Google this is the
Google stock values from uh as you can
see here we started off at 13
20102 so when we look at this first
setup up here uh we have a data set
train equals p Dore CSV and if you
noticed on the original frame um let me
just go back there they had it set to
home Ubuntu downloads Google stock price
train I went ahead and changed that
because we're in the same file where I'm
running the code so I've saved this
particular python code and I don't need
to go through any special paths or have
the full path on there and then of
course we want to take out um certain
values in here and you're going to
notice that we're using um our data set
and we're now in pandas uh so pandas
basically it looks like a spreadsheet um
and in this case we're going to do I
location which is going to get specific
locations the first value is going to
show us that we're pulling all the rows
and the data and the second one is we're
only going to look at columns one and
two and if you remember here from our
data as we switch back on over columns
we always start with zero which is the
date and we're going to be looking at
open and high which would be one and
two I'll just label that right there so
you can see now when you go back and do
this you certainly can extrapolate and
do this on all the columns um but for
the example let's just limit a little
bit here so that we can focus on just
some key aspects of
stock and then we'll go up here and run
the code and uh again I said the first
half is very boring whenever you hit the
Run button it doesn't do anything
because we're still just loading the
data and setting it up now that we've
loaded our data we want to go ahead and
scale it we want to do what they call
feature scaling and in here we're going
to pull it up from the SK learn or the
SK kit pre-processing import min max
scaler and when you look at this you got
to remember that um biases in our data
we want to get rid of that so if you
have something that's like a really high
value U let's just draw a quick graph
and I have something here like the maybe
this stock has a value One stock has a
value of 100 and another stock has a
value of
five um you start to get a bias between
different stocks and so when we do this
we go ahead and say okay 100's going to
be the Max and five is going to be the
men and then everything else goes and
then we change this so we just squish it
down I like the word squish so it's
between one and zero so 100 equals 1 or
one equals 100 and 0 equal 5 and you can
just multiply it's usually just a simple
multiplication we're using uh
multiplication so it's going to be uh
minus5 and then 100 divided or 95
divided by one so or whatever value is
is divided by 95 and uh once we've
actually created our scale we've toing
it's going to be from 0er to one we want
to take our training set and we're going
to create a training set scaled and
we're going to use our scaler SC and
we're going to fit we're going to fit
and transform the training Set uh so we
can now use the SC this this particular
object we'll use it later on our testing
set because remember we have to also
scale that when we go to test our uh
model and see how it works and we'll go
ahead and click on the run again uh it's
not going to have any output yet because
we're just setting up all the
variables okay so we pasted the data in
here and we're going to create the data
structure with the 60 time steps and
output first note we're running 60 time
steps and that is where this value here
also comes in so the first thing we do
is we create our X train and Y train
variables we set them to an empty python
array very important to remember what
kind of array we're in and what we're
working with and then we're going to
come in here we're going to go for I in
range 60 to 1258 there's our 60 60 time
steps and the reason we want to do this
is as we're adding the data in there
there's nothing below the 60 so if we're
going to use 60 time steps uh we have to
start at 60 because it includes
everything anything underneath of it
otherwise you'll get a pointer error and
then we're going to take our X train and
we're going to append training set
scaled this is a scaled value between
zero and one and then as I is equal to
60 this value is going to be um 60 - 60
is 0 so this actually is 0 to I so it's
going to be 0 to 60 1 to 61 let me just
circle this part right here 1 to 61 uh 2
to 62 and so on and so so on and if you
remember I said 0 to 60 that's incorrect
because it does not count remember it
starts at zero so this is a count of 60
so it's actually 59 important to
remember that as we're looking at this
and then the second part of this that
we're looking at so if you remember
correctly here we go we go from uh 0 to
59 of I and then we have a comma a zero
right here and so finally we're just
going to look at the open value now I
know we did put it in there for one to
two um if you remember correctly it
doesn't count the second one so it's
just the open value we're looking at
just open um and then finally we have y
train. aend training set I to zero and
if you remember correctly I to or I
comma zero if you remember correctly
this is 0 to 59 so there's 60 values in
it uh so when we do I down here this is
number 60 so we're going to do this is
we're creating an array and we have 0 to
59 and over here we have number 60 which
is going into the Y train it's being
appended on there and then this just
goes all the way up so this is down here
is uh uh 0 to 59 and we'll call it 60
since that's the value over here and it
goes all the way up to
1258 that's where this value here comes
in that's the length of the data we're
loading so we've loaded two arrays we've
loaded one array that has uh which is
filled with arrays from 0 to 59 and we
loaded one array which is just the value
and what we're looking at you want to
think about this as a Time sequence uh
here's my open open open open open open
what's the next one in the series so
we're looking at the Google stock and
each time it opens we want to know what
the next one 0 through 59 what's 60 1
through 60 what's 61 2 through 62 what's
62 and so on and so on going up and then
once we've loaded those in our for Loop
we go ahead and take X train and Y train
equals np. array XT tr. NP array yrain
we're just converting this back into a
numpy array that way we can use all the
cool tools that we get with numpy array
including reshaping so if we take a look
and see what's going on here we're going
to take our X
train we're going to reshape it wow what
the heck does reshape mean uh that means
we have an array if you remember
correctly um so many numbers by
60 that's how wide it is and so we're
when you when you do XT train. shape
that gets one of the shapes and you get
um X train. shape of one gets the other
shape and we're just making sure the
data is formatted correctly and so you
use this to pull the fact that it's 60
by um in this case where's that value 60
by
1199 1258 minus
60199 and we're making sure that that is
shaped correctly so the data is grouped
into uh
1199 by 60 different arrays and then the
one on the end just means at the end
because this when you're dealing with
shapes and numpy they look at this as
layers and so the in layer needs to be
one value that's like the leaf of a tree
where this is the branch and then it
branches out some more um and then you
get the Leaf np. reshape comes from and
using the existing shapes to form it
we'll go ahead and run this piece of
code again there's no real output and
then we'll import our different Cor
cross modules that we need so from Cross
models we're going to import the
sequential model dealing with sequential
data we have our dense layers we have
actually three layers we're going to
bring in our dents our lstm which is
what we're focusing on and our Dropout
and we'll discuss these three layers
more in just a moment but you do need
the with the lstm you do need the
Dropout and then the final layer will be
the dents but let's go ahead and run
this and they'll bring Port our modules
and you'll see we get an error on here
and if you read it closer it's not
actually an error it's a warning what
does this warning mean these things come
up all the time when you're working with
such Cutting Edge modules that are
completely being updated all the time
we're not going to worry too much about
the warning all it's saying is that the
h5py module which is part of carass is
going to be updated at some point and uh
if you're running new stuff on carass
and you start updating your carass
system you better make sure that your H5
Pi is updated too otherwise you're going
to have an error later on and you can
actually just run an update on the H5
high now if you wanted to not a big deal
we're not going to worry about that
today and I said we were going to jump
in and start looking at what those
layers mean I meant that and uh we're
going to start off with initializing the
RNN and then we'll start adding those
layers in and you'll see that we have
the lstm and then the Dropout lstm then
Dropout lstm then Dropout what the heck
is that doing so let's explore that
we'll start by initializing the RNN
regressor equals sequential because
we're using the sequential model and
we'll run that and load that up and then
we're going to start adding our lstm
layer and some Dropout regularization
and right there should be the que
Dropout regularization and if we go back
here and remember our exploding gradient
well that's what we're talking about the
uh Dropout drops out unnecessary data so
we're not just shifting huge amounts of
data through um the network so and so we
go in here let's just go ahead and uh
add this in I'll go ahead and run this
and we had three of them so let me go
and put all three of them in and then we
can go back over them there's the second
one and let's put one more in let's put
that in and we'll go and put two more in
I meant to I said one more in but it's
actually two more in and then let's add
one more after that and as you can see
each time I run these they don't
actually have an output so let's take a
closer look and see what's going on here
so we're going to add our first lstm
layer in here we're going to have units
50 the units is the positive integer and
it's the dimensionality of the output
space this is what's going out into the
next layer so we might have 60 coming in
but we have 50 going out we have a
return sequence because it is a sequence
data so we want to keep that true and
then you have to tell it what shape it's
in well we already know the shape by
just going in here and looking at xtrain
shape so input shape equals the xtrain
shape of one comma 1 makes it real easy
you don't have to remember all the
numbers that put in 60 or whatever else
is in there you just let it tell the
regressor what model to use and so we
follow our STM with a Dropout layer now
understanding the Dropout layer is kind
of exciting cuz one of the things that
happens is we can overtrain our Network
that means that our neural network will
memorize such specific data that it has
trouble predicting anything that's not
in that specific realm to fix for that
each time we run through the training
mode we're going to take 02 or 20% of
our neurons and just turn them off so
we're only going to train on the other
ones and it's going to be random that
way each time we pass through this we
don't overtrain these nodes come back in
in the next training cycle we randomly
pick a different 20 and finally they see
a big difference as we go from the first
to the second and third and fourth the
first thing is we don't have to input
the shape because the shape's already
the output units is 50 here this Auto
The Next Step automatically knows this
layer is putting out 50 and because it's
the next layer it automatically sets
that says oh 50 is coming out from our L
layer is coming out you know goes into
the regressor and of course we have our
Dropout and that's what's coming into
this one and and so on and so on and so
the next three layers we don't have to
let it know what the shape is it
automatically understands that and we're
going to keep the units the same we're
still going to do 50 units it's still a
sequence coming through 50 units and a
sequence now the next piece of code is
what brings it all together let's go
ahead and take a look at that and we
come in here we put the output layer the
dense layer and if you remember up here
we had the three layers we had uh lstm
Dropout and d uh D just says we're going
to bring this all down into 1 output
instead of putting out a sequence we
just know want to know the answer at
this point and let's go ahead and run
that and so in here you notice all we're
doing is setting things up one step at a
time so far we've brought in our uh way
up here we brought in our data we
brought in our different modules we
formatted the data for training it we've
set it up you know we have our y x train
and our y train we have our source of
data and the answers we're we know so
far that we're going to put in there
we've reshaped that we've come in and
built our Cor Ross we've imported our
different layers and we have in here if
you look we have what uh five total
layers now carass is a little different
than a lot of other systems because a
lot of other systems put this all in one
line and do it automatic but they don't
give you the options of how those layers
interface and they don't give you the
options of how the data comes in carass
is Cutting Edge for this reason so even
though there's a lot of extra steps in
building the model this has a huge
impact on the output and what we can do
with this these new models from karat
so we brought in our dents we have our
full model put together our regressor so
we need to go ahead and compile it and
then we're going to go ahead and fit the
data we're going to compile the pieces
so they all come together and then we're
going to run our training data on there
and actually recreate our regressor so
it's ready to be used so let's go ahead
and compile that and I can go and run
that and uh if you've been looking at
any of our other tutorials on neural
networks you'll see we're going to use
the optimizer atom atom is optimized for
Big Data there's a couple other
optimizers out there uh beyond the scope
of this tutorial but certainly Adam will
work pretty good for this and loss
equals mean squared value so when we're
training it this is what we want to base
the loss on how bad is our error well
we're going to use the mean squared
value for our error and the atom
Optimizer for its differential equations
you don't have to know the math behind
them but certainly it helps to know what
they're doing and where they fit into
the bigger models and then finally we're
going to do our fit fitting the RN into
the training set we have the regressor
do fit X train train y train epics and
batch size so we know where this is this
is our data coming in for the X train
our y train is the answer we're looking
for of our data our sequential input
epic is how many times we're going to go
over the whole data set we created a
whole data set of XT train so this is
each each of those rows which includes a
Time sequence of 60 and badge size
another one of those things where carass
really shines is if you were pulling
this save from a large file instead of
trying to load it all into RAM it can
now pick smaller batches up and load
those indirectly we're not worried about
pulling them off a file today CU this
isn't big enough to uh cause the
computer too much of a problem to run
not too straining on the resources but
as we run this you can imagine what
would happen if I was doing a lot more
than just one column in one set of stock
in this case Google stock imagine if I
was doing this across all the stocks and
I had instead of just the open I had
open close high low and you can actually
find yourself with about 13 different
variables times 60 cuz it's a Time
sequence suddenly you find yourself with
a gig of memory you're loading into your
RAM which will just completely you know
if it's just if you're not on multiple
computers or cluster you're going to
start running into resource problems but
for this we don't have to worry about
that so let's go ahead and run this and
this will actually take a little bit on
my computer it's an older laptop and
give it a second to kick in there there
we go all right so we have epic so this
is going to tell me it's running the
first run through all the data and as
it's going through it's batching them in
32 pieces so 32 lines each time and
there's 1198 I think I said 1199 earlier
but it's 1198 I was off by one and each
one of these is 13 seconds so you can
imagine this is roughly 20 to 30 minutes
runtime on this computer like I said
it's an older laptop running at uh 0.9
GHz on a dual processor and that's fine
what we'll do is I'll go ahead and stop
go get a drink a coffee and come back
and let's see what happens at the end
and where this takes us and like any
good cooking show I've kind of gotten my
latte I also had some other stuff
running in the background so you'll see
these numbers jumped up to like 19
seconds 15 seconds which you can scroll
through and you can see we've run it
through 100 steps or 100 epics so the
question is what does all this mean one
of the first things you'll notice is
that our loss can is over here it kind
of stopped at 0.0014 but you can see it
kind of goes down until we hit about
0.0014 three times in a row so we
guessed our epic pretty close since our
loss has remain the same on there so to
find out what we're looking at we're
going to go ahead and load up our test
data the test data that we didn't
process yet and real stock price data
set test iocation this is the same thing
we did when we prepped the data in the
first place so let's go ahead and go
through this code and we can see we've
labeled it part three making the
predictions and visualizing the results
so the first thing we need to do is go
ahead and read the data in from our test
CSV and you see I've Chang the path on
it for my computer and uh then we'll
call it the real stock price and again
we're doing just the one column here and
the values from I location so it's all
the rows and just the values from these
that one location that's the open Stock
open and let's go ahead and run that so
that's loaded in there and then let's go
ahead and uh create we have our inputs
we're going to create inputs here and
this should all look familiar this is
the same thing we did before we're going
to take our data set total we're going
to do a little Panda concap from the
data Sate train now remember the end of
the data set train is part of the data
going in let's just visualize that just
a little bit here's our train data let
me just put TR for train and it went up
to this value here but each one of these
values generated a bunch of columns it
was 60 across and this value here equals
this one and this value here equals this
one and this value here equals this one
and so we need these top 60 to go into
our new data so to find out what we're
looking at we're going to go ahead and
load up our test data the test data that
we didn't Pro process yet and uh real
stock price data set test ey location
this is the same thing we did when we
prepped the data in the first place so
let's go ahead and go through this code
and you see we've labeled it part three
making the predictions and visualizing
the results so the first thing we need
to do is go ahead and read the data in
from our test CSV you see I've changed
the path on it for my computer and uh
then we'll call it the real stock price
and again we're doing just the one
column here and the values from location
so it's all the rows and just the values
from these that one location that's the
open Stock open let's go ahead and run
that so that's loaded in there and then
let's go ahead and uh create we have our
inputs we're going to create inputs here
and this should all look familiar this
is the same thing we did before we're
going to take our data set total we're
going to do a little Panda concap from
the data SE train now remember the end
of the data set train is part of the
data going in and let's just visualize
that just a little bit here's our train
data let me just put TR for train and it
went up to this value here but each one
of these values generated a bunch of
columns it was 60 across and this value
here equals this one and this value here
equals this one and this value here
equals this one and so we need these top
60 to go into our new data cuz that's
part of the next data or it's actually
the top 59 so that's what this first
setup is over here is we're going in
we're doing the real stock price and
we're going to just take the data set
test and and we're going to load that in
and then the real stock price is our
data test. test location so we're just
looking at that first uh column the open
price and then our data set total we're
going to take pandas and we're going to
concat and we're going to take our data
set train for the open and our data set
test open and this is one way you can
reference these columns we've referenced
them a couple different ways we've
referenced them up here with the one two
but we know it's labeled as a panda set
is open so pandas is great that way lots
of Versatility there and we'll go ahead
and go back up here and run this this
there we go and uh you'll notice this is
the same as what we did before we have
our open data set we pended our two
different or concatenated our two data
sets together we have our inputs equals
data set total length data set total
minus length of data set minus test
minus 60 values so we're going to run
this over all of them and you'll see why
this works because normally when you're
running your test set versus your
training set you run them completely
separate but when we graph this you'll
see that we're just going to be we'll be
looking at the part that uh we didn't
train it with to see how well it graphs
and we have our inputs equals inputs.
reshapes or reshaping like we did before
we're Transforming Our inputs so if you
remember from the transform between zero
and one and uh finally we want to go
ahead and take our X test and we're
going to create that X test and for I in
range 60 to 80 so here's our X test and
we're pending our inputs I to 60 which
remember is 0 to 59 and I comma 0 on the
other side so it's just the First Column
which is our open column and uh once
again we take our X test we convert it
to a numpy array we do the same reshape
we did before and uh then we get down to
the final two lines and here we have
something new right here on these last
two lines let me just highlight those or
or mark them predicted stock price
equals regressor do predict X test so
we're predicting all the stock including
both the training and the testing model
here and then we want to take this
prediction and we want to inverse the
trans form so remember we put them
between zero and one well that's not
going to mean very much to me to look at
a at a float number between 0o want I
want the dollar amounts I want to know
what the cash value is and we'll go
ahead and run this and you'll see it
runs much quicker than the training
that's what's so wonderful about these
neural networks once you put them
together it takes just a second to run
the same neural network that took us
what a half hour to train ahead and plot
the data we're going to plot what we
think it's going to be and we're going
to plot it against the real data what
what the Google stock actually did so
let's go ahead and take a look at that
in code and let's uh pull this code up
so we have our PLT that's our uh oh if
you remember from the very beginning let
me just go back up to the top we have
our matplot library. pyplot as PLT
that's where that comes in and we come
down here we're going to plot let me get
my drawing thing out again we're going
to go ahead and PLT is basically kind of
like an object it's one of the things
that always threw me when I'm doing
graphs in Python because I always think
you have to create an object and then it
loads that class in there well in this
case PLT is like a canvas you're putting
stuff on so if you've done HTML 5 you'll
have the canvas object this is the
canvas so we're going to plot the real
stock price that's what it actually is
and we're going to give that color red
so it's going to be in bright red we're
going to label it real Google stock
price and then we're going to do our
predicted stock and we're going to do it
in blue and it's going to be labeled
predicted and we'll give it a title
because it's always nice to give a title
to your H graph especially if you're
going to present this to somebody you
know to your shareholders in the office
and uh the X label is going to be time
because it's a Time series and we didn't
actually put the actual date and times
on here but that's fine we just know
they're incremented by time and then of
course the Y label is the actual stock
price pt. Legend tells us to build the
legend on here so that the color red and
and real Google stock price show up on
there and then the plot shows us that
actual graph so let's go ahead and run
this and see what that looks like and
you can see here we have a nice graph
and let's talk just a little bit about
this graph before we wrap it up here's
our Legend I was telling you about
that's why we have the legend to showed
the prices we have our title and
everything and you'll notice on the
bottom we have a Time sequence we didn't
put the actual time in here now we could
have we could have gone ahead and um
plotted the X since we know what the the
dates are and plotted this to dates but
we also know it's only the last piece of
data that we're looking at so last piece
of data which ends somewhere probably
around here on the graph I think it's
like about 20% of the data probably less
than that we have the Google price and
the Google price has this little up jump
and then down and you'll see that the
actual Google instead of a a turn down
here just didn't go up as high and
didn't low go uh down so our prediction
has the same pattern but the overall
value is pretty far off as far as um
stock but then again we're only looking
at one column we're only looking at the
open price we're not looking at how many
volumes were traded like I was pointing
out earlier we talk about stock just
right off the bat there's six columns
there's open high low close volume then
there's WEA uh I mean volume share then
there's the adjusted open adjusted High
adjusted low adjusted close they have a
special formula to predict exactly what
it would really be worth based on the
value of the stock and then from there
there's all kinds of other stuff you can
put in here so we're only looking at one
small aspect the opening price of the
stock and as you can see here we did a
pretty good job this curve follows the
curve pretty well it has like a you know
little jumps on it bins they don't quite
match up so this Bend here does not
quite match up with that bend there but
it's pretty aren close we have the basic
shape of it and the prediction isn't too
far off and you can imagine that as we
add more data in and look at different
aspects in the specific domain of stock
we should be able to get a better
representation each time we drill in
deeper of course this took a half hour
for my program my computer to train so
you can imagine that if I was running it
across all those different variables
might take a little bit longer to train
the data not so good for doing a quick
tutorial like this then accelerate your
career in AI and ml with a comprehensive
post-graduate program in artificial
intelligence and machine learning so
enroll now and unlock exciting Ai and
machine learning opportuni the link is
mentioned in the description box below
graphs so computational graphs are
really the heart and soul of neural
networks uh we talk about a
computational graph they are a visual
representation of expressing and
evaluating mathematical equations the
nodes and data flow in a graph
correspond to mathematical operations
and variabl
you'll hear a lot uh some of the terms
you might hear are node and Edge The
Edge being the data flow in this case um
it could also represent an actual value
they have um oh I think in spark they
have a graph x which works just on
Computing edges there's all kinds of
stuff that has evolved from
computational graphs we're focusing just
on carass and on uh neural networks so
we're not going to go into great detail
on everything a computational graph does
it is a core component of a neural
network is what's important to know on
this so carass offers a python
userfriendly front end while maintaining
a strong computation Power by using a
low-level API like tensorflow pie torch
Etc which use computational graphs as a
back end so one this allows for
abstraction of complex problems while
specifying control flow if you've ever
looked at some of the backend or the
original versions of tensor flow uh it's
really a nightmare you have all these
different settings you have to put in
there and create it's a lot of a lot of
backend programming this is like the old
computers when you had to uh tell it how
to dispose of a variable and how to
properly reallocate the memory for use
all that is covered nowadays in our
higher level programming well this is
the same thing with carass is it covers
a lot of this stuff and does things for
you that you would could spend hours on
just trying to figure
out it's useful for calculating
derivatives by using back propagation
we're definitely not going to teach a
class on derivatives uh in this little
video but understanding uh a derivative
is the rate of change so if you have a
particular function you're using in your
neural network a lot of them is just
simple uh uh y = mx plus b um your ukian
geometry where you just have a simple
slope times the intercept and they get
very complicated they have the inverse
tangent function for Activation as
opposed to just a linear ukian model and
you can think about this as you have
your data coming in and you have to
alter it somehow well you alter it going
down to get an answer you end up with an
error and that error goes back up and
you have to have that back propagation
with the derivative you want to know how
it changed so that you can figure out
how to adjust it for the
error a lot of that's hidden so you
don't even have to worry about it with
carass and in today's carass it'll even
if you create your own
um formula for computing an answer it
will automatically compute the back prop
the the derivative for you in a lot of
cases it's easier to implement
distributed computation uh so cross is
really nice way to package it and get it
off on different computers and share it
and it allows parallelism which means
that two operations can run
simultaneously so as we start developing
these backends it can do all kinds of
cool things and utilize multiple cores
gpus on a computer uh to get that
parallel processing
up what are neural networks well like I
said there are already uh we talked
about in computational edges you have a
node and you have a connection or your
Edge so neural networks are algorithms
fashioned after the human brain which
contain multiple layers each layer
contains a node called a neuron which
performs a mathematical operation they
break down complex problems into simple
operations so one an input layer takes
in our data and pre-processes it when we
talk about pre-processing when you're
dealing with neural networks uh you
usually have to pre-process your data so
that it's between minus one and one or
zero and one um into some kind of value
that's usable that occurs before it gets
to the neural network uh in fact 80% of
data science is usually impr prepping
that data and getting it ready for your
different
models two you have hidden layer
performs a non linear transformation of
input now it can do a hidden a linear
transformation it can use just a basic
um ukian geometry and you could think of
a node adding all the different
connections coming in uh so each
connection would have a weight and then
it would add to that weight plus an
intercept um in the node itself so you
can actually use ukian geometry but a
lot of these get really complicated they
have all these different formulas and
they're really cool to look at but when
you start looking at them look at how
they work uh you really don't need to
know the high math behind it um to
figure them out and figure out what
they're doing which is really cool that
means a lot of people can use this
without having to go get a PhD in
mathematics number three the output
layer takes the results from hidden
layer transform them and gives a final
output so sequential models uh so what
makes this a sequential model sequential
models are linear stacks of layers where
one layer leads two to the next it is
simple and easy to implement and you
just have to make sure that the previous
layer is the input to the next layer so
uh you have used for plain stack of
layers where each layer has one input
and one output tensor and this is what
tensor flow is named after is um each
one of these layers is like a tensor
each each node is a tensor and then the
layer is also considered a tensor
values and it's used for simple
classifier declassify models you can
it's also used for regression models too
so it's not just about uh this is
something this is a teapot this is a cat
this is a dog um it's also used for
generating um uh reget the actual values
you know this is worth $10 that's worth
$30 uh the weather's going to be 90 out
or whatever it is so you can use it for
both classifier and declassify uh
models and one more note when we talk
about sequential models the term
sequential is is used a lot and it's
used in different areas and different
notations when you're in data science so
when we talk about time series we'll
talk about sequential that is something
very different uh sequential in this
case means it goes from the input to
layer one to Layer Two to the output so
it's very directional it's important to
note this because if you have a
sequential model can you have a
non-sequential model and the answer is
yes uh if you master the basics of a
sequential model you can just as easily
have another model that shares layers um
you can have another model where they
you have an input coming in and it
splits and then you have one set that's
doing one set of uh nodes maybe they're
doing a yes no kind of node where it's
either putting out a zero or a one a
classifier and the other one might be
regression it's just processing numbers
and then you recombine them for the
output um that's what they call a cross
uh the cross
API so there's a lot of different
availabilities in here and all kinds of
cool things you can do as as far as
encoding and decoding and all kinds of
things and you can share layers and
things like that we're just focusing on
the basic cross model with the
sequential
model so let's dive into the meat of the
matter let's do and do a demo on here uh
today's demo in this demo we will be
performing flower classification using
sequential model and cross and we'll use
our model to classify between five
different types of
flowers
now for this demo and you can do this
demo on whatever platform you want or
whatever um user interface for
developing python um I'm actually using
anaconda and then I'm using Jupiter
notebooks to develop in and if you're
not familiar with this um you can go
under environment once you've created
environment you can come in here to open
a terminal window and if you don't have
the different modules in here you can do
your AA install whatever module it is um
just happened that this particular setup
didn't have a caborn in it which I
already installed uh so here's our
anaconda and then I'm going to go
back and start up my Jupiter
notebook where I already created a uh
new uh python project Python 3 I'm in
Python 3.8 on this particular one um
sequential model for flowers so lots of
fun there uh so we're going to jump
right into this the first thing is to
make sure you have all your modules
installed so if you don't have a numpy
pandas matplot library and Seaborn in
the carass um and sklearn or S kit it's
not actually sklearn you'll need to go
ahead and install all of those now
having done this for years and having
switched environments and doing
different things um I get all my imports
done and then we just run it and if we
get an error we know we have to go back
and install something um right off the
bat though we have numpy pandas M plot
Library Seaborn these are built on top
of each other pandas the data frame and
built on top of numpy the uh um data
array and then we bring in our SK learn
or S kit this is the S kit setup SCI uh
kit even though you use sklearn to bring
it in it's a s kit and then our carass
we have our pre processing the images
image data generator um our model this
is our basic model or sequential
model uh and then we bring in from coros
layers uh import dents um
optimizers these optimizers a lot of
them already come in these are your
different optimizers and it's almost a
lot of this is so automatic now um atom
is the a lot of times the default
because you're dealing with a large data
uh and then we get our SGD which is uh
smaller data does better on smaller
pieces of data and I'm not going to go
into all of these uh different
optimizers we didn't even use these in
the actual demo you just have to be
aware that they are different optimizers
and the Digger the more you dig into
these models um you'll hit a point where
you do need to play with these a little
bit but for the most part leave it at
the default when you're first starting
out and we're doing just the sequential
you'll see here layers
dense and then if we come down a little
bit more uh when they put this together
and they're running the dense layers
you'll also see they have Dropout they
have flatten they have activation uh
they have the uh convolutional layer 2D
Max pooling 2D batch
normalization what are all these layers
uh and when we get to the model we're
going to talk about them uh a lot of
times when you're just starting you can
just uh uh import cross. layers and then
you have your Dropout your flatten uh
your convolutional uh neural network 2D
and we'll we'll cover what these do in
the actual example when we get down
there uh once you take from here those
you need to run your Imports um and load
your different aspects of this and of
course your tensor flow TF because this
is all built on tensor
flow and then finally uh import random
as RN just for random
generation and then we get down here we
have our uh
CV2 that is your um open image or your
open CV they call it for processing
images that's what the CV2
is uh we have our
tqdm the tqdm is for um is a progress
bar just a fancy way um of adding when
you're running a process you can view
the bar going across in the Jupiter uh
setup not really necessary but it's kind
of fun to have um we want to be able
Shuffle some files uh again these are
all different things pill is another um
image processor it goes with the CV2 a
lot of times you'll see both of those
and so we run those we got to bring them
all
in and the next thing is to set up our
directories and so when we come into the
directories there's an important thing
to note on here other than we're looking
at a lot of flowers which is
fun uh is we get down here we have our
directory
archive flowers that just happens to be
where the different files for different
flowers are put in we're denoting an X
and a z and the x is the data of the
image and the Z is the tag for it what
kind of flower is this uh and the image
size is really important because we have
to resize everything if you have a
neural network and if you remember from
our neural networks uh let me flip back
to that
slide when we look at this slide we have
two inputs nodes here uh with an image
you have an input node depending on how
you set it up for each pixel and that
pixel has three different color schemes
usually in it sometimes four so if you
have a picture that's 150 by
150 uh you multiply 150 * 150 * 3 that's
how many nodes input layers coming in I
mean this is a massive input a lot of
times you think oh yeah it's just a a
small amount of data or something like
that uh no it's a image coming in then
you have your hidden layers A lot of
times they match what the image size is
coming in so each one of those is also
just as big and then we get down to just
a single output so that's kind of a a
thing to note in here what's going on
behind the scenes and of course each one
of these layers has a lot of processes
and stuff going
on and then we have our our different uh
directories on here let me go and run
that so I'm just setting the directories
that's all this is um archive flowers
Daisy sunflower flower tulip dandelion
Rose uh just our different directories
that we're going to be looking
at uh and then we want to go ahead and
we need to assign labels remember we
defined x and
z so we're just going to create a uh uh
definition here um and the first thing
is uh return flower type okay just
returns it what kind of flower it is I
guess assign label to it uh but we're
going to go ahead and make our train
data and when you look at this there's a
couple things to take away from here uh
the first one is we're just appending
right onto our numpy array the image
we're going to let numpy handle all that
different aspects as far as 150 by 150
by 3 uh we just dump it right into the
numpy which makes it really easy we
don't have to do anything funky on the
processing and we want to leave it like
that and I'm going to talk about that in
a minute uh and then of course we have
to have the string a pin the label on
there and I want you to notice right
here uh we're going to read the image
in and then we're going to size it and
this is important because we're just
changing this to 150 by 150 we're
resizing the image so it's uniform every
image comes in identical to the other
ones uh this is something that's so
important is um when you're resizing or
reformatting your data you really have
to be aware of what's going on with
images it's not a big deal because with
an image you just resize it so it looks
squishy or spread out or stretched um
the neural network picks up on that and
it doesn't really change how it
processes
it so let's go ahead and run that uh and
now we've got our definition set up on
there and then we want to go ahead and
make our
uh training data uh so make the train
data uh daisy flower daisy directory uh
print length of X so here we go let's go
and run that and we're just loading up
the flower daisy uh so this is going all
in there and it's setting um it's adding
it in to the our setup on there to our x
and z setup and we see we have
769 um and then of course you can see
this nice bar here this is the bar going
across is that little added uh code in
there that just makes it really cool for
doing demos uh not necessarily when
you're building your own model or
something like that but if you're going
to display this to other people adding
that little what was it called
um tqdm I can never remember that uh but
the tqdm module in there is really nice
and we'll go ahead and do sunflowers and
of course you could have just uh created
an array of these um but this has an
interesting problem that's going to come
up and I want to show you something it
doesn't matter how good the people in
the back are how good you are at
programming errors are going to come up
and you got to figure out how to handle
them uh and so when we get all the way
down
to the um where is it dandelion here's
our dandelion directory we're going to
build
um Jupiter has some cool things it does
which makes this really easy to deal
with but at the same time you would want
to go back in there depending on how
many times you rerun this how many times
you pull this so when you're finding
errors
uh I'm going in here there's a couple
things you can do and we're just going
to um oh it wasn't there it is there's
our error I knew there was an error this
processed
1,62 out of
1065 now I can do a couple things one I
could go back into our definition and I
can just put in here try and so if it
has a bad conversion because this is
where the error is coming from uh just
skip it that's one way to do it um when
you're doing a lot of work in data
science and you look at something like
this where you're losing three points of
uh data at the end you just say okay I
lost three points who cares um or you
can go in there and try to delete it um
it really doesn't matter for this
particular demo and so we're just going
to leave that error right alone and skip
over because it's already added all the
other files in there and this is
wonderful thing about Jupiter notebook
is that I can just continue on there and
the x and z which we're creating is
still
uh running and we'll just go right into
the next flower row so all these flowers
are in there um that's just a cool thing
about Jupiter
notebook uh and then we can go ahead and
just take a quick look and
see what we're dealing with and this is
of course really when you're dealing
with other people and showing them stuff
this is just kind of fun where we can
display it on the plot Library here and
we're just going to go through and um
let's see what we got here uh looks like
we're going to do like five of each of
them I think is that how they set this
up um plot Library 5 by two okay oh I
see how they did it okay so two each so
we have 5x two setup on our axes and
we're just going to go in and look at a
couple of these
flowers it's always a good thing to look
at some of your data uh no matter what
you're doing we've reformatted this to
150 by 150 you can see how it really
blurs this one up here on the Tulip that
is that resize to 150 by 150 um and
these are what's actually going in these
are all 150 by 150 images you can check
the dimensions on the side and you can
see uh just a quick sampling of the
flowers we're actually going to process
on here and again like I said at the
beginning most of your work in data
science is
reprocessing this different uh
information so we need to go ahead and
take our lay
labels uh and run a label encoder on
there and then we're just going to Le is
a label encoder one of the things we
imported and then we always use the
fit um to categorical y comma 5 uh
actually here's our array um X so if you
look at this here's our fit we're going
to transform
Z that's our Z array we
created um and then we have Y which
equals that and then we go ahead and do
uh to categorical we want find different
categories and then we create our x uh
inpay of x x = x over
255 so what's going on here there's two
different Transformations one we've
turned our categories into 0 1 2 3 4 5
as the output and we have taken our X
array and remember the X array is three
values of your different
colors this is so important to
understand when we do this across a num
array this takes every one of those
three colors so we have 150 by 150
pixels out of those 150 by 150 pixels
they each have three um color arrays and
those color arrays ra range from 0 to
250 so when we take the xal X over
255 I'm sorry range from 0 to 255 this
converts all those pixels to a number
between zero and one and you really want
to do that when you're working with
neural networks uh now if you do a
linear regression model um it doesn't
affect it as much and so you don't have
to do that conversion if you're doing
straight numbers but when you're running
neural networks if you don't do this
you're going to create a huge bias and
that means they'll do really good on
predicting one or two things and they'll
just totally die on a lot of other
predictions so now we have our um X and
Y values uh X being the data in y being
our no one
output and with any good setup we want
to divide this data into our training so
we have X train uh we have our X test
this is the data we're not going to
program the model with and of course
your y train corresponds to your X train
and your y test corresponds to your X
test the outputs and this is uh when we
do the train test split this was from
the S kit sklearn we imported train test
split and we're just going to go ahead
and do the test size at about a quarter
of the data 0.25 and of course random is
always good this is such a good tool I
mean certainly you can do your own
division um you know you could just take
the first you know 0.25 of the data or
whatever do the length of the data not
real hard to do but this is randomized
so that if you're running this test a
few times you can kind of get an idea
whether it's going to work or
not sometimes what I will do is um I'll
just split the data into three parts and
then I'll test it on two with one being
the uh or train it on two of those parts
with one being the test and I rotate it
so I come up with three different
answers which is a good way of finding
out just how good your model is uh but
for setting up let's stick with the XT
train X test and the SK learn
package and then we're going to go ahead
and uh do a random
seed uh now a lot of times the cross
actually does this automatically but
we're going to go ahead and set it up on
here and you can see we did an NP random
seed um from 42 and we get a nice RN
number um and then we do TF random we
set the seed so you can set your
Randomness at the beginning of your
tensor flow and that's what the tf.
random. set
is so that's a lot of prep um all this
prep and then we finally get to the
exciting part um this is where you
probably spend once you have the data
prepped and you have your pipeline going
and you have everything set up on there
this is the part that's exciting is
building these models
and so we look at this model one we're
going to designate it sequential um they
have the API which is a coros the coros
tensorflow API versus sequential
sequential means we're going one layer
to the next so we're not going to split
the layer and bring it back together it
looks almost the same with the exception
of um bringing it back together so it's
not a huge step to go from this to an
API and the first thing we're going to
look at is um our convolutional neural
Network in 2D uh so what's going on here
there's a lot of stuff that's going on
here um the default for well let's start
with the beginning what is a
convolutional 2d
Network well a convolutional 2d Network
creates a number of small windows and
those small Windows float over the
picture and each one of them is their
own neural network and it's basically um
becomes like a um a categorization and
then it looks at that and it says if we
add these numbers up a certain way uh we
can find out whether this is the right
flower based on this this little window
floating around which looks at different
things and we have filters 32 so this is
actually creating 32 Windows is what
that's
doing and the kernel size is 5x5 so
we're looking at a 5x5 Square remember
it's a 150 by 150 so this narrows it
down to a 5x5 it's a 2d so it has your X
Y coordinates um and when we look at
this 5x5 remember each one of these is
is actually looking at 5x5
by3 uh so we're actually looking at 15
by 15 different um
pixels and padding is just um uh usually
I just ignore that activation by default
is railu we went ahead and put the railu
in
there there's a lot of different
activations railu is for your smaller uh
when you remember I mentioned atom when
you have a lot of data data use an atom
kind of activation or use an atom
processing we're using the railu here uh
it kind of gives you a yes or no but it
it doesn't give you a full yes or no it
has a um a zero and then it kind of
shoots off at an angle very common it's
the most common wand and then of course
here's our input shape 150 by 150 by 3
pixels and then we have to pull it so
whenever you have a two convolutional 2D
um uh layer we have to bring this back
together and pull this into uh neural
network and then we're going to go ahead
and repeat
this uh so we're going to add another
Network here one of the cool things if
you look at this is that it as it comes
in it just kind of automatically assumes
you're going down to the next layer and
so we have another convolutional null
network uh 2D here's our Max pooling
again we're going to do that again again
Max pooling uh and we're just going to
filter on down now one of the things
they did on this one is they changed the
kernel size they change the number of
filters and so each one of these steps
kind of looks at the data a little bit
differently and that's kind of cool
because then you get a little added
filtering on there this is where you
start playing with the model you might
be looking at a convolutional no network
which is great for image
classifications um we get down to here
one of the things we is flattened so we
had we just flatten it remember this is
150 by 150 by 3 well and actually the
pool size changes so it's actually
smaller than that flatten just puts that
into a 1D array uh so instead of being
you know a tensor of this really
complexity with the the pixels and
everything it's just flat and then the
DSE is just another activation on there
um by default it is probably Ru as far
as its
activation and then oh yeah here we go
in sequential they actually added the
activation as railu so this just because
this is sequential this activation is
attached to the dents uh and there's a
lot of different activations but Ru is
the most common one and then we also see
a soft Max uh soft Max is similar but it
has its own kind of variation and one of
the cool things you know what let me
bring this up because if we if you don't
know about these activations this
doesn't make
sense and I just did a quick go gole
search on images of tensorflow
activations um I should probably look at
which website this is but this is the
output of the values uh so as your X as
it adds in all those uh weighted X
values going into the node it's going to
activate it a certain way that's a
sigmoid activation and you can see it
goes between zero and one and has a nice
curve there this also shows the
derivatives um and if we come down the
seven popular activation functions
nonlinear activations there's a lot of
different options on this let me see if
I can find
the O let me see I can find this
specific to
railo so this is a leaky
railu and you can see instead of it just
being zero and then a value between uh
going up it has a little leaky there
otherwise your railu loses some noes
they just become inactive um but you can
see there's a lot of different options
here here's a good one right here with
the ru you can see the rilu function on
the upper on the upper left here and
then the Leaky railo over here on the
right which is very commonly used
also one of the things I use with
processing um language is the S is the
exponential one or the tangent H the
hyperbolic tangent because they have
that nice uh funky curve that comes in
that um has a whole different meaning
and captures word use better again these
are very specific to domain and you can
spend a lot of time I'm playing with
different models for our basic model uh
we'll stick to the railu and the softmax
on here and we'll go and run and build
this
model so now that we've had fun playing
with all these different models that we
can add in there uh we need to go ahead
and have a batch size on here uh
128 epics
10 this means that we're going to send
128 uh rows of data or flowers at a time
to be Pro processed and the Epic 10
that's how many times we're going to
Loop through all the data reduce um the
values and verbose verbose equals one
means that we're going to show what's
going on um value monitor what we're
monitoring we'll see that as we actually
train the model this is what's what's
going to come out of there if you set
the verbos equal to zero um you don't
have to watch it train the model
although it is kind of nice to actually
know what's going on
sometimes
and since we're still working on U
bringing the data in here's our batch
site here's our epics we need to go
ahead and create a data generator uh
this is our image data
generator and it has all the different
settings in here almost all of these are
defaults uh so if you're looking at this
going oh my gosh this is confusing most
of the time you can actually just ignore
most of this um vertical flip so you can
randomly flip pictures you can randomly
horizontally flip them um you can shift
the picture around this kind of helps
gives you multiple data off of them uh
zooming rotation there's all kinds of
different things you can do with images
most of these we're just going to leave
as false we don't really need to do all
that um um setup because we already have
a huge amount of data if you're short
data you can start flipping like a
horizontal picture and it will generate
it's like doubling your data almost um
so the upside is you double your data
the downside is that if you already have
a bias in your data you already have um
5,000 sunflowers and only two roses
that's a huge bias it's also going to
double that bias uh that is the downside
of
that and so we have our model comp model
compile and this you're going to see in
all the carass we're going to take this
model here we're going to take all this
information as far as how we want it to
go and we're going to compile it this
actually builds the model and so we're
going to run that and I want you to
notice uh learning
rate very important this is the default
001 um there's there you really don't
this is how slowly it adjust to find the
right answer and the more data you have
you might actually make this a smaller
number um with larger with you have a
very small sample of data you might go
even larger than that and then we're
going to look at the loss categorically
categorical cross entropy most commonly
used
and this is uh how how much it improves
the model is improving is what this
number means or yeah that's that's
important on there and then the accuracy
we want to know just how good our model
is on the
accuracy and then uh one of the cool
things to do is if you're in a group of
people who are studying the model if
you're in shareholders you don't want to
do this is you can run the model summary
I do this by default and you can see the
different layers that you built into
this model just a quick summary on there
so we went ahead and we're going to go
ahead and create a
um we'll call it history but we want to
do a model fit
generator and so what this history is
doing is this is tracking what's going
on as while it fits the
model now there's a lot of new setups in
here where they just use fit and then
you put the generator in here um we're
going to leave it like this even though
the new default
is a little different on that doesn't
really matter it does the same thing and
we'll go ahead and just run
that and you can see while it's running
right here uh we're going through the
epics we have one of 10 now we're going
through six of 25 here's our loss we're
printing that out so you can see how
it's improving and our accuracy the
accuracy gets better and better and this
is 6 out of 25 this is going to take a
couple minutes to process uh because we
are training 150 by 150 by 3 pixels
across uh six layers or eight layers
whatever it was that is a huge amount of
processing so this will take a few
minutes to process this is when we talk
about the hardware and the problems that
come up in data science and why it's
only now just exploding being able to do
neural networks this is why this process
takes a long
time now you should have seen a jump on
the screen here because I did uh uh
pause the recorder to let this go ahead
and run all the way through its epics
let's go ahead and take a look and see
what these epics are and um if you set
the verbos to uh zero instead of one it
won't show what's going on in the behind
the scenes has it's training it so when
we look at this epic 10 epic so we went
through all the data 10 times uh if I
remember correctly there's roughly a gig
of data there so that's a lot of data
the first thing you're going to notice
is the 270 seconds um that's how much
each of those epics took to run and so
if you divide 60 in there you roughly
get about 5 minutes worth of each epic
so if I have 10 epics that's 50 minutes
almost an hour of
runtime that's a big deal when we talk
about processing uh in on this
particular computer um I actually have
what is it uh uh eight cores with 16
dedicated threads so it runs like a 16
core computer it alternates the threads
going in and it still takes it 5 minutes
for each one of these epics so you start
to see that if you have a lot of data
this is going to be a problem if you
have a number of models you want to Fe
find out how good the models are doing
and what model to use and so each of
those models could take all night to run
in fact I have a model I'm running now
that takes over uh takes about a day and
a half to test each model um and takes
four days to do with the whole data uh
so what I do is I actually take a small
piece of the
data test it out to find out uh get an
idea of of how the different setups are
going to do and then I increase that
size of the data and then increase it
again and I can just take that that
curve and kind of say okay if um the
data is doing this then I need to add in
more dense layers or whatever uh so you
can do a small chunks of data then
figure out what it costs to do a large
set of data and what kind of model you
want the loss as we see here continues
to go down uh this is the error this is
how much errors is in there
it really isn't a um userfriendly number
other than the more it Trends down the
better and so if you continue to see the
loss going down eventually get to the
point where it stops going down and it
goes up and down and kind of waivers a
little bit that point you know you've
run too many epics you're you're
starting to get a bias in there and it's
not going to give you a good model fit
the accuracy just turns us into
something that uh we can use and so the
accuracy is what percentage of guesses
in this case it's categorical so this is
the percentage of guesses are correct um
value loss is similar you know it's a
minus a value loss and then you have the
value accuracy and you'll see the value
accuracy is pretty similar to the
accuracy um just rounds it off basically
and so a lot of times you come down here
and you go okay we're doing 0.5
6 7 and that is 70% accuracy or in this
case 68 uh 59% accuracy that's a very
usable number and it's very important to
have if you're identifying flowers
that's probably good enough if you can
get within a close distance and knowing
what flower you're you're identifying uh
if you're trying to figure out whether
someone's going to die from a heart
attack or not might want to rethink it a
little bit or uh Rey how you're building
your model so if I'm working with a uh
uh a group of um clients um shareholders
in a company or something like that you
don't really want to show them this um
you don't want to show them hey you know
this is what's going on with the
accuracy these are just numbers and so
we want to go and put the finishing
touches just like when you are building
a house and you put in the frame and the
trim on the house it's nice to have
something a nice view of what's going on
and so we'll go ahead and do a pie plot
and we'll just plot the history of the
loss uh the history of the value
loss over here um epics train and test
and so we're just going to compute these
this is really important important uh
and what I want you to notice right here
is when we get to about oh five epics a
little more than five six epics you see
a cross over here and it starts Crossing
as far as the um uh value loss and
what's going on here is you have the
loss in your actual model and your
actual data and you have the value loss
where it's testing it against the the
test data the the data wasn't used to
program your model wasn't used to train
your model on and so when we see this
crossing over this is where the bias is
coming in this is becoming overfitted
and so when you put these two together
uh right around five and six you start
to see how it does this this switch over
here and that's really where you need to
stop right around five yeah six um it's
always hard to guess because at this
point the model's kind of a black box uh
so but you know that right around here
if you're saving your model after each
run you want to use a one that's right
around five epics cuz that's the one
that's going to have the least amount of
bias so this is really important as far
as guessing what's going on with your
model and its accuracy and when to stop
uh it also is you know I don't show
people this mess up here um I show
somebody this kind of model and I say
this is where the training and the
testing comes in on this model uh it
just makes it easier to see and people
can understand what's going
on so that completes our demo and you
can see we did what we were set out to
do we took our flowers and we're able to
classify them uh within about you 68 70%
accuracy whether it's going to be a
dollia sunflower cherry blossom Rose um
a lot of other things you can do with
your output as far as a uh different
tables to see where the errors are
coming from and what problems are coming
up and we're going to take a look at
image classification using carass and
the basic setup we'll actually look at
two different demos on here
uh what's in it for you today what is
image
classification Intel image
classification data creating neural
networks with carass and the vgg16
model what is image
classification the process of image
classification refers to assigning
classes to an entire image images can be
classified based on different categories
like weather it is a nighttime or
daytime shot what the image represents
Etc and you can see here we have uh
mountains looking for mountains we'll
actually be doing some uh uh pictures of
scenery and stuff like that in deep
learning we perform image classification
by using neural networks to extract
features from images and classifies them
based on these
features and you can see here where it
says like what computer sees and this is
oh yeah we see mostly Forest maybe a
little bit of mountains because the way
the image is um and this this is really
where one of the areas that neural
networks really shines um if you try to
run this stuff through uh more like a
linear regression model you'll still get
results uh but the results kind of miss
a lot of things as they as the neural
networks get better and better at what
they do with different tools we have out
there uh so Intel image classification
data the data being used is the Intel
image classification data set which
consist of images of six types of land
areas and so we have Forest building
glaciers and Mountain sea and Street uh
and you can see here there's a couple of
the images out of there as a setup in
the in the um uh Intel image
classification data that they
use and then we're going to go into
creating a neural networks with
carass the convolutional neural network
that we are creating from scratch looks
uh as showing
below you see here we have our input
layer
um they have a listed Max pulling uh so
you have as you're coming in with the
input layer and this the input layer is
actually um before this but the first
layer that it's going to go into it's
going to be a convolutional neural
network uh then you have a Max pooling
that pulls those the the convolutional
neural networks returns uh in this case
they have two of those that is very
standard with convolutional neural
networks uh one of the ones that I was
looking at earlier that was standard
being used by um I want one of the
larger companies I can't remember which
one for doing a large amount of
identification had two convolutional
neural networks each with their Max
pooling and then about 17 dense layers
after it we're not going to do that
heavy duty of a of a code but we'll get
you head in the right direction and that
gives you an idea of what you're
actually going to be looking at when you
look at the flattened part and then the
dents we're talking like 17 dense layers
afterwards uh I find that a lot of the
stuff I've been working on I end up
maxing it out right around nine dense
layers it really depends on what you
have going in and what you're working
with and the vgg16
model uh vgg16 is a pre-trained CNN
model which is used for image
classification it is trained on a large
varied data set and fine-tuned to fit
image classification data sets with ease
and you can see down here we have the
input coming in uh the convolutional
neural network 1:1 one: two and then
pooling and then we do 2:1 2:2
convolutional Network then pooling 3:2
and you can see there's just this huge
layering of convolutional neural
networks and in this case they have five
such layers going in and then three
dents going out or uh more now when they
took this setup this actually one and a
Ward uh back in 2019 for this particular
setup uh and it does it does really good
except that again um we only show the
three dense layers here and as you find
out depending on your data going in what
you have set up uh that really isn't
enough on one of these setups and I'm
going to show you why we restricted it
because it does take up a lot of
processing power and some of these
things so let's go Ahad and rope our
sleeves and we're going to look at both
the setups we're going to start with the
um the first
classification um and then we'll go into
the vgg16 and show you how that's set up
now I'm going to be using anaconda and
let me flip over to my anaconda so you
can see what that looks like now I'm
running in the Anaconda here uh you'll
see that I've set up a main python uh 38
I always put that in there this is where
I'm doing like most of my kind of
playing around uh this is done in Python
version 3.8 we're not going to dig too
much into versions uh at this point you
should already have carass installed on
there usually carass takes a number of
extra
steps and then our usual um uh setup is
the numpy the pandas uh your s your s
kit which is going to be the sklearn
your caborn and I'll I'll show you those
in just a minute um and then I'm just
going to be in the Jupiter lab where
I've created a new um notebook in here
and let's flip on over there to my blank
notebook now I'm there's a couple cool
things to note in here is that um one I
use the the um Anaconda Jupiter notebook
setup because it keeps everything
separate except for carass uh carass is
actually running separately in the back
I believe it's a a c program uh what's
nice about that is that it utilizes the
multiprocessors on the computer and I'll
mention that just in a little bit when
we actually get down to running the
code and when we look in here uh a
couple things to note is here's our uh
um oops I thought I grabbed the other
drawing thing uh but here's our numpy
and our pandas right here in our
operating system this is our s kit you
always import it as sklearn for the
classification report uh we're going to
be using well usually import like
Seaborn brings in all of your pip plot
Library
also kind of nice to throw that in there
I can't remember if we're actually using
caborn if they just the people in the
back just threw that together um and
then we have the sklearn shuffle for
shuffling data here's our map plot
library that the caborn is pretty much
built on um CV2 if you're not familiar
with that that is our image um module
for importing the image and then of
course we have our tense or flow down
here which is what we're really working
with
and then the last thing is just for
visual effect while we're running this
um if you're doing a demo and you're
working with uh the partners of the
shareholders uh this tqdm is really kind
of cool it's an extensible progress bar
for Python and I'll show you that too
remember data science is not I mean you
know most of this code when I'm looking
through this code I'm not going to show
half of this stuff to the shareholders
or anybody I'm working with they don't
really care about pandas and all that we
do because we want to understand
understand how it
works uh so we need to go ahead and
import those different um uh setup on
there and then the next thing is we're
going to go ahead and set up our
classes uh now we remember if we had
Mountain Street Glacier building sea and
Forest those are the different images
that we have coming
in and we're going to go ahead and just
do class name labels and we're going to
kind of match that class name of I for I
class name uh equals the class name
so our labels are going to match the
names up
here uh and then we have the number of
uh
classes and print the class names and
the labels and we'll go ahead and set
the image size this is important that we
resize everything because if you're
remember with neural
networks they take one size data coming
in and so when you're working with
images you really want to make sure
they're all resized to the same uh setup
it might squish them it might stretch
them that generally does not cause a
problem in these uh and some of the
other tricks you can do with if you if
you need more data um and this is one
that's used regularly we're not going to
do it in here is you can also take these
images and not only resize them but you
can til them one way or the other crop
parts of them um so they process
slightly differently and it'll actually
increase your accuracy of some of these
predictions uh and so you can see here
we have Mountain equals zero that's what
this class name label is
Street equals 1 Glacier equals 2
buildings equals 3 C4 Forest equals
5 now we did this as an enumerator so
each one is 0o through five uh a lot of
times we do this instead as um uh uh 0
one 0 one 01 so you have five outputs
and each one's a zero or a one coming
out so the next thing we really want to
do is we want to go ahead and load the
data up and just put a label in there
loading
data just just so you know what we're
doing we going to put in the loading
data down here uh make sure it's well
labeled uh and we'll create a definition
for this and this is all part of your
preprocessing at this point you could
replace this with all kinds of different
things depending on what you're working
on and if you once you download you can
go download this data set uh send a note
to the simply learn team here in YouTube
um and they'll be happy to direct you in
the right direction and make sure you
get this path here um so you have it
right whatever wherever you saved it a
lot of times I'll just abbreviate the
path or put it as a sub thing and just
get rid of the directory um but again
double check your paths um we're going
to separate this into a segment for
training and a segment for testing and
that's actually how it is in the folder
let me just show you what that looks
like so when I have my lengthy path here
where I keep all my programming simply
learn this particular setup we're
working on image classification and
image classification clearly you
probably wouldn't have that lengthy of a
list and when we go in here uh you'll
see sequence train sequence test they've
already split this up this is what we're
going to train the data in and again you
can see buildings Forest Glacier
Mountain Sea Street uh and if we double
click let's go under Forest you can see
all these different Forest uh images and
and there's a lot of variety here I mean
we have wintertime we have summertime
um so it's kind of interesting you know
here's like a Fallen Tree versus um a
road going down the middle that's really
hard to train and if you look at the
buildings A lot of these buildings
you're looking up a skyscraper we're
looking down the
setup here's some trees with one I want
to highlight this one it has trees in it
uh let me just open that up so you can
see it a little
closer the reason I want to highlight
this is I want you to think about this
we have trees growing is this the city
or a
forest um so this kind of imagery makes
it really hard for a classifier and if
you start looking at these you'll see a
lot of these images do have trees and
other things in the foreground weird
angles really a hard thing for a
computer to sort out and figure out
whether it's going to be a forest or a
um
City and so in our loading of data uh
one we have to have the path the
directory we're going to come in here we
have our images and our labels so we're
going to load the images in one section
the labels in another
um and if you look through here it just
goes through the different folders uh in
fact let me do this let
me there we go uh as we look at this
we're just going to Loop through the
three the six different folders that
have the different Landscapes and then
we're going to go through and pull each
file
out and each label uh so we set the
label we set the folder for file and
list uh here's our image path join the
paths this is all kind of n General
stuff um so I'm kind of skipping through
it really
quick and here's our image setup uh if
you remember we're talking about the
images we have our CV2 reader so it
reads the the image in uh it's going to
go ahead and take the image and convert
it to uh from blue green red to red
green green blue this is a CV2 thing um
almost all the time it Imports it and
instead of importing it as a standard
that's used just about everywhere it
Imports it with the BGR versus RGB um
RGB is pretty much a standard in here
you have to remember that was CV2 uh and
then we're going to go ahead and resize
it this is the important part right here
we've set a we've decided what the size
is and we want to make sure all the
images have the same size on
them and then we just take our images
and we're just going to pin the image
pin the label um and then the images
it's going to turn into a numpy array
this just makes it easier to process and
manipulate and then the labels is also a
numpy array and then we just return the
output append images and labels and we
return the output down
here so we've loaded these all into
memory uh we haven't talked to much
there'd be a different setup in there
because there is ways to feed the files
directly into your cross model uh but we
want to go ahead and just load them all
it's
really for today's processing and that
what our computers can handle that's not
a big deal and then we go ahead and set
the uh train images train labels test
images test labels and that's going to
be returned in our output of pinned and
you can see here we did um
uh images and labels set up in there and
it just loads them in there so we'll
have these four different categories let
me just go ahead and run
that
uh so now we've gone ahead and loaded
everything on
there and then if you remember from
before uh we imported just go back up
there Shuffle here we go here's our
sklearn utilities import Shuffle and so
we want to take these labels and shuffle
them around a little bit um just mix
them up so it's not having the same if
you run the same process over and over
uh then you might run into some problems
on
there and just real quick let's go ahead
and do uh um a plot so we just you know
we we've looked at them as far as from
outside of our code we pulled up the
files and I showed you what that was
going on we can goe and just display
them here too and tell you when you're
working with different
people this should be highlighted right
here um this thing is like when I'm
working on code and I'm looking at this
data and I'm trying to figure out what
I'm doing I skip this process the second
I get into a meeting and I'm showing
what's going on to other people I skip
everything we just did so and go right
to here where we want to go ahead and
display some images and take a look at
it and in this display
um I've taken them and I've resized the
images to 20 by
20 that's pretty small uh so we're going
to lose just a massive amount of detail
and you can see here these nice
pixelated images um I might even just
stick with the folder showing them what
images we're
processing uh again this is you got to
be a little careful this maybe resizing
it was a bad idea um in fact let me try
it without resizing it and see what
happens
oops so I took out the image size and
then we put this straight in here one of
the things again this is
um put the D there we go one of the
things again that we want to
know whenever we're working on these
things uh is the
CV2 there are so many different uh image
classification setups it's really a
powerful package when you're doing
images but you do need to switch it
around so that it works with the High
plot and so make sure you take your
numpy array and change it to a u integer
8 format uh cuz it comes in as a float
otherwise you'll get some weird images
down there um and so this is just
basically we split up our we've created
a plot we went ahead and did the plot 20
by 20 um or the plot figure size is 20
by 20 um and then we're doing 25 so a
5x5 subplot um nothing really going on
here too exciting but you can see here
where we get the images and really when
you're showing people what's going on
this is what they want to see uh so you
skip over all the code and you have your
meeting you say okay here's our images
of the
building um don't get caught up in how
much work you do get caught up in what
they want to see so if you want to work
in data science that's really important
to
know and this is where we're going to
start uh having fun uh here's our model
model this is where it gets exciting
when you're digging into these models
and you have here uh let me
get there we
go when you have here if you look here
here's our convolutional neural network
uh
2D and uh 2D is an image you have two
different dimensions x y and even though
there's three colors it's still
considered 2D if you're running a video
you'd be convolutional neural network 3D
if you're doing a series going across um
a Time series that might be
1D and on these you need to go ahead and
have your convolutional n network if you
look here there's a lot of really cool
settings going on to dig into um we have
our input shape so everything's been set
to 150 by 150 uh and it has of course
three different color schemes in it
that's important to notice um
activation default is railu uh this is
small amounts of data being processed on
a bunch of little um neural
networks and right here is the 32 that's
how many of these convolutional null
networks are being strung up on here and
then the
3X3 uh when it's doing its steps it's
actually looking at uh a little 3x3
Square on each image and so that's
what's going on here and with
convolutional noral networks the
window floats across and adds up all
these numbers going across on this data
and then eventually it comes up with 30
in this case 32 different feature
options uh that it's looking for and of
course you can change that 32 you can
change the 3X3 so you might have a
larger setup you know if you're going
across
150 um by 150 that's a lot of steps so
we might run this as 15 by 15 uh there's
all kinds of different things you can do
here we're just putting this together
again that would be something you would
play with to find out which ones are
going to work better on this setup um
and there's a lot of play
involved that's really where it becomes
an art form is guessing at what that's
going to be the second part I mentioned
earlier and I I can only begin to
highlight this um when you get to these
dense layers one is the activation is a
Ru they use a railu and a softmax here
um it's a whole a whole uh setup just
explaining why these are different um
and how they're different because
there's also an exponential there's a
tangent in fact uh there's just a ton of
these and you can build your own custom
activations depending on what you're
doing a lot of different things go into
these activations uh there are two or
three major thoughts on these
activations and Ru and softmax or uh
well Ru uh you're really looking at just
the number you're adding all the numbers
together and you're looking at ukian
geometry um ax + B X2 +
cx3 plus bias with softmax this belongs
to the party of um it's activated or
it's not except it's they call it
softmax because when you get to the to
zero instead of it just being zero uh
it's actually slightly a little bit less
than zero so that when it trains it
doesn't get lost um there's a whole
series of these activations another
activation is the tangent um where it
just drops off and you have like a very
narrow area where you have from minus
one to one or exponential which is 0 to
one so there's a lot of different ways
to do the
activation again we can do that'd be a
whole separate lesson on here we're
looking at the convolutional neural
network um and we're doing the two pools
this is is so common you'll see two two
convolutional n networks stacked on top
of each other each with its own Max pull
underneath and let's go ahead and run
that so we built our model there and
then we need to go ahead and
um compile the model so let's go ahead
and do
that uh we are going to use the atom uh
Optimizer the bigger the data the atom
fits better on there there's some other
Optimizer but I think atom is a default
um I don't really play with the
optimizer too much that's like the if
once you get a model that works really
good you might try some different
optimizers uh but adoms usually the most
and then we're looking at a
loss pretty standard we want to minimize
our Lo we want
to maximize the loss of error and then
we're going to look at accuracy um
everybody likes say accuracy I'm going
to tell you right
now I start talking to people and like
okay what's what's the loss on this and
that and as a data science yeah I want
to know how the Lo what's going on with
that and we'll show you why in a minute
but everybody wants to see accuracy you
want to know how accurate this is uh and
then we're going to run the fit and I
wanted to do this just so I can show you
even though we're in a python setup in
here where Jupiter notebook is using
only a single processor I'm going to
bring over my little CPU Tool uh this is
eight cores on 16 dedicated threat so it
shows up as 16
processors and actually I got to run
this and then move it over so we're
going to run this and hopefully it
doesn't destroy my
mic uh and as it comes in you can see
it's starting to do go through the epics
and we said I set it for five epics and
then this is really nice because carass
uses all the different uh threads
available so it does a really good job
of doing that uh this is going to take a
while if you look at here it's um uh ETA
2 minutes and 25 seconds 24 seconds so
this is roughly two and a half minutes
per epic uh and we're doing five epics
so this is going to be done in roughly
15 minutes I don't know about you but I
don't think you want to sit here for 15
minutes watching The Green bars go
across so we'll go ahead and let that
run and there we go uh there was our 15
minutes it's actually less than that uh
because I did when I went in here
realized that uh uh where was
it here we go here's our model compile
here's our model FL uh fit and here's
our epics uh so I did four epics so a
little bit better more like 10 to 11
minutes instead of uh uh doing the full
uh 15 and when we look at this here's
our model we did talked about the
compiler uh here's our history we're
going to um history equals the model fit
we'll go into that in just a
minute and what we're looking at is we
have our epics um here's our validation
split so as we train it uh we're
weighing the accuracy versus you kind of
pull some data off to the side uh while
you're training it and the reason we do
that is that um you don't want to
overfit and we'll look at that chart in
just a
minute uh here's bat si side
this is just how many images you're
sending through at a time the larger the
batch it actually increases the
processing speed um and there's reasons
to go up or down on the batch size
because of the U the the smaller the
batch there's a certain point where um
you get too large of a batch and it's
trying to fit everything at once uh so I
128 is kind of big um depends on the
computer you're on what it can handle
and then of course we have our train
images and our train labels going in
telling it what we're going to train
on and then we look at our four epics
here uh here's our accuracy we want the
accuracy to go up and we get all the way
up to uh
83 or
83% uh this is actual percentage based
pretty much and we can see over here our
loss we want our loss to go down really
fluctuates uh 5 5 1.2
7748 uh so we have a lot of things going
on there let's go ahead and graph
those turn that off and our our team in
the back did a wonderful job of putting
together um this basic plot setup um
here's our subplot coming in we're going
to be looking at um uh from the history
we're going to send it the accuracy and
the value accuracy see labels and setup
on there um and we're going to also look
at loss and value loss so you can see
what this looks like what's really
interesting about this setup and let me
let me just go ahead and show you
because uh without actually seeing the
plots it doesn't make a whole lot of
sense uh it's just basic plotting of uh
of the data using the pi plot library
and I want you to look at this this is
really interesting um when I ran this
the first time I had very different
results um and they they vary greatly
and you can see here our accuracy
continues to
climb um and there's a cross over
here put it in here right here is our
crossover and I point that out because
as we get to the right of that crossover
where our accuracy um we're like oh yeah
I got 8% we're starting to get an
overfit here that's what this this
switch over
means um as our value um as a training
set versus the value um accuracy stays
the same and so that this is the one
we're actually really want to be aware
of and where it
crosses is kind of where you want to
stop at um and we can see that also with
the train loss versus the value loss
right here we did one Epic and look how
it just flat lines right there with our
loss so really one Epic is probably
enough and you're going to say wow okay
0.8% um certainly if I was working with
the shareholders um telling them that it
has an 80% accuracy isn't quite true and
and we'll look at that a little deeper
it really comes out here that the
accuracy of our actual values is closer
to0 41% right here um even after running
it this number of times and so you
really want to stop right here at that
cross cross over one Epic would have
been enough um so the data is a little
overfitted on this when we do four
epics and uh oops there we are
okay my drawing won't go away um let me
see if I can get there we
go um for some reason I've killed my
drawing ability on my
recorder all right took a couple extra
clicks uh so let's go ahead and take a
look at our actual test loss um so you
see where cross is over that's where I'm
looking at that's where we start
overfitting the model and this is where
if uh we were going to go back and
continually upgrade the model we would
start taking a look at the images and
start rotating them uh we might start
playing with the convolutional neural
network instead of doing the 3X3 window
um we might expand that or you know find
different things that might make a big
difference as far as the way it
processes these things um so let's go
ahead and take a click at our uh our
test loss now remember we had our
training data now we're going to look at
our test images and our test
labels for our test loss here and this
is just model evaluate uh just like we
did fit up here where was it um one more
model fit with our training data going
in now we're going to evaluate it on the
and and this data has not been touched
yet so this model's never seen this data
this is on uh completely new information
as far as the model is concerned of
course we already know what it is from
the labels we
have and this is what I was talking
about here's the actual accuracy right
here
0.48 uh or
4847 so this 49% of the Time guesses
what the image
is uh and I mean really that's a bottom
dollar
uh does this work for what you're
needing does 49% work do we need to
upgrade the model more um in some cases
this might be uh oh what was I doing I
was working on uh stock
evaluations and we were looking at what
stocks were the top
performers well if I get that 50%
correct on top
performers uh I'm good with that um
that's actually pretty good for stock EV
valuation in fact the number I had for
stock was more like U um 30 something
per as far as being a top performer
stock much harder to predict um but at
that point you're like well I'm you'll
make money off of that so again this
number right here depends a lot on the
domain you're working
on and then we want to go ahead and
bring this home a little bit more uh as
far as looking at the different setup in
here and one of the uh from SK learn if
you remember actually let's go back to
the top uh we had the classification
report and this came in from our sklearn
or S kit setup and that's right here you
can see it right here on the
um see there we go uh classification
report right here uh sklearn metrics
import classification report that's
we're going to look at
next a lot of this stuff uh depends on
who your working with so when we start
looking at um
Precision you know this is like for each
value I can't remember what one one one
was probably mountains so if 44% is not
good enough if if you're doing like um
you're in the medical department and
you're doing cancer is ITA is this
cancerous or not and I'm only 44%
accurate not a good deal you know I
would not go with that um so it depends
on what you're working with on the
different labels and what they're used
for Facebook you know 44% I'm guessing
the right person I would hope it does a
little bit better than that um but
here's our main accuracy this is what
almost everybody looks at they say oh
48% that's what's important um again it
depends on what domain you're in and
what you're working
with and now we're going to do the same
model o somehow I got my there it goes I
thought I was going to get stuck in
there again uh this time we're going to
be using the
vgg16 and remember this one is uh all
those layers going into it so it's
basically a bunch of convolutional n
networks getting smaller and smaller on
here uh and so we need to go ahead and
um import all our different stuff from
carass uh we're importing the main one
is the V g16 setup on there just aim
that there we go um
there's kind of a pre-processing images
um applications pre-process input this
is all part of the VG g16 setup on there
uh and once we have all those we need to
go ahead and create our
model and we're just going to create a
vgg16 model in here um inputs model
inputs outputs model inputs I'm not
going to spend as much time as I did on
the other one uh we're going to go
through it really quickly one of the
first things I would do is if you
remember in carass you can treat treat a
model like you would a
layer um and so at this point I would
probably add a lot of dense layers on
after the vgg16 model and create a new
model with all those things in there and
we'll go ahead and uh run this uh CU
here's our model coming in and our setup
and it'll take it just a moment to
compile that what's funny about this is
I'm I'm waiting for it to download the
um package since I prean this um it
takes it a couple minutes to download
the vgg16 model into here um and so we
want to go ahead and train features for
the model we're going to predict the
we're going to predict the train images
and we're going to test features on the
predict test images on
here and then I told you I was going to
create another model too and the people
on the back uh did not disappoint me
they went ahead and did just that and
this is really an important part um this
is worth stopping for I told you I was
going to go through this really quick so
here's our
uh we we have our model
two um coming in and we we've created a
model up here with the vgg16 model
equals model inputs model inputs and so
we have our
vgg16 this has already been
pre-programmed uh and then we come down
here and I want you to notice on this
um right here layer model two layers
minus 4 to1 x layer X um we're basically
taking this model and we're adding stuff
onto it and so uh we've taken we've just
basically duplicated this model we could
have done the same thing by using model
up here as a layer um we could have had
the input go to this model and then have
that go down here so we've added on this
whole setup this whole block of code
from 13 to 17 has been added on to our
vgg16 model and we have a new model uh
with the layer input and X down here
let's go ahead and run that and compile
it and that was a lot to go through
right there uh when you're building
these models this is the part that gets
so
complicated did you get stuck playing in
and yet it's so fun uh it's like a
puzzle how can I Loop these models
together
and in this case you can see right here
that the layers uh we're just copying
layers over and adding each layer in um
this is one way to build a new model and
we'll go ahead and run
that like I said the other way is you
can actually use the model as a layer I
had a little trouble playing with it uh
sometimes when you're using the Straight
model
over you run into issues
um it seems like it's going to work and
then you mess up on the input and the
output layers there's all kinds of
things that come
up let's go ahead and do the new model
we're going to compile it uh again
here's our metric accuracy sparse
categorical loss uh pretty
straightforward just like we did before
you got to compile the
model and just like before we're going
to take our create a history uh the
history is going to be uh new model fit
train
128 and just like before if you remember
when we started running this stuff we're
going to going to have to go ahead and
it's going to light up our uh setup on
here and this is going to take a little
bit to get us all set up uh it's not
going to just happen in in a couple
minutes so let me go ahead and pause the
video and run it and then we'll talk
about what
happened okay now when I ran that these
actually took about six minutes each um
so it's a good thing I put it on hold we
did four epics uh actually had to stopic
say at 10: and switch it to 4 CU I
didn't want to wait an
hour and you can see here our
accuracy um and our loss numbers going
down and just at a
glance it actually performed if you look
at the
accuracy.
2658 um so our accuracy is going down or
you know
26% um 34% 35% and you can see here at
some point it just kind of kicks a
bucket again this is
overfitting that's always an issue when
you're running on uh programming these
different neural
networks and then we're going to go
ahead and plot the accuracy um history
we built that nice little sub routine up
above so we might as well use it and you
can see it right
here um there's that crossover
again and if you look at this look how
the how the um uh the red shifts up how
the uh our loss functions and everything
crosses over we're over after one Epic
um we're
clearly not helping the problem or doing
better um we're just going to it's just
going to Baseline this one actually
shows with the training versus the
loss um value loss maybe second epic so
on here we're now talking more between
the first and the SE second epic and and
that also shows kind of here so
somewhere in here it starts
overfitting and right about now you
should be saying uhoh uh something went
wrong there I thought that um when we
went up here and ran this look at this
we have the accuracy up here is hitting
that
48% and we're down here
um you look at the score down here that
looks closer to 20% not nearly anywhere
in the ballpark of what we're looking
for and we'll go ahead and run it
through the uh the actual test features
here and and there it is um we actually
run this on the Unseen data and
everything8 or
18% um I don't know about you but I
wouldn't want you know at 18% this did a
lot worse than the other one I thought
this is supposed to be the super model
the model that beats all models the
vgg16 that won the awards and everything
well the reason is is that um one we're
not pre-processing the data uh so it
needs to be more there needs to be more
um as far as like rotating the data at
you know 45 degree angle taking partials
of it so you can create a lot more data
to go through here um and that would
actually greatly change the outcome on
here and then we went up here we only
added a couple dense layers um we added
a couple convolutional neural
networks this huge pre-trained setup is
looking for a lot more information
coming in
as far as how it's going to train and so
uh this is one of those things where I
thought it would have done better and I
had to go back and research it and look
at it and say why didn't this work why
am I getting only uh 18% here instead of
uh 44% or better and that would be wise
it doesn't have enough training data
coming in uh and again you can make your
own training data so it's not that we
have a shortage of data it's that that
some of that has to be switched around
and moved around a little bit and this
is interesting interesting right here
too if you look at the
Precision we're getting it on number two
and yet we had zero on everything else
so for some reason is not
seeing uh the different variables in
here so it'd be something else to look
in and try to find track down um and
that probably has to do with the input
but you can see right here we have a
really good solid Point 48 up here uh
and that's where I'd really go with is
starting with this model and then we
look at this model and find out why are
these numbers not coming up better is it
the data coming in um where's the setup
on there and that is the art of data
science right there is finding out which
models work better and why now let's
talk about our first deep learning
project we have image classification
using the cifar 10 data set now this
data set was created by the Canadian
Institute for advanced research the
cifar 10 data set contains 6,32 cross 32
color images in 10 different classes
the 10 different classes are airplanes
cars Birds cats deer and others which
you can see on the screen there are
6,000 images of each class now there are
50,000 training images and 10,000 test
images you can build a convolutional
neural network model using the Caris
library to classify each image into a
category so it is more of an image
recognition kind of a project that is
recommended for beginners who are new to
deep learning the next project in our
list is brain tumor detection
such projects are heavily used in the
healthcare Industries for detecting
diseases before they occur brain tumors
are the consequence of abnormal growths
and uncontrolled cell division in the
brain they can lead to death if they are
not detected early and accurately brain
tumors can be classified into two types
benign and
malignant deep learning can help
Radiologists in tumor Diagnostics
without invasive measures a deep
learning algorithm that has achieved
significant results in image
segmentation and classification is the
convolutional neural network it can be
used to detect a tumor through brain
using magnetic resonance imaging or MRI
image pixels are First Fed as input to
the CNN soft Max's fully connected
layers are used to classify the images
the accuracy of the convolutional neural
network can be obtained with the radial
basis function classifier next we have
an a chatbot now this chatbot is the
world's first open source conversation
platform which comes with a graphical
chart flow Des designer and chat
simulator it's supported channels of web
chat Android iOS and Facebook Messenger
the Anna chat board is available to
answer your questions 24 hours a day 365
days a year Anna is free for personal
and commercial use with the Anna Studio
server simulator and SDK which is
software development kit your
development time is cut from days to
hours using Anna you can create chat
Bots to suit your needs chat Bots play
an important role in customer support
for an e-commerce website you can seeare
realtime order and sipping updates
personalized product recommendations and
targeted offers within the conversation
similarly automobile Brands showrooms
and service centers can use a chatboard
for lead generation scheduling test
drives and roadside
assistance you can check the GitHub
repository that's on the screen to know
more about working on this project the
next project we have in deep learning is
image captioning now image captioning is
the process of generating textual
description of an image it uses both
methods from computer vision to
understand the content of the image and
a language model from the field of
natural language processing to turn the
understanding of the image into words in
the right order Microsoft has built its
own caption bot where you can upload an
image or the URL of an image and it will
display the textual description of the
image on the screen you can see we have
the picture of Alon musk and the caption
bot has generated a description that
says I think it's Alon musk wearing a
shoot and a tie and he seems it ends
with an
emoticon another such application that
suggests a perfect caption and best
hashtags for a picture is caption AI
automatic image caption generation
software can be built using recurring
neural networks and long shortterm
memory networks or
lstms now we have image colorization as
our next project image colorization has
seen significant advancements using deep
learning image colorization is the
process of taking an input of a
grayscale image and then producing an
output of a colored image or a colorized
image colorization is a highly
undetermined problem that requires
mapping a real valued luminance image to
a three-dimensional colored value one
that has not a unique solution chroman
is an example of a picture colorization
Model A generative network is framed in
an adversarial model that learns to
colorize by incorporating perceptual and
semantic understanding of color color
and class distributions the model is
trained using a fully supervised
strategy if you want to implement
chroman check the GitHub link that's on
the screen below next in our list of
projects we have open nmt machine
translation open nmt is an open source
ecosystem for neural machine translation
and neural sequence learning it started
in December 2016 by the Harvard NLP
group and sran the project has since
been used in several research and
Industry applications neural machine
translation or nmt is a new methodology
for machine translation that has led to
significant improvements particularly in
terms of human evaluation compared to
rule-based and statistical machine
translation systems open and empty
provides implementations in two popular
deep learning Frameworks P torch and
tensorflow please feel free to refer to
the below GitHub link to learn more
about neural machine
translation now we have music generation
using deep learning it is possible for a
machine to learn the notes structures
and patterns of Music and start
producing music automatically music 21
is a python toolkit used for
computerated musicology it allows us to
teach the fundamentals of Music Theory
generate music examples and study music
the toolkit provides a simple interface
to acquire the music notation of MIDI
files which stands for musical
instrument digital interface using webet
architecture and long softer mem
networks you can generate music without
human intervention Amper music mubert
and Juke deck produce smart music
powered by Deep learning
algorithms then we have Alpha go now
Alpha go was created by Google Deep Mind
and is the first computer program to
defeat a professional human go player
Alpha go algorithm uses a combination of
machine learning and Tre search
techniques combined with extensive
training both from human and computer
play it uses Monte Carlo tree search
guided by a value Network and a policy
Network both implemented using deep
neural network technology alphao was
initially trained to mimic human play by
attempting to match the moves of expert
players from recorded historical games
it was done using a database of around
30 million moves once it had reached a
certain degree of proficiency by being
set to play large number of games
against other instances of itself using
reinforcement learning to improve its
play next in our list of projects we
have d dream deep dream is a computer
vision program which uses a
convolutional neural network to find and
enhance patterns in images via
algorithmic paradia it then creates a
dream like halogenic appearance in the
overprocessed images deep dream is an
experiment that visualizes the patterns
learned by a neural
network similar to when a child watches
clouds and interprets and tries to
interpret random shapes deep dream over
interprets and enhances the p patterns
it sees in an image do check the given
GitHub link to install dependencies
listed in the notebook and play with the
code locally then we have deep voice
deep voice was developed by BYU which is
an AI system that can clone an
individual's voice the train model takes
just 3 seconds to replicate the output
of a person's voice deep voice 3 is a
fully convolutional attention based
neural text to speech system that
converts text to spectrograms or other
acost parameters to be used with an
audio waveform synthesis method below
you can find the GitHub link for pytorch
implementation of convolutional networks
based text to speech synthesis
models now we have IBM Watson IBM Watson
helps run machine learning models
anywhere across any Cloud using IBM
Watson machine learning you can build
analytical models and neural networks
train with your own data that you can
deploy for use in
applications with its open extensible
model operation Watson machine learning
helps businesses simplify and harness AI
at scale across any Cloud it is used in
healthc care teaching assistant chatbot
as well as for weather forecasting and
finally in our list of deep learning
projects we have the YOLO realtime
object detection you only look once or
YOLO is a state-of-the-art realtime
object detection system it frames object
detection in images as a regression
problem to spatially separated bounding
boxes and a Associated class
probabilities using this approach a
single neural network divides the image
into regions and predicts bounding boxes
and probabilities for each region the
neural network predicts bounding boxes
and class probabilities directly from
full images in one evaluation the base
YOLO model processes images in real time
at 45 frames per second you can use the
Coco data set and tensorflow library to
train and test the model to learn more
about your YOLO object detection check
the GitHub link that's shown on the
screen below then accelerate your career
in Ai and ml with a comprehensive
postgraduate program in artificial
intelligence and machine learning so
enroll now and unlock exciting Ai and
machine learning opportunities the link
is mentioned in the description box
below deep learning interview questions
and we're going to go from the very
basics of neural networks and deep
learning into some of the more commonly
used models so you can have an
understanding of what kind of questions
are going to come up and what you need
to know in interview questions we'll
start with a very general concept of
what is deep learning this is where we
take large volumes of data in this case
on cats and dogs or whatever A lot of
times you use um a training setup to
train your model remember it's kind of
like a magic Black Box going on there
then we use that to extract features or
extract information and in this case
classify the image of a cat and a dog so
the primary takeaway when we're talking
about deep learning is it learns from
large volumes of structured and even
unstructured data and uses complex
algorithms to train neural network it
also performs complex operations to
extract hidden patterns and features and
if we're going to discuss deep learning
in this very uh simplified overview then
we also have to go over what is a neural
network this is a common image you'll
see of a drawing of a forward
propagation neural network and it's it's
a human brain inspired system which
replicate the way humans learn so this
is inspired how our own neurons in our
brain fire but at a much simplified
level obviously it's not ready to take
over the human uh population and and be
our leader yet not for many years it's
very much in its infant stage but it's
inspired by how our brains work um and
they use a lot of other Inspirations you
can study brains of moths and other
animals that they've used to figure out
how to improve these neural networks the
most common one consists of three layers
of network and this is generally how you
view these networks is you have an input
you have a hidden layer and an output
and the neural network is uh broken up
into many pieces but when we Focus just
on the neural network it's always on the
hidden layers that we're making all the
adjustments and figuring out how to best
set up those hidden layers for their
functions to both train faster and to
function better when we look at this of
course we have our input hidden and
output each layer contains neurons
called as nodes perform various
operations and you can see here we have
the list of the nodes we have both our
input nodes and our output nodes and
then our hidden layer nodes and it's
used in deep learning algorithm like CNN
RNN G Etc we'll address some of these
models a little closer at least the most
common models as we go down the list and
we study the Deep learning and the
neural network framework let's start
with what is a multi-layer patron or MLP
a lot of time is it referred to and
you'll see these abbreviations I'll be
honest I have to write them down on a
piece of paper and go through them
because I never remember what they all
mean even though I play with them all
the time what is a multi-layer patron
well if you look at the image on the
right it's very similar to what we just
looked at you have your input layer your
hidden layer and your output layer and
that's exactly what this is it has the
same structure of a single layer Perron
with one or more hidden layers except
the input layer each node in the other
layers uses a nonlinear activation
function what that means is your input
layers your data coming in and then your
activation function is based upon all
those nodes and weights being added
together and then it has the output MLP
uses supervised learning method called
back propagation for training the model
very key word there is back propagation
single layer Perron can classify only
linear separable classes with binary
output 01 but the MLP can classify
nonlinear classes so let's break this
down just a little bit the multi-layer
Perron with an input layer and a hidden
layer and an output layer as you see
that it comes in there it has adds up
all the numbers and weights depending on
how your setup is that then goes to the
next layer that then goes to the next
hidden layer if you have multiple hidden
layers and finally to the output layer
the back propagation takes the error
that it sees so whatever the output is
it says hey this has an error to it it's
wrong and then sends that error
backwards from where it came from and
there's a lot of different functions
used to uh train this based on that
error and how that error goes backwards
in the notes uh so forward is you get
your answers backward is for training
you see this every day even my uh Google
pixel phone has this it they train the
neural network which takes a lot more
data to train than it does to use and
then they load up that neural network
into in this case I have a pixel 2 which
actually has a built-in neural network
for processing pictures and so it's just
the forward propagation I use when it
processes my photos but when they were
training it you use a back propagation
to train it with the errors they had
we'll be coming back to different models
that are used for right now though
multi-layer Patron MLP put that down as
your vocabulary word and of course back
propagation what is data normalization
and why do we need need it this is so
important we spend so much time in
normalizing our data and getting our
data clean and setting it up uh so we
talk about data there's a pre-processing
step to standardize the data so whatever
we have coming in we don't want it to be
a uh you know 1 Gigabyte file here a 2
GB picture here and a 3 kilobyte text
there even as a human I can't process
those all in the same group I have to
reformat them in some way that Loops
them together so that a standardized
format we use this uh data normalization
in pre-processing to reduce and
eliminate data redundancy a lot of times
the data comes in and you end up with
two of the same images or um uh the same
information in different formats then we
want to rescale values to fit into a
particular range for achieving better
convergence what this means is with most
neural networks they form a bias we've
seen this in recently in attacks on
neural networks where they light up one
pixel or one piece of the view and it
skews the whole whole answer so suddenly
um because one pixel is really bright uh
it doesn't know what to do well when we
start rescaling it we put all the values
between say minus one and one and we
change them and refit them to those
values it helps get rid of that bias
helps fix for some of those problems and
then finally we restructure the data and
improve the Integrity we want to make
sure that we're not missing values um or
we don't have partial data coming in one
way to look at this is uh bad data in
bad data out so we want clean data in
and you want good answers coming out one
of the most basic models used is a
boltzman machine so let's address what
is a boltzman machine and if you know we
just did the MLP multi-layer prron so
now we're going to come into almost a
simplified version of that and in this
we have our visible input layer and we
have our hidden layer the bolt spin
machines are almost always shallow
they're usually just two layer neural
Nets that make stochastic decisions
whether a neuron should be on or off
true false yes no first layer is a
visible layer and second layer is the
hidden layer nodes are connected to each
other across layers but no two nodes of
the same layer are connected hence it is
also known as restricted boltzman
machine now that we've covered a basic
MLP or multi-layer Perron and we've gone
over the boltzman machine also known as
the restricted boltzman machine let's
talk a little bit about activation
formulas and this is a huge topic that
can get really complicated but it also
is automated so it's very simple so you
have both a complicated and a simple at
the same time so what is the role of
activation functions in a neural network
activation function decides whether a
neuron should be fired or not that's the
most basic one and that actually changes
a little bit because it's either whether
fired or not in this case activation
function or what value should come out
when it's fired but in these models
we're looking at just the bolts men
restricted layers so this is what causes
them to fire either they don't or they
do it's a yes or no true false All or
Nothing nothing it accepts the weighted
sum of the inputs the bias as input to
any activation function so whatever our
activation function is it needs to have
the sum of the weights times the input
so each input if you remember on that
model and let's just go back to that
model real quick and then you always
have to add a bias and you can look at
the bias if you remember from your ukian
geometry you draw a straight line
formula for that line has a y-coordinate
at the end it's always um CX plus M or
something like that where m is where it
crosses the Y coordinates if you're
doing a straight line with these weights
it's very similar but a lot of times we
just add it in as its own weight we take
it as a node of a one value coming in
and then we compute its new weight and
that's how we compute that bias just
like we compute all the other weights
coming in the node which gets fires
depends on the Y value and then we have
a step function and the step function
this is where remember I said is going
to get complicated and simple all at the
same time we have a lot of different
step functions we have the sigmoid fun
function we have just a standard step
function we have the um rayu it's
pronounced like Ray the ray of from the
Sun and L like a name so Ru function and
we have the tangent H function and if
you look at these they all have
something similar they all either force
it to be um one value or the other they
force it to be in the case of the first
three a zero or one and in the last one
it's either a minus one or one and you
can easily convert that to a zero one
yes no true false and on this one of the
most common ones is the step function
itself because there is no middle value
there is no um uh discrepancy that says
well I'm not quite sure but as you get
into different models probably the most
commonly used used to be the sigmoid was
most commonly used but I see the ru used
more often really depending on what
you're doing you just have to play with
these and find out which one works best
depending on the data in your output the
reason to have a non1 answer or
something kind of in the middle is when
you're looking at this and it's coming
out you can actually process that middle
ground as part of the answer into
another neural network so it might be
that the railu function says hey this is
only a 6 not a one and uh even though
the one is what's going into the next
neural network or the next hidden layer
as an input the 6 value might also be
going in there to let you know hey this
is not a straight up one or straight up
zero is someplace in the middle this is
a little uncertain what's coming out
here so it's a very powerful tool but in
the base basic neural network you
usually just use the step function it's
yes or no let's take a um a big step
back and take a kind of an overview the
next function is what is a cost function
that we're going to cover this is so
important because this is your end
result that you're going to do over and
over again and use to decide whether the
model is working or not whether you need
to try a different step function whether
you need to try a different activation
whether you need to try a fully
different model used uh so what is the
cost function cost function is is a
measure to evaluate how good your
model's performance is it is also
referred as loss or error used to
compute the error of the output layer
during back propagation there's our back
propagation where we're training our
model that's one of our key words mean
squared error is an example of a popular
cost function and so here we have the
cost function c equal half of Yus y
predicted um and then you square that so
the first thing is um you know real
quick if you haven't done statistics
this is not a percentage it's not a
percentage of how accurate it is it's
just a measurement of the error and we
take that error for training it and we
push that error backwards through the
neural network and we use that through
the different training functions
depending on what model you're using to
train the neural network so when you
deploy the network you're usually done
training it because it takes a lot of
computational force to train it um this
is a very simple model and so you deploy
the trained one uh but we want to know
how your error is and so how do we do
that well you split your data part of
your data is for training and part of
your data is for testing and then we can
also test the error on there so it's
very important and then we're going to
go one more step on this we got to look
at both the local and the global setup
it might work great to test your data on
what you have on your computer but
that's different than in the field so
when we're talking about all these
different tests and the error test as
far as your loss you don't you want to
make sure that you're in a closed
environment when you do initial testing
but you also want to open that up and
make sure you follow up with the testing
on the larger scale of data cuz it will
change it might not fit the larger scale
there might be something in there in the
way you brought the data in specifically
or the data group you used or um any of
those could cause an error so it's very
important to remember that we're looking
at both the local and the global context
of our error and just one other side
note on a lot of the newer models of
neural networks by comparing the error
we get on the data our training data
with a portion of the test data we can
actually figure out how good the model
is whether it's overfitted or not we'll
go into that a little bit more as we go
into some of the different models so we
have our output we're able to um figure
out the error on it based on the Square
means usually although there's other uh
functions used so we want to talk about
what is gradient descent another
vocabulary word gradient descent is an
optimation algorithm to minimize the
cost function or to minimize the error
aim is to find the local or Global
Minima of a function determine the
direction the model should take to
reduce the error so as we're looking at
this we have our uh squared error that
we just figured out the co based on the
cost function it says how bad is my
model fitting the data I just put
through it and then we want to reduce
that error so how do you figure out what
direction to do that in well you could
be that you're looking at just that line
of that line of data coming in so that
would be a local Minima we want to know
the error of that particular setup
coming in and then you have your Global
your Global Minima we want to minimize
it based on the overall data we're
putting through it and with this we can
f figure out the global minimum cost we
want to take all those local minimum
costs of each piece of data coming in
and figure out the global one how are we
going to adjust this model to fit all
the data we don't want it to be biased
just on three or four lines of data
coming in we want it to kind of
extrapolate a general answer for all the
data coming in at this of course uh we
mentioned it briefly about back
propagation this is where it really
comes in handy is training our model
neural network technique to minimize the
cost function helps to improve the
performance of the network back
propagates the error and updates the
weights to reduce the error so as you
can see here is a very nice depiction of
a back propagation we have our U
predicted y coming out and then we have
since it's a training set we already
know the answer and the answer comes
back and based on case the square means
was one of the functions we looked at uh
one of the activation functions based on
cost function that cost function then
depending on what you choose for your
back propagation method and there's
there's a number of them will change the
weights it will change the weight going
to each of one of those nodes in the
hidden layer and then based upon the
error that's still being carried back
it'll change the weights going to the
next hidden layer and then it computes
an error level on that and sends that
back up and you're going to say well if
it computes the error into the first
hidden layer and fixes it why would it
stop there well remember we don't want
to create a biased neural network so we
only make small adjustments on these
weights we don't make a big adjustment
that ch changes everything right off the
bat so no matter how far back you go
you're always going to have a small
amount of error and that's still going
to continue to go all the way back up
the hidden layers for right now focus on
the back propagation is taking that
error and moving it backwards on the
neural network to change the weights and
help program it so that it'll have the
correct answers so far we've been
talking about forward propagation Nal
networks everything goes forwards goes
left to right uh but let's let's take a
little detour and let's see what is the
difference between a feed forward
forward neural network and a recurrent
neural network now this is in the
function not when we're training it
using the back propagation so you've got
new information coming in and you want
to get the answer and there's a couple
different networks out there and we want
to know we have a feed forward neural
network and we have a new uh vocabulary
term recurrent neural network a feed
forward neural network signals travel in
one direction from input to Output no
feedback loops considers only the
current input cannot memorize previous
inputs one example of one of these feed
forward neural networks and we've
covered a number of them but one of the
ones this has a big highlight nowadays
is the CNN a convolutional neural
network tensorflow the one put out by
Google is probably most known for their
CNN where the information goes forward
it uh first takes a picture splits it
apart goes through the individual pixels
on the picture so it picks up a
different reading then calculates based
on that goes into a regular feed forward
neural network and then gives your
categorization on there now we're we're
not covering the CNN today but we do
have a video out that you can look up on
YouTube put out by simply learn the
convolutional neural network wonderful
tutorial check that out and learn a lot
more about the convolutional neural
network but you do need to know that the
CNN is a forward propagation neural
network only so it's only moving in One
Direction so we want look at a recurrent
neural network signals travel in both
directions making it a looped Network
considers the current input along with
the previous received inputs for
generating the output of layer has the
ability to memorize past data due to its
internal memory and you can see they
have a nice uh image here we have our um
input and for some reason they always do
the recurrent neural network um in
Reverse from bottom up in the images
kind of a standard although I'm not sure
why your X goes into your hidden layer
and your hidden layer the answer part of
the answer from that it generates feeds
back into the hidden layer so now you
have an input of both X and part of the
Hidden layer and then that feeds into
your output now now if we go back to the
forward let me just go back a slide and
we're looking at uh our forward
propagation Network one of the tricks
you can do to use just a forward
propagation network is if you're in a
what they call a Time sequence that's a
good uh term to remember or a Time
series meaning that it's sequential data
each term comes after the other you can
trick this by creating your input nodes
as with the history so if you know that
uh you have values 1 five and seven
going in and you know what the output is
from one what those outputs are you can
expand the input to include the history
input that's one of the ways to trick a
forward propagation Network into looking
at that but when you do with the
recurrent neural network you let the
hidden layer do that for you it sends
that data and reprocesses it back into
itself what are some of the applications
of recurrent neural network the RNN can
be used for sentiment analysis and text
mining getting up early in the morning
is good for health it's a positive
sentiment one of the catches you really
want to look at this when you're looking
at the language is that I could switch
this around and totally negate the
meaning of what I'm doing so it no
longer be positive so when you're
looking at a sentence knowing the order
of the words is as important as the
meaning of the words you can't just
count how many good words there are
versus bad words to get positive
sentiment you now have to know what
they're addressing and there's lots of
other different uses uh kids are playing
football or soccer as we call it in the
US RN can help you caption an image So
based on previous information coming in
it refeeds that back in and you have a
uh image Setter and then time series
problems like predicting the prices of
stocks in a month or quarter or sale of
product can be solved using an RNN and
this is a really good example you have
whatever your stocks were doing earlier
this month will have a huge effect of
what they're doing today if you're
investing so having an RNN Model A
recurrent neural network feeding into
itself what was happening previously
allows it to take that model and program
in that whole series without having to
to put in the whole a month at a time of
data you only put in one day at a time
but if you keep them in order it will
look back and say oh this because of
what happened yesterday I need some
information from that and I'm going to
use that to help predict today and so on
and so on we're going to go back to our
activation functions remember I told you
uh Ru is one of the most common
functions used uh so let's talk a little
bit more about Ru and also softmax
softmax is an activation function that
generates the output between 0er and one
it divides each output such that the
total sum of the outputs is equal to one
it is often used in the output layers
soft Max L of the N equals e to L on the
N over the absolute value of e to the L
so what does this function mean I mean
what is actually going on here so we
have our output nodes and our output
nodes are giving us uh let's say they
gave us
1.2.9 and4 as a human being I look at
that and I say well the greatest value
is 1.2 so whatever category that is if
you have three different categories
maybe you're not just doing if it's a
cat or it's a dog or um oh let's say
it's a cow remember we had cats and dogs
earlier why the cats and dogs are
hanging out with a cow I don't know but
we have a value and it might say 1.2 is
a cat 0.9 is a dog and 04 is a cow uh
for some reason it SS that there's a
chance of it being any one of these
three items and that's how it comes out
of the output layer well as a human I
can look at 1.2 and say this is
definitely what it is it's definitely a
cat or whatever it is U maybe it's
looking at different kinds of cars might
be a better whether it's a car truck or
motorcycle maybe that'd be a better
example well from a computer standpoint
that may be a little confusing because
they're just numbers waving at us and so
with the soft Max we want all those
numbers to always add up to one so when
I add three numbers together I want the
final output to be one on there and so
it goes through this formula changes
each of these numbers in this case it
changes them to
04634 and .20 they all add up to one and
that's a lot easier to register CU it's
very set it's a set output it's never
going to be more than one it's never
going to be less than zero and so you
can see here that there's probably a
pretty high chance that it's the first
one so you're a human being we have no
problem knowing that but this output can
then also go into say another input so
it might be an automated car that's
picking up images and it says that image
in front of us is probably a big truck
we should deal with it like it's a big
truck it's probably not a motorcycle um
or whatever those categories are that's
the softmax part of it but now we have
the railu well what's where's the railu
coming from well the railu is what's
generating the 1.2 and 0.9 and the 04
and so if you remember our Ru stands for
rectified linear unit and is the most
widely used activation function we
looked at a number of different
activation functions including tangent H
the step function remember I said the
step function is really used if that's
what your actual output is because then
you know it's a zero or one but the
railu if you have that as your output
you know have a discrepancy in there and
if that's going into another neural
network or another process having that
discrepancy is really important and
gives an output of x if x is positive
and zero otherwise so it says my x value
is going to be somewhere between zero or
one and then the uh usually unless it's
really uncertain the output's usually a
one or zero and then you have that
little piece of uncertainty there that
you can send forward to another Network
or you can look at to know that there's
uncertainty involved and is often used
in the hidden layers this is what's
coming out of the Hidden layers into the
output layer usually or as we reference
the convolution neural network the NN
you'd have to go to another video to
review the railu is the most common used
for convolutional part of that Network
it has a bunch of little pieces that are
very simplified looking at all the
different images or different sections
of the map and the ru works really good
for that like I said there's other
formulas used but that this is the most
common one and you'll see that in the
hidden layer is going maybe between one
layer and the next layer so just a quick
recap we have our soft Max which means
that if you have uh numerous categories
only one of them is going to be picked
but you also want to have some value
attached to it how well it picked it and
you put that between zero one so it's
very uh standardized so we have our soft
Max we looked at that let's go back one
we looked at that here where it
transforms in numbers and then we have
our railu function which takes the
information in the summation and puts it
between a zero and a one where it's
either clearly a zero or depending on
how confident our model is it'll go
between the zero and one value what are
hyperparameters oh this is a great
interview question hyperparameters when
you are doing neural networks this is
what you're playing with most of the
time once you gotten the data formatted
correctly a hyperparameter is a
parameter whose value is set before the
learning process begins determines how a
network is trained and the structure of
the network this includes things like
the number of hidden units how many
hidden layers are you going to have and
how many nodes in each layer learning
rate learning rate is usually multiplied
once you figured out the error and how
much you want want to change the weights
we talked about or I mentioned it early
just briefly you don't want to just make
a huge change otherwise you're going to
have a biased model so you only take
little incremental changes and that's
what the learning rate is is those small
incremental changes epics how many times
are you going to go through all the data
in your training set so one Epic is one
trip through all the data and there's a
lot of other things depending on which
model you're working with and which
programming script you're working with
like the python SK learn package will
have it slightly different than say
Google's tensorflow package which will
be a little bit different than the spark
machine learning package so these are
just some examples of the hyper
parameters and so you see in here we
have a nice image of our data coming in
and we train our model then we do a
comparison to see how good our model is
and then we go back and we say hey this
this model is pretty good but it's
biased so then we send it back and we
change our hyperparameters to see if we
can get an unbiased model or we can have
a better prediction on it that matches
our data closer what will happen if
learning rate is set to too low or too
high we have a nice couple graphs here
we have one over here says the learning
rate set too low you can see that it
slowly Works its way down the curve and
on the right you can see a learning rate
set too high it's just bouncing back and
forth when your learning rate is too low
that's what we studied at two slides
over they asked what the learning rate
was training of the model will progress
very slowly as we are making very tiny
updates to the weights we'll take many
updates before reaching the minimum
point so I just mentioned epic going
through all the data might have to go
through all the data a thousand times
instead of 500 times for it to train
learning rate too high causes
undesirable Divergent Behavior to the
loss function due to drastic updates and
weights at times it may fail to converge
or even diverge so if you have your
learning rate set too high and it's
training too quickly maybe you'll get
lucky and it trains after one epic run
but a lot of times it might never be
able to train because the weights are
changing too fast they they flip back
and forth too easy and you see down here
we've introduced uh two new terms conver
converge and diverge a converge means
that our model has reached a point where
it's able to give a fairly good answer
for all the data we put in all those
weights have adjusted and it's minimized
the error diverg means that the data is
so chaotic that it can never manage to
to train to that data the data is just
too chaotic for it to train so we have
two new words there converge and diverge
are important to know also what is
Dropout and batch normalization Dropout
is a technique of droing out hidden and
visible units of a network randomly to
prevent overfitting of data it doubles
the number of iterations needed to
converge the network so here we have our
standard neural network and then after
applying Dropout now it doesn't mean we
actually delete the node the node is
still there and we're still going to use
that Noe what it means is that we're
only going to work with a few of the
notes um a lot of times I think the most
common one right now used is 20% uh so
you'll drop out 20% of the nodes when
you do your training you reverse
propagate your data and then you'll
randomly pick another 20 nodes the next
time you go through an epic data
training so each time you go through one
Epic you will randomly pick 20 of those
nodes not to not to mess with and this
allows for less overfitting of the data
so by randomly doing this you create
some I guess it just kind of pull some
nodes off to the Sid it says we're going
to handle the data later on so we don't
overfit batch normalization is the
technique to improve the performance and
stability of neural network the idea is
to normalize the inputs in every layers
so that they have mean output and
activation of zero and standard
deviation of one this question covers a
lot of different things which is great
it's a great uh interview question
because it pulls in that you have to
understand what the mean value is so a
mean output activation of zero that
means our average activation is zero so
when you normalize it remember usually
we're going between minus one and one on
a lot of these it's a very standard
setup so you have to be very aware that
this is your mean output activation of
zero and then we have our stand standard
deviation of one so we want to keep our
error down to just a one value the
benefits of this doing a batch
normalization is it provides
regularization it trains faster Higher
Learning rates and weights are easier to
initialize what is the difference
between batch gradient descent and
stochastic gradient descent batch
gradient descent batch gradient computes
the gradient using the entire data set
it takes time to converge because the
volume of data is huge and weight update
slowly so you can look at the batches a
lot of times if you're using big data
batch the data in but you still go
through a full epic you still go through
all the data on there so bash gradient
descent means you're going to use it to
fit all the data and look for a
convergence there stochastic gradient
descent stochastic gradient computes the
gradient using a single sample it
converges much faster than batch
gradient because it updates weight more
frequently explain overfitting and
underfitting and how to combat them
overfitting happens when a model learns
a the details and noise and the training
data to the degree that it adversely
impacts the execution of the model on
the new information it is more likely to
occur with nonlinear models that have
more flexibility when learning a Target
function an example of this would be um
if you're looking at say cars and trucks
and motorcycles it might only recognize
trucks that have a certain box-like
shape it might not be able to notice a
flatbed truck unless it's only a
specific kind of flatbed truck or only
Ford trucks because that's what saw on
the training set this means that your
model performs great on your train data
and great on maybe a small test amount
of data but when you go to use it in the
real world it leaves out a lot and start
is not very functional outside of your
small area your small laboratory data
coming in underfitting doing the
opposite when you underfit your data
underfitting alludes to a model that is
neither well Tred on training data nor
can generalize to new information
usually happens when there is less and
improper data to train a model model has
a performance and accuracy so if you're
using underfitted data and you generate
a model and you distribute that in a
commercial Zone you'll have a lot of
people unhappy with you because it's not
going to give them very good answers so
we've explained overfitting and
underfitting so now we want to ask how
to combat them combating overfitting and
underfitting resampling the data to
estimate the model accuracy kfold cross
validation having a validation data set
to EV validate the model so when we do
the resampling we're randomly going to
be picking out data and we'll run it a
few times to see how that works
depending on our random data and how we
U sample the data to generate our model
and then we want to go ahead and
validate the data set by having our
training data and then keeping some data
on the side uh testing data to validate
it how are weights initialized in a
network initializing all weights to zero
all the weights are set to zero this
makes your model similar to a linear
model so if you have linear data coming
in doing a basic setup like that might
work all the neurons in every layer
perform per the same operation given the
same output and making the Deep net
useless right there is a key word it's
going to be useless if you initialize
everything to zero at that point be
looking into some other uh machine
learning tools initializing all weights
randomly here the weights are assigned
randomly by initializing them very close
to zero it gives better accuracy to the
model since every neuron performs
different computations and here we have
the weights are set randomly we have our
input layer the hidden layers and the
output layer and W equals NP random
random n layer size L layer size L minus
one this is the most commonly used is to
randomly generate your weights what are
the different layers in CNN
convolutional neural network first is
the convolutional layer that performs a
convolutional operation we have our
other video out if you want to explore
that more so you go into detail exactly
how the C the convolutional layer works
in the CNN as far as uh creating a
number of smaller uh picture windows
that go over the data uh the second step
is has a railu layer Ru brings
nonlinearity to the network and converts
all the negative pixels to zero output
is rectified feature map so it goes into
a mapping feature there pooling layer
pooling is a down sampling operation
that reduces the dimensionality of the
feature map so we have all our railu
layer which is pulling all these little
Maps out of our convolutional layer is
taking that picture and little creating
little tiny neural networks to look at
different parts of the picture uh then
we need to pull it together and then
finally the fully connected layer so we
flattened our pooling layer out and we
have a fully connected layer recognizes
and classifies the objects in the image
and that's actually your forward
propagation reverse propagation training
model usually I mean there's a number of
different models out there of course
what is pooling in CNN and how does it
work pooling used to reduce the spatial
dimensions of a CNN performs down
sampling operation to reduce the
dimensionality creates a pulled feature
map by sliding a filter Matrix over the
input Matrix I mentioned that briefly on
the previous slide um it's important to
know that you have if you see here they
have a rectified feature map and so each
one of those colors like the yellow
color that might be one of the a smaller
little neural network using the ru
you'll look at it'll just kind of um go
over the main picture and look at all
the different areas on the main picture
so you might step one two three four
spaces um and then you have another one
that's also looking at features and it
has a 2785 each one of those is a map so
it might be the first one might be a map
looking for cat ears and the second one
looking for human eyes when it does this
you then have this rectified feature map
looking at these different features and
the max pooling with a 2 by two filters
and a stride of two stride means instead
of skipping every pixel you're going to
go every two pixels you take the maximum
values and you can see over here when we
look at a pulled feature map one of the
features says hey I had a max value of
eight so somewhere in here we saw a
human eye labeled as eight pretty high
label and maybe seven was a human hand
and maybe four was cat whiskers or
something that we thought might be cat
whiskers four is kind of a low number in
this particular case compared to the
other ones so you have your full pulled
feature map you can see the process here
as we have our stepping we look for the
max value and then we create a pulled
feature map of the maxed values how does
a lstm network work that's long
shortterm memory so the first thing to
know is that an lstms are a special kind
of recurrent neural network capable of
learning long-term dependencies
remembering information for long periods
of time is their default Behavior we did
look at the RNN briefly talked about how
the hidden layer feeds back into itself
with the lstm has a much more
complicated feedback and you can see
here we have um the hidden layer of T
minus one and hidden layer that's what
the H stands for hidden layer of T and
the formula is going in as we can see
here we have the hidden layers we have T
minus one and then h of T where T stands
for time so this is a series remember
working with series and we want to
remember the path and you can see you
have your your input of T and that might
be a frame in a video as a frame comes
in they usually use in this one the
tangent H activation formula but you
also see that it goes through a couple
other formulas the Omega formula and so
when it combines these that thing goes
into the next layer your next hidden
layer that thing goes into the data
that's submitted to the next input so
you have your X of t + one so when you
have that coming in then you have your H
value that's coming forward from the
last process and depending on how many
of these um Omega structures you put in
there depends on how long-term the
memory gets so it's important remember
this is more for your long-term
recurrent neural networks the three
steps in an lstm step one decides what
to forget and what to remember step two
selectively update cell State values So
based on what we want to remember and
forget we want to update those cell
values and then decides what part of the
current state make it to the output so
now we have to also have an output on
there what are Vanishing and exploding
gradients this is a great question that
affects all our neural networks while
training an RNN your slope can become
either too small or too large and this
makes the training difficult when the
slope is too small the problem is known
as Vanishing gradient so our slope we
have our change in X and our change in y
when the slope decreases gradually to a
very small value sometimes negative and
makes training difficult when the slope
tends to grow exponentially instead of
decaying this problem is called called
exploding gradient the slope grows
exponentially you can see a nice graph
of that here issues in gradient problem
Long training time poor performance and
low accuracy what is the difference
between epic batch and iteration and
deep learning epic an epic represents
one iteration over the entire data set
so that's everything you're going to go
ahead and put into that training model
batch we cannot pass the entire data set
into the neural network at once so we
divide the data set into a number of
batches and then iteration if we have
10,000 images as data and a batch size
of 200 then the Epic should run 10,000
times over 200 so that means we have our
total number over the 200 equals 50
iterations so in each epic we're running
over all the data set we're going to
have 50 iterations and each of those
iterations includes a batch of 200
images in this case why tensorflow is
the most preferred library in deep
learning uh well first tensorflow
provides both C++ and python apis that
makes it easier to work on has a faster
compilation time than other deep
learning libraries like carass and torch
tensorflow supports both CPUs and gpus
Computing devices so right now tensor
flow is at the top of the market because
it's so easy to use for both programmer
side and for Hardware side and for the
speed of getting something up and
running what do you mean by tensor and
tensor flow tensor is a mathematical
object represented as arrays of higher
Dimension these arrays of data with
different dimensions and ranks that are
fed as input to the neural network are
called tensors and you can see here we
have a tensor of Dimensions 5 comma 4 so
it's a two-dimensional tensor coming in
um you can look at an image like this
that each one of those pixels is a
different value if it's a black and
white so it might be zero and ones and
then each one represents a black and
white image in a color photo you might
um either find a different value system
or you might have a tensor value that
has the XY coordinates as we see here
plus the colors so you might have three
more different dimensions for the three
different images the red the blue and
the yellow coming in and even as you go
from one layer or one tensor to the next
these layers might change we might
flatten them might bring in numerous in
the case of the convergence neural
network we have all those smaller
different mappings of features that come
in so each one of those layers coming
through is a tensor if it has multiple
Dimensions coming in and weights
attached to it what are the programming
elements in tensor flow well we have our
constants constants are parameters whose
value does not change to define a
constant we use tf. constant command
example a equal tf. constant 2.0 TF
float 32 so it's a tensor float value of
32 b equals TF constant 3.0 print AB if
we did a print of ab we'd have um tf.
constant and then of course uh B is that
instance of it variables variables allow
us to add new trainable parameters to
graph to Define a variable we use tf.
variable command and initialize them
before running the graph in session
example wal TF variable .3 dtype TF
float 32 or b equal a TF variable minus
3 comma dtype float 32 placeholders
placeholders allow us to feed data to a
tensorflow model from outside a model it
permits a value to be assigned later to
define a placeholder we use TF
placeholder command example AAL TF
placeholder Bal a * 2 with the TF
session as sess result equals session
run B comma feed dictionary equals a 3.0
print result uh so we have a nice
example there placeholder session a
session is run to evaluate the nodes
this is called as the tensor flow
runtime so for example you have a equals
TF constant 2.0 Bal TF constant 4.0 Cal
a plus b and at this point you'd go
ahead and create a session equals TF
session and then you could evaluate the
tensor C print session run C that would
input c as an input into your session
what do you understand by a
computational graph everything in tensor
flow is based on creating a
computational graph it has a network of
nodes where each node performs an
operation nodes represent mathematical
operation and edges represent tensors
since data flows in a form of a graph it
is also called a data flow graph and we
have a nice visual of this graph or
graphic image of a computational graph
and you can see here we have our input
nodes our add multiply nodes and our
multiply node at the end and then we
have the edges where the data flows so
we have from a going to C A going to D
you can see you have a two- flowing a
four flowing explain generative
adversarial Network along with an
example suppose there's a wine shop that
purchases wine from dealers which they
will resell later so we have our dealer
going to the wine our shop owner that
then sells it for a profit but there are
some malactor dealers who sell fake wine
in this case the shop owner should be
able to distinguish between fake and
authentic wine the forger will try to
different techniques to sell fake wine
and make sure certain techniques go past
the shop owner's check so here's our
forger fake wine shop owner the shop
owner would probably get some feedback
from the wine experts that some of the
wine is not original the owner would
have to improve how he determines
whether a wine is fake or authentic goal
of forger to create wines that are
indistinguishable from the authentic
ones goal of shop owner to accurately
tell if the wine is real or not there
are two main components of generative
adversarial Network and we'll refer it
to as a noise Vector coming in where we
have our forger who's going to generate
fake wine and then we have our real
authentic wine and of course of course
our shop owner who has to figure out
whether it's real or fake the generator
is a CNN that keeps producing images
that are closer in appearance to the
real images while the discriminator
tries to determine the difference
between real and fake images the
ultimate aim is to make the
discriminator learn to identify real and
fake images what is an autoencoder the
network is trained to reconstruct its
inputs it is a neural network that has
three layers here the input neurons are
equal to the output neuron the Network's
taret Target outside is same as the
input it uses dimensionality reduction
to restructure the input input image
comes in we have our Latin space
representation and then it goes back out
reconstructing the image it works by
compressing the input to a Latin space
representation and then reconstructing
the output from this representation what
is bagging and boosting bagging and
boosting are Ensemble techniques where
the idea is to train multiple models
using the same learning algorithm and
then take a call so we have in here
where we're bagging we take a data set
and we split it we're going to have our
training data and our test data very
standard thing to do then we're going to
randomly select data into the bags and
train your model separately so we might
have bag one model one bag two model two
bag three model three and so on in
boosting the infinis is to select the
data points which give wrong output in
order to improve the accuracy so in
boosting we have our data set again we
split it to test data and train data and
we'll take a bag one and we'll train the
model data points with wrong predictions
then go into Bag two and we then train
that model and repeat and with that we
have come to the end of this deep
learning full course for beginners by
simply learn I hope you found it
valuable and entertaining please ask any
questions about the topics covered in
this video in the comment box below and
our team of experts will assist you in
addressing your problems as soon as
possible so thank you so much for being
here today we'll see you next time until
then keep learning and stay tuned to
Simply learn for more amazing updates
staying ahead in your career requires
continuous learning and upscaling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in cuttingedge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click
here