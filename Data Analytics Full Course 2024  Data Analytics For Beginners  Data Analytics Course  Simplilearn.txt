hello everyone welcome to the data
analytics full course by simply learn if
you're looking to dive into the world of
data and turn information into insights
you are in the right place in 2024 data
analytics is one of the most in demand
skills with companies across all
Industries searching for professionals
who can analyze and interpret data
effectively salaries in this field are
also on the rise with data analysts
earning impressive package as they help
businesses make datadriven decisions
this course will equip you with all the
skills you need to start a successful
career in data analytics craving a
career upgrade subscribe like and
comment
below dive into the link in the
description to FastTrack your Ambitions
whether you're making a switch or aiming
higher simply learn has your
back also if you're interested in a
professional certification course in
data analytics begin your Learning
Journey with simply learn and IIT canus
data analytics certificate course
explore our learning management system
track your progress and meet completion
requirements learn to understand data
and enhance your analytics skills
through live sessions with industry
experts Hands-On labs and master classes
by IIT Conor faculty you can check out
the link mentioned in the description
box below and the pin comments start
learning today and unlock endless
opportunities in data analytics so
without further Ado let's get started
companies around the world are
generating vast volumes of data every
hour this data could be in the form of
log files web server and transactional
data as well as various customer related
data also data is being generated at a
rapid rate from social media websites
and applications such as Facebook
Instagram Twitter and
WhatsApp companies want to use this data
to derive value out of it and make
business
decisions that's where data analytics
comes into
use data analytics is the process of
exploring and analyzing large data sets
to find hidden patterns unseen Trends
discover correlations and valuable
insights to make business
predictions data analytics improves the
speed and efficiency of your
business a few years ago a business
would have gathered information manually
performed statistical and complex
analytics and Unearthed information that
could be used for future decisions
but today that business can identify
insights on the Fly for immediate
decisions most organizations have big
data and many understand the need to
harness that data and extract value out
of it so they use a lot of modern tools
and Technologies to perform data
analytics some of the tools I will
discuss in detail later in this
tutorial now that we have looked at at
what data analytics really is let us
understand the ways in which you can can
use data
analytics first is improved decision
making data analytics eliminates a lot
of guesswork and manual tasks from
choosing the right content planning
marketing campaigns and developing
products organizations can use the
insights they gain from data analytics
to make informed decisions leading to
better outcomes and customer
satisfaction it gives you a 360Â° view of
your customers which helps you
understand the behavior completely
enabling to better meet their
needs second is better customer service
data analytics provides you with more
accurate insights of your customers
allowing you to tailor customer service
to their needs provide more
personalization and build stronger
relationships with them your data can
reveal information about your customers
communication preferences their interest
their concerns and more it helps you
give better recommendations for products
and services
next is efficient operations data
analytics can help you streamline your
processes save money and boost
production when you have an improved
understanding of what your audience
wants you waste less time in creating
ads and content that don't match your
audience interests this helps you
optimize your campaigns create better
content strategies and hence improve
results and finally we have effective
marketing when when you understand your
audience better you can Market to them
more effectively data analytics also
gives you useful insights into how your
campaigns are performing so that you can
fine-tune them for optimal
outcomes you also find out the probable
customers who are the most likely to
interact with a campaign and convert
into
leads now let's discuss the various
steps involved in the data analytics
process as you can see on the screen
there are five process steps now let me
make you understand each of this one by
one so the first step is to understand
the
problem before starting with the
analysis you need to understand the
business problem and Define your goals
asking questions at the outlet is vital
because this would address issues such
as how can we reduce production cost
without sacrificing quality what are
some of the ways to increase sales
opportunities with our current
resources do customers view your brand
in a favorable way answers to these
questions will help you build a clear
road map with lucrative Solutions also
try to find out the key performance
indicators and consider the Matrix to
track along the
way the second step in the process is
data
collection after you have finalized your
goals it's time to start looking for
your data data collection is the process
of gathering information on targeted
variables identified as data
requirements the emphasis is on ensuring
accurate and right data is collected
data collection starts with primary
sources which are also known as internal
sources this is typically structured
data gathered from CRM software Erp
systems marketing automation tools and
others these sources contain information
about customers finances gaps in sales
Etc under external sources you have both
structured and unstructured data so if
you're looking to perform a sentiment
analysis towards your brand you would
gather data from various review websites
or social media
apps the next step is to clean the
data the data which is collected from
various sources is highly likely to
contain incomplete duplicate and missing
values so you need to clean these
unwanted redundant data to make it ready
for analysis so to generate accurate
results analytics professionals must
identify duplicate and anomalous data
and other inconsistencies that could
skew the analysis according to a report
60% of data scientists say most of the
time is spent cleaning the data while
57% of data scientists say it's their
least enjoyable
task now the fourth step in the process
is data exploration and
Analysis once data is cleaned and ready
you can go ahead and explore the the
data using data visualization and
business intelligence tools you can also
use various Data Mining and predictive
modeling techniques to analyze the data
and build
models you can use different supervised
and unsupervised algorithms such as
linear regression logistic regression
decision tree KNN K means clustering and
lots more to build prediction models for
making business
decisions and the final step is to
interpret the results
this part is important because it's how
a business will gain actual value from
the previous four
steps interpreting the results will help
you find unseen Trends and patterns in
the data and gain insights you can have
a validation check if the results are
answering your questions these results
can be shown to your clients and
stakeholders for better understanding
and business
collaboration now that we have looked at
the various steps involved in data
analytics let's now see the different
tool tools that can be used to perform
the above
steps so as you can see we have seven
tools including a few programming
languages that will help you perform
analytics better now let's discuss them
one by
one first we have
python python is an objectoriented open-
Source programming language that
supports a range of libraries for data
manipulation data visualization and data
modeling python programmers have
developed tons of free and open source
libraries that you can use you can find
many of them via the python package
index which is
pypi the repository of python
software python provides the default
package installer called pip or pip
Python has libraries such as numpy for
numerical computation of data pandas to
manipulate data on numerical tables and
time series then you have scipi for
Technical and scientific computations it
also provides psychic learn which is a
machine learning library for creating
classification regression and clustering
algorithms and finally it also has pie
torch and tensorflow for deep
learning up next we have
r r is an open- Source programming
language majorly used for numerical and
statistical analysis it provides a range
of libraries for data analysis and
visualization some of these libraries
are ggplot tidos plotly deer and carrot
then we have tblo tblo is a popular data
visualization and analytics tools that
helps you create a range of
visualizations to interactively present
the data build reports and dashboards to
Showcase insights and Trends it can
connect with multiple data sources and
give hidden business insights and
patterns then we have a competitor of
Tableau which is
powerbi powerbi is a business
intelligence tool developed by Microsoft
that has an easy drag and draw function
ality and supports multiple data sources
with features that make data visually
appealing powerbi supports features that
help you ask questions to your data and
get immediate insights you can also
forecast your data for predicting future
Trends so the next tool is Click
view click view provides interactive
analytics with inmemory storage
technology to analyze vast volumes of
data and use data discoveries to support
decision making
it provides social media Discovery and
interactive guided
analytics it can manipulate huge data
sets instantly with
accuracy up next we have Apache
spark Apache spark is an open source
data analytics engine to process data in
real time and Carry Out complex
analytics using SQL queries and machine
learning
algorithms it supports spark streaming
for realtime analytics and Spark SQL for
writing
queries it also has spark MLB which is a
library that has a repository of machine
learning algorithms and then it has
graphics for graphical
computation and finally we have
SAS SAS is a statistical analysis
software that can help you perform
analytics visualize your data write SQL
queries perform statistical analysis and
build machine learning models to make
future predictions SAS empowers our
customers to move the world forward by
transforming data into
intelligence SAS is investing a lot to
drive software Innovation for analytics
Gartner has positioned SAS as a magic
quadrant leader for data science and
machine learning so let's talk about
what does a data analyst do think of
data as the new oil powering decisions
in every industry data analysts are the
ones who extract and refine these
valuable resources es they gather verify
and organize data to make it easy to
access and analyze using statistical
methods they find Trends and patterns
create reports and visualizations with
tools like tableu or powerbi and provide
insights that help businesses make smart
decisions for example imagine a retail
company trying to understand why sales
dropped last quarter a date analyst
would gather sales Data customer
feedback and market trends they might
discover that a competitor launched a
similar product at a lower price causing
customers to switch with this Insight
the company can adjust its pricing
strategy or improve its product to
regain market share so now let's talk
about how to become a data analyst the
journey involves learning the right
skills understanding industry tools and
getting practical experiences earning a
bachelor's degree or master's degree in
fields like statistics computer science
economics or mathematics give you
essential skills in analytical thinking
statistical analysis and mathematical
modeling universities often offer
courses on data analysis machine
learning and data management but here's
the good news gaining practical
experience is essential internships
allow you to apply your knowledge in
real world settings offering mentorship
and a deeper understanding of business
process University career services job
fairs and online job portals like indeed
glass door and Linkedin can help you
find these opportunities so let's talk
about what skills do you need in order
to become a data analyst so to become a
successful data analyst you need a mix
of Technical and analytical skills at
first we have programming python learn
how to use libraries like pandas for
data manipulation numpy for numerical
data and practice by working on real
world projects such as cleaning data
sets performing exploratory data
analysis and building predictive models
then we have R this language focus on
stat iCal analysis and data
visualization learn packages like ggplot
2 for visualization and DLI for data
manipulation and Carro for machine
learning then we have database
management for database management the
first language you need to learn is SQL
learn how to write queries to extract
data join tables and perform
aggregations practice by working with
databases like MySQL or postre SQL and
understand how to optimize queries for
performance then then we have database
design understand how to design
databases efficiently including
normalization and indexing strategies
practice by designing schemas for sample
projects and optimizing them for
different query
patterns then we have data visualization
TBL and powerbi learn how to create
interactive dashboards build compelling
data stories and learn data Within These
tools practice by creating
visualizations from different data sets
Focus fing on presenting clear and
actionable insights then we have python
libraries like Matt plot IB for basic
plotting and caborn for statistical
visualization and plotly for interactive
plots practice by visualizing various
data sets highlighting Trends and
telling datadriven stories let's now
talk about analytical skills at first
you need to have statistical analysis
understand key statistical Concepts like
hypothesis testing regression analysis
and probability distributions practice
by applying these Concepts to real data
sets and interpreting the results
problem solving develop the ability to
tackle business problems using data
practice by working on case studies and
projects that require data driven
decision making then we have
communication skills data storytelling
learn to present data in compelling way
that highlights key insights practice by
creating reports and presentations that
effectively communicate your findings
presentation skills develop the ability
to create clear informative and engaging
presentations practice by presenting
your analysis to peers or mentors and
incorporating their feedback then we
have technical writing also you should
know how to learn and write clear and
concise reports that explain your
methodologies findings and
recommendation practice by documenting
your projects and sharing your reports
with others for feedback you might be
now wondering where do data analysts
actually work so data analysts are
needed in various Industries like
healthcare Finance marketing and Sports
in healthcare data analysts help
hospitals improve patient outcomes by
analyzing treatment data in finance they
help Banks detect frauding transactions
and in marketing they optimize campaigns
by understanding customer behavior in
sports they help teams improve
performance by analyzing player
statistics so dat analyst can be found
in Tech giants like Google Amazon and
Microsoft Facebook and IBM as as well
the job market is vast and growing with
opportunities in almost every sector so
how much do data analyst actually make
data analysts can expect competitive
salaries often starting around
$60,000 and going up to $100,000 or more
with experience for example an
entry-level data analyst at a tech
company might start with
$65,000 while a senior data analyst with
several years of experience could earn
over
$100,000 the high demand for skill data
analyst means companies are willing to
pay well for these valuable skills now
how to learn data analysis in 20124
start with online courses and
certification online platforms offer
specialized learning and are often
updated with the latest Technologies and
methods craving a career upgrade
subscribe like and comment
below dive into the link in the
description to FasTrack your Ambitions
whether you're making a switch or aiming
higher simply learn has your
back also if you are interested in
making a career in data analytics check
out Simply learns postgraduate program
in data analytics this comprehensive
course is designed to transform you into
a data analytics expert it covers
essential skills such as data
visualization statistical analysis and
machine learning using industry-leading
tools and Technologies like excelr
Python and TBL with expert instructors
real world projects and hands-on
experience you will gain the knowledge
and confidence needed to excel in
today's data driven World join now to
advance your career and become data
analytics professional Also let's talk
about whether practicing your skills
with real world projects are essential
or not well yes building a portfolio to
Showcase your abilities to potential
employees is highly valued for example
you can start with simple projects like
analyzing a public data set and grad
ually move to more complex ones document
your process create visualization
present your findings a strong portfolio
is a great way to demonstrate your
skills and attract job offers platforms
like kaggle offer data science
competitions while GitHub allows you to
contribute to open- source project
personal projects using public data sets
can also enhance your skills and build
your portfolio making you more
attractive for potential employers
freelancing often provides diverse
project experience with direct business
implications platforms like upwork
freelancer and top helps you connect
with clients which needs data analysis
offering flexible experience and
Industry exposure let's talk about
whether networking is essential or not
well building a professional network is
crucial for career growth and gaining
industry insights LinkedIn is a great
platform to connect with professional
join groups and participate in
discussions attending conferences and
workshops also provides networking
opportunities and learning experiences
networking can help you lead to career
opportunities and help you stay updated
with industry Trends engaging with
Community enhances your problem solving
skills and supports your growth
platforms like stack Overflow cross
validated and local or virtual meetups
allow you to discuss data related topics
and build your reputation Community
engagement Fosters skill development and
help you become a knowledgeable and
helpful member of the data community so
you know that now building a strong
resume is your first introduction to
potential employers you must also
highlight your educational background
projects internship technical skills and
certifications as well tailor your
resume for each job and quantify your
achievements to make a strong impression
preparing for interviews involves
understanding the role reviewing your
projects practicing common questions and
participating in mock interviews to
build confidence as you gain experience
opportunities to move into senior roles
such as data scientist data engineer or
anal itics manager will arise continued
education skill diversification and
developing leadership skills are key
strategies for advancements so now let's
talk about what is the data analyst job
outlook in 2024 well the demand for data
driven decision making is increasing day
by day and with it the need for skilled
data analyst as well the big data
analytics Market is predicted to grow by
30% reaching up to 68 billion US by 2025
the growth means more job opportunity
and higher salaries for data analyst
with demand good pay and plenty of
opportunities data analytics is a future
proof career choice data analysts play a
crucial role in helping companies
navigate complex data and make informed
decisions this role offers job security
career growth and the satisfaction of
knowing your work making a real impact
all right let's get started data
modeling my friends it's all about
defining the structure of your database
imagine you've got this raw data coming
from an API or maybe some sales
information it's usually a bit of mess
right just a bunch of numbers letters
and codes representing transactions
customer details and whatnot well data
modeling helps you bring order to that
chaos by aligning the data and
integrating it into your existing
database think of it like this suppose
you are an architect designing a
blueprint for your data you decide how
different pieces of information relate
to each other like how payment connects
to orders or how customer details link
to their purchase history it visually
represents the different data elements
and how they relate to each other
following certain predefined rules and
guidelines but why bother with all of
this you might ask well data modeling is
a foundation for everything you do with
your data it ensures data integrity and
accuracy making sure your machine
learning models aren just training on
garbage data or giving you nonsensical
predictions moreover it aids in database
design application development and data
analysis allowing you to extract
valuable insights from your organized
information and let's not forget about
data governance having a clear
understanding of where your data lives
what types of data you're storing and
the security measures is the key to
compliance with those pesy regulations
and standards now let's talk about the
different types of data models out there
at first we've got a good old model
which is the relational model this uses
tables to store data and establish
relationships between them it's
relatively simple to set up and has
powerful query capabilities making it a
popular choice then we've got the
objectoriented model which combines data
and operations into a single model these
are great for applications with complex
relationship and operations like maybe
modeling the robotics of a car factory
but probably not an AI model that
wouldn't make much sense right we've got
also network data models which are an
extension of hierarchial models it
allows for multiple parent records for
each child representing more complex
relationship and let's not forget about
the entity relationship model which
gives you a clear logical view of your
data by mapping out entities attributes
and relationship it's like a giant web
of connections making it easy to
understand how data flows from one point
to another now when it comes to
implementing data modeling you've got a
bunch of tools at your disposal from
dedicated Solutions like er studio and
DB schema to open source options like PG
modeler and MySQL workbench you've also
got collaborative online tools like
usage chart which is perfect for teams
working together on data models and
let's not forget the good old Oracle SQL
data modeler and IBM infosphere data
architect for those who are in business
intelligence and analytics game but wait
there's more the data modeling process
itself is a journey first you start with
the conceptual modeling where you get a
high level understanding of the data
you're working with and what your
stakeholders need from it it's like the
rough draft of your data blueprint then
we move on to the logical data modeling
where we get technical details
attributes primary Keys foreign Keys
relationship between tables next we have
physical modeling this is where we
design our database schema and optimize
it for performance indexing and
partioning to make sure your data runs
like a well oiled machine
after that you finally create the
database and start ingesting your
business data but the fund doesn't stop
there you've got to keep maintaining and
updating your data model as your needs
change capturing those data changes over
time and making adjustments to keep
everything run smoothly and there you
have it data modeling is the backbone of
your data Universe ensuring that your
application analytics and decision-
making process are built on the solid
foundation of well stru structured and
organized information after establishing
a robust data model the next step is to
delve into the fascinating world of data
analysis this is where the real magic
happens data analysis empowers you to
uncover hidden patterns Trends and
relationship within your meticulously
organized data through data analysis you
can transform raw information into
actionable intelligence that drives
informed decision making it's like
uncovering a treasure drove of knowledge
buried within your data waiting to be
explored and leveraged for strategic
Advantage now let's switch gears and
talk about data analysis well data
analysis is all about evaluating the
data itself it's like being a detective
looking for Clues and patterns in the
data to tell a compelling story you
might run reports create custom queries
or even merge data from multiple sources
to get a complete picture it's like
piecing together puzzle but with numbers
and information instead of colorful
pieces we will now discuss the several
types of data analysis
each serving a different purpose at
first we have descriptive analysis which
help you to see what has already
happened by looking at the past data
then we have the diagnostic analysis to
determine why something happened by
examining the data predictive analysis
uses past data to predict what might
happen in the future and finally
prescriptive analysis combines all
others to make recommendations for the
future where does a data analyst fit
into this intricate web of data modeling
well a data analyst is the master
architect designing the blueprints that
bring order to the chaos of raw data
they Define how different pieces of
information relate to each other
following predefined rules and
guidelines it's like creating a massive
catalog for your organization data
assets ensuring everything is accounted
for and organized to be a master of data
analysis you just need two crucial
skills first you need to have a solid
understanding of the business what the
data means and how it impacts the
organization operation but that's not
all you also need some serious technical
jobs you need to know your way around
databases business intelligence systems
and the tools needed to extract and
analyze the precious data and let's not
forget the demand for skilled data
analyst is soaring according to the
industry reports the average data
analyst salary in the US projected to
reach around
$75,000 with the opportunities for
experienced professionals to earn well
over six figures now you might be
thinking are in data modeling and
Analysis two different skill sets well
you're not wrong but here's when it's
get interesting data analysis and data
modeling data modeling and data analysis
often overlap to truly understand how
data moves from one system to another or
how different field maps together you
might need to flex your data analysis
and dive into the raw data itself this
Synergy has led to the rise of combin
business intelligence and business
analysis role a dynamic duo where data
modeling and data analysis join forces
to create powerful business insights and
intelligence it's like having a
superhero team from where each skill set
compliments the other now you might be
thinking but wait aren't there still
opportunities for more General
functional business analysis roles while
dedicated business intelligence is in
high demand there's still a thriving
market for those who prefer a more
broad-based approach let us now discuss
the career paths in data related rules
the key thing to understand is that data
modeling and data analysis go hand in
hand good modeling ensures you have
accurate structured data skilled
analysis that allows you to truly unlock
the value in the data through insights
companies need expertise in both areas
for a comprehensive data strategy if you
really enjoy digging into Data to find
hidden insights a dedicated business
intelligence analyst rule could be
perfect but there are also General
business analyst role available that
still requires good data modeling skills
whichever path you choose having skills
in both data analysis and modeling is
very valuable with strong abilities in
these areas your data projects are much
more likely to succeed hi guys this is R
from Simply learn and today we're going
to look at three very important data
related roles in the field of data
science and then we're going to pit them
against each other so welcome to data
scientist versus data analyst versus
data engineer now let's have a look at
what's in store for you firstly we'll
talk about the job descriptions the
skill sets required for each role the
salary the roles and responsibilities
and the companies hiring for these
positions so now let's have a look at
each of these roles in detail first off
let's have a look at data scientists now
a data scientist is able to create
machine learning based tools or
processes within the company now they
use Advanced Data techniques such as
clustering division trees neural
networks and so on so that they can
derive business conclusions they are the
senior most member in the team which
involves a data engineer as well as a
data analyst now they need to have
in-depth knowledge of Statistics data
handling and machine learning they also
take inputs from data Engineers as well
as analysts so that they can formulate
actionable insights for the business now
data scientist also needs to have the
same skills as a data analyst and an
engineer but needs to have a lot more
in-depth knowledge and expertise with
these skills next up we have data
analyst now a data analyst is someone
who's able to translate IC data into a
form that everyone in the organization
can understand now this is an
entry-level position in the data
analytics team he or she needs to have
technical skills in programming
languages such as Python and have
knowledge of tools like Excel and
understand the basics of data handling
modeling and Reporting now in due time
they can move up the ranks by taking up
roles of data engineer and data
scientist with some experience that they
can accumulate over the years and
finally we have data engineer now data
engineer is someone who's involved with
pairing data who's involved with
preparing data for analytical or
operational purposes now they are the
intermediary between the data analyst
and the data scientist he or she needs
to have a lot of experience when it
comes to developing constructing and
maintaining architectures now they do
generally work on big data and submit
their reports to the data scientist so
that they can be analyzed now let's have
a look at the skill sets required for
each of these roles first off we have
data scientist now since this role is a
little more coding oriented you need to
know a great deal when it comes to
programming languages programming
languages such as python R SQL SAS Java
and so on now you also need to be well
vered with Frameworks in relations to
Big Data such as Pig spark and Hadoop
peing of adop if you want to learn more
about how it works I suggest you click
on the top right corner and watch our
video on what is AO coming back data
scientists also need to be welled with
machine learning deep learning and other
similar Technologies next up we have
data analyst now this role is much less
technical as compared to a data
scientist as well as a data engineer
considering how it's entry level here
knowing programming languages is a great
bonus so an idea about programming
languages such as python R SQL
JavaScript ssas and so on is a great
benefit at the same time you do need to
be wellers with tools such as SAS Miner
Microsoft Excel ssas SPSS and so on and
finally we have data engineer now being
a data engineer requires you to be well
versed with a bunch of programming
languages as well as Frameworks now you
need to know about programming languages
such as python R SQL SAS Java and so on
while having expertise in Frameworks
such as Hadoop map ruce Hive Pig Apache
spark data streaming nosql and so on now
let's talk about money or the salary
each of these roles get firstly we have
the data scientist who earns a whopping
$137,000 perom then we have the data
analyst who earns $67,000 per Anum which
is a pretty high salary when you
consider that it's only an entry-level
job and a data engineer which is in the
median with
$116,000 perom now let's talk about
roles and responsibilities firstly we
have the data scientist now a data
scientist gets to work with a lot of
unstructured data so they need to mine
and clean the data so that it's usable
they need to be able to design machine
learning models to work on the big data
they need to infer and interpret the
analysis on Big Data to be able to lead
an entire team to achieve the goals of
the organization and deliver conclusions
that have a direct business impact now
let's have a look at the roles and
responsibilities of a data analyst they
need to use queries to gather
information from a database they need to
process the data and provide summary
reports they need to use basic
algorithms for their work such as linear
regression logistic regression and so on
and have core skills in statistics data
monging data visualization and
exploratory data analysis and finally we
have data engineer now they need to mind
through the data so that they can gain
insights from it they need to convert
erroneous data into a usable form so
that they can be further analyzed they
need to write queries data they need to
maintain the design as well as
architecture of the data and create
large data warehouses using ETL or
extract transform load now let's have a
look at some of the companies hiring for
this role firstly for data scientists
you have City Bank Facebook Schneider
Intel Amazon and so on for data analyst
you have infosis Oracle Visa Capital 1
Walmart and so on and for data engineer
you have Google Cisco flast cognizant
Apple Spotify and much much more now now
that we have looked at the various steps
involved in data analytics let's now see
the different tools that can be used to
perform the ADB
steps so as you can see we have seven
tools including a few programming
languages that will help you perform
analytics better now let's discuss them
one by
one first we have
python python is an objectoriented open-
Source programming language that
supports a range of libraries for data
manipulation data visualization and data
modeling python programmers have
developed tons of free and open source
libraries that you can use you can find
many of them via the python package
index which is
pypi the repository of python
software python provides the default
package installer called pip or pip
Python has libraries such as numpy for
numerical computation of data pandas to
manipulate data on numerical tables and
time series then you have scipi for
Technical and scientific computations it
also provides psyit learn which is a
machine learning library for creating
classification regression and clustering
algorithms and finally it also has P
torch and tensorflow for deep
learning up next we have
r r is an open- Source programming
language majorly used for numerical and
statistical analysis it provides a range
of libraries for data analysis and
visualization some of these libraries
are gg+ plot tidos plotly deer and
carrot then we have
tblo Tabo is a popular data
visualization and analytics tools that
helps you create a range of
visualizations to interactively present
the data build reports and dashboards to
Showcase insights and Trends it can
connect with multiple data sources and
give hidden business insights and
patterns then we have a competitor of
Tableau which is powerbi
powerbi is a business intelligence tool
developed by Microsoft that has an easy
drag and draw functionality and supports
multiple data sources with features that
make data visually appealing powerbi
supports features that help you ask
questions to your data and get immediate
insights you can also forecast your data
for predicting future
Trends so the next tool is Click
view click view provides interactive
analytics with inmemory storage
technology to analyze vast volumes of
data and use data discoveries to support
decision
making it provides social media
Discovery and interactive guided
analytics it can manipulate huge data
sets instantly with
accuracy up next we have Apache
spark Apache spark is an open source
data analytics engine to process data in
real time and Carry Out complex
analytics using SQL queries and machine
learning
algorithms it supports spark streaming
for real-time analytics and Spark SQL
for writing SQL
queries it also has spark MLB which is a
library that has a repository of machine
learning algorithms and then it has
graphics for graphical
computation and finally we have
SAS SAS is a statistical analysis
software that can help you perform
analytics visualize your data write SQL
queries perform statistical analysis and
build machine learning models to make
future predictions SAS empowers our
customers to move the world forward by
transforming data into
intelligence SAS is investing a lot to
drive software Innovation for analytics
Gartner has positioned SAS as a magic
quadrant leader for data science and
machine
learning moving on to the applications
of data
analytics data analytics is being used
in LA almost every sector of business
these days let's discuss a few of
them first we have
retail customers expect retailers to
understand exactly what they need and
when they need it data analytics helps
retailers meet those
demands retailers not only have an
in-depth understanding of their
customers but they can also predict
Trends recommend new products and boost
profitability
retailers create assortments based on
customer preferences invoke the most
relevant engagement strategy for each
customer optimize supply chain and
Retail operations at every step of the
customer
Journey the second application is on
Healthcare Healthcare Industries analyze
patient data to provide life-saving
diagnosis and treatment
options they also deal with healthcare
plans and insurance information to drive
key insights using analytics they can
discover new drugs and come up with new
drug development
methods Advanced analytics allows
healthcare companies to improve patient
outcomes and
experience cancer cells and diabetic
retinopathy can be discovered using
Medical
Imaging at number three we have
manufacturing for manufacturers solving
problem is nothing new they fight with
difficult problems and situations on a
daily basis from complex Supply chains
to motion applications to labor
constraints and Equipment breakdowns
they deal with such problems on a
regular basis using data analytics
manufacturing sectors can discover new
cost saving and revenue
opportunities the fourth application is
related to the banking
sector Banking and financial
institutions collect vast volumes of
structured and unstructured data to
derive analytical insights and make
sound financial decisions using
analytics they can find out probable
loan defaulters customer chout rate and
detect fraudulent transactions
immediately the final application is
based on
Logistics logistics companies use data
analytics to develop new business models
that can ease their business and improve
productivity they can optimize routes to
ensure delivery reaches on time in a
cost efficient manner they also focus on
improving order processing capabilities
as well as Performance
Management with that now let's look at
the companies using data analytics on a
daily
basis so we have the e-commerce giant
Amazon then we have Accenture followed
by the American healthcare service
organization
siga then we have the American supplier
of Health Information Technology
Solutions Services devices and Hardware
so learner followed by Target and
Antivirus Company mac
Cafe next we have Rapido which is an
Indian bike rental company based in
Bangalore after that we have Flipkart
and the world's largest retail company
Walmart with that let's understand a
case study from Walmart and how it uses
data analytics to grow its business and
serve its customers better
Walmart is an American multinational
retail company that has over 11,500
stores in 27 countries worldwide and it
has e-commerce websites in 10 different
countries it has more than 5,900 retail
units operating outside the United
States with 55 banners in 26 countries
with more than 7 lakh Associates serving
more than 100 million customers every
week it has over 2.2 million employees
around the world and 1.5 million
employees in the United States alone
Walmart's e-commerce Branch alone
employs more than 3,000 technologists
from Silicon Valley to India England and
South
America more than 240 million customers
shop at Walmart each week online and at
its Banner stores walmart.com sees up to
100 million unique visitors a month
according to Comm score and is growing
every year
Walmart collects over 2.5 petabytes of
data from 1 million customers every hour
that's really
huge now to make sense of all this
information Walmart has created data
Cafe a state of the art analytics Hub
located within its bonv Arkansas
headquarters here over 200 streams of
internal and external data including 40
petabytes of recent transactional data
can can be modeled manipulated and
visualized teams from any part of the
business are invited to bring their
problems to the analytics experts and
then see a solution appear before their
eyes on the nerve centers touchscreen
smartboards Walmart also constantly
analyzes over 100 million keywords to
know what people near each store are
saying on social media to understand the
customer behavior on what they like and
dislike Walmart user modern tools and
Technologies to derive business insights
and improve customer satisfaction some
of these tools include python SAS nosql
databases such as cassendra and
Hadoop Now using all these Technologies
and data analysis techniques Walmart can
better manage its supply chain optimize
product assortment personalize the
shopping experience give relevant
product recommendations and finally
optimize and analyze Transportation
lanes and routes for its Fleet of
trucks with that let's jump into our use
case demo where we will predict the
sales based on Advertising expenditure
using the linear regression model in
R the advertising expenditure has been
made via different mediums such as radio
television and
newspaper we will use the r programming
software to implement the
demo so why
R well r is a free and open- Source
software that can be downloaded from the
arran website it is easy to learn and
use our language is built specifically
for performing statistical analysis data
manipulation and data mining using
packages such as plier deer tider and LU
dat R supports data visualization with
the help of packages such as ggplot
Google v r color Brewer leaflet and GG
map and finally the r software can be
used in a wide range of analytical
modeling including classical statistical
test linear and nonlinear modeling data
clustering time series analysis and
more now let's have a look at our data
that we will be using for this
demo here is our advertising CSV data
set which has four columns you can see
there's tv ads expenditure the next
column is for radio ads then we have the
newspaper ads and the last column is our
Target column that is the
sales so the data set
has in total 200
rows now to understand the data Let me
Give an
example so consider the second row so
suppose you spend around $230 on TV ads
then $ 37.8 on radio ads and $ 69.2
newspaper
ads you can expect to sell nearly 22
units of a particular product
similarly if you are spending $44.5 in
TV advertising $3 39.3 in radio ads and
$45 in newspaper ads you can sell around
10 units of certain item we will analyze
this data using linear regression so
linear regression is a super supervised
learning algorithm which means the data
has labeled columns and is used to
predict numeric continuous variables so
our sales column here is the target
column and it has continuous numeric
variables now let me go to the r studio
and start with the
demo so first I'll create a new file
then I'll select our script
the next step is to install all the
necessary packages that we need for this
demo if you already have the packages
installed in your R Studio you need not
do it again you can just call these
packages using the library function and
pass the package names so
first I will install the deer package
which is used for data manipulation I'll
be using install. packages function and
I'll give the package
name so I'll type install do packages if
you hit tab it will autoc
complete then
under
quotations I'll write
deer I'm not going to run this because I
already have it installed in my R
Studio The Next Step I'll write I'll
call this uh package using Library
function I'll give the package name
depler I'll run
this then I I'll install the broom
package it takes the messy output of
buil-in functions in R such as linear
model or LM then T Test and turns them
into a tidy data
frame so I'll copy the above code I just
paste it
again I'll change it to
broom here also I'll change it to
broom I'll run this
okay then I'll be installing the ca
tools package which will help us build
our linear regression
model I'll paste the same code and I'll
take CA
tools and I'll call that using the
library
function I'll run
this now sometimes people face issues
Within installing this particular
package if you also face this problem do
visit the r Studio Community page now
let me show it to
you so this is the r Studio Community
page and here they have the solution you
can just go through this two
pages all
right after this I will install the
ggplot 2 package which is a very popular
package in our for data visualization
I'm not running install packages because
I have already installed all these
packages
before if you have not so you have to
run install. packages first and then
call the library
function with that let's now load the
data set
for this I will use the read. CSV
function and provide the path location
where my data is located followed by the
data set name and the extension I'll
assign the loaded data set to a
variable let me now go ahead and show
you where my data set is
located so here is my advertising CSV
data set and this is the location I'll
copy this location
I'll move back to our
studio and let me comment this line load
the data
set so I'll take a variable name
adds and then
using read. CSV
function I'll pass the path location
where the data set is present
now one thing to notice we have to
change all the backs slash to forward
slash otherwise R won't accept
it and finally I'll give the data set
name which
is
advertising.
CSV let me run it okay we have
successfully loaded our data set now let
us look at how our data set looks like
using the head
function so I'll give a comment
display the head of the data
set I'll be using the head function and
I'll
pass
ads I'll run it
so you can see the head function has
displayed the first six rows from the
advertising data
set let me now check the dimensions of
the data set so I'll use the dim
function it will give you the total rows
and columns present in the data
set give a comment check the
Dimensions I'll use the dim function and
I'll pass in the ads
variable you can see it has given the
number of rows which is 200 and the
total columns which is
four now if you want to get a summary of
the data set you can use the summary
function so I'll directly type in
summary
and I'll give
ads let me expand
this so actually summary function gives
you information about a few statistics
for each of the
columns so you can see the minimum value
for each column the maximum value for
each column the mean the median first
quartile and the third quartile
values the first quartile or lower
quartile is the value that cuts off the
first 25% of the data when it is sorted
in ascending order the second quartile
is the median which has the value that
cuts off the first
50% and the third
quartile or the upper quartile is the
value that cuts off the first 75% of
data moving ahead
let's do some data visualization now to
visualize our data since our data has
only numeric values using Scatter Plots
would be the best
option so we will visualize our sales
against each of the independent
variables for that I will use the plot
function and give sales in my x-axis and
the independent variable names in the y-
axis let me now do that so I'll give a
comment data visualization
first I'll use the plot function and
then in xaxis using the dollar symbol
I'll give sales then in the Y
AIS I'll give my independent variable
you can see R is automatically giving
you the suggestions I'll select
TV this then I'll take type is equal to
under codes I'll give P which stands for
points and I'll take the color
as
red so you can see under plots we have
our scatter
plot if I zoom
in you can see the red dots are pretty
much aligned in One Direction which
means if you are increasing the
expenditure on TV ads the units sold are
also increasing
equally so the more you spend on TV ads
the more sales you can
expect close
it now let's look at how sales vary
based on radio advertising expenditure
I'll copy
this paste it and under y AIS I'll
change it to
radio and I'll take the color now as
let's say blue
color I'll run
it now
out to zoom
in
now if you look at the blue dots it is
not that linear compared to to our
previous
graph you can see there are a few data
points like
this that show the sales were not good
Even after spending decent money on
radio ads but still you can expect a
decent amount of sales if you are
willing to spend on radio
advertising close
it let's now look at how sales will vary
based on the newspaper advertising
expenditure
I'll change the radio
to newspaper column and this time I'll
take color
as
green I'll run
it me zoom
in you can see the plots are very hazly
present the data is completely nonlinear
and there seems to be a low correlation
between the sales and newspaper advertis
in
expenditure now if you want to look at
these plots at a time you can use the
pairs function so I'll type Pairs and
then pass in my variable name which is
ADS I'll run
it let me zoom in so this is our plot
and you can see
this has all the visualizations so you
can see the TV sales now you can see the
sales that were made with radio
expenditure and with the newspaper
expenditure as
well I'll close
it moving
ahead let's check the correlation
between the variables and see what
inside we can get we will use the core
function or Co function and build a
correlation
Matrix first let me go ahead and install
the core plot package so I'll give a
comment correlation
analysis for this I will have to
install the core plot
package I've already got it installed
then I'll call this function using
Library I'll run
it you can see core plot the version has
been
uploaded now I'll tell you how you can
grab only the numeric columns now data
only has numeric columns but still let
me tell you how you can do it since
correlations are based on numeric
columns only this can be done using the
Supply function so for that we have
already installed the the depler
library I'll give
a variable name
as num. calls which is numeric
columns then I'll pass in the S supply
function I'll give the ads variable and
I'll check if the variable is numeric or
not so I'll use is. numeric
let me run
it and now let's display what's there in
num.
calls you can
see it says TV it's true which means TV
has numeric values even radio has
numeric values similarly for newspaper
and sales
also
then I'll use the correlation function
which is co to display the correlations
between the
variables so I'll give my variable name
as co. data and then I'll take the core
function pass in the ads
variable and I'll only filter
out the numeric
columns so comma numeric columns means
we need all the rows and the selected
columns let me run
it and now to display let me call co.
dat
again so this is our correlation output
as you can see the correlation values
are all above zero which means there is
a positive correlation between the
variables and a change in one of the
independent variables will have a
positive impact on the sales
numbers tv
ads have the maximum correlation with
sales and the value is around
78 then there is radio advertising which
has correlation of about.
57 with sales and newspaper ads have the
lowest correlation compared to the other
two which is at 22
now you can also build a correlation
Matrix using the correlation plot
method this will give you a visual
representation of the correlation
between the variables so let's see how
we can do
that I'll
type core
plot and I'll
give code.
data and I'll pass a
method as color
if I zoom in you can
see this is our correlation
Matrix on the right you can see the
scale so
minus1 is for negative correlation then
there's light red zero which is almost
white color then there's light blue and
finally dark blue for the maximum
positive correlation
the
diagonals are dark blue which represents
the same variables as in rows and in
columns so it's dark
blue tv ads and radio ads have the next
highest correlation while newspaper ads
have the lowest correlation with
sales with that let's jump into the most
important part of this analysis which is
building our regression model first we
will look at a simple linear regression
model where we will take one input
variable that is tv ads I'll be using
the LM function or the linear model
function to build the model so I'll give
a comment simple
linear
regression I'll take a variable name as
model underscore
simple and then using LM function I'll
give my target variable which is sales
and using
till I'll give my independent variable
which is TV and
data as
ads I'll run
it now that we have built our linear
regression model let's check the summary
take summary function and I'll pass in
model undor
simple let me run it now if I expand
this you can see our intercept estimate
is around
7.03 so when the TV advertising budget
is zero we can expect sales to be around
7,30 or 7030
also remember we are operating in units
of
thousand and for every $1,000 increase
in the TV advertising budget we can
expect the average increase in sales to
be around 47
units now the same summary can be
checked using the Tidy function present
in the broom
package so if I call tidy and I'll give
the model name which is model _
simple I'll run it so there you go this
gives us a tid representation of the
summary
figures
now let's build a regression model with
more than one input
variable so we'll build a multiple
linear regression
model I'll take my video name as model
uncore multiple this
time and I'll use the same LM
function I'll pass in the
seals and using
till I'll take all the column names STV
then I'll use an addition operator then
I'll take in
newspaper followed by
the radio
column and then I'll take my data as ads
let's run
it I'll follow the same drill Let Me Now
call the summary function over this
newly created model so I'll write
summary and I'll
select my model name as model undor
multiple let me run it
so the interpretation of our
coefficients is the same as in simple
linear regression model first we see
that our coefficients for TV and radio
advertising budget are statistically
significant since our P value is less
than 05 while the coefficient of
newspaper is not which is around
.86 thus changes in the newspaper budget
does not appear to have any relationship
with changes in
sales however for TV ads our coefficient
suggest that for every $1,000 increase
in TV advertising budget holding any
other predictors constant we can expect
an increase in sales of 45 units on
average similarly the radio coefficient
suggest that for every ,000 increase in
radio advertising holding all the other
predictors constant we can expect an
increase of 188 sales units on an
average now you can also call the Tidy
function over this multiple linear
regression model so let me do that I'll
call tid and I'll pass in model uncore
multiple you can see it has given the
output now you can also find the
coefficients of the model using another
method it's called the coefficient
Matrix here is how you can do that
so take a variable
name
and I'll use the summary
function I'll pass in model _
multiple
and using the dollar symbol I'll take
the parameter as coefficient
ient let me call see coefficient
now so these are the coefficients of
different
variables let me now show you another
example of how you can train a linear
regression model using the ca tools
Library
first I'll take a seed value a random
seed value of
say
101 next I will split the data into
training and testing sets I'll take 70%
for training the data and 30% for
testing the
data so I'll use a variable
sample then I'll call sample do split
take adds and
then then I'll use another parameter
called split
ratio and I'll take the split ratio as 7
which is
70% I'll run
it and
then I'll
use another variable called
train and take the subset of the
sample pass in my adds
variable and I'll select sample is equal
to equal to
True
similarly I'll take another variable
called
test
I'll use my subset function and give the
same parameters but this time I'll take
sample is equal to equal to
false which means the test sample data
set won't have any values that are
present in train data
set I'll run
it now we will use the same LM function
to create our
model so I'll take
model as my variable
and assign it to LM
function so I'll assign the linear model
to the model variable I'll take sales as
my target column use the
til followed by a DOT which means I'm
taking all the variables in terms of the
independent variables and
then I'll select my train data set
with that let's check the summary as
well so this is the summary of our newly
created
model now you can also check the
residual collector from the train model
using the residuals
function let me go ahead and assign a
variable called Rees for residual and
I'll use the residuals
function pass in my
model
then I'll convert the residuals into a
data frame
so I'll use the as. data
frame function and pass in
RS you can check the residuales
so these are the residual
values now it's time to make our
predictions using the test data set I'll
use the predict function for
this let me take another variable called
sales.
predictions and I'll use the predict
function pass in my model followed by
the test data set
now I'll run
it then let me call sales. prediction to
display the
values as you can
see these are my predicted sales
values now let me combine these
predicted sales values to our original
sales for the test data
for that I'll use the cbind function and
pass the column
names I'll take another variable called
results and use the cbind
function I'll take sales.
predictions and I'll consider the sales
column from the test
data let me check the values
now so you have the predicted sales
values and the original values of sales
but you can see the columns don't have
any name assigned to them so let me go
ahead and assign the column names using
the call names function and convert it
into a data frame to make it look
better so I'll use the call
names function and and pass in my
results
variable then I'll take a vector and
give the column names as spread for
predicted values and let's say real for
the original
values me run
it now I'll convert this into a data
frame so I'll use as. data. frame
and give my results
variable now if I display
results you can
see go on
top you can see the colums have been
assigned successfully so on the left you
have the predicted values and on the
right you have the real
values so we have successfully built our
linear regression model and PR predicted
the sales values using linear regression
in
R you can also go ahead and find the
accuracy of this model to know how good
your model is we won't be covering that
as part of this tutorial I'll leave it
for you and encourage you to do some
research on how you can find the
accuracy of a linear regression
model you will come across terms such as
mean squar error root mean squar error
and R squ value if you are able to find
the accuracy please post the results in
the comment section or if you face any
issues with it please post your queries
we'll be happy to help you and if you're
an aspiring data analyst try giving a
short to Simply learns postgraduate
program in data analytics from P
University in collaboration with IBM the
link in the description box and the pin
comment should take you to the
programing go online analytical
processing oap and online transaction
processing in short olp are two popular
database systems that have evolved
separately to serve distinct purposes
while both system offer data storage and
r capabilities they differ significantly
in terms of their architecture data flow
and performance characteristics oap is
designed specifically for data analysis
and Reporting while oltp is designed for
online transaction processing such as
online banking Inventory management and
order processing oap is optimized for
analyzing large volumes of data and
Reporting metrics while olp is optimized
for quickly processing individual
transactions there are many other
differences between these two crucial
data processing systems so in this video
we'll be discussing the major
differences between these two and how
you can Implement these in large
transaction processing systems so
without any further Ado let's get
started also if you're an aspiring data
analyst looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns postgraduate program and
data analysis from P University in
collaboration with IBM should be your
right choice for more details use the
link in the description box below and
with that in mind over to our training
experi wordss approaches stand out in
data processing and data analysis online
analytical processing that is oap and
online transactional processing o TP
although they share the common goal of
handling data these methodologies differ
significantly in their purpose data
structure performance characteristics
and design principles oap is a powerful
tool for organizations seeking to
extract valuable insights from vast
volumes of historic iCal data its
multi-dimensional data model organized
in cubes comprising dimensions and
measures allows for sophisticated
analysis and in-depth Exploration with a
focus on aggregating and summarizing
information oap empowers users to
identify Trends patterns and
correlations across multiple Dimensions
despite longer response times which are
acceptable due to their analytical
nature o AP excels in providing flexible
ad hoc qu capabilities for complex data
Explorations conversely oltp is designed
to handle realtime transactional
processing ensuring the integrity and
consistency of daily operational
activities by adopting a normalized data
model oltp optimizes the storage and
retrieval of individual records in
highspeed environments its primary
objective revolves around efficient data
modification such as inserting updating
and deleting records
to support concurrent transactional
operations with a focus on rapid
response times and maintaining data
accuracy at record level oltp caters to
the needs of time sensitive business
operations understanding the
distinctions between oap and olp is
crucial for organizations to choose the
appropriate data processing approach
based on their specific requirements
today we will be understanding the
difference between oap and o t
by going through the following details
mentioned in the agenda we will get
started with the first one which is its
purpose then data structures data volume
response time query complexity data
modification data granularity
concurrency data backup and recovery and
finally the system design so with the
briefing of O AP and olp discussed and
the agenda for the session discussed
let's Pro point of today's session that
is the major or top 10 differences
between oil AP and oil TP firstly we
will go through the purpose reporting
enabling users to gain insights from
large volumes of historical data it
focuses on providing aggregated and
summarized views of the data
oltp oltp is designed for real-time
transaction processing handling
day-to-day operations such as inserting
updating and deleting individual records
its primary objective is to ensure data
integrity and support high-speed
transactional
operations next is data structure oap
uses a multi-dimensional data model
called AQ it organizes data into
Dimensions such as time geography and
product and measures such as sales and
profit to facilitate multi-dimensional
analysis and drill down capabilities
then we have oltp oltp uses a normalized
data model with tables and relationships
aiming for efficient transaction
processing it minimizes the data
redundancy and ensures data consistency
through the use of normalization
techniques next ahead we have the data
volume oap deals with large volumes of
historical data typically containing
years of data it focuses on analyzing
and summarizing this vast amount of
information next is O TP oltp deals with
relatively smaller volumes of data
usually representing realtime
transactions happening within a shorter
time frame moving ahead we have response
time oap allows for longer response time
since it deals with complex queries and
large data sets users expect analytical
reports to be generated within minutes
or even hours coming into oltp olp
requires worse response times to support
realtime transaction processing users
expect quick responses usually in
milliseconds or seconds a lot faster
than
oap moving ahead we have query
complexity oap queries are usually
complex involving aggregations grouping
filtering and calculations across
multiple Dimensions users need flexible
ad hoc wearing capabilities to perform
data analysis moving ahead we have oltp
olp queries are relatively simple
primarily focused on retrieving or
modifying individual records based on
specific transactional needs queries are
typically short and transaction oriented
moving ahead we have the sixth one that
is data modification oap is Ron or
minimally updated data is loaded into o
AP cubes periodically example daily or
weekly to update the analytical database
with the new
information coming into olp o TP
involves frequent data modification
including insertions updates and
deletions it ensures that the
transaction database remains up toate
and reflects the current state of
business next in the docket we have data
granularity oap deals with aggregated
and summarized data providing with a
high level view of information across
various Dimensions it focuses on Trends
patterns and overall performance
analysis next we have oltp oltp operates
at a detailed level capturing individual
transactional data with a focus on
maintaining accuracy and integrity at a
record
level next we have the eighth point
which is about concurrency oap involves
a low level of concurrent users since
users typically perform separate
analysis and Reporting tasks that
emphasizes on analytical activities
rather than concurrent transactional
processing next oltp olp requires high
level of concurrency to handle multiple
users simultaneously accessing and
modifying the same data it focuses on
maintaining data consistency and
isolation amongst concurrent
transactions nth Point data backup and
Recovery o AP is usually derived from o
TP system and backup and Recovery are
less critical it can be generated from
the transactional database if necessary
in case of oil TP oil TP data is
critical and backup and Recovery
processes are essential
regular backups are taken to ensure data
integrity and provide the ability to
store the system in case of
failures and lastly we have system
design o AP systems are typically
designed with a focus on read intensive
operations they employ specialized data
storage and indexing techniques
optimized for analytical queries and
aggregations oap databases are often
denormalized to improve query
performance and in case of 4tp oltp
systems are designed to handle a high
volume of concurrent read and write
operations they prioritize data
consistency and transactional integrity
often using normalized database
structures to minimize rund dency and
ensure data accuracy in today's
fast-paced digital landscape businesses
face a daunting challenge extracting
valuable insights from massive amounts
of data enter the ETL pipeline the
backbone of data processing and
analytics in this tutorial we will emark
on an accelerating Journey unveiling the
secrets of building a powerful etail
pipeline whether you are a season data
engineer or just starting your data
driven Adventure this video is your
gateway to unlocking the full potential
of your data together we will demystify
the etail process step by step we'll
dive into the extract phase where we
retrieve data from multiple sources
ranging from databases to apis and then
we'll seamlessly transition into
transformation phase where we clean
validate and reshape the data into a
consistent format
but wait there's more we will explore
Cutting Edge techniques for handling
large data sets leveraging cloud-based
Technologies and ensuring quality we aim
to equip you with tools and knowledge to
create robust and scalable ETL pipelines
to handle any data challenge so buckle
up and get ready to revolutionize your
data workflow join us in this
accelerating journey to master the art
of ETL pipelines having said that if
you're an aspiring data analyst looking
for online training and certifications
from prestigious universities and in
collaboration with leading experts then
search no more simply learns
postgraduate program in data analytics
from PUO University in collaboration
with IBM should be a right choice for
more details use the link in the
description box below with that in mind
over to our training experts hey
everyone so without further Ado let's
get started with ATL pipeline so ATL
basically stands for extract transform
and lo so ETL pipelines fall under the
umbrella of data pipelines a data
pipeline is simply a medium of data
extraction filtration transformation
exporting and loading activities through
which the data is delivered from
producer to Consumer to make it a little
simpler the data is produced in two type
let's say you run a vehicle showroom and
you are being a data producer so the
data that you produce is very less and
that could be basically fit into an
Excel sheet this type of data might need
update once in 24 hours or based on your
audit cycle here we call it batch data
and this data is processed using the O
TP model and batch processing tools but
now let's say you're running an entire
vehicle manufacturing plant now the data
you're dealing with is voluminous and
includes various types of data it can be
structured data un data semi-structure
data ranging from space inventory to all
the way up to robotic assembly sensus
data based on requirements this type of
data needs updates maybe every hour
every minute or even every second such
type of data is called realtime data and
needs realtime data streaming Frameworks
and the data is processed using oap
models now ETL is involved in both these
approaches now let's dive in and
understand what exactly is an ETL
pipeline ETL stands basically for
extract transform and load and
representing these three core steps in
data integration and transformation
process let's dive into each phase and
explore their significance firstly
extract the first step in ETL pipeline
is extracting data from various sources
these sources can range from relational
databases data warehouses apis or even
streaming platforms the goal is to
gather raw data and bring it into
centralized location for further
processing tools like Apache Kaa Apache
nii or even custom scripts can be used
to perform the extraction efficiently
next is transform once the data is
extracted it often requires significant
cleaning validation and restructuring
this is transformation phase the
transformation phase ensures that the
data is consistent standardized and
ready for analysis Transformations can
include tasks such as data cleansing
filtering aggregating joining or
applying a complex business rules tools
like Apache spark Talent or python
libraries like pandas are commonly used
for these Transformations lastly we have
the load phase the final step is loading
the transform data into Target systems
such as data warehouse data leag or
database optimized for analysis this
allows business users and analysts to
access and query the data easily loading
can envolve batch processing or
real-time streaming depending upon the
requirements of the Business
Technologies like Apache hadu Amazon red
shift or Google bigquery are often
employed for efficient data loading now
that he understood the core phases let's
explore some key Concepts and best
practices for building robust ETL
pipelines firstly data quality ensuring
data quality is crucial for Reliable
analysis implementing data validation
checks handling missing values and
resolving data inconsistencies are vital
to maintaining data Integrity throughout
the pipeline next is scalability as data
volumes grow exponentially scalability
becomes essential distributed computing
Frameworks like Apache Spa enable
processing large data sets in parall
allowing the pipelines to handle
increasing data loads efficiently
thirdly we have error handling and
monitoring robust error handling
mechanisms such as retry logging and
alerting should be implemented to handle
failures gracefully additionally
monitoring tools can provide realtime
insights into pipeline performance
allowing quick identification and
resolution of issues next we have
incremental loading for continuously
evolving data sets incremental loading
strategies can significantly improve
pipeline efficiency rather than
processing the entire data set each time
only the new or modified data is
extracted and transformed reducing
processing time and resource consumption
and lastly we have data gance and
security incorporating data gance
practices and adhering to security
protocols is crucial for protecting
sensitive data and ensuring compliance
with regulations like gdpr or
hiaa now that we have covered what
exactly is ETL and the ETL stages and
also the best practices for ETL
pipelines let's proceeding with
understanding the popular ETL tools so
the first one amongst the popular ETL
tools is the Apache airflow Apache
airflow flow is an open-source platform
that allows you to schedule Monitor and
manage complex workflows Apache airflow
provides a red set of operators and
connectors enabling seamless integration
with various data sources and
destinations next is Talent a
comprehensive ETL tool that offers a
visual interface for Designing data
integration workflows Talon provides a
vast array of pre-built connectors
Transformations and data quality
features making it ideal choice for
Enterprises and lastly
we have Informatica Informatica is a
widely used Enterprise grade ETL tool
that supports complex data integration
scarios Power Center offers a robust set
of features like metadata management
data profiling and data lineage and
powering organizations so what is data
analysis data analysis is not just a
single step but a set of processes it is
the process of collecting data then
cleaning it when I say cleaning it
simply means removing the err 11 data
and then this data is transformed into
meaningful
information we can simply relate this
process to how you make a jigaw puzzle
just like how you gather all the pieces
together and fit them accordingly to
bring out a beautiful picture data
analysis also works on almost the same
grounds to achieve the goals of data
analysis we use a number of data
analysis tools companies rely on these
tools to gather and transform their data
into meaningful insights so which tool
should you choose to analyze your data
which tool should you learn if you want
to make a career in this field we will
answer that in this session after
extensive research we have come up with
these top 10 data analysis tools here we
will look at the features of each of
these tools and the companies using them
so let's start
off at number 10 we have Microsoft Excel
all of us would have used Microsoft
Excel at some point point right it is
easy to use and one of the best tools
for data analysis developed by Microsoft
Excel is basically a spreadsheet program
using Excel you can create grids of
numbers text and formula it is one of
the widely used tools be it in a small
or large
setup the interface of Microsoft Excel
looks like
this let's now move on to the features
of
excel firstly Excel works with almost
every other piece of software in office
we can easily add Excel spreadsheets to
Word documents and PowerPoint
presentations to create more visually
appealing reports or
presentations the windows version of
excel supports programming through
Microsoft's Visual Basic for
applications
VBA programming with VBA allows
spreadsheet manipulation that is
difficult with standard spreadsheet
techniques in addition to this the user
can automate tasks such as formatting a
data organization in
VBA one of the biggest benefits of excel
is its ability to organize large amounts
of data into orderly logical
spreadsheets and charts by doing so it's
a lot easier to analyze data especially
while creating graphs and other visual
data
representations the visualization can be
generated from specified group of
cells those were few of the features of
Microsoft Excel let's now have a look at
the companies using it most of the
organizations today use Excel few of
them that use it for analysis are the UK
based company Ernest and Young then we
have Urban Pro whpr and
Amazon moving on to our next data
analysis tool at number nine we have
rapid
Miner a data science software platform
rapid Miner provides an integrated
environment for data preparation
analysis machine learning and deep
learning
it is used in almost every business and
Commercial sector rapid Miner also
supports all the steps of the machine
learning
process seen on your screens as the
interface of Rapid
minor moving on to the features of Rapid
minor firstly it offers the ability to
drag and drop it is very convenient to
just drag drop some columns as you are
exploring a data set and working on some
analysis
rapid Miner allows the usage of any data
and it also gives an opportunity to
create models which are used as a basis
for decision-making and formulation of
strategies it has data exploration
features such as graphs descriptive
statistics and visualization which
allows users to get valuable
insights it also has more than 1,500
operators for every data transformation
and Analysis
task let's now have a look at the
company using rapid Miner we have the
Caribbean Airline leward Islands Air
transport next we have the United Health
Group the American online payment
company PayPal and the Australian
Telecom company mobilecon so that was
all about rapid Miner now let's see
which tool we have at number
eight we have talent at number eight
Talent is an open source soft platform
which offers data integration and
management it specializes in Big Data
integration Talent is available both in
open source and premium versions it is
one of the best tools for cloud
computing and Big Data
integration the interface of talent is
as seen on your
screens moving on to the features of
talent firstly automation is one of the
great Boon Talent offers it even
maintains the tasks for the users this
helps with quick deployment and
development it also off offers open-
Source tools Talon lets you download
these tools for free the development
costs reduce significantly as the
process is gradually speed
up Talent provides a unified platform it
allows you to integrate with many
databases SAS and other Technologies
with the help of the data integration
platform you can build flat files
relational databases and Cloud apps 10
times
faster those were the features of talent
the companies using Talent are a from
France L'Oreal Cap Gemini and the
American multinational pizza restaurant
chain
Domino's next on the list at seven we
have Nim constant information minor on
Nim is a free and open- Source data
analytics reporting and integration
platform it can integrate various
components for machine learning and data
mining through its modular data
pipelining concept nime has been used in
Pharmaceutical research and other areas
like CRM customer data analysis business
intelligence text Mining and financial
data
analysis here is how the interface of n
application looks
like now coming to the nine
features nine provides an interactive
graphical user interface to create
visual workflows using the drag androp
feature use of jdbc allows assembly of
nodes blending different data sour
sources including pre-processing such as
ETL that is extraction transformation
loading for modeling data analysis and
visualization with minimal
programming it supports multi-threaded
inmemory data processing n allows users
to visually create data flows
selectively execute some or all analysis
steps and later inspect the results
models and interactive
views Nim server automates workflow
execution and supports team based
collaboration Nim integrates various
other open source projects such as
machine learning algorithms from Becca
hed2 Caris Park and our
project n allows analysis of 300 million
custom addresses 20 million cell images
and 10 million molecular
structures some of the companies hiring
for Nim are United Health Group asml
fractal analytics atos and LEGO Group
let's now move on to the next tool we
have SAS at number six
SAS facilitates analysis reporting and
predictive modeling with the help of
powerful visualizations and dashboards
in SAS data is extracted and categorized
which helps in identifying and analyzing
data
patterns as you can see on your screens
this is how the interface looks
like moving on to the features of
SAS using SAS better analysis of data is
achieved by using Auto automatic code
generation and SAS SQL SAS allows you to
access through Microsoft Office by
letting you create reports using it and
by Distributing them through
it SAS helps with an easy understanding
of complex data and allows you to create
interactive dashboards and
reports let's now have a look at the
companies using SAS we have companies
like genpact iqa Accenture and IBM to
name a
few that was all about SAS so for all
those who joined in late let me just
quickly repeat our list at number 10 we
have Microsoft Excel then at number nine
we have rapid minor at number eight we
have talent at number seven we have nine
and at number six we have SAS so far do
you all agree with this list let us know
in the comment section below let's now
move on to the next five Tools in our
list
so at number five we have both R and
python yes we have two of them in the
fifth
position R is a programming language
which is used for analysis as well it
has traditionally been used in academics
and research python is a highlevel
programming language which has a python
data analysis Library it is used for
everything starting from importing data
from Excel spreadsheets to processing
them for
analysis this is is the interface of
R next up is the interface of the Python
Jupiter
notebook let's now move on to the
features of both R and
python when it comes to the availability
of R and python it is very easy both R
and python are completely free hence it
can be used without any
license R used to compute everything in
memory and hence the computations were
limited but now it has changed both R
and python have options for parallel
computations and good data handling
capabilities as mentioned earlier as
both R and python are open in nature all
the latest features are available
without any
delay moving on to the companies using R
we have Uber Google Facebook to name a
few python is used by many companies
again to name a few we have Amazon
Google and the American photo and video
sharing social networking service
Instagram that was all about RN
python at number four we have Apache
spark Apache spark is an open-source
engine developed specifically for
handling large scale data processing and
analytics spark offers the ability to
access data in a variety of sources
including Hadoop distributor file system
htfs openstack Swift Amazon S3 and
Cassandra it allows you to store and
process data in real time across various
clusters of computers using simple
programming
constructs Apache spark is designed to
accelerate analytics on Hadoop while
providing a complete Suite of
complimentary tools that include a fully
featured machine learning library a
graph processing engine and stream
processing so this is how the interface
of a a spark looks
like now let's look at the important
features of Apache
spark spark stores data in the ram hence
it can access the data quickly and
accelerate the speed of analytics spark
helps to run an application in a Hadoop
cluster up to 100 times faster in memory
and 10 times faster when running on
dis it supports multiple languages and
allows the developers to write
application in Java Scala r or python
Spar comes up with 80 high level
operators for interactive querying Spar
code for batch processing join stream
against historical data or run ad hoc
queries on stream
State analytics can be performed better
as spark has a rich set of SQL queries
machine learning algorithms complex
analytics Etc Apache spark provides fall
tolerance through spark rdd spark
resilient distributed data sets are
designed to handle the failure of any
worker node in the cluster thus it
ensures that the loss of data reduces to
zero conviva Netflix iqa Loy Martin and
eBay are some of the companies that use
a party spark on a daily
basis at number three we have another
important growing data analysis tool
that is Click
view click view software is a product of
Click for business intell and data
visualization click view is a business
Discovery platform that provides
self-service bi for all business users
and
organizations with click view you can
analyze data and use your data
discoveries to support decision making
click viw is a leading business
intelligence and analytics platform in
Gartner magic
quadrant on the screen you can see how
the interface of Click view looks like
now talking about its features click
view provides interactive guided
analytics within memory storage
technology during the process of data
Discovery and interpretation of
collected data The Click view software
helps the user by suggesting possible
interpretations click viw uses a new
patent in memory architecture for data
storage all the data from the different
sources is loaded in the ram of the
system and it is ready to be retrieved
from there it has the capability of
efficient social and mobile data
Discovery social data Discovery offers
to share individual Data Insights within
groups or outou
ofit a user can add annotations as an
addition to someone else's insights on a
particular data report click view
supports mobile data Discovery within an
HTML F enabled touch feature which lets
the user search the data and conduct
data Discovery interactively and explore
other server-based
applications click view performs only
appb and ETL features to perform
analytical operations extract data from
multiple sources transform it for usage
and load it to a data
warehouse the companies that can help
you start your career in Click view are
Mercedes-Benz Cap Gemini City Bank
cognizant and Accenture to name a
few at number two we have
powerbi powerbi is a business analytic
solution that lets you visualize your
data and share insights across your
organization or embed them in your app
or
website it can connect to hundreds of
data sources and bring your data to life
with live dashboards and
reports powerbi is the collective name
for a combination of cloud-based apps
and services that help organizations
collate manage and analyze data from a
variety of sources through a
userfriendly
interface power bi is built on the
foundation of Microsoft Excel and has
several components such as Windows
desktop application called powerbi
desktop and online software is a service
called powerbi service mobile powerbi
apps available on Windows phones and
tablets as well as for IOS and Android
devices here is how the powerbi
interface looks like as you can see
there is a visually interactive sales
report with different charts and graphs
moving on to the features of
powerbi it has an easy drag and drop
functionality with features that make
data visually appealing you can create
reports without having the knowledge of
any programming language powerbi helps
users see not only what's happened in
the past and what's happening in the
present but also what might happen in
the
future it offers a wide range of
detailed and attractive visualizations
to create reports and dashboards you can
select several charts and graphs from
the visualization pan powerbi has
machine learning capabilities with which
it can spot patterns in data and use
those patterns to make informed
predictions and run what if
scenarios powerbi supports multiple data
sources such as Excel Tech CSV Oracle
SQL Server PDF and XML files the
platform integrates with other popular
business management tools like
SharePoint Office 365 and dynamic 365 as
well as other non-microsoft products
like spark Hadoop Google analytics sap
Salesforce and
MailChimp some of the companies using
powerbi are Adobe AXA carlsburg cap mini
and
Nestle moving on to the next tool so any
guesses as to what we have at number one
you can comment in the chat section
below
finally on the top of the pyramid we
have
Tablo Gartner's magic quadrant of 2020
classified tblo as a leader in business
intelligence and data analysis tblo
interactive data visualization software
company was founded in Jan 2003 in
Mountain View
California tblo is a data visualization
software that is used for data science
and business intelligence it can create
a wide range of different visualization
to interact activ L present the data and
showcase
insights the important products of tblo
are tblo desktop tblo public tblo server
tblo online and tblo
reader this is how the interface of tblo
desktop looks
like now coming to the features of
tblo data analysis is very fast with
tblo and the visualizations created are
in the form of dashboards and worksheets
tblo delivers interactive dashboards
that support insights on the
fly it can translate queries to
visualizations and import all ranges and
sizes of data writing simple SQL queries
can help join multiple data sets and
then build reports out of it you can
create transparent filters parameters
and
highlighters Tableau allows you to ask
questions spot Trends and identify
opportunities with the help of tblo
online you can connect with cloud
databases Amazon dread shift and Google
big
query the companies using tblo are Deo
Adobe Cisco LinkedIn and the American
e-commerce giant Amazon to name a few
and there you go those are the top 10
data analysis
tools let's now have a question and
answer session please feel free to post
your queries in the comment section and
we'll respond in the chat
before the question answer session let's
recap quickly in the meanwhile yall can
post your questions in the comment
section
below so at number 10 we have Microsoft
Excel then at number nine we have rapid
minor at number eight we have talent at
number seven we have nine at number six
we have SAS R and python at number five
pares spark at number four click View at
number three powerbi at number two and
finally we have Tablo topping the list
at number one welcome to this tutorial
on Microsoft Excel so we will learn
about functions and formulas we will
learn about conditional formatting data
validation pivote chart and pivote table
now let's look at a scenario here so one
day in a
startup One Professional speaks that
their business is growing and they would
need an efficient way to work with the
data they would have to find a way to
work faster with storing and analyzing
data now to that another colleague
response well we can make use of
Microsoft Excel to do this job
the question is will excel be able to
cater to their business
needs now the colleague responds well we
can make use of excel in several ways
and it also is a coste efficient option
now in that case the colleague who POS
the question says let's go ahead with
Excel and let's train our employees in
Excel and the suggestion is welcomed
which would make the job easier for them
and they would basically decide on using
Excel so they decide on taking a
training right away and basically
starting to learn Excel now before we
move to excel one of the question is why
should we use Excel so let's look at
some of the points here so Excel proves
to be a great platform to perform
various mathematical calculation on
large data sets which is one of the
biggest requirements of various
organizations these days various
features in Excel like searching sorting
filtering makes it easier for you to
play with the data and Excel also allows
you to beautify your data and present it
in the form of charts tables and datab
bars now when it comes to reporting
reporting accounting and Analysis can be
performed with the help of excel
it can help you with your task lists
your calendars and goal planning
worksheets Excel also provides good
security for your data Excel files have
the feature of password protection this
way your information can be
safe now when we talk about what is
Excel and how it can be used so Excel or
you might have heard a spreadsheet can
be basically used for lot of of
different tasks than just storing the
information in so-called tabular
format now Microsoft Excel is an
application that is used for recording
analyzing and visualizing data it is in
the form of a
spreadsheet let's have a look at few of
the functions and formulas used in Excel
and before we do that we can also
quickly take a small tour to understand
how to work with Excel now to do that
what we can do is we can type in in our
search say for example Excel and just
select your Excel app which is installed
and here you see you have lot of options
which says take a tour drop- down list
get started with formulas make your
pivote table going forward with pie
charts and much more so we can click on
this one which says take a tour and that
basically pops up a window which says
welcome to Excel and if you have always
wanted to be better at Excel you have
this which can help you so let's click
on Create and that takes us to the store
window now that says instructions for
screen readers which basically talks
about 10 different steps in which we can
learn Excel and using the spreadsheet
app so there are more than 11 sheets
which we see here at the bottom end and
each one gives us a simple example which
we can work on so for example if I click
on ADD now that takes me to this page
which says how do we add numbers now you
might be provided data which we can
upload by loading a file from our
machine or getting data from a web
Source or even connecting to a database
so there are various options which we
will see in some time so here we have an
option which is called Data you can
click on this one and this basically
has options where you can use existing
connections if you have created some you
can always click on from other sources
and you can get your data from SQL
Server from analysis services from o
data data feed you can get in from XML
from data connection Wizard or also from
Microsoft query you can be running in
different queries here which shows up in
the option which says new query there
are connections which you can use and
that basically will display all the
connections for this particular workbook
which we do not have as of now but we
can create them but let's look at simple
examples now you can follow these
instructions here which says basically
adding up the numbers and that could be
easily done by just placing your cursor
here and what you could do is either you
can type in the formula that is from
which row to which row you would want to
add the data so for example I could just
do a sum here and that shows up all the
different functions which are available
then we can open up a parenthesis and we
can say I would be interested in
totaling the amount
from column D and I would select for
example D4 so I could be doing this and
then I could say D4 onwards till
D7 so that's the data which I'm
interested in you can close your
parenthesis and hit enter and that gives
you the total there is also a shortcut
for this which you can always do is we
can first delete this and you can just
place your cursor here and just use your
alt and equals that automatically
selects the numericals which we can
anytime expand or basically collapse so
I will basically select this which says
this function needs two numbers which is
number one and number two and then you
can hit on enter and that gives you the
total so similarly we can be getting in
the data here by selecting all the
fields so here it also says that you can
use a shortcut now what we can also do
is we can add numbers over 50
by selecting the yellow cell here and
then giving a condition such as so I can
basically use something like some if and
then open a parenthesis I can select I
would be interested in this row and then
I can even drag and drop till here so
that tells me d11 to D15 you can then
put in a comma here and you can give
your condition say for example we would
say I would be interested in numbers
only above 50 and we can select this
close your codes and then just close
your parenthesis and that's your formula
so you can do this and that basically
gives me the total is 100 now similarly
we could do that for the amount here I
could select this now there is also an
option I can click on home and I can go
for something like Auto sum so that's
one more way of doing it which anyway
says sum is alt plus equals so it
automatically adds up your values and I
can try doing a auto sum that
automatically selects my rows and then I
can get my total now as per this
activity here it says try adding another
sum if formula here but add amounts that
are less than 100 and the result should
be 160 so what we can do is we can
basically Al select all the numbers
which are lesser than 100 so the way we
did earlier here there can be always a
shortcut so you can always for example
if you would want to avoid typing in the
formula you can always copy it from here
and then just hit on enter so you are
back into this cell and then I can
basically go here and paste the number
and then as per the requirement we are
required to select anything which is
lesser than 100 so what I could do is I
could select here I could say let's say
G and then I can change this value to
g15 and that's one more way now we see
our selected rows have been changed so I
can hit on enter and I can check what is
the result so we would be interested in
looking for numbers which are lesser
than 100 so I will have to also change
this one to a lesser sign and that
basically gives me the total which
is60 so that's how you can simply add
numbers you can use Auto sum you can
type in the formula you can select the
fields or you can just place your cursor
where you would be looking for a sum and
then you can just do a alt equals and
that basically populates the sum
now let's look at some easy options of
filling your CS or automatically
populating the values in your cells
within your Excel sheet now here we have
an option which says 100 now we can
click on this and that basically says it
is making a sum of column C4 to D4 so if
I click on this one I can check that
this is row number four which shows up
here and I also know this is column C I
also have D so this equals is basically
giving me a sum of C4 to D4 now what we
can do is we can always place our cursor
here at the right corner and then we can
just drag and drop and this basically
gives me a total of all the numbers for
all different rows so this is one short
cut which we can do to get the total
Excel will automatically give the totals
which we call as filling down now what
we can do is in the same way if we would
want to get the totals here we can first
check what is this 200 and this tells me
it is C11 to c14 total so it is totaling
the rows from C11 so column C and 11th
row till 14th row and that's the sum now
what I can all also do is I can
similarly like above we can do a filling
right which basically means bringing
your cursor here and then just dragging
and dropping it all the way where you
would need the totals and this basically
gives me the total there is one more
quick way to check if this is right so
the easiest option would be to select
this cell now what I can also do is I
can just select all of these fields by
just highlight lighting and selecting
all the fields once it is selected press
on crlr and that gives you the total now
if we would be doing this stop down then
I could select all these rows for this
particular column and then I could do a
control D so that's your filling down
and this one was filling right so this
is an easier option of doing a fill when
you would want to have the formula
applied to every row as it occurs in the
first row or the last row we could test
this by for example selecting these
fields I could delete them and I have
here which says 130 I could just place
my cursor here and I could drag it all
the way up and that should also do the
same magic which we were seeing from top
down so this is a simple way wherein you
can fill up your cells and you can also
automatically
propagate or move your computation to
all the
cells let's look at the split option
which basically helps us in splitting
the data when we have some kind of
pattern or when we have some kind of D
limits in our data in say one particular
column and we would want to derive the
values out of it so we can always use
the splitting option now the easiest
option would be so for example we have
our email column which has the email IDs
and which we can clearly see has a first
name do last name now I see that there
is a last name Smith filled up here
first name is empty so what I can also
do is I can just type in say Nancy here
now that's the first name I can again
start typing the second name and as soon
as you do that you would see a faded
list of numbers and that's your clue to
hit enter and once you do that you would
see all the first names have been filled
in here if you would want to maintain
the case sensitiveness you can just go
ahead and delete these and let's type in
as it occurs so let's say Nancy as the
first name go down to the next cell and
just type in Andy and there is your
grade list so just hit on enter and that
basically fills up your first name what
we can also do is we can just select
this particular field and either we can
type in control e which basically fills
up all the options now I can just do a
undo by typing in or clicking contrl Z
and that's basically gone what I can
also do is I can select a particular
field and then I can go into home option
and under home you have an option here
which says fill so you can select this
and then you can do a Flash Fill which
is what we are doing here so click on
Flash Fill and that automatically fills
up the values so in this way you can
work within your spreadsheet and you can
be filling up the values where a d
limiter by default is understood and we
can split the data now however sometimes
you might have some data which has a
different kind of D limiter and there is
again a smarter way of splitting your
data so you can always scroll down here
and and that says splitting a column
based on D limiters so we have some
values in the data column and these
values in each row are separated by
comma so select this your data is
already selected text to columns The
Limited comma is selected and now click
on next so it basically says what is the
destination let's select this one and I
can choose what would you want to have
so that shows me this would be my data
preview now I can basically select this
one I can say finish and say okay and
now if you see our data has been placed
in in the columns appropriately so this
is how you can split your data based on
a d limiter and then organize your data
in a better way now there are some
Advanced options which we can learn
later but this basically tells about
using a formula so this is something if
say if we have some name in one cell and
if you would want to split it into first
name your helper column your middle name
last name so that can also be done using
formulas and this basically tells how
would you extract characters from your
left cell and how would you place them
in your right cell so you can try this
activity which is a little more of
advanc option the benefit is that you
can always use this wherein if you do
some kind of transformation using your
formulas if your original data gets
updated then the split data will also
get updated and that's the benefit of
using formulas where you can place
values from one cell into multiple cells
based on execution of your details in
the
formulas how about using the trans ose
option now you might have heard of
situations where you would want to
switch or turn your rows into columns
and your columns into rows and that's
where transposing comes into picture it
might be useful when you have your data
uh in your X and Y AIS or as I would say
in rows and columns and you would want
to switch your rows to become the
columns and columns to become your rows
so what we can do is the simplest way is
you can select all your values so here
we basically have six columns and I
would say two rows now I can select all
of these and then I can select an empty
field for example the one which is
highlighted here well you can always do
a control alt V that's a shortcut what
you can also do is once you have
selected all your Fields you can just
copy them so just do a contrl c and then
click on an empty set
and then what you can do is you can do a
special paste or paste special so under
your home you have the paste option and
here you can go for pay special and once
you do that you need to select the
transpose option over here and click on
okay and now you will see that the
columns and the rows have been
transposed so your row name was item and
that has become the column heading you
had row name as amount and that has
become the column heading and all your
values have been transposed in this
particular format now there is another
way of doing that and again that's using
your formulas so what you can do is you
can transpose with a formula also and
that basically works when you have
similar kind of data so this has six
columns and basically two rows so you
can basically do this so you can select
this and earlier we were doing a copy
but now what we would want to do is we
would want to just look at the row
numbers which tells me it is c33
c34 and it starts with c and ends in
with your H column so what we can do is
we know that we have six columns and two
rows so transposing that would actually
give me two columns and six rows so what
we can do is we can select
two columns and six rows in our Excel
sheet I can then basically start typing
in the message or I can just go to the
address bar and here I can
say
transpose let's go for c33
h34 it basically selects my data and now
I can just do a control shift and enter
and now if you see see all the values
have been populated now you can just
place your cursor in one of the cells
but if you see the address bar the
formula Remains the Same this is because
this is an array formula so we can read
more about an array formula here it's
basically something which performs
calculations on more than one cell in an
array and in the example here the array
is the original data that is c33 to h34
so your transpose is just changing the
horizontal orientation to the vertical
orientation so this is a very simple way
in which you can basically use the
excel's capability to transpose your
data and convert your rows into columns
and columns into rows apart from working
on additions subtractions filling up
your data sorting the data or basically
splitting your data transposing your
data one of the other requirements is
sorting and filtering your data now that
can be very handy when you're working on
huge data and you would want to sort it
in a particular order say ascending or
descending or might be based on a
particular field or if that field was or
if the cell was highlighted with a
particular color sorting the data so
let's look at how Excel can be used for
sorting and filtering examples are
pretty simple here so let's check that
so if we going to sort and filter and
say this is the data I have say for
example I would want to sort the values
in the department column alphabetically
so what I can do is I can select
Department column and I'm already in the
Home tab I can straight away go here
which says sort and filter I can then
say sort A to Z and that's basically
alphabetically sorting your department
column and once once I do this you would
see the data has been sorted but it's
not just this data we can just do a
control Zed and check what are the
values we have so here we have meat
which is beef and
90,000 110,000 the values then you have
Bakery which should ideally be the first
row if we sort it in an alphabetical
order which goes with Bakery as deserts
you have the values so we can check this
again again so select department and
then just do a sort and filter and let's
say sort A to Z and if you see the data
has changed but it's not just in
changing your First Column but then it
has taken care of all the data however
the data has been sorted based on the
department column so you have Bakery
which aligns with deserts which has the
values and now we have all the data
which has been filtered now what we can
also do is we can sort December's
amounts from largest to smallest so what
we can do is we can basically click any
sell in the December column let's say
20,000 and then what I can do is I can
go into sort filter and then I can say
sort largest to smallest so if you see
Bakery breads is the row which has the
smallest data or might be you have deli
sandwiches so that one looks also
smaller so let's do a larger to smallest
and if I do this you would see the
values have been shifted now so it is no
more based on the department column
because now the data is being sorted
based on the values in the December
column and you see Bakery which was
alphabetically the first one has become
second last so either you can sort the
data based on a department column which
goes based on the values these are all
string values or words so it sorts
alphabetically if you have numbers might
be you can give some values and you can
sort the data you could anytime do a
custom sort and you could basically
select if you would want to select the
data so I could do a custom sort and
then I could choose which is the column
which we would want to use for sorting
what is the Sorting needs to be done is
it cell values is it cell color font
color conditional formatting and then
you can also choose the order so that's
one more way to do that now if you
scroll down that also shows how you can
sort by date or a color so for example
if you would want to sort based on the
expense date so there are different
options so what I can do is I can select
this date field I can just do a right
click
I can go into sort and then I can choose
I would want to sort oldest to newest so
since I selected the date field it
basically has sorted all the data and it
has taken the expense date into
consideration now there are these
filters which you see on the row
headings we could have also used those
so I could have selected this and that
basically says or mentions which are the
dates I would be interested in looking
looking at I also have sort by color
here I can do a sort oldest to newest or
newest to oldest so I could also use
these filters which have been applied
here now we have the data which is in
color so if I would want to basically
select the color columns or color cells
I could select this I can basically do a
right click here and when I go into sort
I could choose put selected color or
cell color on the top and that basically
will make sure that my data is sorted
and it has also sorted that in a
descending order so in this way you can
sort or basically filter your data what
we can also do is we can add filters so
sometimes we can go for formulas which
we would want to use what we can also do
is we can basically select the filter
which has been applied here now how does
the filter come in there so if I would
select a particular row I could select a
particular row and then I could decide
if I would want to just add a filter to
this one and that's how the filter has
come in so we have the filter what we
can do is we can basically click on this
drop down and then you have something
like number filters so we can always go
here we can basically choose one of
these so we can basically choose above
average so I could select this and then
basically it shows me the values we
could also delete the filter by clicking
on this one and we could say well I'm
not interested in this filter anymore so
I could clear the filter and that shows
me all the values or I could say that
let's click on some other field for
example food I can go in here I can go
into number filters and then I could say
well I'm in interested in values which
are below average above average might be
greater than and then I can choose what
is the value so for example if we say
I'm interested in food which is greater
than $25 I could give a value here I
could say okay and now I have applied
the filter similarly you can select this
and then you can just clear your filter
and your data is back so remember no
data is lost
it is just hidden or basically based on
the filter not shown so that's good
enough for us and in this way you can
sort and filter the data so for more
details obviously in all these sheets
you have the links which point to more
information on the web and you can
always refer to these so this is simple
way in which you can sort and filter any
amount of data which has been stored
within your Excel within a particular
sheet now that we have learned about add
fill split transpose sorting and
filtering it will also be good to learn
how to work with tables or basically
converting your data into a tabular
format and then doing some easy
computations so click on this tables
option here now here we see there is
some data which is in five columns and N
number of rows so I can basically select
this data and then what I can do is I
can insert choose the table option and
then it says my table as headers and
we'll be okay with that I'll say okay
and now if you see this is the table
created it basically has different
filters which we have learned earlier
how to use and this is basically my
table which is a collection of cells
which has some special features so we
can easily add rows to this table we can
add columns to this table and we can
even do some calculations so for example
here I can click on this one I can
basically enter some field and then I
can hit on enter and we see that this
row has been inserted wherein we can
easily fill in values for example I
would say
chocolates I could give some value here
say
25,000 might be 35,000 ,000 and then
basically I can give some values here
now what you can also do is you can
continue adding rows in this way and say
for example you would want more columns
so you can select this option here in
the top bottom right corner you can just
drag it towards the right and that
basically has automatically created
columns for my next month months wherein
I can feed in the data so this is a
simpler way wherein you can keep adding
more rows and colums to your data if
that has been converted in a tabular
format now let me just do a control Zed
that basically deletes the columns again
contrl Zed deletes the last row which we
added and I can stop here or I can even
remove my values by doing multiple
control Zed and removing my rows so this
is how I converted my data into a table
and then I can easily work on this what
I can also do is I can do some
calculations so what we can do we have a
table here we have a total field and
what we can do is we can just select one
cell here now as we have learned earlier
we can do a alt and then equals and that
basically says what is this doing so it
says it is calculating the sum of the
last 3 months and if that's what you
would want to do just hit on enter and
it has automatically
calculated the totals for all your rows
for these three columns so the sum
formula is getting filled up now I can
select any particular cell and I can
look in my address bar so it has already
given me the formula where it has
started calculating the sum from the
October column till the December column
and has given me the calculated values
of the columns what we can also do is we
can get total rows in the table now
that's a simpler option so what we can
do is we can select any cell in this
particular table and then we see that
there is a table tools design option
showing up here now I can select this
and then it says well let's get it total
row so let's select this and it
automatically populates the total here
and if you would want the average then
we could select this and from the
drop-down I can select what I'm
interested in so for example I would
want the average values and not the
total I could just select this and that
gives me the average of these values so
we can always do simpler computations
here by converting our data into table
format let's learn about one more
efficient way of working with the data
and that's using your dropdowns so let's
see how dropdowns work here now say for
example you have this data which has the
values in the food column and department
is empty and say for example you would
want to enter the values in Department
however you would want to select the
department should either have produce or
meat and bakery and these are the only
three options which should be available
for any user to fill in the values how
do we do that so we can basically create
a table by pressing controll D so what I
can do is under my
department here I can select one of the
cells and then I can do a control T that
basically converts this into a table I
can say okay and my table is created now
what I can do is once this part is done
we can select all the blank Fields here
where we would want this drop- down to
be applicable now under your data tab
you can go in and select data validation
and this has an option called Data
validation click on this which basically
says allow any value so here I will
select I would want to give a list of
values and then I can type in my values
here which I can say produce say for
example meat and then say Bakery now
these are the values so we can click on
okay and once we have done that we
basically have a drop- down here next to
Apples which will only show us the
values which we can feed in under the
department column so I can go into every
cell and then I can basically choose
what is the department which handles
this and then basically I can select one
of these from the dropdown so this is an
easier option of creating your drop-down
and then feeding in the values from the
set of values which you have defined
here on the right so this is a simple
example of using your drop downs working
with your tables working with your sort
and filter transpose split filling up
your data adding in some data here and
similarly you can use Excel for more
than one use case using its inbuilt
features to easily work with your data
let's see how we can import data or
bring in data into our Excel from your
local machine or from an external web
source so what we can do is we can open
up a blank Excel sheet and say for
example you have been provided a text
file or a CSV file and you would want to
import that data into your Excel sheet
that can be easily done so right now
I've opened an Excel sheet now I can
click on data and here I have an option
which says existing connections from
other data sources so or you can click
on connections if you have already
created some so we can click on from
other sources so this is one opt option
where you can connect to your different
data sources and you can get the data
from one of these what we can also do is
I can click on connections now it says
there is none I can click on ADD it says
well show the connections where
connection files on
network connection files on this
computer so I can say let's get some
files from this computer now if that
does not show up something so say browse
for more and that basically shows you
different options so let's basically
select a folder where I have some data
sets I'll click in here and this is
basically a folder where I have some
data sets now let me select this
particular file and I know it is a CSV
file so let's click on open now if you
would want to verify this you could have
gone and looked into the property of the
file and it says it is a CSV file which
is what we are interested in so I'll
take this file I'll say open now this
basically shows me the text import
wizard option which says is the file
delimited I'll say yes click on next so
I will select comma as my delimiter I
can say text qualifier is none now this
is my data so my data preview is already
showing me the data is
what is the data in the CSV file you can
click on next and then you have an
option which says data format is General
you can go for date format you can go
for advanced options so I'll just say
finish and basically now this has been
created here so we basically have this
and now I can click
on close now once you have done that you
can click on existing connections it
shows shows me the data which we have
here the connection which we have
created say open and then it says do you
want to import this data or bring this
data into existing
worksheet you can also say add this to a
data model if you're doing some data
modeling so click on okay and now this
data is automatically inserted in my
Excel file I can basically save
it
into this particular sheet now what we
can also do is we can also start a new
sheet and that does not have a data and
we can get some other data from web so
what I can do is I can go into my GitHub
and let's say I would be interested in
this CSV file so I can select this and
this is my GitHub path a path on web so
I can click on draw and that basically
gives me the raw path where this
particular file is now you can can
select this copy this particular path
and here you can come back to your Excel
sheet we would be interested in getting
the data from web might be from a text
file where we will have to specify the D
limitter or let's go to web and here I
can give in the web path from where I
would be interested in getting the file
let's give the GitHub path which is
publicly available and then click on
import now once you click on import it
tells me there are two feed data and
value these
are within double quotes separated by
comma so first let's click on import now
once we do this it will basically get
the data from web and put in here it
says existing worksheet so we had
already created a new worksheet so let's
click on okay and now you have the data
coming in but then this basically shows
me in one particular field so what we
can also do is we can just do a control
a that selects all my columns here and
that's my data so we can then
basically filter this out so we can say
text to columns it's a delimited file
click on next we can select comma and
let the text qualifier be
codes it shows me data preview click on
next so you have the general form at it
shows the destination that's the column
click on finish and now your data has
been split and you have the data which
you have imported from web so this is
the data which is coming in from web
this is the data which came in from my
local machine and similarly we can even
create connection with an existing
database so I can basically click on
connections if I would want to do that
so I have an option called connection
here
and it says where the selected
connections are used I can basically
click on ADD I can basically choose if I
would want to get the files from Network
or from computer like we did earlier I
can click on browse for more which
should show me different other options
to create connections say you would want
to create a new SQL Server Connection
you can connect to a new data source
coming in from different place you could
basically choose what kind of connection
would you want so these are all the
different options which we can go for
and we can basically connect to a
database for example if I have some
database and say for example access
database I can see if there are some
files with that particular database and
I can import it so similarly we can also
uh click in here with it says new query
and that also gives you an option of
getting the data from your files from
all these folders from databases so you
can basically click here and then you
can import data from a mySQL database
provided that is set up on your local
machine or on a particular server you
can go from cloud you can get it from
online services you can get it from
other sources which says from web from
your Hadoop file system from active
directory from a blank query and you can
even combine queries wherein you can run
a power query editor you can get the
data from different sources and then you
can bring it into your Excel so in this
way you can get your data from different
sources into your Excel into your
spreadsheet and then you can continue
working on those data sets we have uh
already learned some basic operations
which you can do in Excel and let's
Implement our knowledge by working on
this particular data which is coming in
from housing data set now here if we see
some fields we have agent date listed
area list price and this is basically
the data which has been sorted in newest
to oldest order of date listed so how do
we arrive at this so what I can do is I
can just click on data listed and then I
can either go in here I can select sort
I can get into custom sort and then I
can choose the column based on which I
would want to sort the data so I would
look for the newest data to the oldest
data that means that would be in a
descending order of dates or you could
say the oldest date or the earliest
month will be towards the lower side of
your sheet so here we can select date
listed now I can say let it sort based
on Cell values and the order what we
have here so we have newest to oldest so
let's select this I can say okay and now
if you see the date has been sorted so
we have your 1018 2017 on the top so
that seems to be the latest date and as
we go down we will see an earlier month
hour and earlier month than that in this
date listed so we have sorted our data
into newest to oldest order and that's
based on your date column so the result
shows up here now what we can also do is
we can have different questions which we
would want to answer so for example I
would want to sort the data in ascending
order of area and descending order of of
agent name how do we do that so let's
look into this so this is here I already
have the result here and how did I get
this so I'm looking for ascending order
of area and descending order of agent
name so we can start with any particular
column that does not matter so for
example if I look into this Excel sheet
I have my agent name select this which
we want in a descending order so we
could either do a sort and then go for
descending sort Z2 a we could also use
the filter option here on the top right
and we could do it or I could just say
sort Z2 a and then it has arranged the
data based on the agent column being in
descending order now I can go into area
and then I can again do a sort and I
wanted my area column to be used for
sorting the data in ascending to
descending and that basically not only
changes the order of this particular
column but for my complete data so let's
do that and now if you see we have the
data which has been sorted so we can see
how many values we have here and the
area values which we see and this is how
you get your result so I'll just do a
contrl zed and again and I'm back to my
original data and this is the sorted
result which we are seeing at similarly
we can answer other questions for
example sort the data according to the
following order of area that is we are
saying
County Central and then your county so
we can basically choose in which
particular way we would want to do it so
it is County Central and then again
County so if I look into my sheet three
so here I basically have my data which
is having some South County then you
have your Central and then you have your
North County so we would want to sort
the data to solve our problem which is
according to the following order so
first we go for area then we go for
South County Central and North County so
what we can do is we can basically have
area field selected and and I would want
to sort this particular data so I have
South County Central and North County so
I can basically go for custom
sort and then I can choose which is the
value or column which I'm interested in
let's go for area we will go for
something like cell values well you can
also try to explore conditional
formatting icon if this is what you
would want to use or we can basically go
for just sell values and here we can say
if I would be interested in first
getting my values for South County so
for example I can say custom list and
then I can basically given the new list
here so I can say
s dot
County and then I would want Central and
then then I would want North County so
let's select
this as it exists we can basically say
add and that's basically the order which
we want say okay and then say okay here
and now what we would want is we would
want our data so we can compare that
with the values which we see here it
starts with Kelly you have in the 12th
row something like Lang and that's what
we are doing so we can basically arrange
the data in a particular order by
choosing a custom list and then sorting
your data so that's one more simpler
task what we have done where we have
sorted the data in the order where under
our area column we first wanted South
County then we wanted Central data and
then we wanted North County so this is
how you can do it
now let's look into one more problem so
it says find all the houses in the
central area and we would want to
basically apply a regular filter let's
let's see how do we do that so we can
click on this and here we have the data
so the problem statement is we would
want to find all the houses in Central
Area now how do we do that we can do a
sorting but we would want to use the
filter which you see here is implemented
so how do you do it so you can select
this area and say for example I would
want to apply filter I can just go in
here and I can say let's get a filter on
my first row and now I have filters
applied so we are interested in looking
into the Central Area houses let's go in
here it says all these fields are
selected that means it shows everything
wherein your area has all these values
let's unselect this and then I will only
be interested in the central so let's
say okay and then say okay so now you
see the the area filter has been applied
and we are looking at the central column
so what we have done is we have applied
a symol filter and we are looking at our
data at any point of time if you do not
want the
filter then what I can do is is I can
select this and I can say well I'm
interested in all the data so I could do
this or you can clear the filters from
area and you get your data back so
that's in one way you can filter out
your data so let's look at an example of
sort and filter where we might have to
filter the data based on two columns or
multiple columns with different kind of
values where it could be and and or
condition now say for example well this
is the data I have and this is the
question which we need to answer such as
find the list of all houses in the
central region with pool and South
County without pool now if it was a
simple filter based on one cell I could
have just selected my header row I could
have then applied the
filter and once I have the filter I can
look in area where I have three regions
so I'm only interested in Central and
South County so I can get rid of North
County and that's fine but then we have
two different conditions here so we need
the data in central region to have the
value for pool being true and for South
County the value has to be without pool
now how do we do that so what we can do
is we can first create a copy of of
these headers here so let me do that now
the area has to be South County so
basically I can either just select this
and I can choose this
value and then I can select this and
then I can choose Central so that's the
criteria which I have and the pool value
has to be so central region should be
with pool
so then this one basically has to be
true and this one has to be basically
false so south county is without pool so
let's select one of the values here and
this is my criteria now to get my result
uh we can always place your cursor here
and you can check this is M column and
eighth row okay so we would want the
result here so let let's go ahead and
now click on data and then in filter you
have an option called Advanced and here
what I can do is I can say I would want
to filter the list in the place but
that's not what I would want to do so
I'll say copy to another location and
here if you see the list range will tell
me that this is the data so A1 to J1 26
so a to J column selected and all the
rows criteria range is is basically
based on what I've given here so that is
M from 1 to V which is three so I'm
selecting these columns and then I'm
saying all the way whatever criteria
I've given and copy two I'm saying
M8 to V8 so that basically will give me
my filtered result so you could
basically just say okay and now I I have
my data which has been filtered and I
have my area which is South County that
is without Pool Central with pool again
South County without pool and then if
you look at your central value that's
pool so this is an advanced filter which
we have applied where simply we have
filtered the databased on two columns
and then we have our result so in this
way you can have your customized filter
applied on two different columns and get
your data which can be either replacing
the existing content or in the same
sheet in different set of columns and
rows you can have your result let's look
at one more example of filtering where
you are trying to filter the data based
on a and condition condition being met
in two different columns and then you
would want to filter out the data for
only speci specific columns so the
situation is the agents with a house in
North County that should be County area
having two and a single type family so
we are talking about two bedrooms and we
would basically have a single type
family and here the criteria is that we
would want to only populate these
columns which is Agent area bedroom and
type now what you can do is as I
explained earlier that you can get your
result in the same sheet in a different
location so here I have created these
headers which says agent area bedrooms
and type now this is basically a copy of
all the columns what we have here agent
data listed area list price bedrooms
bath square feet and so on so you can
basically create a copy of the headers
here and this is where we will give our
Advanced criteria to filter the data so
the conditions which need to be met is
we need to look at North County so for
example here in area I can basically go
ahead and select one of the values North
County now the criteria is having two
bedrooms only so let's say bedrooms and
let's say the value should be two and
then basically I saying a single type
family so when you would want the single
type family so here under type I can
give the criteria single type so this is
my and condition so we are saying North
County area having two bedrooms and the
type is single family now this is the
criteria which basically means if I
select this this one tells me that this
is M1 row onwards till V 2 so this is
what we have have and we would want to
filter based on this so let's go ahead
and then go for data filter Advanced and
here in advanced it says filter the list
in place now that's not what we would
want to do so I'll say copy to another
location this basically selects the list
range so which is telling me
A1 2 J 126 so that's
the columns and rows selected criteria
range is based on M1 V2 which we have
given here and copy to I would say for
example from M7 to p7 now this is the
area where I would this is the place
where I want the result let's stay okay
and now I get my data which is based on
the question which has been asked that
you would want the agents with a house
in North County area having two bedrooms
and single type family so in this way
you can basically do Advanced filtering
get your data and get it stored in the
sheet anywhere at a different location
well I could have also done filtering in
place and that would have replaced the
data which we have but that's not what
we want we would want the filtered
result in a different place so this is
how you can do some Advanced filtering
we can also use Excel to filter out the
data in one particular column which
might be conditional or say using some
numerical filters now here say for
example the problem statement is that
you would want to display all the houses
whose list price is
between 45,000 to
600,000 or say for example we would want
to filter out the data to something else
say for example let's say I have I would
want to filter the data between 300,000
and 400,000 so we can basically update
this say for example I'm saying I'm
interested in
300,000
to
400,000 and that's the criteria to
filter out the data now there are two
different ways or there are two easier
ways to do it one is I would want to
look at the list price so I can select
this I can go ahead and to a filter here
and in the list price now this is where
we would want to do the filtering so
it's pretty easy you can click on this
one and then you can go into number
filters and you can choose between now
that's one easier way of doing it so I
could basically select this I could say
I'm looking for Value which is greater
than or equal to 300,000
and then is less than or equal to
400,000 so if I just do this I have
applied my filter and I have my data
which is filtered based on my criteria
right so that's one easier way of doing
it or let me do a control
Z Now let the filter be there which you
can anyways use but what we can also do
is as we have seen earlier methods so
get a
set or get your column headers
here and then you are giving two columns
here now the only difference
between my this set of columns which I
have 1 2 3 4 and then you have S and 10
columns and if you see here we have four
5 6 7 8 9 10 11 right so whenever you
want a and condition you will basically
add the columns where I can give add
condition if it is the same column if it
was a different column then it would be
same number of columns but and
conditions will lie in the same line and
or condition would lie in a different
line now here I can give this value so
I'm looking for listed price being
between 300,000 so I'm saying it should
be greater than or equal to
300,000 and then I can say less than or
equal to 400,000 now that's my criteria
and then I need my result here which is
in
M7 so what I can do is once I have given
my criteria which I'll be using to
filter I can get into Data I can get
into advanced and then I can say copy to
another location so it is selecting my
A1 to J
126 criteria range is based on M1 to
W2 and then I would want my result from
this particular column so let's say okay
and now you have your data filtered out
in a different location in the sheet
which has been filtered based on your
and condition so you can filter out the
data in this way or you could just apply
a filter on a column and give the
conditions now let's solve one more
interesting problem and here we would
want to use Excel where we would have an
and and an or condition so say for
example this is the data given to you
and the question is that you would want
to find all the houses in North County
again that's a spelling mistake but then
they North County area with a list price
greater than
300,000 and having three or four
bedrooms so the bedroom
has conditional so it has R three and
four and then basically you have list
price which is greater than 300,000 now
I could have obviously selected The
Columns and then basically gone for a
filter so I can just do a filtering here
and then I'm looking for list price
being greater than 300,000 so which we
can always give a number filter and I
can say greater than
and then I can say greater than or equal
so I can say greater than and then I can
give 3,30
300,000 and that's basically the filter
which we would want and here I would
want to select the bedrooms which should
be just either three or four so if for
example here I go in here and I unselect
this and then I say three and four right
so so I am getting my data which is
greater than 300,000 and it should have
the beding values which will be either
three or four selected now that's one
way of doing it let's do a control Zed
and get it back to as we were or you can
even just say clear filter so you get
your data back as it was so what we can
do is we can here give the criteria so
for example I have my list price now
this is what I would want as a condition
so let's say greater than
300,000 300,000 and bedrooms should be
three and then I can say greater than
300,000 and then I can say 4 so this one
basically gives me a situation where
your list price has to be greater than
300,000 and bedroom should be either
three or four so we have given our
filtering criteria now to get the result
what we can do is we can go into Data we
can go into advanced and we can say copy
to another location so our list range is
selected which is columns a to J row
number 1 to 126 your criteria range is
given in M1 to V3 where we have
specified and we are saying the result
would be in M7 to V7 so if I do this now
I have got the same data which we were
seeing earlier and here the bedroom
values are three or four and basically
the list price is greater than 300,000
so this is a simpler way in which you
can create your filters and all this
Advanced filtering what we are seeing it
will be saved with your sheet you can
always go back and change this value or
you could do filter ing where one person
has to look into the filter to see what
values have been
selected now that we have looked into
some operations which we can perform in
Excel using filter or sorting the data
creating your tables let's also quickly
look at functions and formulas which can
be used for doing some easy calculations
or computations now Excel can be used in
different kind of data analysis so for
example you have different inbuilt
functions which can be used and we can
always check for a particular function
so for example if I had if I wanted to
look at a particular function I could
just type in here something for example
is and then it shows me all the possible
functions and you can always have a look
at the detail of the function for
example you have is even which will
return true if the number is even if we
would say is logical so I could search
for is logical and that tells me whether
a value is logical value true or false
and returns your value true and false
now we can obviously say subtotal so you
can search for any of these useful
functions and that tells me what this
function can be used for so turns a
subtotal in a list or a database you
have many other such functions such as
integer sum average you may be
interested in working on
truncating some data getting the
absolute value getting the square root
basically getting a count or getting a
max value you can look for any
particular functions within your Excel
sheet now you also have other functions
such as now or time for example let's
look at Now function so I can search
for now and here it is so this is
Returns the current date and time
formatted as a date and time so this is
the function which we would want to use
and if I just give the function it tells
me what is the current time let's first
look at the description of time here so
say for example I would want time it
says converts hours minutes and seconds
given as numbers to an Excel serial
number formatted with a time format so
for example if I would say 2 hours and
then 30 minutes and 30 seconds and if I
do this it has basically converted this
into your time format so you can always
use different inbuild functions for your
work now we will also look at some
Advanced functions like some if or some
ifs you have count if and count ifs and
you can be working on various
functionalities of excel to easily help
you in doing some calculations
computations working with your data
working with your different cell values
so let's look at some example of using
functions like sum or some if so for
that
let's go to this sheet and here we have
some data now I have already applied a
filter which can allow me to filter out
the data so it says find the total units
that were sold in the east region now we
know that in region we have
east and I have multiple regions I could
basically be saying unselect all and
select only East
and say okay and that basically gives me
the units which were sold and if I place
my cursor here and then if I did a auto
sum so it would basically give me the
function which is being use so something
like subtotal and it is basically
working on your rows which is e22
e44 and here we can just do this so that
gives me the total but this is this is
fine you could do that but it would be
good if we know how do we use a function
like sum if to do that so here I'm
seeing this is the subtotal where I'm
looking at the values and basically what
I have done is I have filtered out the
region and then basically I'm getting a
count but this does not give me clearly
how a sum was calculated from all the
values which were listed what we can
also do is let's do a control Zed and
let's get it back so now we have our
data and we would want to get the total
units that were sold in the east region
so what I can do is I can start typing
in my formula and for that I'll use an
inbuilt function so for example I would
be interested in going for sum if now it
says sum if adds the cells specified by
a given condition or criteria when you
talk about some ifs this is when you
could give set of conditions or multiple
criteria so let's look at some if let's
do this now obviously this gives me an
error because the
formula is not right so we have to
basically come in here and let's start
with some if now when I say some if it
shows me there is a function with some
if which we would want want to use and
here once I open up the bracket it tells
me okay what is the range of data which
you are interested in so I'm interested
in all the units that were sold in the
east region so we are interested in the
region which is here so I can basically
be selecting this and this tells me you
are interested in the data here so let's
not take the header value so let's say
B2 and then we can go all the way to the
end so we can basically select this
way that's the data we have select this
and hit enter so now here it has
selected B2 to B4 but we need to
basically now give the criteria so the
criteria is either a value or you can
point it to a cell which has that
particular value so as per our problem
statement we are looking for the units
which are sold in east region so I can
select the value East here and then my
sum if needs the range on which you want
to calculate a sum so let's select this
and now we are interested in finding out
the sum of units so that's basically
this column e column so I can basic
basically type in instead of selecting
so I can say e and I'm interested in E2
to e 44 so that basically selects the
area or all the values and now let's do
this so that basically gives me the sum
is
691 right now this is the criteria where
I have pointed it to a cell and whatever
value that cell has well I could have
done something like this so I could have
selected East giving the value and then
doing it it still does the same thing
and in this way you have more clarity
that you are using some if you are
filtering the data so you have given the
rows you are given the criteria and then
you have given the range on which you
would want to sum up the values now
similarly if the question was what was
the total revenue generated from binder
now we would want to find out what is
the total revenue generated from binders
that means my filtering criteria will be
binder and then I want to find out the
total revenue generated so we have the
revenue generated field also here and we
have we don't have any region to be
filtered we are just looking for binder
so let's again start doing the same
thing so we can go for some if we can
open the bracket now it needs the range
so we are interested in revenue
generated now that's the summation we
want and we would want to get the range
of data so here we can basically
select uh
D2 2 D
44 so that's the data selected now I
would want to give the filtering
criteria so let's say binder and then we
need to give the range on which the sum
needs to be calculated so that's my
Revenue column so that's G so I can say
G to to
g44 and that basically selects the
column and then you get your sum so it
tells me what is the total revenue
generated from binder now I could be
doing this for other things also so say
for example if you would want to filter
out something else you could basically
just drag and drop here and then I could
come here and change this to say instead
of binder I would be interested in
say
pencil if that's the criteria you are
interested in remember to change this so
that you take all the values
and here we will
change it to select the relevant rows
and then this is the data I'm getting so
I know that this is the revenue
generated from pencil this is the
revenue generated from binder now this
is a simple use case where we are using
some if what if we would want to use
some ifs so some ifs let's have a look
at how we get to some ifs so some ifs is
where you would want to work
on doing some calculation but then you
would
want multiple criterias to be met so
let's see how we get this so here what I
can do is let's work on this problem
statement which says what is the total
revenue generated from central region
where the item is a pencil so that's
something which I would want to check
now when we are answering this question
we can also look at the order in which
things have been asked in the question
so it says what is the total revenue
generated from central region so we need
the total revenue generated we know
there is a revenue column we are
interested in getting the total revenue
generated we are saying the filtering
criteria is central region and we say in
that we would be interested only in the
item if it is pencil how do I do it so I
can use some ifs where you can pass in
multiple criteria so let's start with
some
ifs and when I start with some ifs let's
open up the bracket so it says some
range it says criteria range then it
gives one criteria and you can give in
any number of criterias so for example
we are interested in total revenue
generated now that's my G column so
let's follow in the same order so let's
say
G2 so that's my first value and then I
know there are 44 rows here so I can say
g44 and you can obviously check if that
has selected all the rows now that's my
total revenue generated so I would would
want the total revenue generated so I'm
saying setting this sum
range then I need to give the criteria
range so it says from central region so
central region comes into column 2
that's B so let's say B2 to
B44 so that's my criteria range then you
have to give your criteria but we need
to filter out the Reg being Central so
let's select this now either I could
point to a value in the cell or I can
just give the exact value here we can
also give a wild card or matching
pattern so that also works now this one
is fine we are now also interested in
finding the total revenue generated when
the region is Central and the item is
pencil how do we do that so so for item
we know the columns the column is D2 so
let's select D2 2
d44 so that basically selects all the
rows in the D column and we need to give
the filtering criteria so let's do a
comma and then just given our value so
let's say pencil and then let's close
your bracket and that basically gives
you the the result so we need to just
follow the order of our question which
says what is the total revenue generated
so we are looking at the revenue column
we are selecting all the
rows then it says from the central
region so we need to select the region
column and give the filtering criteria
as Central or point to a cell which
contains that value and then it says we
would be interested only in item being
pencil
so then you select the column which has
all the items and provide you a
filtering criteria that is pencil so
that's your easy calculation of using Su
if which we compare with some if here so
some if here was just
having your criteria so basically you
are selecting your rows giving your
filtering criteria and then your sum
range in some ifs we are giving multiple
condition now same thing can be done
here it says how many units were sold by
sales representative Jones or Jones
where the cost of each item was greater
than four so how many units were sold by
sales
representative so when we talk about how
many units that's your e column so let's
start with that so let's say some ifs I
would be interested in E
column then let's give the range so it
says some range so those are the number
of units on which we would want to find
the sum then it says you need to give
the criteria range so we say sales
representative where the name is Jones
so sales representative is in sales rep
column C so let's say C2 to c44
now then we need to give our filtering
criteria so let's say
Jones is the sales representative where
we are interested about whom we are
interested in and then we the question
says where the cost of each item so cost
of each item is what we are interested
in you have unit cost so that's what we
are interested in so that would be F and
then say F2 to F4
4 and then you need to give your cost so
it says where each item is greater
than 4 so let's select this and
let's do this so this tells me
301
now similarly let's answer our third
question which says how many units did
Jones sell
excluding pencil item so we would want
to find out what was the total number of
units units that were
sold and that units or that should not
include the pencil item how do we start
doing this so let's start with some
ifs now we know that you start with some
ifs you need to give the sum range so we
are interested in the number of units so
let's basically go in and select our
number of units which were sold so
that's your column e so I can say E2 to
e44 that's where I would want to perform
the sum now I'm
saying how many units that were sold
where we are talking about sales rep
being Jones so let's see let's select
the column C and then
give the range after that we need to
give our giving filtering criteria which
is Jones and then we are interested in
the items but excluding pencil so items
is in column D so let's say d to
d44 and then we have to give our
criteria so we can say well that should
exclude
pencil so I can basically
say
pencil and let's close
this and let's check so that's my
formula which says that these are the
number of units which the sales
representative whose name is Jones had
sold and that does not include pencil as
an item let's also look at an example of
using count if or count ifs now both of
these can be very useful when you would
want to calculate certain values so for
example if I would want to work on count
if let's try solving this problem now
remember you can answer these questions
using filters and that can be an easier
way but then sometimes you may want to
get the formulas so that you can make
your spreadsheet and your calculation
more dynamic in nature and that will
basically depend on the values in the
columns or rows so for example if I have
find the total number of times Gil has
made a sale now if I look at my data
here it tells me that for every sales
representative there is some value in
the sale and it says sales has greater
than three so for example Jones you have
sales greater than three or you have
Jardine which is sales greater than
three and so on so what we are
interested in is doing a quick count in
finding out the total number of times
Gil has made a sale how do we do that so
we can use this count if function and if
I go into count f it says counts the
number of cells within a range that meet
the given condition now what's our
condition our condition is Gil and we
would want to find out how many times
the name say Gil appears or Gil has made
a sale now I could just say count if and
then open up a bracket I need to give a
range so let's say we are interested in
looking at the range so let's say we
will
choose sales rep so I can say C and then
I can say C2 to
c44 now that's my
range let's not give that in
quotes so you have to give a range so
let's do a count if that selects the
data and then we need to give the
condition so for example let's here give
the name which is Gil and then close
this so that basically tells me it has
five times skill appears here we can
check this so I can go in here I can add
a
filter and then might be I would be
interested in looking at only Gil and
that basically gives me five right so we
can always do that and we can be using
formulas like this now what about this
question so which basically says with
sales representative made a sale more
than three times now we it might be
looking a little confusing when I say
for example let me clear out this
filter now we have sales greater than
three and we would want to find out
which sales representative made a sale
more than three times now I could
basically check for every sales
representative here if they have made a
sale more than three times and what I
can do is I can just say equals I can
start with count if then I need my
filtering criteria so that's your range
so first thing is we will choose for
example let's choose C2 to C
44 and then we need
to give an criteria what is the criteria
we need basically a sales representative
so I can choose the value here in
C2 and then I can close this and then I
can say the sale has to be greater than
three and let's check so it tells me the
Boolean value that yes this guy has made
sales more than three times and what we
can do is we can just drag and drop
which basically gives me the value for
other sales rep you can always check the
value is automatically changing to the
value in sell and for example let's go
in here so this is obviously two so it
says me false right and you can
basically get the values for all your
values so that basically tells me which
sales representative has made a sale
more than three times now like some ifs
we also have count ifs where you can
give multiple criterias so for example
the question is how many orders were
placed from the east region after this
particular date so we have a date
criteria we also have the region
criteria and we need to basically get
the count of number of orders which were
placed now I can in this case use
count
ifs and this basically says that you can
start within criteria so it says how
many orders were placed from east region
after particular date so date is in in
My First Column so for example I could
say a start with two and say for example
let's go
a44 that should have selected all the
rows and then I need to once I've given
the criteria range I need to give the
criteria so we are saying the date has
to be greater than 10th Feb so let's
give it
210
2019 and then you need to say how many
orders were placed so you need to give
the criteria second criteria range so we
are looking at the number of orders
which were placed from the east region
so when I would want to look at the
region that's your column B so I can
basically say B2 2 b
44 and
here I would basically give my criteria
so the criteria is
East let's give that and once you have
done this you would want to find out the
total number of orders so let's select
this and if I do this it tells me 13 now
is that right so we are looking for your
date your region being East and then
getting the total number of orders so
here I can just do count if I'm saying
A2 to
a44 wherein I have given the date
criteria that it should be greater
than 10th Feb because I do not want to
count 10th Feb it says after 10 February
and then you're saying the region has to
be East so we would want to find out the
total number of orders so my region is
east and that gives me the result now
similarly you can also find out how many
times
Gil sold pencils so here we will have to
give the range so let's start with
count
ifs now here I would want that item is
pencils so we can as well select column
D2
d44 then you have to give your criteria
so that's
your
pencil and then we are looking for sales
rep which is Gil so that is my column C
so let's say c 2 2 C
44 and the value should be just
kill so it tells me it's twice where Gil
has sold pencils so we can obviously
check this by going in here choosing my
filter and then let's search for rep
being just
Gil okay and now we are in interested
only in the item being pencil so I can
say well let's get to pencil only and
that tells me twice so you can obviously
reverifying filters but using functions
or using formulas it is always good to
calculate and that can be making your
computation and calculation more
Dynamic let's look at one more
interesting feature of Excel and that's
your conditional formatting now as you
see on the screen conditional formatting
has different rules which can be applied
on your data and that allows you to
basically differentiate or easily
identify data values which are based on
certain criterias or rules so when you
talk about conditional formatting you
have different options such as you can
highlight cell rules you can get get top
and bottom values you can apply
different rules apply different color
scales and you you can easily manage
these rules so conditional formatting is
very useful for people who would want to
work on huge amount of data and easily
perform some data
analysis it's easy to use as it is shown
here and with your conditional
formatting you can format cells based on
a preset condition you can perform
conditional formatting to identify sales
you can highlight a few significant
cells and you can easily perform
conditional
formatting as shown on the left side now
how do we work with conditional
formatting let's have a quick look so
say for example we have our Excel sheet
and if you see here I am highlighting
the salesperson who have generated
Revenue greater than
10,000 so we can be looking at the
values where the revenue generated by a
particular sales person is greater than
10,000 it has a particular color and how
do we get here so for example let's
select this
data and what I could do is I could go
into conditional formatting now I could
basically highlight cell rules and we
you could just say greater than that's
an easier way I could also go ahead and
create a new rule but then I can use one
of this option I can say greater than
and let's give some value might be we
would be interested in looking at any
value greater than 12,000 so let's
choose
12,000 and here it says what color would
you want to select so for example I
would say something
like yellow filled with dark yellow
text and let's say okay so right now
what I'm doing is I have all the values
where the revenue generator was greater
than 10,000 but then I have also
selected all the sales people who have
made or who have generated Revenue
greater than 12,000 so I can just do a
control Zed to see the previous result
now here I had the values which were
greater than 10,000 and the one which we
did just now
basically highlighted the values which
are greater than 12,000 so this is one
simple example now we can look at some
other examples say for example you want
to format cells using three-c color
scale so if you look at the values here
I have a three color scale mainly in
green yellow and red and how do you do
this so for example I can go in here and
I can go to conditional formatting so I
would want to go for color scales and
here you can create different rules so
we can set up a two color scale so we
can say format only values that are
above and below average I can format
only cells that contain something I can
get the top and bottom value so these
are different ways in which I can have a
three color based scale now what I will
do is I will select this and let me show
you the rule which I have so for example
I can go into manage rules and if you
see here there are certain rules which
have been specified now what does that
mean so you would want to specify a
three grade scale so for example if I
would want to look at my first rule it
tells me that I'm choosing three color
scale I can choose lowest value
percentile and highest value and that
basically will select the cells based on
their values so what we could have done
is I can
basically use one of these values I can
delete these rules which I have created
so for example I have all these rules
but you should always carefully remember
that the rules will be applied in the
order shown so for example if I just
delete these rules and then say apply
and say okay my data is back now it does
not have any highlighting now I can go
in here I can say condition sorry
conditional formatting I could go for
color scales or I could basically going
to new rule so I would want the
cells to be using three-c color scale so
let's choose three color scale now when
you say three-c color scale it says what
will be the color of lowest value and we
could choose might be any one of this
let's choose red I can say midpoint is
percentile 50 and then the highest value
is green and if that looks good let's
say okay and now if you see the lowest
values have been high highlighted as red
you have mid values and then you have
the positive value so this is a three
color scale and that easily helps me in
identifying the data based on the cell
values now in conditional formatting
what you can also do is you can
basically color the cells based on their
value so what we are seeing here is if
the
revenue generated is greater than
average then that shows in green and if
the revenue generated is lesser than
average that's shows in Orange now how
do we do that so we can basically again
manage some rules so I can basically
create a new rule now here I can select
one of the options which says format
only values that are above or below
average and that's the option I would
want to select now I can select this and
it's says format values that are above
average so in our case we had it in
green so for example I'll say above
average and then here I can go for a
particular color so you can go for a
particular size so let's go and look
into the formatting so for example let's
choose
yellow say okay now I'm saying
wherever the cell values are above
average it would be yellow instead of
green and let's go in here let's go and
look into manage rules so this is
basically the rule which we are applying
now we can also add a new rule and I
need to select the values so for example
I will
say here so we have had gone for above
now we'll go for below we'll go for
format we will choose red we'll say okay
we'll sa now these are
basically the rules which we have
created and here it says applies to your
data so right now it has not been
applied so for example if I select this
and then I could basically choose my
area just hit on enter and similarly you
can go in here and then select your area
hit on enter and say apply say okay and
now if you see I have really chosen
bright colors but then I have said
whever my revenue generated is above
average it should be in yellow and below
average should be in Red so we wanted
above average to be in green
and below average to be in Orange so
that's what we have here right so you
can always color code your cell values
based on some rules which you are
setting up now similarly you can also
find the top 10 and bottom 10 values and
that's pretty easy so you can just
select this and then you can go into
conditional formatting you can go for
top and bottom values top 10 items
bottom 10 items or you can go in for
more rules so you can say format only
top or bottom ranked values so you have
top 10 now you can choose the color and
for example I'll go for
blue and I'll say okay so now if you see
my top 10 values are blue now similarly
I can add one more rule so so I can say
new rule and I can say let's go for top
or bottom let's go for bottom let's go
for format let's say orange say okay say
okay and that's it so now you have your
values which are top or bottom 10 values
so you're using conditional formatting
where you are basically highlighting
your cell values based on different
colors and here easy conditional
formatting based on different rules
helps us to do that now similarly you
can also have the
values which is
basically showing you how the values are
increasing so what we can do is we can
select our columns either you could
apply this to all the columns now here I
have applied this only to Jan and April
now I could apply this for June so let's
say June so you can go for gradient fill
you can go for solid fill you can
obviously just select the color and that
takes care of the things you can say for
example select this and now this is
selected but I would want to might be
format this so I can go in here I can go
into manage rules now that will tell me
what rule has been
applied in the order so I can just do a
edit Rule and that basically says this
is a solid fill which is color you have
no border this is basically color is
black now I can go for something like
gradient
fill and I can say okay and now if I say
apply and okay so this basically is Like
Your First Column so you can use
conditional formatting for various use
cases and you can highlight the values
so anyone who would look at the value
would automatically notice which are the
higher values which are lower values
might be here the revenue is getting
generated or was getting generated but
did not grow Beyond a particular value
and so on now similarly you can also go
in for different options say for example
here we would want to see if the revenue
was
dropping or if the revenue was
if the revenue decreased or say for
example if the revenue was going up for
this particular sales person so here we
are looking at Carol so in Jan the
revenue Generation by sales was very
high then in Feb it was falling down in
March it was kind of stable then in
April it went way below so we can
obviously work on this wherein we can
grade our cell values so what we can do
is we can go in for highlighting the
cell values now you can go for color
scales you can go for Icon sets and this
is where you can choose your different
shapes so you could choose one of these
shapes so for example I would be
interested in looking at the indicators
like directional I could go using this
three arrows I can go in for this color
I can choose directional and then my
values are
automatically using directional now what
we can also do is we can then go into
manage rules and that basically tells me
what rules have been applied so for
example the latest one is the icon set
which I have chosen it shows the
selected columns I can obviously do a
edit rule and then I can choose so I'm
saying the format style is icon sets I'm
not using a data bar I'm not using color
scale now here I have chosen the style
of icons and then here you can basically
give some values so you have icon which
is green when the value is greater than
or equal to 67
percentage when I say hyphen or minus
it is less than
67 it's way below 33 percentage then you
give this value so you can obviously
edit and easily highlight your cell
values based on this icon set so I can
apply this and that's how I use
conditional formatting so conditional
formatting can be very useful if you
would want to use icon set if you want
to use your data bars if you would want
to highlight particular values if you
would want to color code based on some
calculation if you would want to use a
three color or a two color scale or if
you would want to just find out values
based on some simple calculation so
conditional formatting is used
extensively by data analysts or people
who are working business intelligence
teams or people who would want to use
Excel to easy easily identify the data
easily identify the cells which contain
particular value or finding out less
significant or more significant cells to
then pull out values and carry out your
computations calculations or
analysis and if you're an aspiring data
analyst try giving a sh to Simply Lars
postgraduate program and data analytics
from P University in collaboration with
IPM the link in the description box and
the pin comment take you to the
programming off it and uh so why exactly
do we need to do time serious analysis
typically we would like to predict
something in the future and uh it could
be stock prices it could be the sales or
um anything that needs to be predicted
into the future that is when we use time
series analysis so it is um as the name
suggests it is forecasting and uh
typically when we say predict it need
not be into the future in machine
learning in data analysis when we talk
about predicting we are not necessarily
talking about the future but in Time
series analysis we typically predict the
future so we have some past data and we
want to predict the future that is when
we perform time series analysis so what
are some of the examples uh it could be
daily stock price the shares as we talk
about or it could be the interest rates
weekly interest rates or sales figures
of a company so these are some of the
examples where we use time series data
we have historical data which is
dependent on time and then based on that
we create a model to predict the future
so what exactly is uh time series so
time series data has time as one of the
components as the name suggests so in
this example let's say this is the stock
price data and uh one of the components
so there are two columns here column B
is the price and column A is basically
the time information in this case the
time is a day so that primarily the
closing price of a particular stock has
been recorded on a daily basis so this
is a Time series data and the time
interval is obviously a day time series
or time intervals can be daily weekly
hourly or even sometimes there is
something like a sensor data it could be
every few milliseconds or microsc as
well so the size of the time intervals
can vary but they are fixed so if I'm
saying that the it is daily data then
the interval is fixed as daily if I'm
saying this data is an hourly data then
it is the data is captured every are and
so on so the time intervals are fixed
the interval itself you can uh decide
based on what kind of data we are
capturing so this is a graphical
representation the previous one here we
saw the table representation and this is
how to plot the data data so on the y
axis is let's say the price or the the
stock price and xaxis is the time so
against time if you plot it this is how
a Time series graph would look so as the
name suggests what is time series data
time series data is basically a sequence
of data that is recorded over a specific
intervals of time and based on the past
values so if we want to do an analysis
of Time series F data we try to forecast
a future and uh again as the name
suggest it is time series data which
means that it is time dependent so time
is one of the components of this data
time series data consists of primarily
four components one is the trend then we
have the seasonality then cyclicity and
then last but not least irregularity or
the random component sometimes is also
referred to as a random component so
let's see what each of these components
are so what is Trend trend is overall
change or the pattern of the data which
means that the data may be let me just
uh pull up the pen and uh show you so
let's say you have a data set somewhat
like this a Time series data set
somewhat like this all
right so what is the overall trend there
is an overall trend which is upward
Trend as we call it here right so it is
not like it is continuously increasing
there are times when it is dipping then
there are times when it is increasing
then it is decreasing and so on but
overall over a period of time from the
time we start recording to the time we
end there is a trend right there is an
upward Trend in this case so the trend
need not always be upwards there could
be a downward Trend as well so for
example here there is a downward Trend
right so this is basically what is uh a
trend overall whether the data is
increasing or decreasing all right then
we have the next component which is
seasonality what is seasonality
seasonality as the name suggest once
again changes over a period of time and
periodic changes right so there is a
certain pattern um let's take the sales
of warm clothes for example so if we
plot it along the months so let's say
January February March April May June
July and then let's say it goes up to
December okay so this is our December a
d I will just mark it as D and then you
again have Jan F March and then you get
another December okay and just for
Simplicity let's mark these as December
as the end of the year and then one more
December okay so what will happen when
if you're talking about warm clothes
what happens the sales of warm clothes
will increase probably around December
when it is cold and then they will come
down and then again around December
again they will increase and then the
sales will will come down and then there
will be again an increase and then they
will come down and then again an
increase and then they will come down
let's say this is the sales pattern so
you see here there is a trend as well
there is an upward Trend right the sales
are increasing over let's say these are
multiple years this is for year 1 this
is for year two this is for year three
and so on so for multiple years overall
the trend there is an upward Trend the
sales are increasing but it is not a
continuous increase right so there is a
certain pattern so what is happening
what is the pattern every December the
sales are increasing or they are peing
for that particular year right then
there is a new year again when December
approaches the sales are increasing
again when December approaches the sales
are increasing and so on and so forth so
this is known as seasonality so there is
a certain
fluctuation which is uh which is
periodic in nature so this is known as
seasonality then cyclicity what is
cyclicity now cyclicity is somewhat
similar to seasonality but here the
duration between two cycles is much
longer so seasonality typically is
referred to as an annual kind of a
sequence like for example we saw here so
it is pretty much like every year in the
month of December the sales are
increasing however cyclicity what
happens is first of all the duration is
pretty much not fixed and the duration
the Gap length of time between two
cycles can be much longer so the
recession is an example so we had let's
say recession in 2001 or 2002 perhaps
and then we had one in
2008 and then we had probably in 201
2012 and so on and so forth so it is not
like every year this happens probably so
there is usually when we say recession
there is a slump and then it recovers
and then there is a slump and then it
recovers and probably there is another
bigger slump and so on right so you see
here there this is similar to
seasonality but first of all this length
is much more than a year right that is
number one and it is not fixed as well
it is not like every four years or every
six years that duration is not fixed so
the the duration can vary at the same
time the gap between two cycles is much
longer compared to seasonality all right
so then what is irregularity
irregularity is like the random
component of the time series data so
there is like you have part which is the
trend which tells whether the overall it
is increasing or decreasing then you
have cyclicity and seasonality which is
like kind of a specific pattern right uh
then there is a cyclicity which is again
a pattern but at much longer intervals
plus there is a random component so
which is not really which cannot be
accounted for very easily right so there
will be a random component which can be
really random as the name suggest right
so that is the irregularity component so
these are the various components of Time
series data yes there are conditions
where we cannot use time series analysis
right so is it can we do time series
analysis with any kind of data no not
really so what are the situations where
we are uh we cannot do time series
analysis so there will be some data
which is collected over a period of time
but it's really not changing so it will
not really not make sense to perform any
time series analysis over it right for
example like this one so if we take X as
the time and Y is the value of whatever
the output we talking about and if the Y
value is constant there is really no
analysis that you can do uh leave alone
time series analysis right so that is
one another possibility is yes there is
a change but it is changing as per a
very fixed function like a sine wave or
a COS wave again time series and
analysis will not make sense in this
kind of a situation because there is a
definite pattern here there is a
definite function that the data is
following so it will not make sense to
do a Time series analysis now before
performing any time series analysis uh
the data has to be stationary and uh
typically time series data is not
stationary so in which case you need to
make the data stationary before we apply
any models like ARA model or any of
these right so what exact ly is
stationary data and what is meant by
stationary data let us take a look first
of all what is non-stationary data time
series data if you recall from one of my
earlier slides we said that time series
data has the following four components
the trend seasonality cyclicity and
random random component or irregularity
right so if these components are present
in Time series data it is non-stationary
which means that typically these
components will be present therefore
most of the time A Time series data that
is collected raw data is nonstationary
data so it has to be changed to
stationary Data before we apply any of
these algorithms all right so a
nonstationary Time series data would
look like this which means like for
example here there is an upward Trend
the seasonality component is there and
also the random component and so on so
if the data is not stationary then the
time series for for casting will be
affected so you cannot really perform a
Time series forecasting on a
non-stationary data so how do we
differentiate between a stationary and a
non-stationary Time series data
typically or technically one is of
course you can do it visually in
non-stationary data the the data will be
more flattish seasonality will of course
be there but the trend will not be there
so the data May if we plot that it may
appear somewhat like this right it's
it's a horizontal line along the
horizontal line you will see compared to
the original data which was there was an
upward Trend so it was changing somewhat
like this right so this is
non-stationary data and this is how a
stationary data would look visually what
does this mean technically this means
that stationarity of the data depends on
a few things what the mean the variance
and the co-variance so these are the
three components on which the
stationarity of the data depends so
let's take a look at what each of these
are for stationary data the mean should
not be a function of time which means
that the mean should pretty much remain
constant over a period of time right so
there is there shouldn't be any change
uh so this is how the stationary data
would look and this is how a
non-stationary data would look I've
shown in the previous slide as well so
here the mean is increasing that means
there is an upward Trend okay so that is
one part of it and then the variance of
the series should not be also a function
of time so the variance also should be
pretty much common or should be constant
rather uh so this is a if we visually we
take a look this is how time series
stationary data would look where the
variance is not changing here the
variance is changing therefore this is
nonstationary and we cannot apply time
series forec casting on this kind of
data similarly the co-variance which is
basically of the ith term and the i+ M
term should not be a function of time as
well so Co variance is nothing but not
only the variance at the ath term but
the relation between the variance at the
ath term and the I plus M or the I plus
n term so as again once again visually
this is how it would look if the
co-variance is also changing with
respect to time so these are the three
all three components should be pretty
much constant and that is when you have
stationary data and in order to perform
time series analysis the data should be
stationary okay so let's take a look at
uh the concept of moving average or the
method of moving average and let's see
how it works we'll do simple
calculations so let's say this is our
sample data we have the data for three
months January February March the sales
in hundreds of in thousands rather not
hundreds thousands of dollars is given
here and uh now we want to find the
moving average so how do we find the
moving average we call it as moving
average three so moving average three is
nothing but you take three of the value
or the readings add them up and uh
divide by three basically the way we
take a mean or average of the three
values so that is as simple as that so
that's the average first of all so what
is moving average moving average is if
you now have a series of data you keep
taking the three values the next three
values and then you take the average of
that and then the next three values and
so on and so forth so that is how you
take the moving average so let's take a
little more detailed example of car
sales so this is how we have the car
sales data dat for the entire year let's
say so rather for four years so year one
we have for each quarter quarter 1 2 3 4
and then year two quarter 1 2 3 4 and so
on and so forth so this is how we have
sales data of a particular car let's say
or a showroom and uh we want to forecast
for year five so we have the data for
four years we now want to forecast for
the fifth year let's see how it works
first of all if we plot the data as it
is uh taken the raw data this is how it
would look and uh what do you think it
is is it stationary no right because
there is a trend uper Trend so this is
not a stationary data so we um we need
to later we will see how to make it
stationary but to start with just an
example we will not worry about it for
now we will just go ahead and uh
manually do the forecasting using what
is known as moving average method okay
so we are not applying any algorithm or
anything like that in the next video we
will see how to apply an algorithm how
to make it stationary and so on all
right so um here we see that all the
three or four components that we talked
about um are there there is a trend
there is a seasonality and then of
course there is some random component as
well cyclicity may not be it is possible
that cyclicity is not applicable in all
the situations for sales especially
there may not be or unless you're taking
a sales for maybe 20 30 years cyclicity
may not come into play so we will just
consider uh primarily the trend
seasonality and irregularity right so
Random it is also known as random
irregularity right so we were calling
the random or irregularity component so
these are the three main components
typically in this case we will talk
about so this is the trend component and
um we will see how to do these uh
calculations so let's take a look redraw
the table including the time code we
will add another column which is the
time code and uh is the column and we'll
just number it like 1 2 3 4 up to 16 the
rest of the data Remains the Same okay
so we will do the calculations now now
let us do the moving a average
calculations um or ma four as we call it
for each year so we take all the four
quarters and we take an average of that
so if we add up these four values and
divide by four you get the moving
average of 3.4 so we start by putting
the value here so that will be for the
third quarter let's say 1 2 3 the third
quarter then we will go on to the next
one so we take the next four values as
you see here and take the average of
that which is the moving average for the
next quarter and so on and so forth now
if we just do the moving average uh it
is not centered so what we do is we
basically add one more column and we
calculate the centered move moving
average as shown here so here what we do
is we take the average of two values and
then just adding these values here so
for example the first value for the
third quarter is actually the average of
the third and the fourth quarter so we
have 3.5 now it get centered so
similarly the next value would be 3.6 +
3.9 ided 2 so which is 3.7 and so on and
so forth okay so that is the centered
moving average this is done primarily to
smooth the data so that there are not
too many rough edges so that is what we
do here so if we visualize this data now
uh this is how it looks right so if we
take the centered moving average as you
can see there is a gradual increase if
this was not the case if we had not
centered it the changes would have been
much sharper so that is the basically
the smoothing that we are talking about
now let's go and or do the forecast for
the fifth year so in order to do the
forecast what we will do is we will take
the centered moving average as our
Baseline and then start doing a few more
calculations that are required in order
to come up with the prediction so what
we are going to do is we are going to
use this multiplicity or multiplicative
model in this case and this is how it it
looks so we take the product of
seasonality and uh the trend and the
irregularity components and we just to
multiply that and in order to get that
this product of these two We have
basically the actual value divided by
CMA YT value divided by CMA will give
you the predicted value or YT is equal
to the product of all three components
therefore St into ITT is equal to YT by
CMA so this is like this is equal to YT
right so therefore if we want St into it
the product of seasonality and
irregularity is equal to YT by CMA so
that is how we will work it out I also
have an Excel sheet of the actual data
so let me just pull that up all right so
this is how the data looks in Excel as
you can see here year 1 quarter 1 2 3 4
year 2 quarter 1 2 3 4 and so on and
this is the sales data and then this is
the moving average as I mentioned this
is how we calculate and this is the
centered moving average so this is the
primary component that we will start
working with and then we will calculate
since we want the product of into YT
that is equal to YT by CMA so if you see
these values are nothing but the YT
value divided by CMA so in this case it
is 4 by 3.5 which is 1.14 similarly 4.5
by 3.7 1.22 and so on and so forth so we
take we have the product St into it and
uh then the next step is to uh calculate
the average of um respective quarters so
that is what we are doing here a average
of respective quarters and then we need
to calculate the deseasonalized values
so in order to get deseasonalized value
we need to divide YT by St that was
calculated so for example here it is 2.8
by9 so we got the decaled value here and
uh then we get the trend and then we get
the predicted uh values so in order to
get the predicted value which is
basically we predict the values for
known values as well like for example
year one quarter 1 we know the value but
now that we have our model we predict
ourselves and see how close it is so we
predicted as 2.89 whereas the actual
value is 2.8 then we have 2.59 the
actual value is 2.1 and so on just to
see how our model works and then
continue that into the fifth year
because for fifth year we don't have a
reference value okay and if we plot this
we will come to know how well our
calculations are how well our manual
model in this case we did not really use
a model but we did on our own manually
so it will tell us the trend so for
example the predicted value is this gray
color here and you can see that it is
actually pretty much following the
actual value which is the blue color
right and the gray color is the
predicted value so the wherever we know
the values up to year four we can see
that our predicted values are following
or pretty much very close to the actual
values and then from here onwards when
the year five starts the blue color line
is not there because we don't have the
actual values only the predicted values
so we can see that since it was
following the trend pretty much for the
last four years we can safely assume
that it has understood the pattern and
it is predicting correctly for the next
one year the next four quarters right so
that is what we are doing here so these
four quarters we did not have actual
data but we have the predicted values so
let's go back and see how this is
working in this using the slides so this
is we already saw this part and I think
it was easier to see in the Excel sheet
so we calculated the St it the product
of St and it using the formula like here
y by YT by CMA we got that and then we
got ST which is basically YT so this is
average of the first quarters for all
the four years and uh similarly this is
the average of the second quarter for
all the four years and so on so these
values are repeating there are they are
calculated only once they get repeated
as you can see here and uh then we get
the
deseasonalized data and that is
basically YT by St so we calculated St
here and we have YT so YT by St will
give you the desaly data and uh we have
got rid of the seasonal and The
Irregular components so far now what we
are left with is the TR and uh before we
start the time series forecasting or
time series analysis as I mentioned
earlier we need to completely get rid of
the non-stationary components so we are
still left with the trend component so
now let us also remove the trend
component in order to do that we have to
find the or we have to calculate the
intercept and slope the data because
that is required to calculate the trend
and uh how are we going to do that we
will actually use um what is known as a
regression tool or Analytics tool that
is available in Excel so you remember we
have our data in Excel so let me take
you to the Excel and uh here we need to
calculate the intercept and the slope in
order to do that we have to use the
regression mechanism and in order to use
the regression mechanism we have to use
the Analytics tool that comes with Excel
so how do you activate this tool so this
is how you would need to activate the
Tool uh from Excel you need to go to
options and uh in options there will be
addin and uh in addin you will have um
analysis tool pack and you select this
and um you just say go it will open up a
box like this you say analysis tool pack
and you say Okay And now when you come
back in to the regular view of excel in
the data tab you will see data analysis
activated so you need to go to file
options and addins and then analysis
tool pack typically since I've already
added it it is coming at the top but it
would come under inactive application
addin so when you're doing it for the
first time so don't use VBA you just say
analysis tool pack there are two options
one with VBA like this one and one
without VBA so just use the one without
VBA and then instead of just saying okay
just take care that you click on this go
and not just okay so you say go then it
will give you these options only then
you select just the analysis tool pack
and then you say okay all right so and
then when you come back to the main view
you click on data okay so this is your
normal home view perhaps so you need to
come to data and here is where you will
see data analysis available to you and
then if you click on that there are a
bunch of possibilities what kind of data
analysis you want to do if there are
options are given right now we just want
to do regression because we want to find
the slope and The Intercept so select
regression and you say
okay and and you will get these options
for input y range and input X range
input y range is the value YT so you
just select this and you can select up
to here and press enter and input X
range you can for now you start with uh
the baseline or you can also start with
the D seasoned values so you can just
click on these and say
okay I have already calculated it so
these are the intercept and the
coefficients that we are getting for
these values and we will actually use
that to calculate our Trend here right
so which is in the J column so our trend
is equal to intercept plus slope into
the time code so The Intercept is uh out
here as we can see in our slide as well
so if you see here this is our intercept
and the lower value is the slope we have
calculated here and it's show in the
slides as well so intercept the formula
is shown here so our trend is equal to
intercept plus slope into time code time
code is nothing but this one t column A
1 2 3 4 okay so that's how you calculate
the trend and that's how you use the
data analysis tool from Excel using
these two we calculate the predicted
values and using this formula which is
basically trend is equal to intercept
plus slope into time time code and then
we can go and plot it see how it is
looking and therefore so we see here
that the predicted values are pretty
close to the actual values and um
therefore we can safely assume that our
calculations which are like our manual
model is working and hence we we go
ahead and predict for the fifth year so
till 4 years we know the actual value as
well so we can compare how our model is
performing and for the fifth year we
don't have reference values so we can
use our equations to calculate the
values or predict the values for the
fifth year and we can go ahead and
safely calculate those values and when
we plot for the fifth year as well the
predicted values we see that they are
pretty much they captured the pattern
and we can safely assume that the
predictions are fairly accurate as we
can also see from the graph in the Excel
sheet that we have already seen Okay so
so let's go and plot it so this is how
the plot looks this is the CMA or the
centered moving average the green color
and then the blue color is the actual
data red color is the predicted value
predicted by our handcrafted model okay
so remember we did not use any regular
forecasting model or any tool we have
done this manually and uh the actual
tool will be used in the next video this
is just to give you an idea about how
behind the scenes or under the hood how
fusting works a Time series analysis how
it is performed okay so it looks like it
has captured the trend properly so up to
here is the known reference we have
reference and from here onwards it's
purely predicted and uh as I mentioned
earlier we can safely assume that the
values are accurate and predicted
properly for the fifth year so let's go
ahead and Implement a Time series
forecast in r first of all we will be
using the arima model to do the forecast
of uh this time series data so let us
try to understand what is arima model so
ARA is actually an acronym it stands for
autor regressive integrated moving
average so that is what is ARA model and
it is specified by three parameters
which is p d and q p stands for auto
regressive so let me just mark this so
there are three components here Auto
regressive integrated moving average
okay so these three parameters
correspond to those three components so
the P stands for auto regressive D for
integrated and Q for moving average so
let us see what exactly this is so these
three factors are p is the number of
Auto regressive terms or ar we will see
that in a little bit and D is how many
uh levels of differences that we need to
do or differentiation we need to do and
Q is the number of lagged forecast error
so we'll see what exactly each of these
are so AR is the number of Auto
regressive terms and which is basically
denoted by the p and then we have D
which is for the number of times it has
to be
differentiated and then we have q which
is for the moving average so what
exactly AR terms so in terms of the
regression model Auto regressive
components uh refer to the prior values
of the current value well what we mean
by that is here when we talk about time
series data focus on the fact that there
is regression so what exactly happens in
regression we try to do something like
if it a simple linear regression we do
some equation like Y is equal mx + C
where there are actually there are two
variables one is the dependent variable
and then there is an independent
variable let me just complete this
equation as well MX Plus see right so
this is a normal regression curve or a
simple regression curve now here we are
talking about Auto regression or Auto
regressive so Auto regressive as the
name suggests is regression of itself so
which means that here you have only one
variable which is your maybe the cost of
the flights or whatever it is right and
the other variable is basically time
dependent and therefore the value at any
given time and that we will denote as YT
for for example so there is no X here
there is only one variable and which is
y and we say YT which is basically the
predicted value at a time interval T for
example is dependent on the previous
value so for example there may be A1 and
then YT minus1 and then there will be
like plus A2 and right plus A2 and YT
minus 2 and uh all right and then + A3
into y tus 3 all right so basically here
what we saying is there's only one
variable here but there is a regression
component so we are doing a regression
on itself so that's how the term Auto
regression comes into play so only thing
is that it is dependent on the previous
time values so there is a lag let's say
this is the first lag second lag third
lag and so on so the current value which
is YT is dependent on the previous time
lag values so that is what is AO
regression component so this is what is
shown here for example in this case
instead of Y we are calling it as X so
that's the same and this is represented
by some equation of that sort depending
on how many lags we take so that is the
AR component and the term p is basically
determines how many lags we are
considering so that's the term E4 now
what is d d is the degree of
differencing so here differencing is
like to for the nor seasonal differences
right so for example if you take the
values like this which are given for 5 4
6 and so on and so forth if you take the
differencing of one after another like
for example 5 - 4 or 4 - 5 the next
value with the previous value so 4 - 5
so this is known as the first order
differencing so the result is minus1
similarly 6 - 4 is 2 7 - 6 is 1 so this
is first order differencing and uh here
we call it as D is equal to 1 okay and
same way we can have second order third
order and so on then the last one is q q
is the actually by we call it moving
average but in reality it is actually
the error of the model so we also
sometimes represent as ET all right so
now ARA model works on the assumption
that the data is stationary which means
that the trend and seasonality of the
data has been removed that is correct
okay so this we have discussed in the
first part how what exactly is
stationary data and how do we remove the
nonstationary part of it now in order to
test whether the data is stationary or
not there are two important components
that are considered one is the
autocorrelation function and other is
the partial autocorrelation function so
this is referred to as ACF and pacf cor
right so what is autocorrelation and
what is the definition autocorrelation
is basically the similar ity between
values of a same variable across
observations as the name suggests now
how do we actually find the aut
correlation function the value right so
this is basically done by plotting and
autocorrelation function also tells you
how correlated points are with each
other based on how many time steps they
are separated by and so on that is
basically the time lag that we were
talking about and it is also used to
determine how past and future data
points are related and the value of the
autocorrelation function can vary from
min 1 to 1 so if we plot this is how it
would look autocorrelation function
would look somewhat like this and there
is actually a readily available function
in R so we will see that and you can use
that to plot your aut correlation
function okay so that is ACF and we will
see that in our R studio in a little bit
and similarly you have partial
autocorrelation function so partial
autocorrelation function is the degree
of association between two variables
while adjusting the effect of one or
more a additional variables so this
again can be measured and it can also be
plotted and its value once again can go
from minus1 to 1 and it gives the
partial correlation of Time series with
its own lagged values so lag again we
have discussed in the previous uh couple
of slides this is how a PF plot would
look in our studio we will see that as
well and once we get into the r studio
and with that let's get into our studio
and take a look at our use case before
we go into the code let's just quickly
understand what exactly is the objective
of this use case so we are going to
predict some values or forecast some
values and we have the data of the
airline ticket sales of the previous
years and now we will try to find the or
predict the or forecast the values for
the future years all right so we will
basically identify the time series
components like Trend seasonality and uh
random Behavior we will actually
visualize this in our studio and then we
will actually forecast the values based
on the past values or history data
historical data so these are the steps
that we follow we will see in our studio
in a little bit just quickly let's go
through what are the steps we load the
data and it is a Time series data if we
try to find out what class it belongs to
the data is actually air passengers data
that is already comes preloaded with our
studio so we will be using that and we
can take a look at the data and then
what is the starting point what is the
end point so these are all functions
that are really available we'll be using
and then what is the frequency there
basically frequency is 12 which is like
yearly data right so every month the
data has been collected so for each year
it is 12 and then we can check for many
missing values if there are any and then
we can take a look at the summary of the
data this is what we do in exploratory
data analysis and then we can plot the
data visualize the data how it is
looking and we will see how the data has
some Trend seasonality and so on and so
forth all right then we can take a look
at the cycle of the data using the cycle
function and we can see that it is every
month that's the cycle end of every 12
months a new cycle begins so each month
of the year is uh the data is available
then we can do box plots to see for each
month how the data is varying over the
various 10 or 12 years that we will be
looking at this data and uh from
exploratory data analys es we can
identify that there is a trend there is
a seasonality component and how the
seasonality component varies also we can
see from the box plots and we can
decompose the data we can use the
decompose function rather to see the
various components like the seasonality
trend and the irregularity part okay so
we will see all of this in our studio
this is how they will look this is the
Once you decompose and this is how you
will actually you can visualize the data
this is the actual data and this is the
trend as you can see it's going upwards
this is the seasonal component and this
is your random or irregularity right so
we call it irregularity or we can also
call it random as you can see here yes
so the data must have a constant
variance and mean which means that it is
stationary before we start any analysis
time series analysis and uh without so
basically yeah if it is stationary only
then it is easy to model the data
perform time series analysis so we can
then go ahead and fit the model as uh we
discussed earlier we'll be using ARA
model there are some techniques to find
out what should be the parameters so we
will see that when we go into our studio
so the auto ARA function basically tells
us what should be the parameters right
so these parameters are the p d and q
that we talked about that's what is
being shown here so if we use autoa it
will basically take all possible values
of this PDQ these parameters and it will
find out what is the best value and then
it will recommend so that is the
advantage of using Auto ARA all right so
like in this case it will tell us what
if we use this parameter Trace we set
the parameter Trace is equal to true
then it will basically tell us what is
the value of this AIC which has to be
minimum so the lower the value the
better so for each of these combination
of p d and q it will give us the values
here and then it will recommend to us
which is the best model okay because
whichever has the lowest value of this
AIC it will recommend that as your best
uh PDQ values so once we have that we
can see that we will basically we can
potentially get a model or the equation
model is nothing but the equation and
based on the parameters that we get and
we can do some Diagnostics we can do
some plotting to see how whether there
is a plot for the residuals so which
shows the stationarity and then we can
also take a look at the ACF and PF we
can plot the ACF and PF and then we can
do some forecasting for the future years
so in this case we have up to 1960 and
then we can see how we can forecast for
the next 10 years which is 1970 up to
1970 and once we have done this can we
validate this model yes definitely we
can validate this model and uh to
validate the findings we use uh junkbox
test and this is how you just call box.
test and then you pass these parameters
and you will get the values that will be
returned which will tell us whether this
how accurate this model is how accurate
the predictions are so the values of P
are quite insignificant in this case we
will see that and that also indicates
that our model is free of
autocorrelation and that will basically
be it so let's go back and into our R
studio and uh go through these steps in
uh real time so we have to import this
Library forecast package is not
installed you have to go here and
install the forecast package okay so
that's the easy way to install rather
than to click on this install I will not
do it now because I've already installed
so the first time that's only one time
then after that you just have to load it
into memory and then keep going so we
will load this data called air passenger
ERS so by calling this data method and
if you see the the data passengers is
loaded here and if we check for the
class it is a Time series data TS data
so we can check for the dates we can
also view the data in a little bit and
start date is 1949 and
January and our end date is 1960
December and the frequency is 12 which
is like collected monthly so that is the
frequency which is uh 12 here and then
we check if there are any um missing
values there are no missing values and
then we take a look at the summary of
the data this is all exploratory data
analysis and then if you just display
the data this is how it
looks and then we need to decompose this
data so we will kind of uh store this in
an object TS data and then use that to
decompose and store the new Val values
let me just clear this for now and uh if
we decompose basically as we have seen
in the slides decomposing is breaking it
into the trend seasonality and The
Irregular or random components then you
can go ahead and plot it so when you
plot it you can see here let me Zoom
this this is our original plot or
observed value as it is known as then we
have decomposed the three parts which is
basically the trend as you can see there
is a trend then the seasonal component
so this is a some regularly occurring
pattern and then there is this random
value which is basically you cannot
really give any equation or function or
anything like that so that's what this
plotting has done and then you can
actually plot them individually as well
so these are the individual plots for
the trend for the seasonal component and
the random component all right so now
let's take a look at the original data
and see how the trend is in a way so if
we do this linear regression line it
will show that that it is going upwards
and we can also take a look at the cycle
that are there which is nothing but we
have a frequency of 12 right so the
Cycles will display that it is January
February to December and then back to
January February and so on and so forth
and if we do box plots for the monthly
data you will see that for each of the
months right and over the 10 years that
the data that we have we will see that
there is a certain pattern right this is
also you way to find the seasonality
component so while January February
sales are relatively low around July
August the sales pick up so especially
in July I think the sales are the
highest and this seems to be happening
pretty much every year right so this is
every year in July there seems to be a
peak in the sales and then it goes down
and slightly higher in December and so
on so that is Again part of our
exploratory data analysis and once again
let's just plot the the data now as I
said in order to fit into an ARA model
we need the values of PD and Q now one
way of doing it is there are multiple
ways actually of doing it the earlier
method of doing it was you draw the
autocorrelation function plot and then
partial autocorrelation function plot
and then observe that and where does
this change and then identify what
should be the values of p and Q and so
on now R really has a very beautiful
method which we can use to avoid all
that manual process that we used to do
earlier so what R will do is there is a
method called Auto ARA and if we just
call this Auto ARA method and it will
basically go and test the ARA model for
all possible values of this parameters
PDQ and then it will suggest you what
should be the best model and it will
return that best model with the right
values of PD and so you we as data
scientists don't have to do any manual
you know trial and error kind of uh
stuff okay so we got the model now and
uh this is the model it it has PDQ
values are 211 PDQ and this is the
seasonal part of it so we can ignore it
for now and so if we want to actually
understand how this has returned these
values 211 as the best one there is uh
another functionality or feature where
we can use this Trace function or Trace
parameter so if you pass to Auto ARA the
trace parameter what it will do is it
will show you how it is doing this
calculation what is the value of the AIC
basically AIC is what you know defines
the accuracy of the model the lower the
better okay so for each combination of
PDQ it will show us the value of AIC so
let's run it before instead of me
talking so much let's run this if we run
auto ARA with Trace you see here there
is a red mark here that means it is
performing it's executing this and here
we see the display right so it starts
with certain values of PDQ and then itth
finds that value is too high so it
starts with again with some zero one one
Z and so on and so forth and ultimately
it tells us okay this is our best model
you see here it says this is our best
model 211 let's go back and see did we
get the same one yes we got the same one
when we ran without Trace as well right
now why is 211 let us see where is 211
here is our 211 and if you compare the
values you see that 10 17 is pretty much
the lowest value and therefore it is
saying this is our best model all other
values are higher so that's how you kind
of U get your model and now that you
have your model what you have to do you
need to predict the values right so
before that let us just do some test of
these values so for that you install T
Series again if you are doing it for the
first time you would rather use this
package and install and say T Series and
install it and then you just use this
Library function to load it into your
memory all right so now that we got our
model using Auto ARA let us go ahead and
forecast and also test the model and
also plot the ACF and PF remember we
talked about this but we did not really
use it we don't have to use that but at
least we will visualize it and uh for
some of the stuff we may need this T
Series Library so if you are doing this
for the first time you may have to
install it and my recommendation is
don't use it in the code you go here and
install T Series and I will not do it
now because I've already installed it
but this is a preferred method and once
you install it you just load it using
this libraries function and then you can
plot your residuals and this is how the
residuals look and you can plot your ACF
and PF okay so this is how your PF looks
and this is how your ACF looks for now
there is really nothing else we need to
do with ACF and PF this just to
visualize how that how it looks but as I
mentioned earlier we were actually using
these visualizations or these graphs to
identify the values of P e d and q and
how that was done it's uh out of scope
of this video so we will leave it at
that and uh then we will forecast for
the next 10 years how do we forast that
so we call forast and we pass the model
and we pass what is the level of
accuracy that you need which is 95% and
for how many periods right so basically
we want for 10 years which is like 10
into 12 time periods so that's what we
are doing here and now now we can plot
the forecast value so you see this is
the original value up to I think 62 or
whatever and then it goes up to 72 this
blue color is the predicted value let's
go and zoom it up so that we can see it
better so from here onwards we focusing
and you can see that it looks like our
model has kind of learned the pattern
and this pattern looks very similar to
what we see in the actual data now how
do we test test our model so we can do
what is known as a box test and we pass
our model here residuals basically with
different lags and from those values
here the P values here we find that they
are reasonably low the P values which
means our model is fairly
accurate applications of data analytics
now the sky's limit on this in today's
world almost every business Act of life
your music on your Spotify are driven by
data analytics but some of the big
players when you go in there job hunting
are going to be your fraud analysis uh
if you want to go make a lot of money
and you're good at it and you like
dealing with numbers uh go join the
banks and track down the criminals who
are stealing money it's a lot of you
know it's a big thing to protect credit
cards protect uh sales purchases bad
checks any of those things when you can
track them down is
huge Health Care
exploding uh there is everything from
trying to find cures for uh the covid
virus or any of the viruses out there uh
using your cell phone to diagnose
different ailments uh that way you don't
have to go in and see the doctor you can
actually just go in there and take a
picture of the funky growth on your arm
hopefully it's not too big and then they
send it in there and the data analytics
goes in there looks at it and says oh
this is what this is this is the
professional you need to go see or don't
need to see and that's just one aspect
of healthcare uh the database is being
generated by Healthcare and getting the
right doctors and helping the doctors
analyze whether something is uh benign
or malignant if it's cancerous all those
things are now part of the ongoing
Health Care growth in data
analytics Inventory management think one
of those huge warehouses where they're
shipping out all the goods how do you
inventory that in such a way so that uh
you maximize the stuff that's being
purchased the most near the entrance and
all the other stuff towards the back or
even even pre- ship it uh so it's huge
to be able to inventory the manage your
inventory and pretty soon they'll just
have a drone come in there and start
picking up some of those boxes and move
them around
also delivery Logistics again this goes
from uh getting from point A to point B
uh you can combine it with our inventory
so you pre- ship stuff if you know a
certain area is more likely to purchase
it how do you get it the delivery to the
most destinations the quickest in the
short amount of time and then they even
pre-stack the trucks going out and
that's all done with data analytics how
do we stack all that stuff so it comes
out in the right
order targeted marketing huge industry
any kind of marketing whether you're
generating uh the right content for the
marketing who are you targeting with
that marketing researching the people
what they want so you know what products
to Market out there all those things are
huge and these are just a few examples
you can probably go Way Beyond this from
tracking forest fires to a ology and
studying the stars all of this is part
of data analytics now and plays a huge
role in all these different
areas uh City Planning is another one
you know you can see a nice organized
City like this one where you can get in
and out of the neighborhoods if you're a
fir Tru uh police officers need to be
able to get in out you want your tourist
to be able to come in yet you still want
the place to look nice and you have the
right commercial development the right
Industrial Development like enough
residence for people to stay all those
things are part of your City Planning
again huge in data
analytics so Sky a limit on what you use
it for let's take a look at types of
data
analytics and this can be broken up in
so many ways uh but we're going to start
with looking at the most basic questions
that you're going to be asking in data
analytics and the first one is you want
descriptive analytics what has happened
hindsight uh how many cells per call
ratio coming out of the call center if
we have 500 tourists in in a forest and
you have a certain temperature how many
fires were started how many times did
the police have to show up to certain
houses um all that's descriptive the
next one is predictive Predictive
Analytics is what will happen next we
want to predict uh this is great if you
want have a ice cream store and you want
to predict how many people to work at
the ice cream store in a certain day
based on the temperature coming up in
the time of the year and then one of the
biggest growing and most important parts
of the industry is now prescriptive
analytics and you can think of that as
combining the first two we have
descriptive and we have
predictive then you get
prescriptive analytics how can we make
it happen foresight what can we change
to make this work
better in all the industries we looked
at before we can start asking questions
uh especially in City development
there's a good one if we want to have
our city generate more income and we
want that income to be commercial based
uh what kind of commercial buildings do
we need to build in that area that are
going to bring people over do we need
huge warehouse sales Costco sales
buildings or do we need little momod
joints that are going to bring in uh
people from the country to come shop
there or do you want an industrial setup
what do you need to bring that IND
industry in there is there car industry
available in that area uh if it's not a
car industry what other Industries are
in that area all those things are
prescriptive we're guessing we're
guessing what can we do to fix it what
can we do to fix crime in area with
education what kind of education are we
going to use to help people understand
what's going on so that we lower the
rate of crime and we help our
communities grow better that's all
prescriptive it's all guessing we went
foresight into how can we make it happen
how can we make this
better and we really can't not go into
enough detail on these three because a
lot of people stumble on this when they
come in and are doing Analytics whether
you're the manager shareholder or the uh
data scientist coming in you really need
to understand the descriptive analytics
where you're studying the total units of
furniture sold and the profit that was
made in the past uh here we go into
Predictive Analytics predicting the
total units that would sell and the
profit we can expect in the future gear
up for how many employees we need how
much money we're going to make and
prescriptive analytics finding ways to
improve the sales and the profit so we
can uh sell maybe a different kind of
furniture uh we're going to guess at
what the area is looking for and how
that marketing is going to
change data analytics process steps so
let's take a look at some of the basic
processing and what that looks like when
you're working with this
data so there's five basic steps uh the
five steps of processing and and this
changes and there's a lot of things that
go on when they talk about um agile
programming the whole concept of agile
is you take some kind of framework like
this and then you build on it depending
on what your business needs so the first
step is data
collection and usually with a large
company you might have somebody who uh
is responsible for the database
management um you might have another one
where they're pulling apis and they're
pulling data off of U maybe the Census
Bureau uh maybe something very very um
specific uh domain specific so if you're
analyzing cancerous growths and how to
understand them then the data collection
is going to be those measurements they
take from the MRI or it might be even
the MRI images they've used those also
uh so there's a lot of things with data
collection and how to control that and
make sure it has uh what you need and
it's clean and you don't have Mis
information coming
in uh once you have the data collected
there's a data
preparation uh so stage two is we take
that data and we format it into
something we can use probably one of the
biggest formats that you see is when
you're you're processing text how do you
process text well you use what they call
a one hot encoder and each word is
represented uh by a a yes no kind of
setup so it' be like a long array of
bits um that's one way to prepare it and
so you know bit number one is the bit
number two is has or whatever it is
other preparations might be if you're
using neural networks you might be um
taking integers or float numbers and
converting them to a value between zero
and one that way you don't have one of
them creating a bias in there uh so
there's a lot of different things that
go into Data preparation that is 80% of
data science so we talk about the data
analytics which is a little bit more on
the math side and usually say talk about
a data scientist kind of being the
overall preparer of this stuff you're
going to spend 80% of your data
preparation data exploration uh that's
the fun part this is where you're
exploring things uh and it is
maybe 10 to 15% of what you do with the
data you spend with the data exploration
it is probably uh the most important
step because this is where you got to
start asking questions uh if you ask
your questions wrong you're going to get
some wrong information if you're working
with a company and they want to know the
marketing values then you really got to
focus on hey how do we generate money
for this company or fraud how do we
lower the fraud rate while still
generating a profit four data modeling
this this is where we start actually
getting into the data code uh which
model to use that predicts what's going
to
happen uh and then result interpretation
we want to be able to interpret those
results you usually see that in your
matplot library where you create nice
beautiful images so it shows up on their
dashboard for the marketing manager or
for the CEO so they can take a quick
look and say hey I can see what's going
on there you want to reduce it to
something they can easily read uh they
don't want to hear the say ific terms
they want to see something they can use
and we'll talk about that a little bit
more when we start looking at some of
this in a
demo since this is data analysis with
python we got to ask the question why
python for data analytics I mean there's
C++ there's Java there's Donnet from
Microsoft why do people go to python for
it so the number of reasons one it's
easy to learn with simple
syntax uh you don't have a very high
type set like you do in Java and other
coding so it allows you to kind of be a
little lazy in your programming uh that
doesn't mean that it can't be set that
way and that you don't have to be
careful it just makes means you can spin
up a code much quicker in Python the
same amount of code to do something in
Python A lot of times is one two or
three or four lines where when I did the
same thing say in Java I found myself
with 10 12 13 20 lines depending on what
it was it's very scalable and flexible
uh so there's our flexibility because
you can do a lot with it and you can
easily scale it up you can go from
something on your machine to using uh Pi
spark into the spark environment and
spread that across hundreds if not
thousands of servers across terabytes of
data or pedabytes of data so it's very
scalable there's a huge collection of
libraries this one's always interesting
because Java has a huge collection of
libraries C has a huge collection of
libraries net does and they always in
competition to get those libraries out
uh Scala for your spark all those have
huge collections of libraries this is
always changing uh but because Python's
open source you almost always have easy
to access libraries that anybody can use
you don't have to go check your
licensing and have special licensing
like you do in some
packages graphics and visualization they
have a really powerful package for that
so it makes it easy to create nice
displays for people to
read and community support because
python is open source has a huge
community that supports it you can do a
quick Google and probably find a
solution for almost anything you're
working
on python libraries let's bring it
together we have data analytics and we
have python so when we're talking data
analytics we're talking python libraries
for data analytics and the big five
players are numpy pandas matplot Library
scipi which is going to be in the
background so we're not going to talk
too much about the scientific formulas
in scipi andsit
so numpy supports in dimensional arrays
provides numerical Computing tools
useful for linear algebra and for year
transform um and you can think of this
as just a grid of numbers um you can
even have uh a grid inside a grid or
data it's not even numbers because you
can also put uh words and characters and
just about anything into that array but
you can think of a grid and then you can
have a grid inside a grid and you end up
with a nice threedimensional array if
you want to talk three-dimensional array
you can think of images you have your
three channels of color four if you have
an alpha and then you have your XY
coordinates for the image we're looking
at so you can go x y and then what are
the three channels to generate that
color and numpy isn't restricted to
three dimensions you could imagine uh
watching a movie well now you have your
movie clips and they each have their X
number of frames and each of those
frames have X number of XY coordinates
for the pictures in each frame and then
you have your three dimensions for the
colors so numpy is just a great way to
work with in dimensional
arrays now closely with numpy is pandas
uh useful for handling missing data
perform mathematical operations provides
functions to manipulate data pandas is
becoming huge because it is basically a
data frame and if you're working with
big data and you're working in spark or
any of the other major packages out
there you realize that the data frame is
very Central to a lot of that and you
can look at it as a Excel spreadsheet
you have your columns you have your rows
or indexes and uh you can do all kinds
of different manipulations of the data
within uh including filling in missling
data which is a big thing when you're
dealing with large pools or lakes of
data where they might be collected
differently from different uh
locations and Matt plot Library we did
kick over the scip which is a lot of
mathematical computations which usually
runs in the background of the of numpy
and pandas um although you do use them
they're useful for a lot of other things
in there but the map plot Library that's
the final part that's what you want to
show people and this is your plotting
library in Python several toolkits
extend matap plot Library
functionality there's like a hundred
different toolkits to extend matplot
Library which range from uh how to
properly display star constellations
from astronomy there's a very specific
one built just for that all the way to
some uh very generic ones we'll actually
add Seaborn in when we do the labs in a
minute several toolkits extend met plot
Library functionality and it creates
interactive
visualization uh so there's all kinds of
cool things you can do as far as just
displaying graphs and there's even some
that you can create interactive graphs
we won't do the interactive graphs but
you'll see you'll get a a pretty good
grasp of some of the different things
you can do in matplot
library look let's jump over to the demo
which is my favorite roll up our sleeves
get our hands in on what we're doing now
there's a lot of options when we're
dealing with python uh you can use py
charm as a really popular
one uh and you'll see this all over the
place um so it's one of the main ones
that's out there and there's a lot of
other ones I used to use net beans which
is kind of lost favor uh don't even have
it installed on my new
computer but the most popular one right
now for data science now py charm's
really popular for py on General
development for data science we usually
go to Jupiter uh notebook or anaconda
and we're going to jump into Anaconda
because that's my favorite one to go to
because it has a lot of external tools
for us we're not going to dig into those
but we will pop in there so you can see
what it looks like so with Anaconda we
have our Jupiter lab we have our um
notebook these are identical Jupiter lab
is an upgrade to the notebook with
multiple tabs that's all it is and we'll
be using the notebook and you can see
that py charm is so popular with um
python that we even have it highlighted
here in anaconda's part of the setup uh
Jupiter notebook can also be a
standalone uh so we're actually going to
be running Jupiter notebook and then you
have your different environments um I
have we're going to be under main Pi 36
there's a root one and I usually label
it Pi
36 the reason is is currently as of
writing this tensor flow only works in
36 and not in 37 or 38 for for doing
neural networks but you can actually
have multiple environments which is nice
they're they separate the kernel so it
helps protect your computer when you're
doing development and this is just a
great way to do a display or a demo
especially if you're looking for that
job pull up your laptop open it up or if
you're doing a meeting get it broadcast
up to the big screen so that the CEO can
see what you're looking
at and when we launch the notebook uh it
actually opens up a file browser in
whatever web browser you have this
happens to be Chrome and then you can
just go under new there's a lot of
different options depending on what you
have installed Python 3 and this just
creates an Untitled uh version of this
and you can see here I'm actually in a
simply learn folder for other work I've
done for simply
learn uh and that's where I save all my
stuff and I can browse through other
folders making it really easy to jump
from one project to another and under
here we'll go ahead and change the name
of this and we'll go ahead and rename it
data analytics data analytics just so I
can remember what I was
doing which is probably about 50 of the
folders in here right or files in here
right now uh so let's go ahead and jump
in there and take a look at some of
these different uh tools that we were
looking
at and as we go through the demo let's
start with the uh numpy uh the least
visually exciting and I'm going to zoom
in here so you can see what we're
doing and the first thing we want to do
is import import
numpy and we'll import it as NP that is
the most common numpy
terminology and let's go and change the
view so we also have the line numbers um
I don't know why we probably won't need
them but not for easy reference uh and
then we'll create a onedimensional array
we just call this array one and it
equals np. array and you put your array
information in here in this case we'll
spell it out uh you can actually do like
a range and other ways there's lot of
way to generate these arrays but we'll
just do one two three so three
integers and if we
print our array
one we can go ahead and run this and you
can see right here it prints one two
three you can see why this is a really
nice interface to show other people what
you're doing uh with the Jupiter
notebook uh so this is the basic we've
created an array this is a
onedimensional array and then array is 1
two 3 one of the nice things about the
jupyter notebook is whatever ran in this
first setup is still running it's still
in the kernel so it still has the nump
imported as NP and it still has our
variable um arr1 for array one equal to
NP array of 1 two 3 so we go to the next
cell we can check the type of the array
we're just going to print we say hey
what's what what what is this um setup
in here and we want
type um and then we want what is the
type type of array one let's go ahead
and run that and it says class numpy in
the array so it's its own class that's
all we're doing is is checking to see
what that class
is and if you're going to look at the uh
array class uh probably the biggest
thing you do I don't know how many times
I find myself uh doing this uh because I
forget what I'm working on and I forget
I'm working with a three-dimensional or
four-dimensional array uh and I have to
reformat somehow so it works with
whatever things I have and so we do the
array shape uh the array shape is just
three because it has three members and
it's a one-dimensional array that's all
that
is and with the numpy array we can
easily access um stick with the print
statement if you actually put a variable
in Jupiter notebook and it's the last
one in the cell it will the same as a
print statement so if I do this where
array one of two it's the same as doing
print array of two that's those are
identical statements in our Jupiter
notebook we'll go and stick with the
print on this one and it's three so
there's our print space two and we have
0 1 2 2al 3 we can easily change that so
we have array one of place
two equals
5 and then if we print our array
one uh you can see right down here when
it comes out it's 1 2 and five five and
there I leftt the print statement off
because it's the last variable in the
list um it'll always print the variable
if you just put it in like that that's a
Jupiter notebook thing don't do that in
pie charm I've forgotten before doing a
demo and we talked about multiple
Dimension so we'll do an array um
two-dimensional array and this is again
a numpy
array and in the numpy array we need um
our first Dimension we'll do one 2 3 and
our second dimension uh 3 4 five and you
can see right here that when we hit the
uh we'll do this we'll just do array
two and we can run that and there's our
array two 1 2 3 3 4 5 we can also do
array
two of uh
one and then we can do let's do zero
doesn't really matter which one actually
let's do uh two there we go and if I run
this it'll print out five uh cuz here we
are this is 0o uh 0 one 2 3 is on our
zero Row 3 4 five is on our one row
always start with zero and then the two
012 goes to the
five and then maybe we forgot what we
were working with so we'll go do array
2.
shape and if we do array two of
shape uh we'll go and run that we'll see
we have two rows and each row has three
elements uh two dimensional array two
three if you looked up here when we did
it before it just had three comma
nothing when you have a single entity it
always saves it as a tuple with a blank
space uh but you can see right here we
have 2 comma
3 and if you remember from up here we
just did this array two of oh let's go
what is it one
2 we run that we get the five you can
also count backwards this is kind of fun
and you'll see I just kind of Switched
something on you because you can also do
one comma two to get to the same spot um
now two is the last one 0 one two it's
the last one in there we can count
backwards and do minus one and if we run
this we get the same answer whether we
count it as uh let's go back up here
whether we count this as Z 0 1 2 or we
count backwards as min-1 -2 -3 and you
can see that if I change this -1 to a
minus two and run
that I get four which is going backwards
minus one minus two so there's a lot of
different ways to reference what we're
working on inside the numpy
array it's really a cool tool it's got a
lot of things you can do with
it and we talked about the fact that it
can also hold things that are not values
and we'll call this array s for strings
equals uh np.
array put our setup in there brackets
and let's
go
China
um
[Music]
India
USA uh
Mexico doesn't matter we can make
whatever we want on here and if we print
that
out we run this you can see that we get
our number array China India USA Mexico
it even gives us our dtype of a
U6 and a lot of times when you're
messing with data we'll call this array
R for range just to kind of keep it
uniform
np. a range so this is a command inside
numpy to create a range of
numbers and if you're testing data Maybe
you want maybe have equal time
increments um that are spaced a certain
point apart but in this case we're just
going to do
integers and we're going to do a uh a
setup from 0 20 skipping every other one
and we'll print it out and see what that
looks
like and you can see here we have 0 2 4
6 8 10 12 14 16 18 like you expected it
skips every one and just a quick
note there's no 20 on here uh why well
this starts at zero and counts up to 20
so if you're used to another language
where explicitly says less than or less
than equal to 20 like forx equal 0 um
x++ uh X is less than 20 that's what
this is it just assumes X is less than
20 on
here and if we want to create a very
uniform uh set you know 0 2 4 6 what
happens if I want to create numbers uh
from 0 to 10 but I need 20 increments in
there uh we can do that with line space
so we can create um an R uh we'll call
this
L equals I don't think we'll actually
use any of this again so I don't know
why I'm creating unique um identifiers
for it uh but we'll do
NP uh Lin
space and we're going to do
0 to 10 or 0 to nine uh remember it
doesn't it goes up to 10 and then we
want to let's say we have 20
different um increments in there so
we're creating a we have a data set and
we know it's over a certain time period
and we need to divide that time period
by 20 and it happens to just have 10
pieces in it um and here we go you can
see right here we have TW or has 20
pieces in it but it's over 10 years we
got to divide it in the middle and you
can see it does it goes 0.52 remember
yeah there's our 10 on the end so it
goes up to
10 uh and then we can also do random
there's np. random if you're doing
neural
networks uh usually you start it by
seating it with random
numbers and we'll just do np. random and
we'll just call this array we'll stop
giving it unique numbers we'll print
that one out and run it and you can see
we have random numbers they are 0o to
one so you'll see that all these numbers
are under one and you can easily alter
that by multiplying them out or
something like that if you want to do
like 0o to 100 um you can also round
them up if it's integer 0 to 100 there's
all kinds of things you can do but it
generates a random float between 0 and
one and you have a couple options you
could reshape that um or you can just
generate them uh in whatever shape you
want and so we can see here uh we did
three and four and so you can see three
rows by four
variables same thing as doing a reshape
of 12 variables to three and
four and if you're going to do that you
might need an empty data set um I have
had this come up many times where I need
to start off with zero and I don't know
you know because I'm going to be adding
stuff in there or it might be zero and
one or one is uh if you're removing the
background of an image you might want
the background is zero and then you
figure out where the image is and you
set all those boxes to one and you
create a mask so creating mask over
images is really big and doing that with
a a numpy array of zero
and we can also
uh give it a
space and we'll just do this all in one
shot this time and we'll do the same
thing like we did
before zeros and in this case we'll do
uh 2 comma three and so when we run
this forgot the asteris around it I knew
was forgetting something there we go so
when we run this uh you can see here we
have are 10 zeros in a row and maybe
this is a mask for an image and so it
has uh two rows of three digits in it so
it's a very small image um little tiny
pixel and maybe you're looking to do
something the opposite way uh instead of
uh creating a mask of zeros and filling
in with ones uh maybe you want to create
a mask of ones and fill them in with
zeros and we'll just do just like we did
before we'll do comma 4 and when we run
this you'll see it's all ones and we
could even do this even U we'll do it
this way let's do
10 10 x 10 icon and then you have your
three colors you so creates quite a
large array there for doing pictures and
stuff like that when you add that third
dimension
in um if we take that off it's a little
bit easier to
see we'll do 10 again
and you can easily see how we have 10
rows of 10
ones and you can also do something uh
like create an
array and we'll do 0 one
2 and then in this array um we actually
print it right out we want a repeat so
you can actually do a repeat of the
array and maybe need this array um let's
repeat it three
times so there's our repeat of an array
repeat three
times and if we run this you'll see we
have 00111
222 and whenever I think of a repeat I
don't really think of repeating being
the first digit three times the second
digit I really always think of it as um
012 012 012 it catches me every time uh
but the actual code for that one is
going to be tile uh again if we do AR
range
three and we run this you can see how
you can generate 012 012
012 and if you're dealing with um an
identity Matrix um we can do that also
if you're big on you're doing your
matrixes and we'll just
identity I guess we'll go and spill it
out today
Matrix and the command we're looking for
is is um I ey and we'll do three and
then we'll just go ahead and print this
out there we go there's our identity
Matrix and it comes out by a 3X3 array
because there's our
Matrix uh and then it puts the ones down
the middle and for doing your different
Matrix math and we can manipulate that a
little bit too um we talk
about uh matrixes
we might not want ones across the middle
in which case we now have uh the
diagonal so we can do an np.
diagonal and we do a diagonal uh let's
put in the
diagonal one 2 3 4 five and when we run
this again this generates a value and by
just putting that value in there is the
same as putting print around it or
putting array equals and then print
array and you can see it generates a
diagonal 1 2 3 4 5 and there's your uh
your beginning of your Matrix array for
working with
matrixes and we can actually go in
reverse uh let's create an array equals
remember our
random random. random and we'll do a 5x5
array uh oops there we go five by
five and just so you can see what that
looks
like helps if I tpe don't mistype the
numbers
which is in this case I just need to
take out the brackets and there you go
you have your your 5x5 array set up in
there and we can now because we're
working with matrixes we might want to
do this in reverse and extract the
diagonals which would be the 79 the 0678
and so
on and we simply type in np.
diagonal we put our array in there um
and this will of course print it out
because it returns it as a variable and
you can see here here's our diagonal
going across from our
Matrix and we did talk about shape
earlier if you remember you can do um
print the shape out you can also do the
dimensions uh so in Dimensions very
similar to shape it comes out and just
has two Dimensions we can also look at
the size so if we do size on here we can
run that and you can see has a size of
25 two dimensions and of course 5x5 and
that was from the shape from earlier
that we looked at uh there's our 5x5
shape and if you remember earlier we did
random well you can also do uh random I
talked a little bit about manipulating
zero to one and how you can get
different answers you can also do
straight for the integer part and we'll
do minus uh 10 2
10
4 and so we're going to Generate random
integers between minus 10 to 10 uh we're
going to generate four of those and when
we run that we have seven 7 - 3 - 6 - 3
they're all between - 10 and 10 and
there's four of
them and now we jump into some of the
functionality of arrays uh which is
really great because this is where they
come
in here's your array and you can add 10
to it and if I run this um there takes
my original array from up
here with the integers and adds 10 to
all of those values so now we have oh
this is the decimal that's right this is
a random decimal I had stored in ARR
um but this takes a random decim the
random numbers I had from 0 to one and
adds 10 to them and we can just as
easily do uh minus
10 uh we could even
do times
2 and we could do divide by two and it
would it'll take that random number we
generated and cut it in half so now
these numbers are under
0.5 uh another way you can change the
numbers to what you need on
there and as you dig deeper into numpy
we can also do exponential so as an
exponential function uh which should
generate some interesting numbers off of
the random so we're taking them to the
power I don't even remember what the
original numbers in the um array were
because we did the random numbers up
there here's our original numbers and if
you build an exponential on there uh uh
this is where you get e to the X on this
and just like you can do e to the X you
can also do the log so if you're doing
logarithmic
functions that reinforce learning you
might be doing some kind of log setup on
there and you can see the logarithmic of
these different array
numbers and if you're working with uh
log base
2 you can do you can just change it in
there n p log 2 you have to look it up
because this is not log one two 3 four
five um it is Log and log two uh so just
a quick note that's not a variable going
in that is an actual command there's a
number of them in there and you'll have
to go look and see uh what the
documentation is but you can also do log
10 so here's log value
10 uh some other really cool functions
you can do with this is your sign so we
can take a sign value of all of our
different uh values in there and if you
have sign you of course have cosine
we can run that uh so here's a cosine of
those and if you're doing activations in
your numpy array and you're doing a
tangent activation uh there's your
tangent for
that and the tangent activation is
actually uh from neural networks that's
one of the ways you can activate it
because it forms a nice curve between uh
from whether you're generating one to
negative one uh with some discrepancy in
the
middle just jumping a little bit in
there and to neural
networks and then we get into let me
just put the array back out there so
that we can see it uh while we're doing
this as we're getting into this you can
also sum the values so we have NP
sum and you can do a summation of all
the values in this array and you'll see
that if you added all these together
they'd equal
12519 so on I don't know what the whole
setup is in
there uh but you can see right here the
the summation of this
one of the things you can also do is by
axes so we could do axis equals zero and
if we run the summation of the axis
equal
zero and you can think of that uh in
numpy as the row so that would be uh or
you can think of that in numpy as being
the columns we're summing these columns
going across and you can also change
this to
one and now we're summing the rows
and so that is the summation of this row
and so forth and so forth going
down and maybe you don't need to um know
the summation maybe what you're looking
for is the
minimum uh so here's our minimal you're
looking for and this comes up a lot
because you have like your errors we
want to find the minimal error inside of
this array and just like um the other
one we can do a is equals zero
and you can see here
0.645 is the smallest number in this
First Column is 0645 and so
on and if you have a minimum well you
might also want to know the max maybe
we're looking for the maximum profit and
here we go you can see maximum 79 is a
maximum on this first column and just
like we did before you can change this
to a one on axes you can take the axes
out of here and just find the max value
for the whole array and the max value in
here was 8344 so on so
on and since we're talking data
analytics uh we want to go ahead and
look at the mean uh pretty much the same
as the average this is the mean across
the whole thing and just like we did
before we could also do axis equals
zero and then you'll see this is the
mean of this axis and so on and we have
mean we might want to know the median
and there's our median our most common
numbers uh if we have median we might
want to know the standard deviation or
if we have the average a lot of times
you do the means and the standard
deviation um we can run that and there's
our standard deviations along the axes
we're can also do it across the whole
array uh if we're going to do standard
deviations there's also uh
variance which is your
V and there's our variance across the
different
levels and so if we looked at that we
looked at variance we looked at standard
deviation the median and the means
there's more but those are the most
common ones used with data analytics um
and then going through your data and
figuring out uh uh what you're going to
present to the
shareholders and some other things we
can do is we can actually take slices uh
you'll hear that
terminology and a slice might be um like
we have a 5x5 array but maybe we don't
want the whole array maybe we want uh
from one on we don't want the zero in
there so we got up to four and maybe on
the second part we just
want two to row three and see this
notation right here says one to the end
and if we run this you can see how that
generates a single row to the end and
then row two and three now remember it
doesn't include three that's why we only
get the one column so if you wanted two
and three you would need to go ahead and
go two to four so it goes up to four we
could also do this in Reverse just like
we learned earlier we can go minus one
oops and when we go to minus one it's a
same thing because we have 0 1 2 3 4
this is the same thing as two to four
goes two to the last
one also very common with arrays is
you're going to want to sort them so we
still have our array up here that we
rent romly generated and we might want
to
um sort it and we'll go and throw an
axis back in there uh axis equals one if
we run this you can see from the axes
that it sorts it uh the point 2 being
the lowest value to the highest value by
the row we can also change this of
course to axis zero if you're sorting it
by columns so maybe your values are
based on columns and then of course you
can do the whole
array and we can sort that don't usually
do that but you know I guess sometimes
you might that might come
up and so you can see right here we have
a nice sorted array uh something else
let's just go ahead and reprint our
array so we can look at it again
starting to get too many boxes up there
uh something else you can do with an
array is we can take and transpose it
this comes up more than you would think
when you transpose it you'll see that um
the rows and the column are transposed
so where
7957 064 is the column now we've
switched it and we have 79.4 2 as the
index you can see this really more
dramatic if we take a
slice and we'll just do a slice of the
first couple and then we'll just do all
the other um the full rows and if we run
this you can see how it comes up a
little bit different and we'll just do
the same slice up here so you can see
how those two look next to each
other there we go there's our slice run
uh and so you can see the slice comes up
and it has uh one two 3 four five
columns now we have 1 2 3 four five rows
and three columns versus three
rows and the original version when they
first started putting this um together
uh was a function so the original
version was transpose and this still
worked you can still see it generates
the same value as just a capital T so
many times we flip this data because
we'll have an XY value or we'll have an
image or something like that and it's
being read one way into the next process
and the next one needs it the opposite
uh so this actually happens a lot you
need to know how to transpose the data
really
quick and we can go ahead oh let's just
take um here's our transpose we'll just
stick with the transpose on here and
instead of uh doing this we might need
to do something called flattening why
would you flatten your data uh if this
is an array going into a neural
network you might want to send it in as
one set of values instead of two rows
and you can see here is all the values
as a single array it just flattens it
down into one
array so we covered our scientific uh
means transpose median um some different
variations on here some of the other
things we want to do is what happens if
we want to pin to our array uh so let's
create a new array I'm getting tired of
looking at the same set of random
numbers we generated earlier um so we'll
go and create a new array here something
a little simpler so it's easier to see
what we're
doing and four five six
S8 uh that's good enough we'll just do
four five six
S8 and if we print this
array there it is 4 five 6 78
and we might want to append something to
the array so we have our array we need
to extend it you got to be very careful
about appending things to your array and
there's a number of reasons for that uh
one is runtime because of the way the
numpy array is set up a lot of times you
build your data and then push it into
the numpy array instead of continually
adding on to the array um and then it
also usually it automatically generates
a copy for protecting your data so
there's a lot of reasons to be careful
about app pending this way uh but you
can certainly do it and we can just take
our array we're going to create a new
array array one and if we print array
one and we append eight to it you'll see
four five six 7 and then there's our a
appended on to the
end and if you want to append something
to an array um you'd probably also want
to
whoops array one let's try that again
there we go now we have the eight to
pended on to the end um so you can see
four five 6 seven eight and then we
pined another eight on
there and if you're going to append
something you might want to um go ahead
and insert instead of appending it might
be you need to keep a certain order and
we can do the same thing we do our
array um and we're going to pin or
insert at the beginning and let's go
ahead and insert uh one two three one
two 3 and we go ahead and print our
array to we run it and you can see 1 2 3
a pin is inserted at the beginning uh
inserts a lot more powerful and that you
can put it anywhere in the array we can
move it to the one spot and there we go
one two three uh we can do a minus one
just for fun and you'll see it comes up
uh one two three and we're counting
backwards by one I imagine you can do a
minus
0 and run this and it turns out that
minus 0 puts it back at the beginning
because that's why reg is a zero just
takes a minus sign
off and just like we add numbers on we
might want to delete numbers and so uh
let's do an np. delete Well's let's keep
it a little bit make it a little easy
here um to watch we'll go and create an
array three and we'll do NP delete we
were just working with array uh
two and what we want to do is delete
zero space uh so if you look at this
here's our array two array two starts
with one and when we delete the space on
here and print that out uh we deleted
the one right out of
there and we can also do something like
this where we can do it as a slice and
we can do let's do one comma 3 and if we
run one comma 3 you'll see we've deleted
the one space and the three space out
which deleted our two and
four now keep in mind when you're mess
with um adding lines and deleting
lines uh you have to be really careful
because there's a time element involved
um as far as where the data is coming
from and it's really easy to delete the
wrong data and corrupt what you're
working on or to insert stuff where you
don't want it um so there's always a
warning when we talk about manipulating
numpy
arrays and just like anything else we're
doing uh we'll create an array C which
equals we'll just do our um our numpy
array that we just created our numpy
array three and we can do copy so you
can make a copy of it U maybe you want
to protect your original data or maybe
you're making a mask and so you copy the
array and then the new array make all
these alterations and change it from
values to 0er to one to mask over the
first one and of course we if we do um
array C since it equals a copy of uh
array three it's the same thing 1 3 5 6
7 8
and now we're getting into uh combine
and split arrays I end up doing a lot of
this and I don't know how many times I
end up fiddling with this and having a
mess uh so but but you do it a lot you
know you combine your arrays you split
them you might need one set of data for
one thing another set of data for the
other so let's go and create two arrays
array one array two and I want you to
note and the terminology we're going to
look for is concatenate what that means
is we're going to take um we'll call
this a ray cat I like a ray cat there we
go um our array cat or concatenated
array we're taking array one and two and
it's very important to really pay
attention to your axes and your counts I
can't merge two arrays that have like
the if their axes are messed up and I'm
merging on axis zero it's going to give
me an error and I'll have to reshape
them so you got to make sure that
whatever you're concatenating together
works and what that
means as you can see here we have one 2
3 4 1 2 3 4 and then 5 6 7 8 5 6 7 8
along the zero axis these each are four
values um so it's a 2x4 value and if we
go ahead and switch this to one you can
see how that's that flips it a little
bit so now we have 1 2 3 4 5 6 7
8 it's interesting that we chose that
one if I did something like
this where this is
now there we go and we can catenate it
um run this and it gives me an answer
okay because I have two by two and I'm
using axis one but if I switch this to
axis zero where now it's got three and
five it gives me an error so you got to
be really careful on that to make sure
that your whatever axes you are putting
together that they match um so like I
said this one oops axis one axis one has
two entities and since we're going on
axis one or by row you can see that it
lets it merge it right onto the end
there and you could imagine this if this
was a XY plot of value or the x value
going in and the predicted yvalue coming
out and then you have another prediction
and you want to combine them this works
really easy for that we'll go back and
let's just put this back to where we had
it oops I forgot how many changes I made
there we go 'll just put it Oops I
messed up in my concatenation order
[Music]
here there we
go okay so you can see that we went
through the different concatenation axes
is really important when you're doing
your concatenation values on here and
we'll switch this back to one just
because I like the looks of that better
there we go two
rows now there are other commands in
here um so we can do uh cap V equals
npv v stack this is nothing more than
your
concatenation uh but instead we don't
have to put the axes in there uh because
it's v stands for vertical and so if we
print out
cat V and we run this you can see we get
the 1 2 3 4 1 2 3 4 and that would be
the same as making this axis zero for
vertical stack and if you're going to
have a vertical stack uh you can also
have an H
stack so if we change this to from v
stack to oops here we go H stack and
we'll just change this from cat to cat
and I run
this it's the same as doing axis zero
the process is identical in the
background um this is like a legacy
setup uh your B stack and your H stack
most people just use concatenate and
then put the axes in there CU it's much
uh has a lot more clarity and um is more
more commonly used
nowadays the last section in numpy we're
going to cover uh
is is kind of uh data exploration um and
that'll make a little bit more sense in
just a moment sometimes they call them
set operations but let's say we have an
array 1 2 3 4 5 6 3 whatever it is uh
you we generate a nice little array here
and what I want to go ahead and do is
find the unique values in that
array uh so maybe I'm generating what
they call a one hot encoder and so these
values then all become I need to know
how long my bit array is going to be so
each word how many how many each word is
represented by a number and then I want
to know just how many of those words are
in there if we're doing word count very
popular thing to do
um and you can see here when we do
unique uh we have one two three four
five six those are our unique
values uh some of the things we can do
with the unique values is we can also
instead of doing just unique we can do
uniques or unique values and counts of
each unique
value and this is very similar to what
we just did up here where we uh we're
doing NP unique uh but we're going to
add a little bit more into there and
it's just part of the arguments in this
and we want to do
return counts equals true so in instead
of just returning the unique values uh
we want to know how many of those unique
values are in each one and we'll go
ahead and
print our
uniques and
print our
counts when we run that uh you can see
here we have our unique value 1 2 3 4 5
six just like we had before and then
there's two of the first of two ones two
twos two 3es two fours one five two
sixes and so on and you can go through
and actually look at that if you want to
count them um but a quick way to find
out your um distribution of different
values so you might want to know how
often the word the' is used versus the
word and if each word is represented as
a unique
number and along the set variables we
might want to know um let me just put a
note up here we're going to start
looking
at uh
intersection and we might want to also
know
differentiation and
neither so when we're whoops
neighbor neither um so what we're
looking at now is we want to know hey
where do these two arrays intersect and
we have 1 2 3 4 5 3 4 5 6 7 we might
want to know what is common between the
two arrays
um and so when we do that we have um
NP
intersect and it's a ond array
onedimensional
array and then we need to go ahead and
put array uh one array
two and if we run
this we can see they intersect at 345
that's what they have
common uh and because we're going to go
ahead and go through these and look at a
couple different options let's change
this from intersect
1D and we'll do the same thing we'll go
and print this so we might want to know
the intersection uh where they have
commonalities another uh unique word is
Union of 1D uh so instead of uh
intersect we want to know all the values
that are in both of them so here's our
Union of 1D when we run that you can see
we have 1 2 3 4 5 six 7 so it's all the
different values in there
and the last one of the last words we
have two more to go uh is we want to
know what the set difference
is uh so that's where the you'll see if
you remember set we talked about that
being the what they call these things um
so the set
difference of a 1D array when we run
that you can see that one is only in one
array and two is only in one
array and if we want to know uh what in
Array one but not in Array two we might
want to know what is in Array one but
not two and what's in two but not
one uh and this would be the set X or 1D
on here uh so we have the four different
options here where we can do an
intersection what do they both have in
common uh we can do a union what are all
the unique values in both arrays we can
see the difference what's in Array one
but not array two so set diff 1D and
then set X or what is not in one but is
in two and what is in not in two but in
one so we dug a lot in numpy because
we're talking um there's a lot of
different little mathematical things
going on in numpy a lot of this can also
be done in pandas although usually the
heavy lifting is left for numpy because
that's what it's designed for let's go
ahead and open up another Python
3 setup in here and so we want to
explore uh what happens when you want to
display this this this is where it
starts getting in my opinion a little
fun because you're actually playing with
it and you have something to show people
and we'll go ahead and rename this we're
going to call this uh
pandas uh and pip plot so pandas pip
plot just so I can remember for next
time and we want to go ahead and import
the necessary libraries we're going to
import pandas as PD now remember this is
a data frame so we're talking rows and
columns and you'll see how uh pandas
work so nicely uh you're actually
showing data to people and then we're
going to have numpy in the background
numpy works with pandas uh so a lot of
times you just import them by
default caborn sits on top of the map
plot Library uh so sometimes we use the
caborn because it kind of extends it's
one of the 100 packages that extends the
map plot Library probably the most
common used because it has a lot of
built-in functionality um almost by
default I usually just put caborn in
there in case I need it and of course we
have uh map plot Library as pip plot as
PLT and note we have as PD as NP as SNS
as PLT those are pretty standard so when
you're doing your Imports I would
probably keep those just so other people
can read your code and it makes sense to
them that's pretty much a standard
nowadays and then we have the strange
line here uh it says uh amber sign
matplot Library
inline that is for Jupiter notebook only
so if you're running this in a different
package it'll have a popup when it goes
to display the matplot library um you
can with the most current version of
Jupiter usually leave that out and it
will still display it right on the page
as we go and we'll see what that looks
like and then we're going to go ahead
and just uh do the um caborn the SNS do
set and we're going to set the color
codes equals true let them uh just keep
the default ones so we don't have to
think about it too
much and we of course have to run this
um the reason we run this is because
these values are all set if we don't run
this and I access one of these um
afterward it'll it'll crash the cool
thing about Jupiter uh notebooks is if
you forgot to import one of these you
forgot to install it because you do have
to install this under your anaconda
setup or whatever setup you're in you
can flip over to Anaconda and run your
install for these um and then just come
back and run it you don't have to close
anything
out and we'll go ahead and paste this
one in here real quick where we have car
equals pd. read CSV
and then we have uh the actual path this
path of course will vary depending on
what you are working with uh so it's
wherever you saved the file at and you
can see here I have um like my one drive
documents simply Learn Python data
analytic using python SL car CSV it's
quite a long
file when we open that up what we get is
we get a CSV file and we have the make
the model the GE the engine fuel type uh
engine horsepower cylinders and so on um
and this is just a comma separated file
so each row is like a row of data think
of it as a um
spreadsheet and then each one is a
column of data on here and as you can
see right here it has the uh make model
so it has columns for a header on
here now your pandas just does an
excellent job of automatically pulling a
lot of this in so you start seeing the
pandas on here you realize that you are
already like halfway done with getting
your data in uh I just love pandas for
that reason numpy also has it you can
load a CSV directly into numpy um but
we're working with pandas and this is
where really gets cool is I can come
down here and I can print uh you
remember our print statement we can
actually get rid of it and we're just
going to do car head because it's going
to print that out the head is going to
print the top values of that data file
we just ran in and so you can see right
here it does a nice print out it's all
nice and inline because we're in jupyter
Notebook I can scroll back and forth and
look at the different data uh and just
like we expected we have our column it
brought the header right in one thing to
note is the index it automatically
created an index 0 1 2 3 4 and so on and
we're just looking at the head so we got
0 1 2 3
4 um you can change this you might want
to just look at the top two we can run
that there's our top two
BMWs um another thing we can do is
instead of head we can do
tail and look at the last three values
that are in that data file and uh you
can see right here it numbered them all
the way up to
11,933 oh my goodness they put a lot of
data in this file I didn't even look to
see how big the file was uh so you can
really easily get through and view the
different data in here when you're
talking about Big
Data you almost never just print out car
uh in fact let's see what happens when
we do if we run this and we just run the
car it's huge uh in fact it's so big
that the pandas automatically truncates
it and just does head plus tail so you
can see the two um so we really don't
want to look at the whole thing we'll go
and go back to we'll stick with the head
displaying our data there we go so
there's a head of our data it gives us a
quick look to see what's actually in
there um I can zoom out if we want so
you can actually get a better
view although we'll keep it zoomed in so
you can see the code I'm working
on and then from the uh data standpoint
we course want to look at um data types
uh what's going on with our data what
does it look like uh now this you know
you show your when you're talking to
your
shareholders they like to see these nice
easy to read charts they look like a
spreadsheet sheet uh so it's a nice way
of displaying pieces of the
chart when we talk about the data types
now we're getting into the data science
side of it what are we working with well
we have uh make model we have an integer
64 for the year uh engine fuel type is
an object if we go up here you can see
that there most of them are um like you
know it's a set manual rear wheel drive
uh so they might be very limited number
of types in there
uh and so forth and you it's either
going to be a float 64 an integer or an
object is the way it's going to read it
on
here and the next thing you want to know
is like your
columns and since it loaded the columns
automatically uh we have here the make
the model the year the engine the size
all the way up to the
MSRP and um just out of something you'll
see come up a lot is whenever you're in
pandas you type in values it converts it
from a pandas uh list to a numpy
array and that's true of any of these uh
so then you end up in a numpy array so
you'll see a little switch in there in
the way that the data is actually stored
and that's true of any of these uh in
this case so we want car.
columns you have a total list of your
car columns and like any good data um
scientist we want to start looking at
analytical summary of the data set
what's going on with our data so we can
start trying to um piece Mill it
together so we can do
car uh
describe and then we'll do is we'll do
include equals all uh so nice Panda
command is to describe your data if
you're working with r this should start
looking familiar uh and we come down
here and you can see um count there's uh
make the model the year um how many of
each one
how many unique values of each one uh
the top value of each one what's most
common the frequency the mean um clearly
on some of these it's an object so it
really can't tell you what the um
average is you it' just be the top ones
the average I guess um the year what's
the average year on there um all this
stuff comes down here your standard
deviation your minimum value your
maximum value uh what's in the lower
quarter 50% Mark where where's that line
at and what's in the upper 75% the top
25% going into the
max now this next part is just cool uh
this is what we always wanted computers
to be back like in the 90s instead of
5,000 lines of code to do this maybe not
5,000 all right I built my own plot uh
Library back in 95 and the amount of
code for doing a simple plot was um I
don't know probably about 100 lines of
code this is being done in one line of
code we have our car which is our pandas
we generated that it's our data frame
and we have hist for histogram that is
the power of Seaborn now it's still
going to generate a numpy graph but
caborn sits on top and then we can do
the figure size this is just um so it
fits nicely on the paper on here and we
do something simple like this and you
can see here where comes up and does say
map plot library and does subplots and
everything but we're looking at a
histogram of all the different pieces in
our database and we have our engine
cylinders um that's always a good one
because you can see like they have some
that are they had a null on there so
they came out as zero um maybe a couple
maybe one of them had a two-cylinder
engine way back when four is a common uh
six a little less common and then you
see the 8 cylinder uh 12 cylinder
engines well that's got to be a
Speedster or something uh but you can
see right here it just breaks it down so
now you have uh how many cars with how
many whatever it is cylinders horsepower
uh and so on and it does a nice job
displaying it you can see if you're
working with your uh um you're going
into your uh demo it's really nice just
to be able to type that in and boom
there it is I can see it all the way
across and we might want to zero in uh
and use like a box plot and this time
we'll go ahead and call the um Seaborn
SNS box plot and we're going to go ahead
and do um vehicle size versus um engine
horsepower XY plot and the data comes
from the car so if we run this we end up
with a nice box plot you see our midsize
Compact and large you can see the
variation there's our outlier showing up
there on the compact that must be a
high-end sports car uh large car might
have a couple engines and again we have
all these outliers and then your
deviation on
them very powerful and quick way to zero
in on one small piece of data and
display it for people who need to have
it reduced to something they can see and
look at and understand and that's our
Seaborn boxplot or SNS dobox
plot and then if we're going to back out
and we want a quick look at um what they
call pair plotting uh we can run that
and you can see with the caborn it just
does all the work for you uh it takes it
just a moment for it to pull the data in
and compile it
and once it does it creates a nice grid
um and this grid if you look at uh this
one space here which is you might not be
able to see the small number says engine
horsepower this is engine horsepower uh
to the year was built and it's just
flipped so everything to the right of
the middle diagonal is just the rotation
of what's on the left and as you expect
um the engine horsepower um gets bigger
and bigger and bigger as time goes on so
the the year it was built further up in
the year the more likely you are to have
a heavy horsepower
engine and you can quickly look at
Trends with our pair plot coming up uh
and look how fast that was that was it
took it a couple you know moment to
process U but right away I get a nice
view of all these different um
information which I can look at visually
in in kind of see how things group and
look now if I was doing a meeting I
probably would show all the data
um one of the things I've learned over
the years is um people myself included
love to show all our work you know we
were taught in school show all your work
prove what you know the CEO doesn't want
to see a huge U grid of of graphs I
guarantee it uh so we want to do is we
want to go ahead and
drop um the stuff that might not be
interested in and we're going to I'm not
really a car person our guy in the back
is obviously so you have your engine
fuel type we're going to drop that we're
going to drop Market category vehicle
style popularity number of doors vehicle
size um and we have the axes in here if
you remember from numpy we have to
include that axes to make it clear what
we're working on that's also true with
pandas and then we'll look at just the
what it looks like um from the head and
you can see that we dropped out those
categories and now we have the make
model year uh and so forth um and we
took out the engine fuel type Market
category Etc
uh and this should look familiar to you
now when you start working with pandas I
just love pandas for this reason look
how easy it is it just displays it as a
nice um uh spreadsheet for you you can
just look at it and view it very easily
uh it's also the same kind of view
you're going to get if you're working in
spark or P spark which is python for
spark across Big Data this is a kind of
thing that they they come up with this
is why pandas is so
powerful and we may look at this and
decide we don't like these columns and
so you can go in here and we can
actually rename the
columns simple command car equals car
rename uh columns equals engine
horsepower equals
horsepower this is just your standard
python
dictionary um so it just Maps them out
and you know instead of having like a
lengthy here we had um engine horsepower
we just want horsepower we don't need to
know it's the engine horsepower engine
cylinders we don't need to know that
it's for the engine cuz there's only one
thing we're describing if we're talking
about cars and that's
cylinders uh we'll go ahead and just run
this and again here's our car head and
you can see how that changed we have
model year and horsepower versus model
year engine horsepower engine cylinders
and just
cylinders again we want to keep reducing
this so it's more and more readable the
more readable you get it the better um
and of course we can also adjust the
size a little bit so that when it prints
out instead of splitting it on two lines
we get like a single line we can do that
also that's just your control mouse up
or plus sign you use in Chrome that's a
chrome
command and if you remember from numpy
we had shape well pandas works the same
way uh we can look at the shape of the
data so we now have um
11,914 rows and 10 columns uh so you see
some similarities because pandas is
built on
numpy and questions that come up just
like you did a numpy we might want to
know duplicate rows and so we can do car
and look at this switch here um we're
doing a selection this is a panda
selection with the brackets but we want
to select it based on car. duplicated so
how many duplicates on
there so it's starting to look a little
bit different as far as how we access
some of the data on here this can be a
logical statement and we get the number
of duplicate rows we have 989 rows by 10
columns again
and this is one of those troubleshooting
things that we end up doing uh a lot
more than we really feel like we should
uh we might go ahead and do like a car
count uh just to see how many rows we're
dealing with and then right after that
we might want to go ahead and say hey um
let's drop duplicates so remember we did
all the duplicates on there so car
equals car. drop duplicates and then we
can print the head again we'll just do
car head here and you can see the data
on there um looks the same as
four uh and just note that we did car
equals car draw duplicates there are
commands in here where you can do where
it changes the actual value and it works
on some of them and not on others
depending on what you're doing but by
default it always returns a copy so when
we do this we're reassigning it to car
and you can see it's the same header but
we want to go ahead and do count and see
how the count changes let's go ahead and
run this and you can see here instead of
11 914 we have
10,925 uh so we've removed about 100
cars that were duplicated just slightly
under 100
there and then as we're prepping our
data we might want to know um car is
null uh so it's going to count the
values of null and then we want to sum
that up and when we do that uh we do the
car is n function. suum uh we end up
with uh HP the horsepower is 6 9 have
null values and 30 have cylinders have
null values now if you don't put the sum
at the end it's just going to return a
mask with the true false of is it null
or is it not by zero and one so you're
summing up the ones underneath each
column and this of course uh then you
have to decide what you're going to do
with the uh null values there's a lot of
different options it might be that you
need to put in the average or means uh
maybe you want to put in the median
value um there's a lot of different ways
to fill it usually when you first start
out with the data a lot of them you just
drop your null values and you can see
here car. drop na which is equal to all
and then we're going to go ahead and
count it and you can see that we've
dropped almost another 100 values so
from
10925 to
10827 maybe 75 or so values uh so we've
clean the this is really a big part of
cleaning dat data you need to know how
to get rid of your null values or at
least count them and what to do with
them and of course if we go back to um
uh counting our null values we should
now have uh null null values there we go
and you'll see there's zero null values
I don't know how many times I've been
running a model that doesn't take null
values and it crashes and I just sit
there and look at it trying to get why
did that crash it should have worked uh
it's because I forgot to remove the null
values
so we've been jumping around a lot we're
going to go back to uh finding outliers
and let's go ahead and bring that back
into our Seaborn and if you remember we
did a box plot earlier uh this time
we're going to do a box plot just on the
price and you can see here um our price
value and we have the deviation with the
two thinner bars on each side of the
main value and then as we get up here we
have all these outliers um in fact we
have one way out here that's um probably
a really expensive high-end car is what
we're looking at
if you were doing um fraud analysis you
would be jumping on all over these
outliers why are these deviation from
the standard what are these people doing
again this is probably like I said a
really high-end expensive car out here
that's what we're looking at and we can
also look at the um box plot for the
horsepower we'll put that in down
here and run that and you can see again
here's our horsepower and it just jumps
and there's these really odd huge muscle
cars out here that are
outliers and we're going to jump into
making this a little bit more um as you
start displaying your data or your
information to your shareholders uh
we're going to look at plotting a
histogram for the number of cars per
brand and the first thing we want to go
ahead and do is we have with our car go
back over here here we go uh we have our
make value counts largest plot um and we
want to do a kind equals bar uh fig size
105 and right off the bat we jump up
here we see Chevrolet it's going against
what was it it's um figure the value
counts and we want the largest value so
here's our value counts and compared to
what the different cars are Chevrolet
puts out a lot of different kinds of
cars I didn't realize that they made
that many cars or different types and
then for readability uh let's go ahead
and add a title number of cars by make
number of cars and make if you had
looked at this the first time you would
have been like well what the heck am I
looking at well we're looking at the
number of cars by make and then you can
see here now we're talking about the
type of cars and the different uh ones
were put out Lotus I guess only had a
few different kinds of cars over there
very high-end
cars and then as a doing data analytics
and as a data scientist one of the
things I am most interested in is the
relationship between the
variables uh so this is always a place
to start we want to know what's going on
with our variables and how they connect
with each other uh so the first thing
we're going to do is we're going to go
ahead and set a figure size because we
want to make sure it fits our graph um
we'll just go ahead and set this one
plot Figure Set to figure size 2010 if
you never used the map plot Library
which is sitting behind Seaborn uh
whatever is in the PLT this is what's
loaded it's like a canvas you're
painting on so the second you load that
uh pip plot as PL l t anything you do to
that is affecting everything on
it uh and then we want to go ahead uh
since we're using
caborn we'll go ahead and create a
variable C for uh relationships or
correspondence and car
docr that's a correlation in Seaborn on
top of pandas again one line and you get
the whole correlation on there and
because we're working with caborn the
let's put it into a nice heat map if
you're not familiar with heat maps that
means we're just using color as part of
our um uh setup so we have a nice
visual and we can see here that the
Seaborn connected to the pandas prints
out a nice chart we'll talk a little bit
about the color here in a second it
prints out a nice chart this is the
chart I look at as a data scientist
these are the numbers I want to look at
uh and we'll just highlight one of them
um here's cylinders versus horsepower
the closer to one the higher the
correlation so 788 pretty high
correlation between the number of
cylinders and how heavy the horsepower
is I'm betting if you looked at the year
versus uh horsepower we just look at
that one here's year and horsepower 314
not as so much but if you combine them
uh you don't actually add them but if
you combine them you'll start to see an
increase in Horsepower per year and
cylinders you could probably get a
correlation there and just like 78 is a
positive correlation
uh you might notice if we look at
cylinders and or let's look at
horsepower and mileage uh so if we go
here to horsepower to mileage you get a
nice um negative we'll do cylinders
that's a bigger number with cylinders to
the miles per gallon it's a minus. 6 so
it's a negative correlation the closer
to minus one the more the negative
correlation
is and then the chart you would actually
show people is a nice heat map this is
all our colors and it's just those
numbers put into a heat map the darker
the color the higher the correlation you
can see straight down the middle um
obviously the year correlates directly
with the year horsepower with horsepower
and so on that's why it's a one the
closer to the one the higher the
correlation between the two pieces of
data now this is a good introduction uh
pandas goes Way Beyond this most the
functionality and numpy since Panda sits
on it is also in pandas and then it even
has additional features in it we use
Seaborn pretty extensively sitting on
top over our pip plot uh so keep in mind
that our pip plot has a ton of other
features in it that we didn't even touch
on in here uh we couldn't even if you
had a sole course in it uh there's just
so many things hidden in there depending
on what your domain you're working on uh
but you can see here here's our Seaborn
and here's our Matt plot Library that's
all our Graphics that we did and then
the Seaborn worked really nicely with
the pandas uh we really like that in the
world of data science python stands out
for its ease of use and Powerful data
analysis capabilities reflecting on the
2023 stack Overflow developer survey
Python's popularity is unmatched
especially among data analyst and
scientist consider the success story of
Ana a data analyst whose expertise in
pythons pandas and numpy libraries led
to the development of a predictive model
that significantly influenced her
startup strategy this example highlight
Python's practical application in
extracting meaningful insights from data
a skill highly valued in the industry as
you gear up for a python data analyst
interview it's crucial to demonstrate
not only your technical Mastery of
python but also your ability to apply
its functionalities to real world
problems this guide focuses on essential
interview questions and answers drawing
from real life applications and
challenges to prepare you for what lies
ahead through understanding practical
use cases like anas you will gain
insights into the impactful role python
plays in datadriven decision making so
we'll start with the first question that
is what is Python and why is it
preferred for data analysis so python is
like a versatile tool that's not only
easy to use but also powerful enough to
tle Big projects it's a programming
language which means it's a way for us
to write instructions that a computer
can understand and execute what makes
Python special is how it combines
Simplicity and power it's as if the
tools in this section of the supermarket
are designed to fit comfortably in your
hand and work exactly as you expect
making your job easier so here I will be
giving you an overview of the answers
you can provide to the interviewer in
the interview so now you might wonder
why do people working with data love
shopping in the python section so much
so let's consider some points for that
that would be easy to learn so python
Simplicity means you can learn how to
use it quickly even if you are never
programmed before it's like learning to
cook using recipes that requires only a
few ingredients then comes versatile
tool set python offers a variety of
tools known as libraries specifically
designed for data analysis imagine
having a Swiss army knife but for data
these tools can help you clean sort
analyze and visualize data all in one
place then we have Community Support so
there's a huge community of people who
use Python for data analysis so if you
ever get stuck or need advice there's a
good chance someone has being in your
shoes and can help it's like having a
group of friends who are always there to
lend a hand and then comes integration
and automation so python makes it easy
to automate repetitive task and
integrate your data analysis work into
larger projects this can save you a ton
of time and effort allowing you to focus
on the fun parts of your project then
comes the in demand skill because so
many businesses rely on data to make
decisions knowing how to analyze data
with python is a valuable skill that can
open doors to exciting job
opportunities so this is how you can
answer the interview about why is python
preferred for data analysis so moving to
the next question that is explain the
difference between a list tle and
dictionary in Python so I will give you
an overview you could use this or just
mold into your language and provide the
answer to the interview so when
discussing the structures that is list
uple and dictionary in Python it's like
look at different ways to organize
information each serves a unique purpose
list are AK to an adjustable shelving
unit you can add remove or change the
items on freely it's ordered meaning you
can refer to each shelf by its position
for example you could see an example
here that is mycore list and we have put
pencil notebook and eraser in the list
so it allows you to manipulate the
contents as needed now coming to the
tles so tles resemble a fixed display
case once you have arranged your items
inside their order and presence are set
you cannot change them this immutability
is useful for ensuring that certain data
remains constant so you could see the
example so I have changed the example so
you could see here that myle has pencil
notebook and eraser so it signifies data
that you don't intend to alter now
coming to dictionaries so they are like
a flying cabinet or the filling cabinet
with labeled folders each folder or key
hold specific items values making it
easy to locate what you need quickly
without concern for the order in which
you edit them so you could see an
example that is myor dictionary that has
pen and it is pointing out for the blue
then book that is pointing out to
mathematics that provides a way to
access items directly by the label or
key now moving to the next item or the
next question that is how do you handle
missing values in a pandas data frame so
when working with data in Python using
pandas it's common to encounter missing
values think of missing values like
puzzle pieces that are lost to complete
the puzzle or analyze your data you need
to decide what to do with these missing
pieces so strategies to handle missing
values the first one will be remove so
if the missing value is like a lost
puzzle piece that isn't crucial you can
simply remove that row or column this is
straightforward but can lead to losing
valuable information if not done
cautiously for example you could just
use DF do drop na so this is how you
could use it now coming to the next
opment that is fill in so if you think
of the missing piece as something you
can guess or approximate you can fill in
the missing value with a guess this
could be the average of the ordered
pieces the most frequently appearing
piece or a placeholder like zero for
example you could use DF do fill Na and
you could pass the value inside it so
the choice between removing and filling
in depends on your specific data set and
and what you are trying to achieve if
maintaining the Integrity of your data
without the missing pieces is crucial
filling in might be the best option if
the presence of incomplete data could
skew your results removing those entries
might be safer so this is how you could
tackle this now moving to the next
question that is what are Lambda
functions in Python and where would you
use them so imagine you are in the
kitchen about to make a quick snack you
could pull out a big food processor for
a small job but sometimes it's easier to
use a simple knife in Python Lambda
functions are like that handy knife a
quick one and a onetime tool for small
tasks so let's understand the Lambda
function Lambda functions are small
Anonymous functions defined within the
Lambda keyword they can have any number
of arguments but only one expression
perfect for when you need a simple
function for a short period you could
see the syntax that is basic form Lambda
arguments colon expression now we'll see
its examples of use number one is
sorting imagine you have a list of names
with the ages and you want to sort them
by age Lambda function makes this simple
so you could see the code here that is
sorted and under we have pass list comma
keyal to Lambda X col X of 1 so this is
how you could do that then comes the
quick calculations so you need to
quickly apply a mathematical operation
like squaring a number without writing a
full function so you could have a
example here so this is how you could
have a square of a number
so when to use Lambda functions so now
we'll see when to use Lambda functions
number one is short one time use that is
ideal for when you need a function once
or in a limited scope and number two
would be Simplicity when the operation
is simple enough to be expressed in a
single line and then comes the inline
operations that is useful for inline
operations that requires a function so
let's move to the next question that is
question number five and the question is
demonstrate how to ose list
comprehension to simplify code that
generates a list of square for all even
numbers from 0 to 10 so imagine you are
tasked with creating a beautiful Garden
Path using Square Stepping Stones
however you only want to use stones that
fit certain criteria such as being even
numbered in size in Python list
comprehensions offer a concise way to
create this path with just the stones
you need so let's understand list
comprehension so list comprehension
provide a sent way to create list they
combine a loop conditionals and the list
operation into one line it's like
selecting specific items based on
criteria to create a new collection and
now we'll see the basic Syntax for that
that is in the form you will write the
expression for item and then the itable
and if condition so let's see an example
so we'll create a list of square for all
even numbers from 0 to 10 so first we'll
do this with the traditional approach
and here we have an array and then we'll
use the for Loop and then if statement
and then we'll append if the number is
even we will Square it now coming to the
list comprehension approach so you could
see the expression here so first is the
condition then the for Loop and then the
if conditioner so this is how the list
comprehension work so now we will
discuss about the key points of list
comprehension so the number one is
efficiency so list comprehensions make
your code more readable and efficient
number two is simplification they
simplify multiple lines of code into a
single readable line and then comes
versatility useful for filtering items
or applying operations two elements and
now coming to the next question that is
sixth one explain the concept of deep
and shallow copy in Python so imagine
you have a magical notebook whatever you
write in it also appears in a link
notebook if you have a shallow copy of
the notebook changes to the content like
adding a new page in one will show up in
the another however if you change
something on a page already existing in
both notebooks only the original
notebooks page is affected a deep copy
is like having two completely
independent magical not notebooks
whatever you do in one notebook whether
adding new page or changing existing
ones doesn't affect the other notebook
at all so let's understand shallow copy
and deep copy so shallow copy creates a
new object but does not create copies of
nested objects found within instead it
just copies the reference to these
objects so changes to nested objects in
the original or the copy will reflect in
the other so you could see an example
for that that is import copy and then we
have the original list and and the
shallow coped list is equal to copy.
copy of the original list now moving to
the Deep copy so it creates a new object
and recursively copies all objects found
within the original object nested
objects included so changes in the
original or the copy do not affect each
other at all so let's see the example
that is deepor copy list equal to copy.
deep copy and inside we have passed
original list now we'll discuss about
the key points so shallow copy that is
good for having memory when creating
copies of large objects where nested
objects aren modified and about the Deep
copy so it used when you need a fully
independent copy of an object including
all nested objects so this was about
deep copy and shallow copy now moving to
the next question that is what is pate
and why is it important in Python
Programming imagine you are playing a
game where everyone agrees on certain
rules to make the game more fun and fair
for everyone in the world of Python
Programming Pate serves a simp SAR
purpose but for writing code Pate stands
for python enhancement proposal 8 and
it's essentially a set of rules and
guidelines for formatting python code so
let's understand Pate so Pate is like
the rule book for python code style it
covers recommendations on how to format
your code such as how to name variables
how to incident your code and other best
practices think of it as the etiquette
for Python Programming ensure that
everyone writes code in a consistent and
readable way and let's discuss why p P
it matters so the number one reason is
readability just like a well organized
book is easier to read code that follows
pepit is easier for humans to understand
and read it's about making your code not
just functional but also presentable and
approachable then comes collaboration
when everyone uses the same style guide
it's much easier to work on projects
together this consistency removes
confusion and helps prevent errors that
arise from misinterpreting the code and
then comes maintenance so code that's
easy to read and understand stand is
also easier to maintain and update if
your code follows Pate anyone including
future you can quickly understand and
modify it as needed so following pet is
like keeping your codes desk clean and
organized it makes your python programs
more enjoyable and easier for you and
others to read and work on just as
playing a game is more fun when everyone
knows and follows the rule coding in
Python is more effective and enjoyable
when everyone ades to pep it so this was
about pepet now moving to the next
question that is describe how to perform
a merge operation between two pandas
data frames so imagine you are at a
dinner party and you have two guest list
one list has the names of guest and
their favorite dishes the other list has
names with their preferred drink choices
to plan your dinner menu you need to
combine these list into one so you think
each guest food and drink preferences
side by side in Python's Panda's Library
this combining of list or tables is
called a merge operation so let's
understand merge in pandas so first
we'll see pandas so it's a powerful
library in Python for data analysis and
manipulation and merge operation is like
joining two tables based on a common
column like guest names in an example so
that you can see all the information in
one place so let's see the steps to
perform a merge number one step is
identify the common key that is find the
column common to both data frames such
as guest name then choose the type of
merge decide how you want to combine
your list do you want to include all
guest only those present on both list or
some other criteria and then use the
merge function apply pandas merge
function to bring your data frames
together based on your criteria you
could see the example here that is first
we will identify the common key and then
we'll choose the type of merge as you
could see like outer joint inner joint
or both the criterias and then we have
used the merge option so this is done
that is to perform a merge operation
between two pandas data frame now now
moving to the next question that is
ninth question and that is can you
explain what nump is and how it is
better than a regular list in Python so
imagine you have a toolbox in this
toolbox you have a regular Hammer that
is Python's list and a power hammer that
is numpy both can drive Nails into a
board that is perform numerical
operations but the power hammer does it
faster and with less effort especially
when you have a load of nails large data
set numpy stands for numerical Python
and it's a library especially designed
for numerical operations it introduces
an object called an ND array that is n
dimensional array that is more efficient
than Python's regular list for a few
reasons we'll see those reasons number
one is performance so numpy arrays are
stored at one continuous place in memory
allowing faster access in comparison to
list so this means operations on numpy
arrays can be done much quicker than on
list then comes functionality numai
comes with a vast library of
mathematical functions that can be
performed on array making it incredibly
powerful for scientific Computing and
then comes memory efficiency a nump AR
is more memory efficient than a list
allowing you to work with larger data
set so this is why numpy is better than
a regular list in Python now going to
the next question that is how to improve
the performance of a slow running python
script so imagine you are in a race with
a backpack full of tools you think you
might need as you run you realize the
backpack is slowing you down so to speed
up you would streamline your backpack
carrying only words essential and
reconized for quick access optimizing a
python script follows a similar
principle so here's how number one is
profile before optimizing first find out
where the heavy items are use profiling
tools to identify the slow parts of your
script it's like checking your backpack
to see what's making it too heavy then
you can use efficient data structures
some data structures are faster than
others for certain tasks it's like
choosing a lightweight compact
multi-tool or carrying separate tools
for each job you could leverage nump for
numerical t task if your script does a
lot of mathematical calculations using
numpy as you discussed earlier you can
significantly speed things up similar to
spping Out of manual tool for power tool
and then you could avoid Global
variables accessing Global variables is
slower than accessing local ones it's
second to having the tools you
frequently use in an easily accessible
pocket rather than in the bottom of a
backpack and you could use buil-in
functions and libraries Python's
build-in functions and standard
libraries are optimized for Speed using
them instead of custom Solutions can be
like using a well paved road instead of
hacking your way through the jungle and
then you could use limit of Loops so
Loops essentially nested ones can slow
down a script where possible use
Python's list comprehensions or numpy's
factorized operations it's like choosing
a path that goes directly to your
destination instead of wandering around
so by focusing on these strategies you
could significantly improve the speed of
your python script making your coding
race more of a Sprint than a marathon
number moving to the 11th question that
is what is the purpose of the group by
function in pandas and provide an
example of its use so let's say you have
a big box of crayons sorted by color you
want to count how many crayons you have
in each color in Python pandas Library
the group by function helps you do
exactly that with your data so the group
by function groups data in a data frame
based on a certain criteria like how you
might group crayons by color so once
group you can apply functions to each
group independently such as counting
them summing of values or calculating
averages so here how it works number one
is group so the first group by that
separates the data into groups based on
your choosen criteria it's like sorting
your crayons into groups of the same
color now coming to the apply so next it
applies a function to each group this
could be calculating the average sum
count Etc if we sticking with the cron
analogy this is like counting how many
crons you have in each color group and
then comes combine finally it combines
the results into a new data frame you
end up with a summary of your original
data organized by the grouping criteria
so it's like having a neat list that
tells the count of each color of GRS so
now moving to the next question that
would be 12th so how can you create
visualizations using matte PL LI or
seone provide an example so imagine you
have just taken a beautiful hike and you
want to share the experience with your
friends you could describe every detail
in words or you could show them a photo
that captures the essense of the
adventure in data analysis M plot
and the camera and filters for data
allowing you to create visual
representations that capture the
insights from your data set so talking
about matlo Li that is the camera mlo Li
is a fling library in Python that allows
you to create a wide range of static
animated and interactive visualizations
think of it as your camera for data
visualization giving you the tools to
create basic graphs like line charts bar
charts and Scatter fls and then comes
the cbone but before that you could see
the example here that is import M Pro
lab splt then we have X and Y and they
have the numbers there and then you
could float the graph there and then
comes the cbone the filters so cbone is
built on top of matal lip and provides a
high level interface for drawing
attractive and informative statistical
Graphics so cbone is like adding filters
to your photos making your visualization
more aesthetically pleasing and easier
to interpret with less code and you
could see the example here that is
import cbone as SNS and then you could
set the theme and the line Flo now
coming to the next one that is 13th
question so now guys let's understand
overfitting so overfitting occurs when a
machine learning model learns the
details and noise in the training data
to the extent that is performs poorly on
new data it's like memorizing the
answers to a test rather than
understanding the subject and now we'll
see how to prevent overfitting so number
one is use more data so having more data
can help the model learn better and
generalize well it's like learning more
about types of fruit to improve your
overall understanding then comes
simplify the model Sometimes using a
simpler model can prevent overfitting
this is AK to focusing on the basic
characteristics of fruit like color and
shape rather than memorizing every sport
on the apple skin and then comes cross
validation this technique involves
rotating the data set through training
and validation faces it's like
practicing with different sets of root
every time to ensure you really
understand them not just memorize them
and then comes regularization so this
technique adds a penalty on the more
complex features of the model think of
it as discouraging the model from paying
too much attention to the less common
characteristics of the fruit and then
comes early stopping during training if
you notice that your model's performance
on a validation set starts to worsen
stop the training it's like realizing
you are starting to focus too much on
the details that don't matter and
stopping your studying to refocus and in
conclusion or fitting in like studying
too hard on the specifics and missing
the big picture by using strategies like
Gathering more data simplifying your
model and practicing with cross
validation you can help your machine
learning model to learn better and be
more adaptable just like a well-rounded
student who understands the essence of
the subject not just the details now
moving to the next question that is 14th
describe a situation where you had to
clean a large data set and what steps
did you take so imagine you have just
been given a giant old full of various
objects to organize and clean up some
items are valuable and need to be kept
some are duplicates and others are just
plain junk cleaning a large data set in
Python is somewhat similar you're aiming
to tidy up the data so that it's useful
for analysis so we'll see the steps for
cleaning a large data set that is remove
duplicate data that is just like
removing duplicate items in the Attic
that you don't need more than one off in
data sets duplicates can skew your
analysis so for that we'll use the
action
as we have seen the number one step that
is remove duplicate data and we'll see
the action that is we will use data
frame. dropcore duplicates in pandas to
remove them and then we can use another
method that is handle missing values so
imagine finding boxes that are supposed
to contain items but some are empty you
decide whether to fill them label them
as empty or remove them all together for
that we could do the action that is
options include data frame do fill na to
fill Miss missing values or data frame
or Draw P to remove rows or columns with
missing values and then we have correct
data types sometimes objects are stored
in the wrong boxes similarly a numeric
column might be incorrectly formatted as
text so for that we'll use data frame do
as type to correct data types and then
we have standardized the data ensuring
everything is organized and labeled
consistently like making sure all book
titles follow the same format for this
section we will apply function to clean
and standardize text numbers and dates
and we have remove or correct outliers
if you find something completely out of
place like a snowboard in a box of
summer clothes you decide if it's an
error or if it should be stored
somewhere else so the action we can use
is use statistical methods to detect
outliers and decide whether to keep
adjust or remove them and in conclusion
cleaning a large data set involves
removing unnecessary Parts like
duplicates and outliers filling in gaps
that is missing ing values and
organizing correcting data types and
standardizing it's about making sure the
data is accurate consistent and ready
for analysis just as organizing and
cleaning the attic that makes it easier
to find and use what you need now moving
to the next question that is 15th how do
you split a data set into training and
testing sets using psych learn imagine
you are preparing for a magic show
before performing in front of a live
audience you practice your tricks with a
friend to get feedback similarly when
you are working with machine machine
learning models you need to practice or
train your model on a portion of your
data and then test it on another portion
to see how well it performs so this
process helps ensure your model can
perform magic with new unseen data not
just the data it learned from so we'll
see using psyit learn to split data so
psyit learn is like your magic kit for
machine learning in Python it has a tool
called Trainor testore split that makes
dividing your data into training and
testing sets a breeze so we'll see the
steps to split your data set number one
is prepare your data gather your data
set it's like getting all your magic
probes ready for the show then import
the tool bring the Trainor testore split
function into a project and then we have
split the data decide what portion of
your data you want to train on and what
portion to test on the common split is
80% for training and 20% for testing and
you could see the code here and here x
represent your features that is the
information you use to make predictions
and why presents your target what you
are trying to predict and then we have
text size equal to 0.2 that tells the
function to reserve 20% of the data for
testing and in conclusion splitting your
data set into training and testing sets
is a crucial step in the machine
learning process it's like rehearsing
your magic tricks before the Big Show
ensuring your model is well prepared to
amaze with its prediction on the new
data using secondy learns train
underscore testore split this process is
simple and straightforward allowing you
to focus on perfecting your model's
performance that is 16 explain the
difference between supervised and
unsupervised learning with examples so
imagine you are learning to paint in
supervised learning you have an
instructor who gives you a picture to
paint and guides you telling you when
you are putting or getting it right and
where you needs to improve so this is
like having labeled data in machine
learning where you know the correct
answers or labels for your training data
and you're trying to predict those
answers for new data so in supervised
learning however it's like you are given
brush and paints but no pictures to
replicate and no instructions you're
free to explore and create patterns or
structures on your own in machine
learning this means working with data
without predefined labels and the goal
is to identify patterns or groupings
within the data such as clustering
similar customers together so key points
are here are supervised learning you
have label data and the goal is to learn
a mapping from inputs to outputs and
examples include regression and
classification problems so now moving to
to the now key points for unsupervised
learning that is we work with unlabelled
data and the goal is to discover
underlying patterns of structures the
examples include clustering and
dimensionality reduction now moving to
the next question that is 17th question
and what are the main steps in a machine
learning project so embarking on a
machine learning project is like
planning a journey you need a map to
guide you through different terrains and
landmarks and here's how this journey
typically enforce number one is Define
the problem so first figure out your
destination what do you want to achieve
with the machine learning model and this
step is about understanding the problem
you're trying to resolve for second
Point gather and prepare data so next
back your packpack you need data for
your journey collects data for your
problem and then clean and pre-process
it to make it useful for training a
model then choose a model now choose a
model of Transportation different models
like decision trees neural network Etc
have their strength and are suited for
different types of problems and then
train the model time to hit the road
training a model is like teaching it the
rules of the road you will feel create
the data you prepared earlier allowing
it to learn from it and then evaluate
the model check your map and Compass how
well is your model performing use
separate testing data to evaluate its
accuracy and make sure it's learning
correctly and then fine-tune the model
so find the best route based on the
evaluation adjust your model's
parameters or approach to improve its
performance and then deploy the model
you have reached your destination once
your model is trained and fine tuned you
can deploy it to start making
predictions or insight on new data and
then and then we can monitor and update
that is keep an eye on the road
conditions over time your model might
need adjustment or retraining as it
encounters new data or as the underlying
data patterns change so this is about
the main steps in a machine learning
projects now moving to the next question
that is how do we evaluate the
performance model so imagine you have
baked a batch of CC case for the first
time to know if they are good you need
people to taste them and give you
feedback similarly after you trained a
machine learning model you need to
evaluate its performance to see how well
it's doing for its job like predicting
outcomes or classifying data so the
steps would be number one is accuracy so
this is like asking out of all the C is
how many were just right in
classification problems accuracy
measures how often the models
predictions are correct and then
confusion metrics imagine a chart
showing the types of feedback to Sweet
just write too blend a confusion metrix
help you understand not just the
successes but but also where and how the
predictions went wrong and then comes
precision and record Precision ask of
all the cookies be labeled at just right
how many truly are just right recoil ask
of all the cookies that are actually
just right how many did we correctly
indentified it's about being correct
when you say it's right and finding all
the right ones and then comes F1 score
sometimes you want a single number to
balance precision and record especially
if you have unsure which one is more
important Tye one score is like an
overall grade for a quick case that
considers both taste and appeal and then
comes Roc curve and Au imagine plotting
a graph showcasing your cookie making
attempts balancing how adventurous you
are with flavors against the mistakes
you make so the ROC curve flotats true
positive rates against false positive
rates and the Au area under the curve
gives you a single measure of how good
your model is across different
thresholds now moving to the next
question that is pride an example of
using SQL in conjunction with python for
data analysis so imagine you are a
detective trying to solve a mystery you
have a huge room full of file cabinets
your database with information of
various cases now you need a smart
efficient way to find exactly what you
are looking for without ranging through
every file by hand here's where python
and SQL join forces to help you out so
the python and SQL the detective Duo so
SQL is like your search tool for query
of the database directly pinpoint the
exact information you need such as
details of a specific case and you know
python that is like your assistant
detective that helps you to analyze the
information draw insights and present
them in a way that's easy to understand
so now we'll see a simple scenario you
are interested in finding out the number
of cases solved each month to see when
your team was the most effective so use
SQL to retrieve data so you write a SQL
query to extract the number of cases
solved by month from your database so
you could see how we have written here
that is select month and count as case
sold and from Case Files we status is
equal to sold Group by month so now we
will analyze and visualize with python
so you use Python to connect to the
database run your SQL query and then use
libraries like pandas for analysis and M
PL LI or cbone for visualization so you
could see here that we have imported
pandas and met Li and then we have
connected the SQL here and then we have
run the SQL query and load into a data
frame and then we have ploted the graph
so so you could see the example how we
have used SQL in conjunction with python
for data analysis now moving to the next
question that is 20th question and how
do you handle error handling in Python
and can you explain the use of try
accept and finally blocks so let's dive
in imine you're walking through a forest
on a path you think you know well
suddenly you find a tree that has fallen
across your path and you need a plan to
deal with the unexpected obstacle in
Python situation this is like running
into an error in your code so the try
except Captain finally blocks help you
navigate these unexpected areas
gracefully so the tri block this is
where you walk along your planed path
you place the code that you think might
cause an error inside the tri Block it's
like saying I will try to go this way
but then might be a tree in my path and
then we are coming to the accept block
so if you encounter fallen tree and
error the accept block is your alternate
route this blow catches the error and
lets you handle it preventing the
program from crashing you can specify by
different types of trees to look out for
and different detours so you could see
the example here that is try try working
the path except oh no a tree take a
detour so you'll take a detour from here
and then we have the finally block
whether you encounter a tree or node the
finally block is like reaching the end
of your walk and taking a moment to
reflect so this block runs no matter
what even if there was an error it's
often used for clean up actions like
closing a file or releasing resources so
you could see the code here that is
finally relax at the end thank you for
joining the data analytics full course
by simply learn by completing this
course you have taken a significant step
towards rewarding career in analytics
with the high demand for dat exports in
2024 you're now well prepared to learn a
top job in the field and enjoy a
lucrative
salary staying ahead in your career
requires continuous learning and
upscaling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in cuttingedge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
the path to Career Success click the
link in the description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click here