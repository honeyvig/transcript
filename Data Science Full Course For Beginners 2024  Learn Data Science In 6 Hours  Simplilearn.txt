hey everyone welcome to Simply lears
YouTube channel in this session we will
learn the data science full goes craving
a career upgrade subscribe like and
comment
below dive into the link in the
description to FastTrack your Ambitions
whether you're making a switch or aiming
higher simply learn has your best
back but before we begin if you are an
aspiring data scientist who's looking
out for online training and
certification in data science from the
best universities and Industry experts
then search no more simply learns
postgraduate program in data science
from Caltech University in collaboration
with IBM should be the right choice for
more details on this program please use
the link in the description box below
now with that having said back to the
prpd now we will get started by having a
brief introduction to data science
followed by that we will understand what
is artificial intelligence machine
learning and deep learning followed by
that we will understand the thinnest
lines of differences between the
artificial intelligence machine learning
deep learning and data science followed
by that we will have the basics of
python for data science then we will
understand the data analyst data
scientist data engineer J profiles and
what are the differences between them
then we will get started with data
science life cycle moving ahead we will
have statistics for data science types
of distribution in statistics base
theorem then we will have the brief
understanding of artificial intelligence
through the artificial intelligence
quick tutorial then we have the machine
learning introduction followed by that
we will have a brief understanding of
the different types of machine learning
then we will learn supervised and
unsupervised machine learning and the
differences between them next we have
the reinforced machine learning and
followed by that we will understand the
differences between supervised machine
learning unsupervised machine learning
and reinforced learning then we have the
regression analysis linear regression
logistic regression linear versus
logistic regression classification of
machine learning and decision Tree in
machine learning moving ahead we will
understand random for best algorithm K
means clustering algorithm knif based
classifier and once we are done with all
these algorithms we will start off by
Deep learning then we will learn the
natural language processing and the
important python libraries for data
science followed by that we will have a
brief understanding of what exactly is
data visualization and one of the most
important data visualization tool the T
once we have a better understanding of
the fundamentals we will start off by
learning how to build a data science
engineer resume so once we are good with
the resume building then we have the
most important aspect of this particular
fill course which is the most frequently
Asked data science interview questions
by the most biggest and top industries
in the company and these interview
questions should keep you on the safest
side to bag your dream job opportunity I
hope I made myself clear with the agenda
with that let me tell you guys that we
have daily updates on multiple
Technologies if you're a tech in a
continuous hunt for latest technological
Trends then consider getting subscribed
to our YouTube channel and don't forget
to hit that Bell icon to never miss an
update from Simply Lear now over to our
training experts do you know how Netflix
decides which movies suit your taste
better or how Google Maps can provide
roots with low traffic at given moment
in seconds this is all possible thanks
to data science the popularity of data
science exploded in the 2010 causing an
approximate 15% year on-ear growth in
the job market and is expected to grow
even higher in the coming decade now
what exactly is data science data
science is the field of study that works
with massive amounts of data utilizing
relevant tools and techniques to derive
valuable data the day-to-day work of a
data scientist involves collecting
analyzing and interpreting this data to
help businesses achieve their goals
companies across various Industries
generate vast amounts of data so its
application range all the way from
education Healthcare and entertainment
to finance and marketing when it comes
to the subfields of data science it
comprises many Technologies like
artificial intelligence machine learning
natural language processing and deep
learning each of them fulfilling
different Purp purposes and having
varied functionalities the role of a
data scientist was voted the sexiest job
of the 21st century and it is for a good
reason the demand for professionals who
can make sense of vast amount of data is
more than ever in this day and age
companies like Microsoft Google and
Amazon hire data scientists as they deal
with problem solving models involving
massive amounts of data daily this is
why data science professionals are in
high demand with average salaries going
as high as
$1,864 per an with 2.5 million terabytes
of data being transferred over the
internet on a daily basis the data
science Industry is related to grow even
higher promoting further Innovation and
job
opportunities Google had gathered five
exabytes of data between the beginning
of time 2003 this amount of data data
started to be produced every 2 Days in
2010 and every 40 minutes by 2021 the
responsibility of a data scientist is to
gather clean and present data and have a
keen business sense and analytical
abilities let us have a discussion about
it in the upcoming slides how can you
become a data scientist as a beginner I
guarantee you after watching this video
you will have a clear understanding on
how to drive your career as a data
scientist hey guys welcome to Simply
learn before proceeding please make sure
you subscribe to Simply learn's YouTube
channel and press the Bell icon to never
miss any updates today we are going to
cover significance of data scientist in
Industries after that prerequisites and
Technologies required for a data
scientist and finally salary of a data
scientist I have a query for you which
technology is used by Google Maps to
predict traffic jams deep learning
machine learning natural language
processing data structure please leave
the answer in the comment section below
and stay tuned to get the answer
significance now we will see how top
industries are involved in the field of
data science by 2025 the data science
Industry is anticipated to grow to a
value of $16 billion there is an
abundance of data science jobs all over
the world now let's list out crucial
areas where data science is used media
and entertainment the major player in
the media and entertainment sector such
as YouTube Netflix hotstar Etc have
begun to use data science to better
understand their audience and provide
them with recommendations that are both
relevant and personalized e-commerce
data science has aided retail companies
in better meeting their expectations as
they bring a unique combination of deep
data knowledge technology skills and
statistical experience data scientists
are in high demand in the retail
industry top recruiters are are Amazon
flip cart Walmart mintra Etc digital
marketing large volumes of data are
currently being fetched from its users
through search Pages social networks
online traffic display networks movies
web pages Etc a high level of business
intelligence is needed to analyze such
large amount of data and this can only
be done with the proper use of data
science approaches top recruiters are
Amazon Flipkart Facebook Google Etc
cyber security data science and AI are
now being used by the cyber security
industry to prevent the growing usage of
algorithms for harmful purposes top
recruiters includes IBM Microsoft
Accenture Cisco and many more before
moving forward what is the response to
the Google Map question that I asked
answer is machine learning coming to
prerequisites now that we know
significance of data science in industry
let us explore the prerequisites and
Technologies required for a data
scientist seeing the demand of data
scientist in every industry it is
obvious that the scope of a data
scientist is very high so how to start
there is no necessity that you should be
knowing any technology or programming
language you can be a Layman too data
scientists typically have a variety of
educational and professional experiences
most should be proficient in four
crucial areas important skill is
mathematical expertise three concepts
like linear algebra multivariable
calculus and optimization technique are
crucial because they aid in our
understanding of numerous machine
learning algorithms that are crucial to
data science similar to that knowing
statistics is crucial because they are
used in data analysis additionally
important to statistics probability is
regarded as a must for mastering machine
learning next is computer science in the
field of computer science there is a lot
to learn but one of the key inquiries
that arises in relation to programming
is r or Python language there are many
factors to consider when deciding which
language to choose for data science
because both have a comprehensive
collection of libraries to implement
complex machine learning algorithms in
addition to learning a programming
language you should learn the following
computer science skill fundamentals of
algorithm and data structures
distributed computing machine learning
deep learning Linux SQL mongodb Etc
domain expertise most individuals
wrongly believe that domain expertise is
not crucial to data science yet it is
consider the following scenario if you
are interested in working as a data
scientist in the banking Industries and
you already know a lot about it for
instance you are knowledgeable about
stock trading Finance Etc this will be
very advantageous for you and the bank
itself will favor you over other
replicant and finally communication
skill it covers both spoken and written
Communication in a data science project
the project must be explained to others
when finding from the analysis have been
reached this can occasionally be a
report that you provide to your team or
employer at work sometimes it might be a
blog entry it is frequently a
presentation to a group of co-workers
regardless a data science project always
involves some form of communication of
the project findings therefore having a
good communication skill is a
requirement for being a data scientist
apart from all this practicing is very
important keep using different tools
also start reading blocks on data
science start building projects on data
science which can be added to your
resume also you can find many
interesting courses on data science by
simply learn salary reward is the result
of good work now we shall discuss
salaries that a data scientist will get
it should come as no surprise that that
data scientist may add significantly to
a business every step of the process
from data processing to data cleansing
requires persistence a lot of athematic
and statistics as well as scattering of
engineering skills one of the most
important factors in a data scientist
salary is experience at the beginner
level a data scientist can make
$95,000 annually the typical annual
compensation for a mid-level data
scientist is between 130 ,000 and
$195,000 a seasoned data scientist
typically earns between
$165,000 and
$250,000 per year in India at the
beginner level a data scientist can make
9 lakh 40,000 rupees on average per year
at mid level data scientist will get 20
lakhs rupees per anom and if you are at
the advanced level you will get paid an
average of rupees 25 lakhs annually this
salary will vary in different countries
the top hiring businesses in the US that
provide the highest salaries for data
scientist are apple with
$180,000 perom next is Twitter with $170
perom meta technology
$170 annually LinkedIn
$160,000 per anom F Pro technology
provides 150,000 perom IBM provides 14
lakhs per anom and accenter will provide
you with 19 lakhs perom and finally
American Express will provide on average
of 13 lakhs per an if you are an
aspiring data scientist who's looking
out for online training and
certification in data science from the
best universities and Industry experts
then search no more simply learns
postgraduate program in data science
from Caltech University in collaboration
with IBM should be the right choice for
details on this program please use the
link in the description box below deep
learning deep learning was first
introduced in the 1940s deep learning
did not develop suddenly it developed
slowly and steadily over seven decades
many thesis and discoveries were made on
deep learning from the 1940s to 2000
thanks to companies like Facebook and
Google the term deep learning has gained
popularity and may give the perception
that it is a relatively new concept deep
learning can be considered as a type of
machine learning and artificial
intelligence or AI that imitates how
humans gain certain types of knowledge
deep learning includes statistics and
predictive modeling deep learning makes
processes quicker and simpler which is
advantageous to data scientists to
gather analyze and interpret massive
amounts of data having the fundamentals
discussed let's move into the different
types of deep learning neural networks
are the main component of deep learning
but neural networks comprise three main
types which contain artificial neural
networks orn convolution neural networks
or CNN and recurrent neural networks or
RNN artificial neural networks are
inspired biologically by the animal
brain convolutional neural networks
surpass other neural networks when given
inputs such as images Voice or audio it
analyzes images by processing data
recurrent neural net networks uses
sequential data or series of data
convolutional neural networks and
recurrent neural networks are used in
natural language processes speech
recognition image recognition and many
more machine learning the evolution of
ml started with the mathematical
modeling of neural networks that served
as the basis for the invention of
machine learning in 1943 neuroscientist
Warren mccullock and logician Walter
pittz attempted to quantitatively map
out how humans make decisions and carry
out thinking processes therefore the
term machine learning is not new machine
learning is a branch of artificial
intelligence and computer science that
uses data and algorithms to imitate how
humans learn gradually increasing the
systems accuracy there are three types
of machine learning which include
supervised learning what is supervised
learning well here machines are trained
using label data machines predict output
bed based on this data now coming to
unsupervised learning models are not
supervised using a training data set it
is comparable to the learning process
that occurs in the human brain while
learning something new and the third
type of machine learning is
reinforcement learning here the agent
learns from feedback it learns to behave
in a given environment based on actions
and the result of the action this
feature can be observed in robotics now
coming to the evolution of AI the
potenti of artificial intelligence
wasn't explored until the 1950s although
the idea has been known for centuries
the term artificial intelligence has
been around for a decade still it wasn't
until British polymath Alan Turing posed
the question of why machines couldn't
use knowledge like humans do to solve
problems and make decisions we can
Define artificial intelligence as a
technique of turning a computer-based
robot to work and act like humans now
let's have a glance at the types of
artificial intelligence weak AI performs
only specific tasks like Apple Siri
Google assistant and Amazon's Alexa you
might have used all of these
Technologies but the types I am
mentioning after this are under
experiment General AI can also be
addressed as artificial general
intelligence it is equivalent to human
intelligence hence an AGI system is
capable of carrying out any task that a
human can strong AI aspires to build
machines that are indistinguishable from
the human mind both General and strong
AI are hypothetical right now rigorous
research is going on on this matter
there are many branches of artificial
intelligence which include machine
learning deep learning natural language
processing robotics expert systems fuzzy
logic therefore the correct answer for
which is not a branch of artificial
intelligence is option a data analysis
now that we have covered deep learning
machine learning and artificial
intelligence the final topic is data
science Concepts like deep learning
machine learning and artificial
intelligence can be considered a subset
of data science let us cover the
evolution of data science the phrase
data science was coined in the early
1960s to characterize a new profession
that would enable the comprehension and
Analysis of the massive volumes of data
being gathered at the time since its
Beginnings data science has expanded to
incorporate ideas and methods from other
fields including artificial intelligence
machine learning deep learning and so
forth data science can be defined as the
domain of study that handles vast
volumes of data using modern tools and
techniques to find unseen patterns
derive meaningful information and make
business decisions therefore data
science comprises machine learning
artificial intelligence and deep
learning there are a lot of areas where
data science can be used one of the very
common one is fraud detection or fraud
prevention there are a lot of fraudulent
activities or transactions primarily on
the Internet it's very easy to commit
fraud and therefore we can use data
science to either prevent or detect
fraud there are certain algorithms
machine learning algorithms that can be
used like for example some outlier
techniques clustering techniques that
can be used to detect fraud and prevent
fraud as well so who is a data scientist
rather it is actually a very generic
role that defines somebody who is
working with data is known as a data
scientist but there can be very specific
activities and the roles can be actually
much more specific what exactly a person
does within the area of data science can
be much more specific but broadly
anybody working in the area of data
science is known as a data scientist so
what does a data scientist do these are
some of the activities
data acquisition data preparation data
mining data modeling and then model
maintenance we will talk about each of
these in a great detail but at a very
high level the first step obviously is
to get the raw data which is known as
data acquisition it can be all kinds of
format and could be multiple sources but
obviously that raw data cannot be used
as it is for performing data mining
activities or data modeling activities
so the data has to be cleaned and
prepared for using in the data models or
in the data mining activity so that is
the data preparation then we actually do
the data mining which can also include
some exploratory activities and then if
we have to do stuff like machine
learning then you need to build a
machine learning model and test the
model get insights out of it and then if
um the model is fine you deploy it and
then you need to maintain the model
because over a period of time it is
possible that you need to tweak the
model because of change in the process
or change in the dat and so on so that
all comes under the model maintenance so
let's take deeper look at each of these
activities let's start with data
acquisition so in the stage of data
acquisition basically the data scientist
will collect raw data from all possible
sources so this could be typically an
rdbms which is a relational database or
it can also be a non rdbms or could be
flat files or unstructured data and so
on so we need to bring all that data
from different sources if required we
need to do some kind of homogeneous
formatting so that it all fits into in
in know looks at least format from a
format perspective it looks homogeneous
so that may be requiring some kind of
transformation very often this is loaded
into what is known as data warehouse so
this can also be sometimes referred to
as ETL or extract transform and load so
a data warehouse is like a common place
where data from different sources proc
is brought together so that people can
perform data science activities like
reporting or data mining or the
statistical analysis and so on so data
from various sources is put in a
centralized place which is known as a
data warehouse so that is also known as
ETL and in order to do this there can be
the data scientists can take help of
some ETL tools there are some existing
tools that a data scientist can take
help of like for example data stage or
Talent OR inform matica these are pretty
good tools for performing these ETL
activities and getting the data the next
stage now that you have the raw data
into a data warehouse you still probably
are not in a position to straight away
use this data for performing the data
mining activities so that is where data
preparation comes into play and there
are multiple reasons for that one of
them could be the data is dirty there
are some missing values and so on and so
forth so a lot of time is actually spent
in this particular stage so a data
scientist spends a lot of time almost 60
to 70% of the time in this part of the
project or the process which is data
preparation so there are again within
this there can be multiple sub
activities starting from let's say data
cleaning you will probably have missing
values the data there is some columns
the values are missing or the values are
incorrect or there are null values and
so on and so forth so that is basically
the data cleaning part of it then you
need to perform certain Transformations
like for example normalizing the data
and so on right so you could probably
have to modify categorical values into
numerical values and so on and so forth
so these are transformational activities
then we may have to handle outliers so
the data could be such that there are a
few values which are Way Beyond the
normal behavior of the data for whatever
reason either people have keyed in wrong
values or for some reason some of the
values are completely out of range so
those are known as outliers so there are
certain ways of handling these outliers
and detecting and handling these
outliers so this is a part of what is
known as exploratory analysis so you
quickly explore the data to find out are
there so and you can use visual tools
like plots and identify what are the
outliers and see how we can get rid of
the outliers and so on then the next
part could be data inte data Integrity
is to validate for example if there are
some primary keys that all the primary
keys are populated there are some
foreign Keys then at least most of the
foreign Keys should be populated and
otherwise when we are trying to query
the data you may get wrong values and so
on so that is the data Integrity part of
it and then we have what is known as
data reduction sometimes we may have
duplicate values we may have columns
that may be duplicated because they
coming from different sources the same
values are there and so on so a lot of
this can be done using what is known as
data reduction and thereby you can
reduce the size of the data drastically
because very often this could be rant
data which can be removed and so on so
let's take a look at what are the
various techniques that are used for
data cleaning so we need to ensure that
the data is valid and it is consistent
and uniform and accurate so these are
the various parameters that we need to
ensure as as a part of the data cleaning
process now what are the techniques that
that are used for data cleaning or so we
will see what each of these are in this
particular case and uh so what is the
data set that we have we have a data
about a bank and it's customer details
so let's take an example and see how we
go about cleaning the data and in this
particular example we're assuming we are
using python so let's assume we loaded
this data which is the raw file. CSV
this is how the customer data looks like
and um we will see for example we take a
closer look at the geography column we
will see that there are quite a few
blank spaces so how do we go about when
we have some blank spaces or if it is a
string value then we put a empty string
here or we just use a space or empty
string if they are numerical values then
we we need to come up with a strategy uh
for example we put the mean value so
wherever it is missing we find the mean
for that particular column so in this
case let's assume we have credit score
and we see that quite a few of these
values are missing so what do we do here
we find the mean for this column for all
the existing values and we found that
the mean is equal to
6386 so we kind of write a piece of code
to replace wherever there are blank
values NN is basically like null and uh
we just go ahead and say fill it with
the mean value so this is the piece of
code we are writing to fill it so all
the blanks or all the null values get
replaced with the mean value now one of
the reasons for doing this is that very
often if you have some such situation
many of your statistical functions may
not even work so that's the reason you
need to fill up these values or either
get rid of these records or fill up
these values with something meaningful
so this is one mechanism which is
basically using a mean there are a few
others as we move forward we can see
what are the other ways for example we
can also say that any missing value in a
particular row if even one column the
value is missing you just drop that
particular row or delete all rows where
even a single column has missing values
so that is one way of dealing now the
problem here can be that if a lot of
data has let's say one or two columns
missing and and uh we Dro many such rows
then overall you may lose out on let's
say 60% of the data as some value or the
other missing 60% of the rows then it
may not be a good idea to delete all the
rows like in that manner because then
you're losing pretty much 60% of your
data Therefore your analysis won't be
accurate but if it is only 5 or 10% then
this will work another way is only to
drop values where or rather drop rows
where all the column colums are empty
which makes sense because that means
that record is of really no use because
it has no information in it so there can
be some situations like that so we can
provide a condition saying that drop the
records where all the columns are blank
or not applicable we can also specify
some kind of a threshold let's say you
have 10 or 20 columns in a row you can
specify that maybe five columns are
blank or null then you drop that record
so again we need to take care that such
a condition such a situation the amount
of data that has been removed or
excluded is not large if it is like
maybe 5% maximum 10% then it's okay but
by doing this if you're losing out on a
large chunk of data then it may not be a
good idea you need to come up with
something better what else we need to do
next is so the data preparation part is
done so now we get into the data mining
part so what exactly we do in data
mining primarily we come up with ways to
take meaningful decisions so data mining
will give us insights into the data what
is existing there and then we can do
additional stuff like maybe machine
learning and so on to get perform
Advanced analytics and so on so the one
of the first steps we do is what is
known as data Discovery and uh which is
basically like exploratory analysis so
we can use tools like tblo for doing
some of this so let's just take a quick
look at how we go about that so tblo is
excellent data mining or actually more
of a reporting or a bi tool and you can
download a trial version of tblo at
table.com or there is also tblo public
which is free and you can actually use
and play around however if you want to
use it for Enterprise purpose then it is
a commercial software so you need to
purchase license and you can then run
some of the data mining activities say
your data source your data is in some
Excel sheet so you can select the source
as Microsoft Excel or any other format
and the data will be brought into the
tblo environment and then it will show
you what is known as dimensions and
measures so dimensions are all the
descriptive columns so and tblo is
intelligent enough to actually identify
these dimensions and measure so measures
are the numerical value so as you can
see here customer ID gender geography
these are all Dimensions non- numerical
values where as age balance credit score
and so on are numeric values so they
come under measures so you got your data
into Tableau and then you want to let's
say build a small model and you want to
let's say solve a particular problem so
what is the problem statement all right
let's say we want to analyze why
customers are leaving the bank which is
known as exit and we want to analyze and
see if what are some of the factors for
exiting the bank and we want to let's
assume consider these three of them like
let's say gender credit card and
geography these as a criteria and
analyze if these are in any way
impacting or have some bearing on the
customer exiting or the customer exit
Behavior okay so let's um use tblo and
very quickly we will be able to find out
how these uh parameters are affecting
all right so let's see so this is our
customer data so from our Excel sheet we
have data set about let's say 10,000
rows and we want to find out what is the
criteria let's start with genda let's
say we want to first use gender as a
criteria so Tablo really offers an easy
drag and drop kind of a mechanism so
that makes it really really easy to
perform this kind of analysis so what we
need to do is exited says whether the
customer has exited or not so it has a
value of zero and one and then of course
you have gender and so on so we will
take these two and simply drag and drop
okay so exited and then we will put
gender and if we drag and drop into the
analysis side of of TBL Loop all right
so here what we are doing is we are
showing male female as two different
columns here and zero for people who did
not exist and one for people who exited
and that is color coded so the blue
color means people who did not exit and
uh this yellow color means people who
did exit all right so now if we pull the
data here create like bar graphs this is
how it would look uh so what is yellow
let's go back so yellow is uh who exited
and uh for the male only
16.45% have exited and we can also draw
a reference line that will help us or
even provide aliases so these are a lot
of fancy stuff that is um provided by
tblo you can create alosis and U so that
it looks good rather than basic labels
and you can also add a reference line so
if you add a reference line something
like this from here we can make out that
on an average female customers exit more
than the male customers right so that is
what we are seeing here on an average so
we have uh analyzed based on gender we
do see that there is some difference in
the male and female Behavior now let's
take the next criteria which is the
credit card so let's see if having a
credit card has any impact on the
customer exit Behavior so just like
before we drag and drop the credit card
has credit card column if we drag and
drop here and then we will see that
there is pretty much no difference
between people having credit card and
not having credit card 20.8 1% of people
who have no credit card have exited and
similarly 20.18% of people who have
credit card have also exited so the
credit card is not having much of an
impact that's what this piece of
analysis shows last we will basically go
and check how the geography is impacting
so once again we can drag and drop
geography column onto this side and uh
if we see here there are geographies
like I think there are about three
geographies like France Germany and uh
Spain and um we see that there is some
kind of a impact with the geography as
well okay so what we derive from this is
that the credit card is really we can
ignore the credit card variable or
feature from our analysis because that
doesn't have any impact but gender and
geography we can keep and do further
analysis okay all right so what are some
of the advantages of data mining bit
more detailed analysis can help us in
predicting the future Trends and it also
helps in identifying customer Behavior
patterns okay so you can take informed
decisions because the data is telling
you or providing you with some insights
and then you take a decision based on
that if there is any fraudulent activity
data mining will help in quickly
identifying such a fraud as well and of
course it will also help us in
identifying the right algorithm for
performing more Advanced Data Mining
activities like machine learning and so
on all right so the next activity now
that we have the data we have prepared
the data and perform some data mining
activity the next step is model building
let's take a look at model building so
what is a model building if we want to
perform a more detailed data mining
activity like maybe perform some machine
learning then you need to build a model
and how do you build a model first thing
is you need to select which algorithm
you want to use to solve uh the problem
on hand and also what kind of data that
is available and so on and so forth so
you need to make a a choice of the
algorithm and based on that you go ahead
and create a model train the model and
so on now machine learning is kind of at
a very high level classified into
supervised and unsupervised so if we
want to predict a continuous value could
be a price or a temperature or or a
height or a length or things like that
so those are continuous values and if
you want to find some of those then you
use techniques like regression linear
regression simple linear regression
multiple linear regression and so on so
these are the algorithms on the other
hand there will be situ situations or
there may be situations where you need
to perform unsupervised learning case of
unsupervised learning you don't have any
historical labeled data so to learn from
so that is when you use unsupervised
learning and uh some of the algorithms
in unsupervised learning are clustering
K means clustering is the most common
algorithm used in unsupervised learning
and similarly in supervised learning if
you want to perform some activity on
categorical values like for example that
is not measured but it is counted like
you want to classify whether this image
is a cat or a dog whether you want to
classify whether this customer will buy
the product or not or you want to
classify whether this email is Spam or
not spam so these are examples of
categorical values and uh these are
examples of classification then you have
algorithms like logistic regression K
nearest neighbor or KNN and support
Vector machine so these are some of the
algorithms that are used in this case
and similarly in case of unsupervised
learning if you need to perform on
categorical values you have some
algorithms like Association analysis and
hidden Marco model okay so in order to
understand this better let's take an
example and uh take you through the
whole process and then we will also see
how the code can be written to perform
this now let's take our example here
where we want to perform supervised
learning which is basically we want to
do a multi-linear r regression which
means there are multiple independent
variables and then you want to perform a
linear regression to predict certain
value so in this particular example we
have World happiness data so this is a
data about the happiness quotient of
people from various countries and we are
trying to predict and see whether our
how our model will perform so what is
the question that we need to ask first
of all how to describe the data and then
can we make a predictive model to
calculate the happiness score right so
based on this we can then decide on what
algorithm to use and what model to use
and so on so variables that are
available or used in this model this is
a list of variables that are available
there is a happiness rank I'll load the
data and or I'll show you the data in a
little bit so it becomes clear what are
these so there is what is known as a
happiness rank happiness score which is
happiness score is more like absolute
value whereas rank is what is the
ranking and then which country we are
talking about and within that country
which region and what kind of economy
and whether the family which family and
health details and freedom trust
generosity and so on and so forth so
there are multiple variables that are
available to us and uh the specific
details probably are not required and
there can be um in another example the
variables can be completely different so
we don't have to go into the details of
what exactly these variables are but
just enough to understand that we have a
bunch of these variables and now we need
to use either all or some of these
variables and then which we also
sometimes refer to as features and then
we need to build our model and train our
model all right so let's assume we will
use python in order to perform this
analysis or perform this machine
learning activity and I will actually
show you in our lab in in a little bit
this whole thing we will run the Live
code but quickly I will run you through
the slides and then we will go into the
lab so what are we doing here first
thing we need to do is import a bunch of
libraries in Python which are required
to perform our analysis most of these
are for manipulating the data the
preparing the data and then pyit learn
or SK learn is the library which we will
use actually for this particular machine
learning activity which is linear
regression so we have numpy we have
pandas and so on and so forth so all
these libraries are imported and then we
load our data and the data is in the
form of a CSV file and there are
different files for each year so we have
data for 2015 16 and 17 and uh so we
will load this data and then combine
them concatenate them to prepare a
single data frame and uh here we are
making an assumption that you are
familiar with python so it becomes
easier if you are familiar with Python
programming language or at least some
programming language so that you can at
least understand by looking at the code
so we are reading the file each of these
files for each year and this is
basically we creating a list of all the
names of the columns we will be using
later on you will see in the code so we
have loaded 2015 then
2016 and then also 2017 so we have
created um three data frames and then we
concatenate all these three data frames
this is what we are doing here then we
identify which of these columns are
required which for example some of the
categorical values do we really need we
probably don't then we drop those
columns so that we don't unnecessarily
use all the columns and make the
computation complicated we can then
create some plots using plotly library
and it has some powerful features
including creation or creation of maps
and so on just to understand the pattern
the happiness quotient or how the
happiness is across all the countries so
it's a nice visualization we can see
each of these countries how they are in
terms of their happiness score this is
the legend here so the lighter colored
countries have lower ranking and so
these are the lower ranking ones and
these are higher ranking which means
that the ones with this dark colors are
the happiest ones so as you can see here
Australia and maybe this side uh us and
so on are the happiest ones okay the
other thing that we need to do is the
correlation between the happiness score
and happiness rank we can find a
correlation using a scatter plot and we
find that yes they are kind of inversely
proportion which is obvious so if the
score is high happiness score is high
then they are ranked number one for
example highest is scored as number one
so that's the idea behind this so the
happiness score given here and the
happiness rank is actually given here so
they are inversely proportional because
the higher the score the the absolute
value of the rank will be lower right
number one has the highest value of the
score and so on so they are inversely
correlated but there is a strong what
this graph shows is that there is a
strong correlation between happiness
Rank and happiness score and then we do
some more plots to visualize this we
determine that probably Rank and score
are pretty much conveying the same
message so we don't need both of them so
we will kind of drop one of them and uh
that is what we are doing here so we
drop the happiness Rank and similarly so
this is one example of how we can remove
some columns which are not adding value
so we will see in the code as well how
that works moving on this is a
correlation between pretty much each of
the columns with the other columns so
this is a correlation you can plot using
plot function and uh we will see here
that for example happiness score and
happiness score are correlated strongest
correlation right because every variable
will be highly correlated to itself so
that's the reason so the darker the
color is the higher the correlation and
as so the and correlation in numerical
terms goes from 0 to one so one is the
highest value and it can only be between
0 and one correlation between two
variables can be only have a value
between 0 and one so the numerical value
can go from 0 to one and one here is
dark color and um zero is kind of dark
but it is blue color from Red it goes
down the dark blue color indicates
pretty much no correlation so the from
this heat map we see that happiness and
economy and family are probably also
heal probably are the most correlated
and then it keeps decreasing after
Freedom kind of keeps decreasing and
coming to pretty much uh zero all right
so that is a correlation graph and then
we can probably use this to find out
which are the columns that need to be
dropped which do not have very high
correlation and uh we take only those
columns that we will need so this is the
code for dropping some of the columns
once we have prepared the data when we
have the required columns then we use
psyit learn to actually split the data
first of all this is a normal machine
learning process you need to split the
data into training and test data set in
this case we are splitting into 8020 so
80 is the training data set and 20 is
the test data set so that's what we are
doing here so we use uh train test split
method or function so you have all your
training data uh in xor train the labels
in Yore train similarly xor test has the
test data the inputs whereas the labels
are in Yore test so that's how and this
value whether it is 8020 or 50/50 that
is all individual preference so in our
case we are using 8020 all right and uh
then the next is to create a linear
regression instance so this is what we
are doing we're creating an instance of
linear regression and then we train the
model using the fit function and uh we
are passing X and Y which is the x value
and the label data regular input and the
lab data label information then we do
the test We Run The or we perform the
evaluation on the test data set so this
is what we are doing with the test data
set and then we will evaluate how
accurate the model is and using the
psyched learn functionality itself we
can also see what are the various
parameters and what are the various
coefficients because in linear
regression you will get like a equation
of like a straight line Y is equal to
Beta 0 plus beta 1 X1 Plus beta 2 X2
those beta 1 beta 2 Beta 3 are known as
the coefficients and beta 0 is The
Intercept after the training you can
actually get these information of the
model what is The Intercept value what
are the coefficients and so on by using
these functions so let's take quickly go
into the lab and take a look at our code
okay so this is my lab this is my
Jupiter notebook where the code I have
the actual code and and I will take you
through this code to run this linear
regression on the world happiness data
so we will import a bunch of libraries
numpy pandas plot plotly and so on also
yeah psychic learn that's also very
important so that's the first step then
I will import my data and uh the data is
in three parts there are three files one
for each year 2015 2016 and 2017 and it
is a CSV file so I've imported my data
let's take a look at the data quickly
glance at data so this is how it looks
we have the country region happiness
Rank and then happiness score there are
some standard errors and then what is
the per capita family and so on so and
then we will keep going we will create a
list of all these column names we will
be using later so for now just I will
run this code no need of major
explanation at this point we know that
some of these columns probably are not
required so you can use this drop
functionality to remove some of the
columns which we don't need like for
example region and standard error will
not be contributing to our model so we
will basically drop those values out
here so we use the drop and then we
created a vector of with these names
column names that's what we are passing
here instead of giving the names of the
columns here we can pass a vector so
that's what we are doing so this will
drop from our data frame it will remove
region and standard error these two
columns then the next step we will read
the data for 2016 and also
2017 and then we will concatenate this
data so let's do that so we have now
data frame called Happiness which is a
concatenation of both all the three
files let's take a quick look at the
data now so most of the unwanted columns
have been removed and you have all the
data in one place for all the three
years and this how the data looks and if
you want to take a a look at the summary
of The Columns you can say describe and
uh you will get this information for
example for each of the columns what is
the count what what is the mean value
standard deviation especially the
numeric values okay not the categorical
values so this is a quick way to see how
the data is and uh initial little bit of
exploratory analysis can be done here so
what is the maximum value what's the
minimum value and so on for each of the
columns all right so then we go ahead
and create some visualizations using
plotly so let us go and build a plot so
if we see here now this is the relation
correlation between happiness Rank and
happiness score this is what we have
seen in the slides as well we can see
that there is a tight correlation
between them only thing is it is inverse
correlation but otherwise they are very
tightly correlated which also says that
they both probably provide the same
information so there is not not much of
value add so we'll go ahead and drop the
happiness rank as well from our columns
so that's what we're doing here and now
we can do the creation of the
correlation heat map let us plot the
correlation heat map to see how each of
these columns is correlated to the
others and we as we have seen in the
slides this is how it looks so happiness
score is very highly correlated so this
is the we have seen in the slide as well
so blue color indicates pretty much zero
or very low correlation deep red color
indicates very high correlation and the
value correlation is a numeric value and
the value goes from 0 to one if the two
items or two features or columns are
highly correlated then they will be as
close to one as possible and two columns
that are not at all correlated will be
as close to zero as possible so that's
how it is for example here happiness
score and happiness score every column
or every feature will be highly
correlated to itself so it is like
between them there will be correlation
value will be one so that's why we see
deep red color but then others are for
example with higher values are economy
and then health and then maybe family
and freedom so these are generosity and
Trust are not very highly correlated to
happiness score so that is uh one quick
exploratory analysis we can do and uh
therefore we can drop the country and
the happiness rank because they also
again don't have any major impact on the
analysis on our analysis so now we have
prepared our data there was no need to
clean the data because the data was
clean but if there were some missing
values and so on as we have discussed in
the slides we would have had to perform
some of the data cleaning activities as
well but in this case the data was clean
all we needed to do was just the
preparation part so we removed some
unwanted columns and we did some
exploratory data analysis now we are
ready to perform the machine learning
activity so we use psyit learn for doing
the machine learning psyit learn is
python library that is available for
performing our uh machine learning once
again we will import some of these
libraries like pandas and numpy and U
also psyit learn first step we will do
is split the data in 2080 format so you
have all the test data which is 20% of
the data is test data and 80% is your
training data so this test size
indicates how much of it is the what is
the size of the test data the remaining
which is point here we are saying 0 2
therefore that means training is8 so
training data is 80% all right so we
have executed that split the data and
now we create an instance of the linear
regression model so LM is our linear
regression model and we pass X and Y the
training data set and call the function
fit so that the model gets trained so
now once that is done training is done
training is completed and now what we we
have to do is we need to predict the
values for the test data so the next
step is using so you see here fit will
basically run the Training Method
predict will actually predict the values
so we are passing the input values which
is the independent variables and we are
asking for the values of the dependent
variable which is which we are capturing
in Yore PR and we use the predict method
here lm. predict so this will give us
all the predicted y values and remember
we already have Yore test has the actual
values which are the labels so that we
can use these two to compare and find
out how much of it is error so that's
what we are doing here we are trying to
find the difference between the
predicted value and the actual value
Yore test is the actual value for the
test data and Yore predict is the
predicted value we just found out the
predicted right so we will run that and
we can do a quick check as to how the
data looks how is the difference so in
some some cases it is positive some
cases it is negative but in most of the
cases I think the difference is very
small this is exponential to the power
of 0 minus 04 and so on so looks like
our model has performed reasonably well
we can now check some of the parameters
of our model like the intercept and the
coefficients so that's what we are doing
here so these are the coefficients of
the various parameters that we are the
coefficients of the various independent
variables okay so these are the values
then we can quickly go ahead and list
them down as well against the
corresponding independent variable so
the coefficients against the
corresponding independent variable so
1.51 is the coefficient for economy
99983 is for family coefficient for
family and health and so on and so forth
right so that's what this is showing now
we can use the functionality readily
available functionality of psychic learn
and then plot that to find some of the
parameters which determine the accuracy
of this model like for example what is
the mean square error and so on so
that's what we are doing here so let's
just go ahead and run this so you can
see here that the root mean square error
is pretty low which is a good sign and
uh which is one of the measures of uh
how well our model is performing we can
do one more quick plot to just see how
the actual values and the predicted
values are looking and once again you
can see that as we have seen from the
root mean square error root square is
very very low that means that the actual
values and the predicted values are
pretty much matching up almost matching
up and this plot also shows the same so
this line is going through the predicted
values and the actual values and the
difference is very very low so again
this is actual data this is one example
where the the accuracy is high and the
predicted values are pretty much
matching with the actual values but in
real life you may find that these values
are slightly more scattered and you may
get the error value can be relatively On
The Higher Side the root me square error
okay so this was a good and quick
example of uh the code to perform data
science activity or a machine learning
or data mining activity in this case we
did what is known as linear regression
so let's go back to our slides and see
what else is there so we saw this these
are the coefficients of each of the
features in our code and uh we have seen
the root mean square error as well and
and uh with we can take just few hundred
countries of certain values and actually
predict to see if how the model is
performing and I think we have done this
as well and in this case as we have seen
pretty much the predicted values and the
actual values are pretty much matching
which means our model is almost 100%
accurate as I mentioned real life it may
not be the case but in this particular
case we have got a pretty good model
which is very good also subsequently we
can assume that this is how the equation
in linear regression the model is
nothing but an equation like Y is equal
to Beta 0 + beta 1 X1 plus beta 2 X2
plus beta 3x3 and so on so this is what
we are showing here so this is our
intercept which is beta 0 and then we
have beta 1 into economy value beta 2
into the family value beta 3 into health
value and so on so that is what is shown
here okay so I think the next step once
we have the results from the data mining
or Mission learning activity the next
step is to communicate these results to
the appropriate stakeholders so that is
what we will see here now so how do we
communicate usually you take these
results and then either prepare a
presentation or put it in a document and
then show them these actionable results
or actionable insights and uh you need
to find out who are your target audience
and uh put all the results in context
and maybe if there was a problem
statement you need to put those results
in the context of the problem statement
what was our initial goal that we wanted
to achieve so that we need to
communicate here based on you remember
we started off with what is the question
and what is the data and so on and then
what is the answer so we we need to put
the results and then what is the
methodology that we have used all that
has to be put and clearly communicated
in business terms so that the people
understand very well from a business
perspective so once the model building
is done once the results are published
and communicated the last part is
maintenance of this model now very often
what can happen is the model may have to
be subsequently updated or modified
because of multiple reasons either the
the data has changed the way the data
comes has changed or the process has
changed or for whatever reason the
accuracy may keep changing once you have
trained the model model the for example
we got a very high accuracy but then
over a period of time there can be
various factors which can cause that so
from time to time we need to check
whether the model is performing well or
not the accuracy needs to be tested once
in a while and if required you may have
to rebuild or retrain the model so you
do the assessment you you see if it
needs any tweaks or changes and then if
it is required you need to probably
retrain the model with the latest data
that you have and then you deploy it you
build the model train it and then you
deploy it so that is like the
maintenance cycle that you may have to
take the model data analyst versus data
engineer versus data scientist which one
to choose this is one of the most
popular questions asked by leers looking
for a career in data and
analytics I'm sure YouTube would have
come across these job roles in the ever
growing data science landscape though
they all deal with data these jobs are
are not the same there are significant
differences between what a data analyst
data engineer and a data scientist does
we will look at these job rules and the
differences in
detail
first let's look at some data analytics
and data science
Trends the analytics and data science
Market is thriving data analytics data
engineering and data science are the key
trends in today's accelerating Market as
per status.com the global bigd data
analytics Market Revenue will grow at a
cagr of 30% with Revenue reaching over
$68 billion US by
2025 according to technavio the
Enterprise data management Market is
expected to increase by
64.8 billion US by
2025 as per markets andm markets.com the
big data market size is projected to
grow from 62.6 billion in
2021 to
$73.49
6 now another report from Research Drive
says that the data science platform
Market is estimated to reach $
224.210 engineers and data scientists
are going to increase in 2022 and over
the coming
years now let's learn the major
differences between data analyst versus
data engineer versus data
scientist so who are
they a data analyst analyzes and
interprets vast volumes of data in order
to extract meaningful information out of
it they find solutions to a business
problem and make critical business
decisions then insights provided by data
analysts are important to companies that
want to understand the needs of their
end
customers we talking about who a data
engineer is a data engineer on the other
hand builds infrastructure and scalable
pipelines to manage the flow of data and
prepare it for
analysis so basically they optimize the
systems that enable data analyst and
data scientists to perform their job
efficiently
data scientists are professionals who
analyze and visualize existing data and
use algorithms to build predictive
models for making future decisions they
also engage with Business Leaders to
understand their needs and present
complex
findings with that let's look at the
primary roles and responsibilities of
these three job
roles data analysts are responsible to
collect clean store and process data the
disc hidden patterns from data by
performing exploratory data analysis and
visualize data by creating charts and
graphs acquiring data from primary and
secondary sources is one of their key
tasks they build reports and dashboards
and also maintain
databases now talking about the roles
and responsibilities of a data engineer
a data engineer performs data
acquisition the design build and test
data as well to develop and maintain
data
architecture data Engineers are tasked
with testing integrating managing and
optimizing data from a variety of
sources so they integrate data into
existing data pipelines prepare data for
modeling and perform various ETL
operations now talking about the roles
and responsibilities of a data
scientist so data scientists develop
machine learning models to identify
Trends in data for making decisions they
develop hypothesis and use the knowledge
of Statistics data visualization and
machine learning to forecast the future
for the business data scientists
visualize data and use storytelling
techniques and also write programs to
automate data collection and
processing now move on to the skills
possessed by data analysts data
engineers and data
scientists to become a data analyst you
need to have good hands-on experience
with writing SQL queries you should have
excellent micros oft Excel skills for
analyzing data data analysts are also
good at programming and they need to
know how to visualize data solve
business problems and possess domain
knowledge data Engineers should have a
solid understanding of SQL mongodb and
programming they need to have a good
command of data architecture scripting
data warehousing and ETL data Engineers
are also good at Hadoop based Analytics
now talking about the skills for a data
scientist so a data scientist should
have experience with programming in
Python and R this should have a very
good understanding of mathematics and
statistics as well data scientists need
to possess analytical thinking and data
visualization skills as
well Machine learning deep learning and
decision making are other critical
skills every data scientist should
have now we look at the salaries of a
data scien a data analyst as well as a
data
engineer so a data analyst in the United
States earns over $770,000
perom while in India a data analyst can
earn nearly 7 lakh 25,000 rupees
perom a data engineer in the United
States can earn over $112,500 per year
and in India you can earn over 9 lakh
rupees
perom talking about the salary of a data
scientist a data scientist in the United
States earns over
$117,000 perom and in India a data
scientist can earn over 11 lakh rupees
perom coming to the final section of
this video we'll look at the top
companies hiring for data analysts data
engineers and data
scientists so we have the first company
is Google then we have Tesla next we
have the e-commerce giant Amazon the
internet giant Facebook or the social
media giant Facebook we have the tech
giant Oracle we also have Verizon and
arbnb so these are some of the top
companies that hire for the three roles
if you are an aspiring data scientist
who's looking out for online training
and certification in data science from
the best universities and Industry
experts then search no more simply
learns post graduate program in data
science from Caltech University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below now let's talk
about the life cycle of a data science
project okay the first step is the
concept study in this step it involves
understanding the business problem
asking questions get a good
understanding of the business model meet
up with all the stakeholders
understand what kind of data is
available and all that is a part of the
first step so here are a few examples we
want to see what are the various
specifications and then what is the end
goal what is the budget is there an
example of this kind of a problem that
has been maybe solved earlier so all
this is a part of the concept study and
another example could be a very specific
one to predict the price of a 1.35 karat
diamond and there may be relevant
information inputs that are available
and we want to predict the price the
next step in this process is data
preparation data Gathering and data
preparation also known as data monging
or sometimes it is also known as data
manipulation so what happens here is the
raw data that is available may not be
usable in its current format for various
reasons so that is why in this step a
data scientist would explore the data he
will take a look at some sample data
maybe pick there are millions of Records
pick a few thousand records and see how
the data is looking are there any gaps
is the structure appropriate to be fed
into the system are there some columns
which are probably not adding value may
not be required for the analysis very
often these are like names of the
customers they will probably not add any
value or much value from an analysis
perspective the structure of the data
Maybe the data is coming from multiple
data sources and the structures may not
be matching what are the other problems
there may be gaps in the data so the
data all the columns all the cells are
not filled if you're talking about
structured data there are several blank
records or blank columns so if you use
that data directly you'll get errors or
you'll get inaccurate results so how do
you either get rid of the data or how do
you fill this gaps with something
meaningful so all that is a part of data
monging or data manipulation so these
are some additional subtopics within
that so data integration is one of them
if there are any conflicts in the data
there may be data may be redundant yeah
data redundancy is another issue there
may be you have let's say data coming
from two different systems and both of
them have customer table for example
customer information so when you merge
them there is a duplication issue so how
do we resolve that so that is one data
transformation as I said there will be
situations where data is coming from
multiple sources and then when we merge
them together they may not be matching
so we need to do some transformations to
make sure everything is similar we may
have to do some data reduction if the
data size is too big you may have to
come up with ways to reduce it
meaningfully without losing information
then data cleanings so there will be
either wrong values or you null values
or there are missing values so how do
you handle all of that a few examples of
very specific stuff so there are missing
values how do you handle missing values
or null values here in this particular
slide we are seeing three types of
issues one is missing value then you
have null value you see the difference
between the two right so in the missing
value there is nothing blank null value
it says null now the system cannot
handle if there are null values
similarly there is improper data so it's
supposed to be numeric value but there
is a string or a non-numeric value so
how do we clean and prepare the data so
that our system can work flawlessly so
there are multiple ways and and there is
no one common way of doing this it can
vary from Project to project it can vary
from what exactly is the problem you're
trying to solve it can vary from data
scientist to data scientist organization
to organization so these are like some
standard practices people come up with
and and of course there will be a lot of
trial and error somebody would have
tried out something and it worked and
it'll continue to use that mechanism so
that's how we need to take care of data
cleaning now what are the various ways
of doing you know if if values are
missing how do you take care of that now
if the data is too large and um only a
few records have some missing values
then it is okay to just get rid of those
entire rows for example so if you have a
million record and out of which 100
records don't have full data so there
are some missing values in about 100
records so it's absolutely fine because
it's a small percentage of the data so
you can get rid of the entire records
which have missing values but that's not
a very common situation very often you
will have multiple or at least you know
large number of data set for example out
of million records you may have 50,000
records which are like having missing
values now that's a significant amount
you cannot get rid of all those records
your analysis will be inaccurate so how
do you handle such situation so there
are again multiple ways of doing it one
is you can probably if a particular
values are missing in a particular
column you can probably take the mean
value for that particular column and
fill all the missing values with the
mean value so that first of all you
don't get errors because of missing
values and second you don't get results
that are way off because these values
are completely different from what is
there so that is one way then a few
other could be either taking the median
value or depending on what kind of data
we are talking about so something
meaningful we will have put in there if
we are doing some machine learning
activity then obviously as a part of
data preparation you need to split the
data into training and test data set the
reason being if you try to test with a
data set which the system has already
seen as a part of training then it will
tend to give reasonably accurate results
because it has already seen that data
and that is not a good measure of the
accuracy of the system so typically you
take the entire data set the input data
set and split it into two parts and
again the ratio can vary from person to
person individual preferences some
people like to split it into 50/50 some
people like it as
6333 and 33.3 is basically 2/3 and 1/3
and some people do it it as 8020 80 for
training and 20 for testing so you split
the data perform the training with the
80% and then use the remaining 20% for
testing all right so that is one more
data preparation activity that needs to
be done before you start analyzing or
applying the data or putting the data
through the model then the next step is
model planning now these models can be
statistical models this could be machine
learning models so you need to decide
what kind of models you're going to use
again it depends on what is a problem
you're trying to solve if it is a
regression problem you need to think of
a regression algorithm and come up with
a regression model so it could be linear
regression or if you're talking about
classification then you need to pick up
an appropriate classification algorithm
like logistic regression or decision
tree or svm and then you need to train
that particular model so that is the
model building or model planning process
and the cleaned up data has to be fed
into the model and apart from cleaning
you may also have to in order to
determine what kind of model you will
use you have to perform some exploratory
data analysis to understand the
relationship between the various
variables and uh see if the data is
appropriate and so on right so that is
the additional preparatory step that
needs to be done so little bit of
details about exploratory data anal
analysis so what exactly is exploratory
data analysis basically to as the name
suggests you're just exploring you just
Reed the data and you're trying to
explore and uh find out what are the
data types and what is the is the data
clean in in each of the columns what is
the maximum minimum value so for example
there are out of the box functionality
available in tools like R so if you just
ask for a summary of the table it will
tell you for each column it will give
some details as to what is the mean
value what is the maximum value and so
on and so forth so this exercise or this
exploratory analysis is to get an
understanding of your data and then you
can take steps to during this process
you find there are a lot of missing
values you need to take steps to fix
those you will also get an idea about
what kind of model to be used and so on
and so forth what are the various
techniques used for exploratory data
analysis typically these would be
visualization techniques like you use
histograms uh you can use plots you can
use cater plots so these are very quick
ways of identifying the patterns or a
few of the trends of the data and so on
and then once your data is ready you
you've decided on the model what kind of
model what kind of algorithm you're
going to use if you're trying to do
machine learning you need to pass your
80% the training data or rather you use
that training data to train your model
and the training process itself is
iterative so so the training process you
may have to perform multiple times and
once the training is done and you feel
it is giving good accuracy then you move
on to test so you take the remaining 20%
of the data remember we split the data
into training and test so the test data
is now used to check the accuracy or how
well our model is performing and if if
there are further issues let's say and
model is still during testing of the
accuracy is not good then you may want
to retrain your model or use a different
model so this whole thing again can be
iterative but if the test process is
passed or if the model passes the test
then it can go into production and it
will be deployed all right so what are
the various tools that we use for model
planning R is an excellent tool in a lot
of ways whether you're doing regular
statistical analysis or machine learning
or any of these activities are in along
with our studio provides a very powerful
environment to do data analysis
including visualization it has a very
good integrated visualization or plot
mechanism which can be used for doing
exploratory data analysis and then later
on to do analysis detailed analysis and
machine learning and so on and so forth
then of course you can write python
programs python offers a rich library
for performing data analysis and machine
learning and so on mat lab is a very
popular tool as well especially during
education so this is a very easy to
learn tool so matlb is another uh tool
that can be used and then last but not
least SAS SAS is again very powerful it
is uh proprietory tool and it has all
the components that are required to
perform very good statistical analysis
or perform data science so those are the
various tools that would be required for
or that that can be used for model
building and uh so the next step is
model building so we have done the
planning part we said okay what is
algorithm we are going to use what kind
of model we going to use now we need to
actually train this model or build the
model rather so that it can then be
deployed so what are the various uh ways
or what are the various types of model
building activities so it could be let's
say in this particular example that we
have taken you want to find out the
price of 1.35 karat diamond so this is
let's say a linear regression problem
you have data for various carrots of
diamond and you use that information you
pass it through a linear regression
model or you create a linear regression
model which can then predict your price
for 1.35 karat so this is one example of
model building and then little bit
details of how linear regression works
so linear regression is basically coming
up with a relation between an
independent variable and a dependent
variable so it is pretty much like
coming up with equation of a a straight
line which is the best fit for the given
data so like for example here Y is equal
to mx + C so Y is the dependent variable
and X is the independent variable we
need to determine the values of M and C
for our given data so that is what the
training process of uh this model does
at the end of the training process you
have a certain value of M and c and um
that is used for predicting the values
of any new data that comes all right so
the way it works is we use the training
and the test data set to train the model
and then validate whether the model is
working fine or not using test data and
uh if it is working fine then it is
taken to the next level which is put in
production if not the model has to be
retrained if the accuracy is not good
enough then the model is retrained maybe
with more data or you come up with a
newer model or algorithm and then repeat
that process so it is an iterative
process once the training is completed
training and test then this model is
deployed and we can use this particular
model to determine what is the price of
1.35 5 karat diamond remember that was
our problem statement so now that we
have the best fit for this given data we
have the price of 1.35 karat diamond
which is 10,000 so this is one example
of how this whole process works now how
do we build the model there are multiple
ways you can use Python for example and
use libraries like pandas or numpy to
build the model and implement it this
will be available able as a separate
tutorial a separate video in this
playlist so stay tuned for that moving
on once we have the results the next
step is to communicate this results to
the appropriate stakeholders so it is
basically taking this results and
preparing like a presentation or a
dashboard and communicating these
results to the concerned people so
finishing or getting the results of the
analysis is not the last step but you
need to as a data scientist take this
results and present it to the team that
has given you this problem in the first
place and explain your findings explain
the findings of this exercise and
recommend maybe what steps they need to
take in order to overcome this problem
or solve this problem so that is the
pretty much once that is accepted and
the last step is to operationalize so if
everything is fine your data scientists
presentations are accepted then they put
it into practice and thereby they will
be able to improve or solve the problem
that they stated in step one okay so
quick summary of the life cycle you have
a concept study which is basically
understanding the problem asking the
right questions and trying to see if
there is uh enough data to solve this
problem and then even maybe gather the
data then data preparation the raw data
needs to be manipulated you need to do
data monging so that you have have the
data in a certain proper format to be
used by the model or our analytics
system and then you need to do the model
planning what kind of a model what
algorithm you will use for a given
problem and then the model building so
the exact execution of that model it
happens in step four and you implement
and execute that model and uh put the
data through the analysis in this step
and then you get the results this
results are then communicated packaged
and presented and communicated to the
stakeholders and once that is accepted
that is operationalized so that is the
final let's begin this lesson by
defining the term statistics statistics
is a mathematical science pertaining to
the collection presentation analysis and
interpretation of data it's widely used
to understand the complex problems of
the real world and simplify them to make
well-informed decisions several
statistical principles functions and
algorithms can be used to analyze
primary data build a statistical model
and predict the
outcomes an analysis of any situation
can be done in two ways statistical
analysis or a non-statistical analysis
statistical analysis is the science of
collecting exploring and presenting
large amounts of data to identify the
patterns and Trends statistical analysis
is also called quantitative analysis
non-statistical analysis provides
generic information and includes text
sound still images and moving images
non-statistical analysis is also called
qualitative analysis although both forms
of analysis provide results statistical
analysis gives more insight and a
clearer picture a feature that makes it
vital for
businesses there are two major
categories of Statistics descriptive
statistics and inferential statistics
descriptive statistics helps organize
data and focuses on the main
characteristics of the data it provides
a summary of the data numerically or
graphically numerical measures such as
average mode standard deviation or SD
and correlation are used to describe the
features of a data set suppose you want
to study the height of students in a
classroom in the descriptive statistics
you would record the height of every
person in the classroom and then find
out the maximum height minimum height
height and average height of the
population inferential statistics
generalizes the larger data set and
applies probability Theory to draw a
conclusion it allows you to infer
population parameters based on the
sample statistics and to model
relationships within the data modeling
allows you to develop mathematical
equations which describe the inner
relationships between two or more
variables consider the same example of
calculating the height of students in
the classroom in inferential St
statistics you would categorize height
as tall medium and small and then take
only a small sample from the population
to study the height of students in the
classroom the field of Statistics
touches our lives in many ways from the
daily routines in our homes to the
business of making the greatest cities
run the effective statistics are
everywhere there are various statistical
terms that one should be aware of while
dealing with Statistics population
sample variable quantitative variable
qualitative variable discrete variable
continuous
variable a population is the group from
which data is to be
collected a sample is a subset of a
population a variable is a feature that
is characteristic of any member of the
population differing in quality or
quantity from another member a variable
differing in quantity is called a
quantitative variable for example the
weight of a person number of people in a
car a variable differing in quality is
called a qualitative variable or
attribute for example color the degree
of damage of a car in an
accident a discrete variable is one
which no value can be assumed between
the two given values for example the
number of children in a
family a continuous variable is one in
which any value can be assumed between
the two given values for example the
time taken for a 100 meter run typically
there are four types of statistical
measures used to describe the data they
are measures of frequency measures of
central tendency measures of spread
measures of position let's learn each in
detail frequency of the data indicates
the number of times a particular data
value occurs in the given data set the
measures of frequency are number and
percentage
central tendency indicates whether the
data values tend to accumulate in the
middle of the distribution or toward the
end the measures of central tendency are
mean median and
mode spread describes how similar or
varied the set of observed values are
for a particular variable the measures
of spread are standard deviation
variance and quartiles the measure of
spread are also called measures of
dispersion position identifies the exact
location of a particular data value in
the given data set the measures of
position are percentiles quartiles and
standard scores statistical analysis
system or SAS provides a list of
procedures to perform descriptive
statistics they are as follows proc
print proc contents proc means proc
frequency proc univariant proc
gchart proc box plot proc
gplot proc print it prints all the
variables in a SAS data set proc
contents it describes the structure of a
data
set proc means it provides data
summarization tools to compute
descriptive statistics for variables
across all observations and within the
groups of
observations proc frequency it produces
oneway to inway frequency and cross
tabulation tables frequen ques can also
be an output of a SAS data
set proc univariate it goes beyond what
proc means does and is useful in
conducting some basic statistical
analyses and includes highresolution
graphical
features proc gchart the g- chart
procedure produces six types of charts
block charts horizontal vertical bar
charts Pi donut charts and Star Charts
these charts graphically represent the
the value of a statistic calculated for
one or more variables in an input SAS
data set theed variables can be either
numeric or
character proc box plot the box plot
procedure creates side bys side box and
whisker plots of measurements organized
in groups a box and whisker plot
displays the mean quartiles and minimum
and maximum observations for a
group proc gplot gplot procedure creates
two- dimensional graphs including simple
Scatter Plots overlay plots in which
multiple sets of data points are
displayed on one set of axis plots
against the second vertical axis bubble
plots and logarithmic plots in this demo
you'll learn how to use descriptive
statistics to analyze the mean from the
electronic data set let's import the
electronic data set into the SAS console
in the left plane rightclick the
electronic. xlsx data set and click
import
data the code to import the data
generates automatically copy the code
and paste it in the new
window the proc means procedure is used
to analyze the mean of the imported data
set the keyword data identifies the
input data set in this demo the input
data set is
electronic the output obtained is shown
on the
screen note that the number of
observations mean standard deviation and
maximum and minimum values of the
electronic data set are
obtained this concludes the demo on how
to use descriptive statistics to analyze
the mean from the electronic data set so
far you have learned about descriptive
statistics let's now learn about
inferential
statistics hypothesis testing is an
inferential statistical technique to
determine whether there is enough
evidence in a data sample to infer that
a certain condition holds true for the
entire population to understand the
characteristics of the general
population we take a random sample and
analyze the properties of the sample we
then test whether or not the identified
conclusions correctly represent the
population as a whole the population of
hypothesis testing is to choose between
two competing hypothesis about the value
of a population
parameter for example one hypothesis
might claim that the wages of men and
women are equal while the other might
claim that women make more than
men hypothesis testing is formulated in
terms of two hypotheses null hypothesis
which is referred to as H null
alternative hypothesis which is referred
to as
H1 the null hypothesis is assumed to be
true unless there is strong evidence to
the contrary the alternative hypothesis
is assumed to be true when the null
hypothesis is proven false let's
understand the null hypothesis and
alternative hypothesis using a general
example null hypothesis attempts to show
that no variation exists between
variables and alternative hypothesis is
any hypothesis other than the null for
example say a pharmaceutical company has
introduced a medicine in the market for
a particular disease and people have
been using it for a considerable period
of time and it's generally considered
safe if the medicine is proved to be
safe then it is referred to as null
hypothesis to reject null hypothesis we
should prove that the medicine is unsafe
if the null hypothesis is rejected then
the alternative hypothesis is
used before you perform any statistical
tests with variables it's significant to
recognize the nature of the variables
involved based on the nature of the
variables it's classified into four
types they are categorical or nominal
variables ordinal variables interval
variables and ratio
variables nominal variables are ones
which have two or more categories and
it's impossible to order the values
examples of nominal variables include
gender and blood group ordinal variables
have values ordered logically however
the relative distance between two data
values is not not clear examples of
ordinal variables include considering
the size of a coffee cup large medium
and small and considering the ratings of
a product bad good and best interval
variables are similar to ordinal
variables except that the values are
measured in a way where their
differences are meaningful with an
interval scale equal differences between
scale values do have equal quantitative
meaning for this reason an interval
scale provides more quantitative
information than the ordinal scale the
interval scale does not have a true zero
point a true zero point means that a
value of zero on the scale represents
zero quantity of the construct being
assessed examples of interval variables
include the Fahrenheit scale used to
measure temperature and distance between
two compartments in a
train ratio scales are similar to
interval scales in that equal
differences between scale values have
equal quantitative meaning however ratio
scales also have a true zero point which
give them an additional property for
example the system of inches used with a
common ruler is an example of a ratio
scale there is a true zero point because
0 in does in fact indicate a complete
absence of
Link in this demo you'll learn how to
perform the hypothesis testing using
SAS in this example let's check against
the length of certain observations from
a random sample
the keyword data identifies the input
data
set the input statement is used to
declare the Aging variable and cards to
read data into
SAS let's perform a t test to check the
null hypothesis
let's assume that the null hypothesis to
be that the mean days to deliver a
product is 6
days so null hypothesis equals six Alpha
value is the probability of making an
error which is 5% standard and hence
Alpha equals
0.05 the variable statement names the
variable to be used used in the
analysis the output is shown on the
screen note that the P value is greater
than the alpha value which is
0.05 therefore we fail to reject the
null
hypothesis this concludes the demo on
how to perform the hypothesis testing
using
SAS let's now learn about hypothesis
testing procedures there are two types
of hypothesis testing procedures they
are parametric tests and non-parametric
tests in statistical inference or
hypothesis testing the traditional tests
such as test and Anova are called
parametric tests they depend on the
specification of a probability
distribution except for a set of free
parameters in simple words you can say
that if the population information is
known completely by its parameter then
it is called a parametric test if the
population or parameter information is
not known and you are still required to
test the hypothesis of the population
then it's called a nonparametric test
non-parametric tests do not require any
strict distributional assumptions there
are various parametric tests they are as
follows T Test an Nova chai squar linear
regression let's understand them in
detail T Test a t test determines if two
sets of data are significantly different
from each other the T test is used in
the following
situations to test if the mean is
significantly different than a
hypothesized value to test if the mean
for two independent groups is
significantly different to test if the
mean for two dependent or pair comped
groups is significantly
different for example let's say you have
to find out which region spends the
highest amount of money on shopping it's
impractical to ask everyone in the
different regions about their shopping
expenditure in this case you can
calculate the highest shopping
expenditure by collecting sample
observations from each region with the
help of the T Test you can check if the
difference between the regions are
significant or a statistical fluke
Anova Anova is a generalized version of
the T Test and used when the mean of the
interval dependent variable is different
to the categorical independent variable
when we want to check variance between
two or more groups we apply the Anova
test for example let's look at the same
example of the test example now you want
to check how much people in various
regions spend every month on shopping in
this case there are four groups namely
East West North and South with the help
of the Anova test you can check if the
difference between the regions is
significant or a statistical
fluke chai
square chai square is a statistical test
used to compare observed data with data
you would expect to obtain according to
a specific
hypothesis let's understand the high
Square test through an example you have
a data set of male Shoppers and female
Shoppers let's say you need to assess
whether the probability of females
purchasing items of $500 or more is
significantly different from the
probability of males purchasing items of
$500 or more linear regression there are
two types of linear regression simple
linear regression and multiple linear
regression simple linear regression is
used when one wants to test how well a
variable predicts another variable
multiple linear regression allows one to
test how well multiple variables or
independent variables predict a variable
of interest when using multiple linear
regression We additionally assume the
predictor variables are
independent for example finding
relationship between any two variables
say sales and profit is called Simple
linear
regression finding relationship between
any three variables say sales cost
telemarketing is called multiple linear
regression
some of the non-parametric tests are
will coxen rank sum test and crcll
Wallace h test will coxen rank sum test
the W coxen signed rank test is a
non-parametric statistical hypothesis
test used to compare two related samples
or matched samples to assess whether or
not their population mean ranks differ
in W coxen rank some test you can test
the null hypothesis on the basis of the
ranks of the
observations crusco Wallis h test crusco
Wallis h test is a rank-based
non-parametric test used to compare
independent samples of equal or
different sample sizes in this test you
can test the null hypothesis on the
basis of the ranks of the independent
samples the advantages of parametric
tests are as follows provide information
about the population in terms of
parameters and confidence
intervals easier to use in modeling
analyzing and for describing data with
Central Tendencies and data
Transformations Express the relationship
between two or more
variables don't need to convert data
into rank order to
test the disadvantages of parametric
tests are as follows only support
normally distributed data only
applicable on variables not
attributes let's Now list the advantages
and disadvantages of non-parametric
tests the advantages of non-parametric
tests are as follows simple and easy to
understand do not involve population
parameters and sampling Theory make
fewer
assumptions provide results similar to
parametric
procedures the disadvantages of
non-parametric tests are as follows not
as efficient as parametric tests
difficult to perform operations on large
samples manually we'll discuss the type
types of distribution in
statistics but before we move ahead
let's have a brief introduction on what
is probability distribution a
probability distribution is a list of
all of the possible outcomes of a random
variable along with the corresponding
probability
values and it is used in many fields but
we rarely do explain what they are so in
this video we'll discuss the three main
types of probability distribution that
is normal binomial and poison
distribution so let's move ahead
so what is normal
distribution normal distribution is a
continuous probability density that has
a probability density function which
gives us a symmetrical bell curve now
data can be distributed or spread out in
different ways but there are many cases
where the data tends to be around a
central value with no bias to the left
or right which means that it doesn't
show any particular spikes towards the
left or the right and it gets close to a
normal distribution half of the data
will fall on the left of the mean and
the other half will fall on the right
now let's take a look at a graph which
shows the height distribution in a
glass as you can see the average height
is in the middle and the data to the
left of the average height represents
the short people and the data to the
right of it represents the taller
people the Y AIS shows us the likelihood
of any of these Heights occurring the
average height has the most distribution
or it has the most number of cases in
the class and as the height decreases or
increases the number of people who have
that height also
decreases this kind of a distribution is
called a normal distribution where the
average or the mean is always the
highest point and any other point after
that or before that is significantly
lower
the resulting data gives us a bell curve
and as you can see there is no abrupt
bias or spike in the data anywhere
except for the average height so this
kind of a curve is called a bell curve
and it's usually seen in a normal
distribution the reason we call this a
normal distribution is because the data
is normally distributed with the average
being the highest and all the other data
points having a lower
likelihood now we came across two terms
which are associated with normal
distribution continuous probability
density and probability density function
what is continuous probability density
continuous probability density is a
probability distribution where the
random variable X can take any given
value because there are infinite values
that X could assume the probability of X
taking on any specific value zero for
example let's let's say you have a
continuous probability density for a
man's height what is the probability
that a man will have the exact height of
70
in it is impossible to find this out
because the probability of one man
measuring exactly 70 in is very low it
is more probable that he will measure
around 70.1 in or maybe
6997 in and it doesn't stop
there the fact is that it's impossible
to exactly measure any variable that's
on a continuous scale and because of
this it's impossible to figure out the
probability of one exact measurement
which is occurring in a continuous
probability
density next we have the probability
density function it's nothing but a
function or an expression which is used
to define the range of values that a
continuous random variable can take an
example of this would be to garge the
risk and reward of a
stock a probability density function is
a statistical measure which is used to G
the likelihood of a discrete value a
discrete variable can be measured
exactly while a continuous variable can
have infinite
values however for both continuous as
well as discrete varibles we can define
a
function which gives us the range of
values within
which these variables will fall and that
function is known as the probability
density function
now let's take a look at standard
deviation what is standard deviation
standard deviation is used to measure
how the values in your data differ from
one another or how spread out your data
is a standard deviation is a statistic
that measures the dispersion of a data
set relative to its mean the standard
deviation is calculated as the square
root of variance by determining each
data Point's deviation relative to the
mean
if the data points are further from the
mean that means that there's a higher
deviation within the data set and then
the data is set to be more spread
out this leads to a higher standard
deviation too let's take an example of
income in rural and urban areas in rural
areas let's say such as farming areas
the income doesn't differ that much more
or less everyone earns the same because
of this a bell curve has a very low
standard deviation and it has a very
narrow
Peak however in urban areas the we
distribution is very uneven some people
can have very high incomes and can be
earning a lot while other people can
have very low incomes the furthermore
the data distribution between these two
income points is going to be more spread
out because there are lot more people
living there who work in various fields
and who have various incomes because of
this our standard deviation is more
spread out and our bell curve will also
have a wider
Peak now how can we find the standard
deviation standard deviation is obtained
by subtracting each data value from the
mean and finding the squared average of
these values let's look at how we can do
this with the help of an example these
values correspond to the height of
various
dogs we can find the mean by finding the
average of all these values which is
nothing but adding all the values and
dividing it by the total number of
values the mean that we get is
394 this means that the average height
of a dog is 394
mm to find the standard deviation first
we need to subtract the height from the
mean this will tell us how far from the
mean our data points actually
are next we will square up all of these
differences and add them up and again
divide it by the total number of values
that we have this is called the
variance the variance that we get in
this case is
21704 finally when we find the square
root of this value we will get the
standard deviation the standard
deviation here is 147 the standard
deviation will tell us how our data
points differ from the average and it
gives us a basic value suggesting how
spread out our data is from the very
middle or from the mean so when we plot
these values this value 147 will mean
that a curve will have a width of of 147
points around the
mean now what is the standard normal
distribution the standard normal
distribution is a type of normal
distribution that has a mean of zero and
a standard deviation of one this means
that the normal distribution has its
Center at zero and it has intervals
which increase by one all normal
distributions like the standard normal
distribution are unimodal and
symmetrically distributed with a
bell-shaped curve however a normal
distribution can take on any value as
its mean and standard deviation in the
standard normal distribution however the
mean and standard deviation are always
fixed when you standardize a normal
distribution the mean becomes zero and
the standard deviation becomes one this
allows you to easily calculate the
probability of certain values occurring
in your distribution or to compare data
sets with different mean and standard
deviations the curve shows a standard
normal distribution as you can see again
the data is centered at
zero this does not mean that the data
necessarily starts at zero this means
that after standardizing this point is
where a mean will lie in a standard
normal distribution the standard
deviation is one so all the data points
will increase or decrease in steps of
one let's better understand a standard
normal distribution with the help of an
example
again as you can see the data is
centered around zero which is nothing
but the
mean let's again consider the weights of
students in class 8th the average weight
here is around 50 kgs and the data
increases and decreases in steps of five
the data over here in this curve is
evenly distributed along these steps
this is what a standard normal
distribution will look
like we already know that the mean of
our data is 50 and because the data is
in increasing and decreasing in equal
steps we can just standardize it and
take it to mean that the data is
increasing and decreasing in steps of
one this is what a standard normal
distribution look looks like and when
you have a data which looks like this
you can always standardize it and
convert it into a standard normal
distribution now standard normal
distribution has a couple of properties
which makes calculation comparatively
easy the first one is that 68% of the
values fall within the first standard
deviation which means that 68% of all
data values on this Curve will fall
between the range of -1 to 1 or the
first interval ranging from -1 to
1 the second property is that 95% of the
rest of the values are within the second
standard deviation of from the second
negative point to the second positive
point and
finally
99.7% of the values fall within the
third standard deviation or from the
third negative point to the third
positive point this makes calculations
on standard normal distribution fairly
easy you can compare scores on different
distributions with different means and
standard deviations you can normalize
scores for statistical decision making
using standard normal distribution you
can find the probability of observations
in a distribution which fall above or
below a given value and finally you can
find the probability that a mean
significantly differs from a population
mean now let's take a look at
zcore so what is a zcore a zcore is used
to tell us how far from the mean our
data point actually
is it is calculated using the mean and
standard deviation so it can be said
that the Z score is how many standard
deviations below the mean are data is
basically by using the Z score we can
get an approximate location of where our
data point lies on the graph with
regards to the mean now the Z score is
given by subtracting the data point from
the mean and dividing it by standard
deviation this can also be written as x
- mu / Sigma now any normal distribution
can be standardized by converting its
values into Z scores the Z score will
tell you how many standard deviation
from the mean each values lie while data
points are referred to as X in a normal
distribution they called Zed or Z scores
in the Zed distribution a z score is a
standard score that will tell you how
many standard deviations away from the
mean and individual point will lie a
positive Z score will mean that your x
value is greater than the mean and a
negative Z score will mean that your x
value is less than the mean a z square
of Zer will mean that your x value is
equal to the mean
and again to standardize a value from a
normal distribution all we have to do is
convert it to a z score by subtracting
the mean from our individual value and
dividing it by the standard deviation
now let's see how we can find the Z
score from data points with the help of
a solved
example let's do a case
study in this case study we'll be taking
the summary of daily travel time of a
person who's commuting to and from work
all these values are in minutes and
using these values we have to calculate
the mean the standard deviation and the
Z score these values are as shown as we
can see there are 13 values in total
let's start by finding the mean the mean
is the average and it can be gotten by
adding all of these values and dividing
it by the total number of
values this gives us a value of
38.6 the mean tells us the average of
all our data points which means on an
average he travels for 38.6 minutes to
reach work next let's subtract the
individual values from our mean and
calculate the variance and standard
deviation the values on the left give us
the values that we get after subtracting
it from the mean and the variance can be
calculated by squaring all of these
values adding up all of the squared
values and dividing it by the total
number of values at the end of the day
we get a variance of
140 to calculate the standard deviation
all we have to do is take a square root
of the variance which gives us a value
of
11.8 now the means signifies the average
of our values and we already know this
it gives us the average time which is
taken to travel but the standard
deviation will tell us the average value
of how much our data points differ from
the mean it tells us the deviation
within our own data and it tells us how
far away on an average a point is from
the mean
now the value that we get is 11.8 which
means that on an average a single data
point is around 11.8 data points away
from the
mean now let's calculate the Z
score the Z score is given by
subtracting individual data points from
the mean and dividing it by the standard
deviation we know that we have a
standard deviation of 11.8 and a mean of
38.6
using these values we can calculate the
Z scores for individual X values now we
know that a negative Z score means that
our x value is lower than our mean but
what does the number 1.06 mean this
means that the Z score for 26 is
1.06 standard
deviations away from the mean the
negative symbol here means that our x
value is less than the mean and by how
less 1.06 * the standard deviation now
we know that the negative value of a z
score means that our x value is less
than our mean but what does the number
1.06 mean this means that the Z score is
1.06 * the standard deviation less than
the
mean the same thing can be said for the
Z score of 33 it is 0.47 * the standard
deviation less
than the
mean the z s of 65 is 2.23 * the
standard deviation more than the mean
that means it has to be added to the
mean the reason that we know it's more
than the mean is because this has a
positive value so this means that using
Z scores we can know where our data
points fall relative to other points on
the graph the Z score will tell us how
far away from the mean a point is in
steps of our standard deviation
Basics and
terminology the first one is
outcome whenever we do an experiment
like flipping a coin or rolling a dice
we get an outcome for example if we flip
a coin we get an outcome of heads or
tails and if we roll a die we get an
outcome of 1 2 3 4 5 or
six random experiment a random
experiment is any well- defined
procedure that produces an observable
outcome that could not be perfectly
predicted in advance a random experiment
must be well defined to eliminate any
vagueness or
surprise it must produce a definite
observable outcome so that you know what
happened after the random experiment is
run random
events consider a simple
example let us say that we toss a coin
up in the air what can happen when it
gets back it either gives a head or a
tail these two are known as outcome and
the occurrence of an outcome is an event
thus the event is the outcome of some
phenomenon the last one is sample
space a sample space is a collection or
a set of possible outcomes of a random
experiment the sample space is
represented using the symbol
S the subset of all possible outcomes of
an experiment is called events and a
sample space may contain a number of
outcomes that depends on the experiment
if it contains a finite number of
outcomes comes then it is known as a
discrete or finite sample
spaces now let's discuss what is random
variable a random variable is a
numerical description of the outcome of
a statistical experiment a random
variable that may assume only a finite
number of values is set to be discrete
one that may assume any value in some
interval on the real number line is set
to be
continuous let's see an example let X be
a random variable defined as a sum of
numbers when two dices are
rolled X can assume the values 2 3 4 5 6
7 8 9 10 11 and 12 notice there's no one
here because the sum of the two dice can
never be one now that we know the basics
let's move on to binomial
distribution the binomial distribution
is used when there are exactly two
mutually exclusive outcomes of a trial
these outcomes are appropriately labeled
success and failure the B Li
distribution is used to obtain the
probability of observing X successes in
N number of trials with the probability
of success on a single trial denoted by
P the banal distribution assumes that P
is fixed for all the
trials here's a real life example of a
bomal distribution suppose you purchase
a lottery ticket then either you are
going to win the lottery or not in other
words the outcome will be either success
or failure that can be proved through
bomal distribution
there are four important conditions that
needs to be fulfilled for an experiment
to be a binomial
experiment the first one is there should
be a fixed number of end trials carried
out the outcome of a given trial is only
two that is either a success or a
failure the probability of success
remains constant from trial to trial it
does not changes from one trial to
another and the trials are independent
the outcome of a trial is not affected
by the of any other
trial to calculate the binomial
coefficient we use the formula which is
NCR into p^ R into 1 - P to the^ n minus
r where R is the number of success in N
number of Trials and P is the
probability of sees 1 - P denotes the
probability of a failure now let's use
this formula to solve an
example suppose a die is tossed three
times what is the probability of No 5
turning up 15 and 3 5 is turning up to
calculate the No 5 turning up here R is
equal to 0 and N is equal to 3
substituting the value in the formula we
have 3 c0 into 1X 6 ^ 0 into 5x 6^ 3
where 1X 6 is the probability of success
and 5x 6 is the probability of failure
calculating this equation we'll get the
value to be 0.5
787 in a similar manner to calculate the
probability of 15 turn up we'll replace
r with 1 and N will be 3 so P X1 will be
equal to 3 C1 into 1X 6 ^ 1 into 5x 6 ^
2 which will come out to be
0.347 and for 35 turning up we
substitute Ral to 3 and the formula will
remain the same and we'll get the value
to be
0.46 now that we are done with the
concepts of binomial probability
distribution here's a problem for you to
solve post your answers in the comment
section and let us
know a pois distribution is a
probability distribution used in
statistics to show how many times an
event is likely to happen over a given
period of time to put it another way
it's a count
distribution po distribution are
frequently used to comprehend
independent event at a constant rate
over a given interval of
time the posant distribution was
developed by French mathematician Simon
in this poison in
1837 a poison distribution is used in
cases where the chances of any
individual event being a success is very
small the number of defective pencils
per box of a 6,000 pencil the number of
plane crash in India in one year or the
number of printing mistakes in each page
of a book all of these example can have
use of poison
distribution the poison distribution can
be used to calculate How likely it is
that something will happen X number of
times a random variable X has a poison
distribution with parameter Lambda and
the formula for that is e ^ minus Lambda
into Lambda ^ x / X factorial where X
can be the number of times the event is
happening the value of e is taken as
27182 let's discuss some application of
poon
distribution if you want to calculate
the number of of deaths per day or week
due to rare disease in the hospital you
can use the poison distribution in a
similar manner the count of bacteria per
CC in blood or the number of computers
infected as virus per week the number of
mishandled baggage per thousand
passengers can also have an application
for poison
distribution let's discuss one example
to see how you can calculate the poison
distribution suppose on an average a
cancer kills five people each year in
India
what is the probability that one person
is killed this year we'll assume all
these events are independent random
events so by the
formula we have x equal to 1 because we
have to calculate the probability of 1
person that is killed this year so p x =
1 will be equal to e^ - 5 into 5 ^ 1 / 1
factorial which will come out to be
0.033 which will be near to
3.3% so the probability that only 1
person is killed this year due to cancer
is 2.3% if you are an aspiring data
scientist who's looking out for online
training and certification in data
science from the best universities and
Industry experts then search no more
simply learns postgraduate program in
data science from Caltech University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below
hello everyone welcome to another
session by simply learn today we're
going to discuss the base theorem an
important subtopic that comes under
probability Theory we'll start this
video by talking about probability and
conditional probability after that we'll
move on to the base theorem and
understand its formula and a real life
example where the base theorem can be
used so let's get started what is
probability probability is a branch of
mathematics concerning numerical
descriptions of How likely an event is
to occur or How likely it is that a
proposition is true the probability of
an event is a number between 0 and one
where roughly speaking zero indicates
the impossibility of the event and one
indicates
certainty the higher the probability of
an event the more likely is that the
event will occur let's look at an
example
a simple example is the tossing of a
fair unbiased coin since the coin is
fair the outcome that is heads and the
tails are both equally probable the
probability of heads equals the
probability of the Tails and since no
other outcomes are possible the
probability of either heads or tails can
be said to be 1 by two which is also 50%
the probability of an event can be
calculated by number of ways it can
happen divided by the total number of
outcomes now that we know about the
probability let's see if you can answer
this question what is the probability of
drawing a Jack and a queen consecutively
from a deck of 52 cards without
replacement here are your options post
your answers in the comment section and
let us know now let's move on to
conditional probability let A and B be
the two events associated with a random
experiment then the probability of A's
occurrence under the condition that B
has already occurred and probability of
B is not equal to zero is called the
condition probability
it is denoted by P
A/B thus we can say that p a/ b is equal
to p a intersection B divided P of B
where P A/B is the probability of
occurrence of a given that B has already
occurred and PB is the probability of
occurrence of
B to know more about conditional
probability you can check our previous
video which is specifically on
conditional probability now let's move
on to base theorem
the base theorem is a mathematical
formula for calculating condition
probability in probability and
statistics in other words it is used to
figure out how likely an event is
associated on its proximity to
another base law or base rule are the
other names of this theorem the formula
for the base theorem can be written in a
variety of ways the most common version
is p A/B is equal to P of b/ a into P of
a / P of B where p a/ b is the
conditional probability of event a
occurring given that b is true and P A
and B of B are the probabilities of A
and B occurring independently of one
another let's solve a problem using the
base theorem to understand it
better there is a cricket match tomorrow
and in recent years it has rained only 5
days each year unfortunately the
meteorologist has predicted the rain for
tomorrow now when it rains the
meteorologist correctly forecast rain
90% of the time and when it doesn't rain
he incorrectly forecast rain 10% of the
time let's calculate what is the
probability that it will rain on the
match day so the two sample spaces here
are the events that it rains and it does
not rain additionally a third event is
also there that meteorologist predict
the rain so the notation for these
events appear below event A1 is equal to
it Reigns on the match Day event A2 that
it does not rain on the match day and
event B is the meteorologist predicting
the rain now in terms of probability we
know the following probability of a is 5
by 365 that it Reigns 5 days in a year
which will come out to be
0.0136 P A2 is 360 by 365 that is no
days for 360 days in an year which will
come out to be
986 B b/ A1
is9 this signifies when it rains the
meterologist predict the rain 90% of the
time
in a similar manner PB by A2 is01 that
it does not rain the meterologist
predicts the rain 10% of the
time combining all this we can calculate
p a 1/b that is the probability It Will
Rain on the given Match Day given a
forecast of Rain by meterologist the
answer can be determined using the base
theorem as shown below so here's the
formula of the base theorem and putting
all the values that we have calculated
in the previous Slide the probability
that it will rain on the match day given
a forecast of the rain by meterologist
will come out to be
0.111 which will be equal to
11.11% so there's an 11% chance that it
will rain on the match day given that
the meteorologist has predicted the rain
I hope this example is clear to you it's
a weekend and John decided to watch the
latest movie recommended by Netflix at
his friend's place before heading out he
asked Siri about the weather and
realized it would rain so he decided to
take his Tesla for the long journey and
switched to autopilot on the highway
after coming home from the eventful day
he started wondering how technology has
made his life easy he did some research
on the internet and found out that
Netflix Siri and Tesla are all using AI
so what is ai ai or artificial
intelligence is nothing but making
computers based machines think and act
like humans artificial intelligence is
not a new term John McCarthy a computer
scientist coined the term artificial
intelligence back in
1956 but it took time to evolve as it
demanded heavy computing power
artificial intelligence is not confined
to just movie recommendations and
virtual assistance broadly classifying
there are three types of AI artificial
narrow intelligence also called weak AI
is the stage where machines can perform
a specific task Netflix Siri chatbots
spatial recommendation systems are all
examples of artificial narrow
intelligence next up we have artificial
general intelligence referred to as an
intelligent agent's capacity to
comprehend or pick up any intellectual
skill that a human can we are halfway
into successfully implementing this
space IBM's Watson's supercomputer and
gpt3 fall under this category and lastly
artificial
superintelligence it is the stage where
machines surpass human intelligence you
might have seen this in movies and
imagined how the the world would be if
machines occupy it fascinated by this
John did more research and found out
that machine learning deep learning and
natural language processing are all
connected with artificial intelligence
machine learning a subset of AI is the
process of automating and enhancing how
computers learn from their experiences
without human health machine learning
can be used in email spam detection
medical diagnosis Etc deep learning can
be considered a subset of machine
learning it is a field that is based on
learning and improving on its own by
examining computer algorithms while
machine learning uses simpler Concepts
deep learning works with artificial
neural networks which are designed to
imitate the human brain this technology
can be applied in face recognition
speech recognition and many more
applications natural language processing
popularly known as NLP can be defined as
the ability of machines to learn human
language and translate it chatbots fall
under this
category artificial intelligence is
advancing in every crucial field like
healthcare education robotics banking
e-commerce and the list goes on like in
healthcare AI is used to identify
diseases helping healthcare service
providers and their patients make better
treatment in lifestyle decisions coming
to the education sector AI is helping
teachers automate grading organizing and
facilitating parent Guardian
conversations in robotics AI powered
robots employ real-time updates to
detect obstructions in their path and
instantaneously design their routes
artificial intelligence provides
Advanced data analytics that is
transforming banking by reducing fraud
and enhancing compliance with this
growing demand for AI more and more
Industries are looking for AI Engineers
who can help them develop intelligent
systems and offer them lucrative
salaries going north of
$120,000 the future of AI looks
promising with the AI Market expected to
reach $190 billion by
2025 we know humans learn from their
past experiences and machines follow
instructions given by
humans but what if humans can train the
machines to learn from their past data
and do what humans can do and much
faster well that's called machine
learning but it's a lot more than just
learning it's also about understanding
and reasoning so today we will learn
about the basics of machine learning so
that's Paul he loves listening to new
songs he either likes them or dislikes
them Paul decides this on the basis of
the song's Tempo genre intensity and the
gender of voice for Simplicity let's
just use Tempo and intensity for now so
here Tempo is on the x-axis ranging from
relaxed to fast whereas intensity is on
the y- AIS ranging from light to Soaring
we see that Paul like likes the song
with fast tempo and soaring intensity
while he dislikes the song with relaxed
Tempo and light intensity so now we know
Paul's choices let's say Paul listens to
a new song Let's name it as song a song
a has fast tempo and a soaring intensity
so it lies somewhere here looking at the
data can you guess whether Paul will
like the song or not correct so Paul
likes this song by looking at Paul's
past choices we were able to classify
the unknown song very easily right let's
say now Paul listens to a new song Let's
label it as song b so song b lies
somewhere here with medium Tempo and
medium intensity neither relaxed nor
fast neither light nor soaring now can
you guess whether Paul likes it or not
not able to guess whether Paul will like
it or dislike it are the choices unclear
correct we could easily classify song A
but when the choice became complicated
as in the case of song b yes and that's
where machine learning comes in let's
see how in the same example for song b
if you draw a circle around the song b
we see that there are four votes for
like whereas one vote for dislike if we
go for the majority votes we can say
that Paul will definitely like the song
that's all this was a basic machine
learning algorithm also it's called K
nearest neighbors so this is just a
small example in one of the many machine
learning algorithms quite easy right
believe me it is but what happens when
the choices become complicated as in the
case of song b that's when machine
learning comes in it learns the data
builds the prediction model and when the
new data point comes in it can easily
predict for it more the data better the
model higher will be the accuracy there
are many ways in which the machine
learns it could be either supervised
learning unsupervised learning or
reinforcement learning let's first
quickly understand supervised learning
suppose your friend gives you 1 million
coins of three different currencies say
one rupe 1 euro and 1 dirham each coin
has different weights for example a coin
of 1 rupee weighs 3 g 1 EO weighs 7 G
and 1 Dam weighs 4 G your model will
predict the currency of the coin here
your weight becomes the feature of coins
while currency becomes the label when
you feed this data to the machine
learning model it learns which feature
is associated with which label for
example it will learn that if a coin is
of 3 G it will be a 1 rupee coin let's
give a new coin to the machine on the
basis of the weight of the new coin your
model will predict the currency hence
supervised learning uses labeled data to
train the model here the machine knew
the features of the object and also the
labels associated with those features on
this note let's move to unsupervised
learning and see the difference suppose
you have Cricket data set of various
players with their respective scores and
wickets taken when we feed this data set
to the machine the machine identifies
the pattern of player performance so it
plots this data with the respective
wickets on the x-axis while runs on the
y axis by looking at the data you'll
clearly see that there are two clusters
the one cluster are the players who
scored High runs and took less wickets
while the other cluster is of the
players who scored less runs but took
many wickets so here we interpret these
two clusters as batsmen and Bowlers the
important point to note here is that
there were no labels of batsmen and
Bowlers hence the learning with
unlabeled data is unsupervised learning
so we saw supervised learning where the
data was labeled and the unsupervised
learning where the data was unlabeled
and then there is reinforcement learning
which is a reward-based learning or we
can say that it works on the principle
of feedback here let's say you provide
the system with an image of a dog and
ask it to identify it the system
identifies it as a cat so you give a
negative feedback to the machine saying
that it's a dog's image the machine will
learn from the feedback and finally if
it comes across any other image of a dog
it'll be able to classify it correctly
that is reinforcement learning to
generalize machine learning model let's
see a flowchart input is given to a
machine learning model which then gives
the output according to the algorithm
applied if it's right we take the output
as a final result else we provide
feedback to the training model and ask
it to predict until it learns I hope
you've understood supervised and
unsupervised learning so let's have a
quick quiz you have to determine whether
the given scenarios uses supervised or
unsupervised learning simple right
scenario one Facebook recognizes your
friend in a picture from an album of
tagged
photographs scenario 2 Netflix
recommends new movies based on someone's
Past movie
choices scenario three analyzing Bank
data for suspicious transactions and
flagging the fraud transactions think
wisely and comment below your answers
moving on don't you sometimes wonder how
is machine learning possible in today's
era well that's because today we have
humongous data available everybody's
online either making a transaction or
just surfing the internet and that's
generating a huge amount of data every
minute and that data my friend is the
key to analysis also the memory handling
capabilities of computers have largely
increased which helps them to process
such huge amount of data at hand without
any delay and yes computers now have
great computation Powers so there are a
lot of applications of machine learning
out there to name a few machine learning
is used in healthcare where Diagnostics
are predicted for doctor's review the
sentiment analysis that the tech Giants
are doing on social media is another
interesting application of machine
learning fraud detection in the finance
sector and also to predict customer
churn in the e-commerce sector while
booking a gap you must have encountered
searge pricing often where it says the
fair of your trip has been updated
continue booking yes please I'm getting
late for office well that's an
interesting machine learning model which
is used by Global Taxi giant Uber and
others where they have differential
pricing in real time based on demand the
number of cars available bad weather
Rush R Etc so they use the surge pricing
model to ensure that those who need a
cab can get one also it uses predictive
modeling to predict where the demand
will be high with a goal that drivers
can take care of the demand and search
pricing can be minimized
great hey Siri can you remind me to book
a cab at 6:00 p.m. today okay I'll
remind you thanks no problem if you are
an aspiring data scientist who's looking
out for online training and
certification in data science from the
best universities and Industry experts
then search no more simply learns
postgraduate program in data science
from calc University in collaboration
with IBM should be the right choice for
more details on this program please use
the link in the description box below
let's dive in a little deeper and see
how machine Learning Works let's say you
provide a system with the input data
that carries the photos of various kinds
of fruits now you want the system to
figure out what are the different fruits
and group them accordingly so what the
system does it analyzes the input data
then it tries to find patterns patterns
like shapes size and
color based on these patterns the system
will try to predict the different types
of fruit and segregate them finally it
keeps track of all such decisions it
took in the process to make sure it's
learning the next time you ask the same
system to predict and segregate the
different types of fruits it won't have
to go through the entire process again
that's how machine learning
works now let's look into the types of
machine learning machine learning is
primarily of three types first one is
supervised machine learning as the name
suggest you have to supervise your
machine learning while you train it to
work on its own it requires labeled
training data next up is unsupervised
learning wherein there will be training
data but it won't be labeled finally
there's reinforcement learning wherein
the system learns on its own let's talk
about all these types in detail let's
try to understand how supervised
Learning Works look at the pictures very
very carefully the monitor depicts the
model or the system that we are going to
train this is how the training is done
we provide a data set that contains
pictures of a kind of a fruit say an
apple then we provide another data set
which lets the model know that these
pictures were that of a fruit called
Apple this ends the training phase now
what we will do is we provide a new set
of data which only contains pictures of
apple now here comes the fun part the
system can actually tell you what fruit
it is and it will remember this and
apply this knowledge in future as as
well that's how supervised Learning
Works you are training the model to do a
certain kind of an operation on its own
this kind of a model is generally used
into filtering spam mails from your
email accounts as well yes surpris
aren't you so let's move on to
unsupervised learning now let's say we
have a data set which is cluttered in
this case we have a collection of
pictures of different fruits we feed
this data to the model and the model
analyzes the data to figure out patterns
in it in the end it categorizes the
photos into three types as you can see
in the image based on their
similarities so you provide the data to
the system and let the system do the
rest of the work simple isn't it this
kind of a model is used by flip cart to
figure out the products that are well
suited for you honestly speaking this is
my favorite type of machine learning out
of all the three and this type has been
widely shown in most of the Sci-Fi
movies lately let's find out how it
works imagine a newborn baby you put a
burning candle in front of the baby the
baby does not know that if it touches
the flame its fingers might get burned
so it does that anyway and gets hurt the
next time you put that candle in front
of the baby it will remember what
happened the last time and would not
repeat what it did that's exactly how
reinforcement learning works we provide
the machine with a data set wherein we
ask it to identify a particular kind of
a fruit in this case an Apple so what it
does as a response it tells us that it's
a mango but as we all know it's a
completely wrong answer so as a feedback
we tell the system that it's wrong it's
not a mango it's an apple what it does
it learns from the feedback and keeps
that in mind when the next time when we
ask a same question it gives us the
right answer it is able to tell us that
it's actually an apple that is a
reinforced response so that's how
reinforcement learning works it learns
from its mistakes and experiences this
model is used in games like Prince of
Persia or Assassin's Creed or FIFA where
in the level of difficulty increases as
you get better with the games just to
make it more clear for you let's look at
a comparison between supervised and
unsupervised learning firstly the data
involved in case of supervised learning
is labeled as we mentioned in the
examples previously we provide the
system with a photo of an apple and let
the system know that this is actually an
apple that is called label data so the
system learns from the label data and
makes future
predictions now unsupervised learning
does not require any kind of label data
because its work is to look for patterns
in the input data and organize it the
next point is that you get a feedback in
case of supervised learning that is once
you get the output the system tends to
remember that and uses it for the next
operation that does not happen for
unsupervised learning and the last point
is that supervised learning is mostly
used to predict data whereas
unsupervised learning is used to find
out hidden patterns or structures in
data I think this would have made a lot
of things clear for you regarding
supervised and unsupervised
learning now let's talk about a question
that everyone needs to answer before
building a machine learning model what
kind of a machine learning solution
should we
use yes you should be very careful with
selecting the right kind of solution for
your model because if you don't you
might end up losing a lot of time energy
and processing costs
I won't be naming the actual Solutions
because you guys aren't familiar with
them yet so we will be looking at it
based on supervised unsupervised and
reinforcement learning so let's look
into the factors that might help us
select the right kind of machine
learning solution first factor is the
problem statement describes the kind of
model you will be building or as the
name suggests it tells you what the
problem is for example let's say the
problem is to predict the future stock
market prices so for anyone who is new
to machine learning would have trouble
figuring out the right solution but with
time and practice you will understand
that for a problem statement like this
solution based on supervised learning
would work the best for obvious reasons
then comes the size quality and nature
of the data if the data is cluttered you
go for unsupervised if the data is very
large and categorical you normally go
for supervised learning Solutions
finally we choose a solution based on
their complexity
as for the problem statement wherein we
predict the stock market prices it can
also be solved by using reinforcement
learning but that would be very very
difficult and time consuming unlike
supervised
learning algorithms are not types of
machine learning in the most simplest
language they are methods of solving a
particular problem so the first kind of
method is classification which falls
under supervised learning classification
is used when the output you are looking
for is a yes or a no or in the form a or
b or true or false like if a shopkeeper
wants to predict if a particular
customer will come back to his shop or
not he will use a classification
algorithm the algorithms that fall under
classification are decision tree knife
base random Forest logistic regression
and
KNN the next kind is regression this
kind of a method is used when the
predicted data is numer iCal in nature
like if the shopkeeper wants to predict
the price of a product based on its
demand it would go for
regression the last method is
clustering clustering is a kind of
unsupervised learning again it is used
when the data needs to be
organized most of the recommendation
system used by flip cart Amazon Etc make
use of
clustering another major application of
it is in search engines the search
engines study your old search history to
figure out your preference es and
provide you the best search
results one of the algorithms that fall
under clustering is K
means now that we know the various
algorithms let's look into four key
algorithms that are used widely we will
understand them with very simple
examples the four algorithms that we
will try to understand are K nearest
neighbor linear regression decision tree
and knife Pace let's start with our
first machine learning solution K
nearest neighbor K nearest neighbor is
again a kind of a classification
algorithm as you can see on the screen
the similar data points form
clusters the blue
one the red
one and the green one there are three
different clusters now if we get a new
and unknown data point it is classified
based on the cluster closest to it or
the most similar to it k in Ann is the
number of nearest neighboring data
points we wish to compare the unknown
data with let's make it clear with an
example let's say we have three clusters
in a cost to durability graph first
cluster is of
footballs the second one is of tennis
balls and the third one is of
basketballs from the graph we can say
that the cost of footballs is high and
the durability is less the cost of
tennis balls is very less but the
durability is high and the cost of
basketballs is as high as the durability
now let's say we have an unknown data
point we have a black spot which can be
one kind of the balls but we don't know
what kind it is so what we'll do we'll
try to classify this using KN andn so if
we take K is equal to 5 we draw a circle
keeping the unknown data point is the
center and we make sure that we have
five balls inside that Circle in this
case we have a football a basketball and
three tennis balls now since we have the
highest number of tennis balls inside
the circle the classified ball would be
a tennis
ball so that's how kest neighbor
classification is done linear regression
is again a type of supervised learning
algorithm this algorithm is used to
establish linear relationship between
variables one of which would be
dependent and the other one would be
independent like if we want to predict
the weight of a person based on his
height weight would be the dependent
variable and height would be
independent let's have a look at it
through an
example let's say we have a graph here
showing a relationship between height
and weight of a person let's put the y-
axis as
age and the x-axis as
weight so the green dots are the VAR
ious data points these green dots are
the data points and D is the mean
squared error that is the perpendicular
distances from the line to the data
points are the error
values this error tells us how much the
predicted values vary from the original
value Let's ignore this blue line for a
while so let's say if this is our
regression line you can see the distance
from all the data points from this line
is very
high so if we take this line as a
regression line the error in the
prediction will be too
high so in this case the model will not
be able to give us a good prediction
let's say we draw another regression
line here like this even in this case
you can see that the perpendicular
distance of the data points from the
line is very high so the error value
will still come as high as the last one
so this model will also not be able to
give us a good
prediction so what to
do so finally we draw a line which is
this blue line so here we can see that
the distance of the data points from the
line is very less lips to the other two
lines we
drew so the value of D for this line
will be very less so in this case if we
take any value on the x- axis the
corresponding value on the Y AIS will be
our
prediction and given the fact that the D
is very low our prediction should be
good
also this is how regression works we
draw a line a regression line that is in
such a way that the value of D is the
least eventually giving us good
predictions this algorithm that is
decision tree is a kind of an algorithm
you can very strongly relate to
it uses a kind of a branching method to
realize the problem and make decisions
based on the conditions let's take this
graph as an example imagine yourself
sitting at home getting bored you feel
like going for a swim what you do is you
check if it's sunny outside so that's
your first condition if the answer to
that condition is yes you go for a swim
if it's not sunny in the next question
you would ask yourself is if it's
raining outside so that's condition
number two if it's actually raining you
cancel the plan and stay indoors if it's
not raining then you would probably go
outside and have a walk so that's the
final note that's how decision tree
algorithm works you probably use this
every day it realizes a problem and then
takes the decisions based on the answers
to every
conditions knbis algorithm is mostly
used in cases where a prediction needs
to be done on a very large data set it
makes use of conditional
probability conditional probability is
the probability of an event say a
happening given that another even B has
already happened this algorithm is most
commonly used in filtering spam mails in
your email account let's say you receive
a mail the model goes through your old
spam mail
records then it uses space theorem to
predict if the present maale is a spam
mail or not so PC of a is the
probability of even C occurring when a
has already occurred B a of C is the
probability of event a occurring when C
has already occurred and P C is the
probability of event C occurring and Pa
is the probability of event a occurring
let's try to understand knif base with a
better example night base can be used to
determine on which days to play cricket
based on the probabilities of a day
being rainy windy or sunny the model
tells us if a match is possible if we
consider all the weather conditions to
be event a for
us and the probability of a match being
possible event
C so the model applies the probabilities
of event A and C into the base theorem
and predicts if a game of cricket is
possible on a particular day or not in
this case if the probability of C of a
is more than 0.5 we can be able to play
a game of cricket if it's less than 0.5
we won't be able to do that that's how
KN V algorithm works we're going to
cover reinforcement learning today and
what's in it for you we'll start with
why reinforcement learning we'll look at
what is reinforcement learning we'll see
what the different kinds of learning
strategies are that are being used today
in computer models under supervised
versus unsupervised versus
reinforcement we'll cover important
terms specific to reinforcement learning
we'll talk about markov's decision
process and we'll take a look at a
reinforcement learning example well
we'll teach a tic-tac-toe how to play
why reinforcement learning training a
machine learning model requires a lot of
data which might not always be available
to us further the data provided might
not be reliable learning from a small
subset of actions will not help expand
the vast realm of solutions that may
work for a particular problem and you
can see here we have the robot learning
to walk um very complicated setup when
you're learning how to walk and you'll
start asking questions like if I'm
taking one step forward and left
what happens if I pick up a 50b object
how does that change how a robot would
walk these things are very difficult to
program because there's no actual
information on it until it's actually
tried out learning from a small subset
of actions will not help expand the vast
real of solutions that may work for a
particular
problem and we'll see here it learned
how to walk this is going to slow the
growth that technology is capable of
machines need to learn to perform
actions by themselves and not just learn
off
humans and you see the objective climb a
mountain real interesting point here is
that as human beings we can go into a
very unknown environment and we can
adjust for it and kind of explore and
play with it most of the models the
non-reinforcement models in computer um
machine learning aren't able to do that
very well uh there's a couple of them
that can be used or integrated see how
it goes is what we're talking about with
reinforcement learning so what is
reinforcement learning reinforcement
learning is a subbranch of machine
learning that trains a model to return
an Optimum solution for a problem by
taking a sequence of decisions by itself
consider a robot learning to go from one
place to another the robot is given a
scenario and must arrive at a solution
by itself the robot can take different
paths to reach the
destination it will know the best path
by the time taken on each path it might
even come up with a unique solution all
by itself and that's really important as
we're looking for Unique Solutions uh we
want the best solution but you can't
find it unless you try it so we're
looking at uh our different systems or
different model we have supervised
versus unsupervised versus reinforcement
learning and with the supervised
learning that is probably the most
controlled environment uh we have a lot
of different supervised learning models
whether there's linear regression neural
networks um there's all kinds of things
in between decision trees the data
provided is labeled data with output
values specified and this is important
because when we talk about supervised
learning you already know the answer for
all this information you already know
the picture has a motorcycle in it so
you're supervised learning you already
know that um the outcome for tomorrow
for you know going back a week you're
looking at stock you can already have
like the graph of what the next day
looks like so you have an answer for
it and you have labeled data which is
used you have an external supervision
and solves Problems by mapping labeled
input to know an output so very
controlled unsupervised learning and
unsupervised learning is really
interesting because it's now taking part
in many other models they start with an
you can actually insert an unsupervised
learning model um in almost either
supervised or reinforcement learning as
part of the system which is really cool
uh data provided is unlabeled data the
outputs are not specified machine makes
its own predictions used to solve
association with clustering problems
unlabeled data is used no supervision
solves Problems by understanding
patterns and discovering
output uh so you can look at this and
you can think um some of these things go
with each other they belong together so
it's looking for what connects in
different ways and there's a lot of
different algorithms that look at this
um when you start getting into those
there's some really cool images that
come up of what unsupervised learning is
how it can pick out say uh the area of a
donut one model will see the area of the
donut and the other one will divide it
into three sections based on this
location versus what's next to it so
there's a lot of stuff that goes in with
unsupervised learning and then we're
looking at reinforcement learning
probably the biggest industry in today's
market uh in machine learning or growing
Market it's very in it's very infant
stage
uh as far as how it works and what it's
going to be capable of the machine
learns from its environment using
rewards and errors used to solve reward
based problems no predefined data is
used no supervision follows Trail and
error problem solving approach uh so
again we have a random at first you
start with a random I try this it works
and this is my reward doesn't work very
well maybe or maybe it doesn't even get
you where you're trying to get it to do
and you get your reward back and then
looks at that and says well let's try
something else and it starts to play
with these different things finding the
best route so let's take a look at
important terms in today's reinforcement
model and this has become pretty
standardized over the last uh few years
so these are really good to know we have
the agent uh agent is the model that is
being trained via reinforcement learning
so this is your actual uh entity that
has however you're doing it whether
using a neural network or a a q table or
whatever combination thereof this is the
actual agent that you're using this is
the
model and you have your environment uh
the training situation that the model
must optimize to is called its
environment uh and you can see here I
guess we have a robot who's trying to
get a chest full of gems or whatever and
that's the output and then you have your
action this is all possible steps that
can be taken by the model and it picks
one action and you can see here it's
picked three different uh routes to get
to the chest of diamonds and
gems we have a state the current
position condition returned by the
model and you could look at this uh if
you're playing like a video game this is
the screen you're looking at uh so when
you go back here uh the environment is a
whole game board so if you're playing
one of those Mobius games you might have
the whole game board going on uh but
then you have your current position
where are you on that game board what's
around that what's around you um if if
you were talking about a robot the
environment might be moving around the
yard where it is in the yard and what it
can see what input it has in that
location that would be the current
position condition returned by the model
and then the reward uh to help the model
move in the right direction it is
rewarded points are given to it to
appraise some kind of action so yeah you
did good or if uh didn't do as good
trying to maximize the reward and have
the best reward
possible and then policy policy
determines how an agent will behave at
any time it acts as a mapping between
action and present State this is part of
the model what what is your action that
you're you're going to take what's the
policy you're using to have an output
from your agent one of the reasons they
separate uh policy as its own entity is
that you usually have a prediction um of
a different options and then the policy
well how am I going to pick the best
based on those predictions I'm going to
guess at different options and we'll
actually weigh those options in and find
the best option we think will work uh so
it's a little tricky but the policy
thing is actually pretty cool how it
works let's go Ahad and take out look at
a reinforcement learning example and
just in looking at this we're going to
take a look uh consider what a dog um
that we want to train uh so the dog
would be like the agent so you have your
your puppy or whatever uh and then your
environment is going to be the whole
house or whatever it is where you're
training them and then you have an
action we want to teach the dog to
fetch so action equals
fetching uh and then we have a little
biscuit so we can get the dog to perform
various actions by offering incentives
such as a dog biscuit as a
reward the dog will follow a policy to
maximize this reward and hence will
follow every command and might even
learn new actions like begging by itself
uh so you have B you know so we start
off with fetching it goes oh I get a
biscuit for that it tries something else
and you get a handshake or begging or
something like that and it goes oh this
is also reward-based and so it kind of
explores things to find out what will
bring it as biscuit and that's very much
like how a reinforced model goes is it
uh looks for different rewards how do I
find can I try different things and find
a reward that
works the dog also will want to run
around and playing explorers environment
uh this quality of model is called
exploration there's a little Randomness
going on in
Exploration and explores new parts of
the house climbing on the sofa doesn't
get a reward in fact it usually gets
kicked off the
sofa so let's talk a little bit about
markov's decision process uh markov's
decision process is a reinforcement
learning policy used to map a current
state to an action where the agent
continuously interacts with the
environment to produce new Solutions and
receive Rewards and you'll see here's
all of our different uh uh vocabulary we
just went over we have a reward our
state our agent our environment and our
action and so even though the
environment kind of contains everything
um that you you really when you're
actually writing the program your
environment's going to put out a reward
in state that goes into the agent uh the
agent then looks at this uh state or it
looks at the reward usually um first and
it says okay I got rewarded for whatever
I just did or didn't get rewarded and
then it looks at the state then it comes
back and if you remember from policy the
policy comes in um and then we have a
reward the policy is that part that's
connected at the bottom and so it looks
at that policy and it says hey what's a
good action that will probably be
similar to what I did or um sometimes
are completely random but what's a good
action that's going to bring me a
different
reward so taking the time to just
understand these different pieces as
they go is pretty important in most of
the models today um and so a lot of them
actually have templates based on this
you can pull in and start using um
pretty straight forward as far as once
you start seeing how it works uh you can
see your environment send it says hey
this is the agent did this if you're a
character in a game this happened and it
shoots out a reward in a state the agent
looks at the reward looks at the new
state and then takes a little guess and
says I'm going to try this action and
then that action goes back into the
environment it affects the environment
the environment then changes depending
on what the action was and then it has a
new state and a new reward that goes
back to the agent so in the diagram
shown we need to find the shortest path
between note A and D each path has a
reward associated with it and the path
with a maximum reward is what we want to
choose the nodes AB c d denote the nodes
to travel from node uh A to B is an
action reward is the cost of each path
and policy is each path
taken and you can see here a can go uh
to b or a can go to C right off the bat
or you can go right to D and if you
explored all three of these uh you would
find that a going to D was a zero reward
um a going to C and D would generate a
different reward or you could go AC b d
there's a lot of options here um and so
when we start looking at this diagram
you start to
realize that even though uh today's
reinforced learning models do really
good at um finding an answer they end up
trying almost all the different
directions you see and so they take up a
lot of work uh or a lot of processing
time for reinforcement learning they're
right now in their infant stage and
they're really good at solving simple
problems and we'll take a look at one of
those in just a minute in a tic tac toe
game uh but you can see here uh once
it's gone through these and it's
explored it's going to find the
ACD is the best reward it gets a full 30
points for it so let's go ahead and take
a look at a reinforcement learning
demo uh in this demo we're going to use
reinforcement learning to make a tic tac
toe game you will be playing this game
Against the Machine learning
model and we'll go ahead we're doing it
in Python so let's go ahead and go
through I always uh not always actually
have a lot of python tools let's go
through um Anaconda which will open up a
Jupiter notebook seems like a lot of
steps but it's worth it to keep all my
stuff separate and it's also has a nice
display when you're in the Jupiter
notebook for doing
python so here's our Anaconda Navigator
I open up the notebook which is going to
take me to a web page and I've gotone in
here and created a new uh python folder
in this case I've already done it and
enabled it to change the name to
tic-tac-toe uh and then for this example
uh we're going to go ahead
and import a couple things we're going
to um import numpy as NP we'll go ahead
and import pickle numpy of course is our
number array and then pickle is just a
nice way sometimes for storing uh
different information uh different
states that we're going to go through on
here uh and so we're going to create a
class called State we're going to start
with
that and there's a lot of lines of code
to this uh class that we're going to put
in here don't let that scare you too
much there's not as much here um it
looks like there's going to be a lot
here but there really is just a lot of
setup going on in the in our class date
and so we have up here we're going to
initialize it um we have our board um
it's a Tic Tac Toe board so we're only
dealing with nine spots on the board uh
we have player one player
two uh is in we're going to create a
board hash uh we'll look at that in just
a minute we're just going to sore some
information in there symbol of player
equals one um so there's a few things
going on as far as the
initialization uh then something simple
we're just going to get the hash um of
the board we're going to get the
information from the board on there
which is uh columns and rows we want to
know when a winner occurs uh so if you
get three in a row that's what this
whole section here is for uh me go ahead
and scroll up a little bit and you can
get a copy of this code if you send a
note over to Simply learn we'll send you
over um this particular file and you can
play with it yourself and see how it's
put together I don't want to spend a
huge amount of time on this uh because
this is just some real General python
coding uh but you can see here we're
just going through um all the rows and
you add them together and if it equals
three three in a row same thing with
columns um diagonal so you got to check
the diagonal that's what all this stuff
do does here is it just goes through the
different areas actually let me go ahead
and
put there we
go um and then it comes down here and we
do our sum and it says true uh minus
three just says did somebody win or is
it a tie so you got add up all the
numbers on there anyway just in case
they're all filled up and next we also
need to know available positions um
these are ones that don't no one's ever
used before this way when you try
something or the computer tries
something uh it's not going to give it
an illegal move that's what the
available positions is doing uh then we
want to update our state so you have
your position going in we're just
sending in the position that you just
chose and you'll see there's a little
user interface we put in there you P
pick the row and column in
there and again I mean this is a lot of
code uh so really it's kind of a thing
you'd want to go through and play with a
little bit and just read through it get
a copy of it great way to understand how
this works and here is a given reward um
so we're going to give a reward result
equals self winner this is one of the
hearts of what's going on here uh is we
have a result self. winner so if there's
a winner then we have a result if the
result equals one here's our
feedback uh if it doesn't equal one then
it gets a zero so it only gets a reward
in this particular case if it
wins and that's important to know
because different uh systems of
reinforced learning do rewarding a lot
differently depending on what you're
trying to do this is a very simple
example with a a 3X3 board imagine if
you're playing a video game uh certainly
you only have so many actions but your
environment is huge you have a lot going
on in the environment and suddenly a
reward system like this is going to be
just um is going to have have to change
a little bit it's going to have to have
different rewards and different setup
and there's all kinds of advanced ways
to do that as far as weighing you add
weights to it and so they can add the
weights up depending on where the reward
comes in so it might be that you
actually get a reward in this case you
get the reward at the end of the game
and I'm spending just a little bit of
time on this because this is an
important thing to note but there's
different ways to add up those rewards
it might have like if you take a certain
path um the first re reward is going to
be weighed a little bit less than the
last reward because the last reward is
actually winning the game or scoring or
whatever it is so this reward system
gets really complicated on some of the
more advanced uh
setups um in this case though you can
see right here that they give a um a 0.1
and a 0. five
reward um just for getting a picking the
right value and something that's
actually valid instead of picking an
invalid value so rewards uh again that's
like key it's huge how do you feed the
rewards back in uh then we have a board
reset that's pretty straightforward it
just goes back and resets the board to
the beginning because it's going to try
out all these different things while
it's learning it's going to do it by
trial and error so you have to keep
resetting it and then of course there's
the play we want to go ahead and play uh
rounds equals 100 depends on what you
want to do on here um you can set this
different you obviously set that to
higher level but this is just going to
go through through and you'll see in
here uh that we have player one and
player two this is this is the computer
playing itself uh one of the more
powerful ways to learn to play a game or
even learn something that isn't a game
is to have two of these models that are
basically trying to beat each other and
so they they keep finding explore new
things this one works for this one so
this one tries new things it beats this
we've seen this in um chess I think with
a big one where they had the two players
in chess with reinforcement learning uh
was one of the ways they train one of
the top um computer chess playing
algorithms uh so this is just what this
is it's going to choose an action it's
going to try something and the more it
tries stuff um the more we're going to
record the hash we actually have a board
hash where they self get the hash setup
on here where stores all the
information and then once you get to a
win one of them wins it gets reward uh
then we go back and reset and try again
and then kind of the fun part we
actually get down here is uh we're going
to play with a human so we'll get a
chance to come in here and see what that
looks like when you put your own
information in and then it just comes in
here does the same thing it did above it
gives it a reward for its things um or
sees if it wins or ties um looks at
available positions all that kind of fun
stuff and then finally we want to show
the board uh so it's going to print the
the board out each
time really um as an integration is not
that exciting what's exciting uh in here
is one looking at this reward system
whoops Play One More up the reward
system is really the heart of this how
do you reward the different uh setup and
the other one is when it's playing it's
got to take an action and so what it
chooses for an action is also the heart
of reinforcement learning how do we
choose that action and those are really
key to right now where reinforcement
learning is um in today's uh technology
is uh figuring this out how do we reward
it and how do we guess the next best
action so we have our uh environment and
you can see the environment is we're
going to be or the state uh which is
kind of like what's going on we're going
to return the state depending on what
happens and we want to go ahead and
create our agent uh this clay our player
so each one is let me go and grab that
and so we look at a class player um this
is where a lot of the magic is really
going on is what how is this player
figuring out how to maneuver around the
board and then the board of course
returns a state uh that it can look at
and a reward uh so we want to take a
look at this we have a name uh self
State this is class player and when you
say class player we're not talking about
a human player we're talking about um
just a a the computer players and this
is kind of interesting so remember I
told you depending on what you're doing
there's going to be a Decay gamma
um explore rate uh these are what I'm
talking about is how do we train it
um as you try different moves it gets to
the end the first move is important but
it's not as important as the last one
and so you could say that the last one
has the heaviest weight and then as you
as you get there the first one the see
the first move gives you a five reward
the second gives you a two reward and
the third one gives you a 10 reward
because that's the final ending you got
it the 10's going to count more than the
first step uh and here's our uh we're
going to you know get the board
information coming in and then choose an
action this was the second part that I
was talking about that was so important
uh so once you have your training going
on we have to do a little Randomness and
you can see right here is our NP random
uh uniform so it's picking out a random
number take a random action this is
going to just pick which row and which
colum it is um and so choosing the
action this one you can see we're just
doing random States um Choice length of
positions action position and then it
skips in there and takes a look at the
board uh for p and positions you get
it's actually storing the different
boards each time you go through so it
has a record of what it did so it can
properly weigh the values
and this simply just ains a hash State
what's the last state Pinn it to the uh
um to our states on here here's our
feedback reward so the reward comes in
and it's going to take a look at this
and say is it none uh what is the reward
and here is that formula remember I was
telling you about up here um that was
important because it has Decay gamma
times the reward this is where as it
goes through each step this and this is
really important this is this is kind of
the heart of this of what I was talking
about earlier uh you have step
one and this might have a reward of two
you have step two I should probably
should have done ABC this has a step
three uh step
four so on till you get to step in and
this might have a reward of
10 uh so reward of
10 we're going to add that but we're not
adding uh let's say this one right here
uh let's say this reward here right
before 10 was um let's say it's also 10
that just makes the the math easy so we
had 10 and 10 we had 10 this is 10 and
10 n whatever it is but it's time it's
0.9 uh so instead of putting a full 10
here we only do nine that's uh 0. n
times
10 and so this
formula um as far as the Decay times the
reward minus the cell State value uh it
basically adds in it says here's one or
here's two I'm sorry I should have done
this ABC it would have been easier uh so
the first move goes in here and it puts
two in
here uh then we have our s uh setup on
here you can see how this gets pretty
complicated in the math but this is
really the key is how do we train our
states and we want the the final State
the win to get the most points if you
win you get most points uh and the first
step gets the least amount of points so
you're really training this almost in
Reverse you're training you're training
it from the last place where you have
like it says okay this is now where I
need to sum up my rewards and I want to
sum them up going in reverse and I want
to find the answer in Reverse kind of an
interesting uh uh play on the mind when
you're trying to figure F this stuff
out and of course we want to go ahead
and reset the board down here uh and
save the policy load
policy these are the different things
that are going in between the agent and
the state to figure out what's going on
let's go ahead and load that up and then
finally we want to go ahead and create a
human
player and the human player is going to
be a little different uh in that uh you
choose an action row and column here's
your action uh if action is if action in
positions meaning positions that are
available uh you return the action if
not it just keeps asking you until you
get an action that actually works and
then we're going to go ahead and append
to the hash state which uh we don't need
to worry about because it Returns the
action up
here and feed forward uh again this is
because it's a
human um at the end of the game bat
propagator and update State values this
part isn't being done because it's not
programming uh the model uh the model is
getting its own rewards so we've gone
ahead and loaded this in here uh so
here's all our pieces and the first
thing we want to
do is set up uh P1 player one uh P2
player two and then we're going to send
our players to our state so now it has
P1 P2 and it's going to play and it's
going to play 50,000 rounds now we can
probably do a lot less than this and
it's not going to get the full results
in fact you know what uh let's go ahead
and just do five um just to play with it
because I want to show you something
here oops somewhere in there I forgot to
load
something there we go I must have forgot
to run this
run oops forgot a reference there for
the board rows and columns 3x3
um there is actually in the state it
references that we just tack it on on
the end it was supposed to be at the
beginning uh so now I've only set this
up with um see where are we going here
I've only set this up to
train five times and the reason I did
that is we're going to uh come in and
actually play it and then I'm going to
change that and we can see how it
differs on
there there we go I didn't you make it
through a run and we're going to go
ahead and save the
policy um so now we have our player one
and our player two policy uh the way we
set it up it has two separate policies
loaded up in
there and then we're going to come in
here and we're going to do uh player one
is going to be the computer experience
rate zero load policy one human player
human and we're going to go ahead and
play this and I remember I only went
through it um uh just one round of
training in fact minimal training and so
it puts an X there and I'm going to go
ahead and do row zero column one you can
see this is very uh basic on here and so
I put in my zero and then I'm going to
go zero block it zero zero and you can
see right here it let me win uh just
like that I was able to win
zero two and woo human winds so only
trained it five times we're going to run
this again and this time uh instead of
five let's do 5,000 or 50,000 I think
that's what the guys in the back had and
this takes a while to train it this is
where reinforcement learning really
falls apart look how simple this game is
we're talking about uh 3 by3 set of
columns and so for me to train it on
this um I could do a q table which would
take which would go much quicker um you
could build a quick Q table with almost
all the different options on there and
uh you would probably get a the same
result much quicker we're just using
this as an example so when we look at
reinforcement learning you need to be
very careful what you apply it to it
sounds like a good deal until you do
like a large neural network where you're
doing um you set the neural network to a
learning increment of one one so every
time it goes through it
learns and then you do your action so
you pick from the learning uh setup and
you actually try actions on the learning
setup until you get the what you think
is going to be the best action so you
actually feed what you think is right
back through the neural network there's
a whole layer there which is really fun
to play
with and then it has an output well
think of all those processes I mean that
is just a huge amount of work it's going
to do uh let's go ahead and Skip ahead
here give it a moment it's gonna take a
a minute or two to go ahead and
run now to train it uh we will went
ahead and let it run and it took a while
this this took um I got a pretty
powerful processor and it took about
five minutes plus to run it and we'll go
ahead and uh run our player setup on
here oops I brought in the last whoops I
brought in the last round so give me
just a moment to redo the policy save
there we go I forgot to save the policy
back in
there and then go ahead and run our
player again so we we've saved the
policy then we want to go ahead and load
the policy for P1 as the computer and we
can see the computer's gone in the
bottom right corner I'm going to go
ahead and go uh one one which is the
center and it's gone right up the top
and if you have ever played tic-tac-toe
you know the computer has me uh but
we'll go ahead and play it out out row
zero column
two there it is and then it's gone here
and so I'm going to go ahead and go row
01 two no 01 there we go and column zero
that's where I wanted oh and it says I
okay you your action there we go boom uh
so you can see here we've got a didn't
catch the win on this it said Tai um
kind of funny they didn't catch the win
on
there but if we play play this a bunch
of times you'll find it's going to win
more and more the more we train it the
more the reinforcement
happens this lengthy training process uh
is really the stopper on reinforcement
learning as this changes reinforcement
learning will be one of the more
powerful uh packages evolving over the
next decade or two in fact I would even
go as far as to say it is the most
important uh machine learning tool and
artificial intelligence tool out there
as it learns not only a simple Tic Tac
Toe board but we start learning
environments and the environment would
be like in language if you're
translating a language or something from
one language to the other so much of it
is lost if you don't know the context
it's in what's the environments it's in
and so being able to attach environment
and context and all those things
together is going to require
reinforcement learning to
do so again if you want to get a copy of
the Tic Tac Toe board it's kind of fun
to play with uh run it you can test it
out you can do um you know test it for
different uh uh values you can switch
from P1
computer uh where we loaded the policy
one to load the policy 2 and just see
how it varies there's all kinds of
things you can do on there supervised
learning users label data to train
machine learning models label data means
that the output is already known to you
the model just needs to map the inputs
to the outputs an example of supervised
learning can be to train a machine that
identifies the image of an animal below
below you can see we have a trained
model that identifies the picture of a
cat unsupervised learning uses
unlabelled data to train machines
unlabeled data means there is no fixed
output variable the model learns from
the data discovers patterns and features
in the data and Returns the output here
is an example of an unsupervised
learning technique that uses the images
of vehicles to classify if it's a bus or
a truck so the model learns by
identifying the paths of a vehicle such
as the length and width of the vehicle
the front and rear end covers roof hoods
the types of Wheels used Etc based on
these features the model classifies if
the vehicle is a bus or a
truck reinforcement learning trains a
machine to take suitable actions and
maximize reward in a particular
situation it uses an agent and an
environment to produce actions and
rewards the agent has a start and an end
state but there might be different paths
for reaching the end State like a maze
in this learning technique there is no
predefined target variable an example of
reinforcement learning is to train a
machine that can identify the shape of
an object given a list of different
objects such as square triangle
rectangle or a circle in the example
shown the model tries to predict the
shape of the object which is a square
here now let's look at the different
machine learning algorithms that come
under these learning techniques some of
the commonly used supervised learning
algorithm orms are linear regression
logistic regression support Vector
machines K nearest neighbors decision
tree random forest and knife
base examples of unsupervised learning
algorithms are K means clustering
hierarchical clustering DB scan
principle component analysis and others
choosing the right algorithm depends on
the type of problem you're trying to
solve some of the important
reinforcement learning algorithms are Q
learning Monte Carlo sarsa and deep Q
Network now let's look at the approach
in which these machine learning
techniques
work so supervised learning takes
labeled inputs and Maps it to known
outputs which means you already know the
target
variable unsupervised learning finds
patterns and understands the trends in
the data to discover the output so the
model tries to label the data based on
the features of the input
data while reinforcement learning
follows trial and error method to get
the desired solution after accomplishing
a task the agent receives an award an
example could be to train a dog to catch
the ball if the dog learns to catch a
ball you give it a reward such as a
biscuit now let's discuss the training
process for each of these learning
methods so supervised learning methods
need external supervision to train
machine learning models and hence the
name
supervised they need guidance and
additional information to return the
result unsup wise learning techniques do
not need any supervision to train models
they learn on their own and predict the
output similarly reinforcement learning
methods do not need any supervision to
train machine learning models and with
that let's focus on the types of
problems that can be solved using these
three types of machine learning
techniques so supervised learning is
generally used for classification and
regression problems we'll see the
examples in the next
slide and unsupervised learning is used
for cluster and Association problems
while reinforcement learning is
reward-based so for every task or for
every step completed there will be a
reward received by the agent and if the
task is not achieved correctly there
will be some penalty
used now let's look at a few
applications of supervised unsupervised
and reinforcement
learning as we saw earlier supervised
learning are used to solve
classification and regression problems
for example You can predict the weather
for a particular day based on humidity
precipitation wind speed and pressure
values you can use supervised learning
algorithms to forecast sales for the
next month or the next quarter for
different products similarly you can use
it for stock price analysis or
identifying if a cancer cell is
malignant or
benign now talking about the
applications of unsupervised learning we
have customer segmentation So based on
customer Behavior likes dislikes and
interest you can segment and cluster
similar customers into a group another
example where unsupervised learning
algorithms are used is customer churn
analysis now let's see what applications
we have in reinforcement learning so
reinforcement learning algorithms are
widely used in the gaming Industries to
build games it is also used to train
robots to perform human tasks if you are
an aspiring data scientist who's looking
out for online training and
certification in data science from the
best universities and Industry experts
then search no more simply learns
postgraduate program in data science
from calch University in collaboration
with IBM should be the right choice for
more details on this program please use
the link in the description box below
often professionals want to know if
there is a relationship between two or
more variables for instance is there a
relationship between the grade on the
third French exam a student takes and
the grade on the final exam if yes then
how is it related and how strongly
regression can be used here to arrive at
a conclusion this is an example of by
variate data that is two variables
however statisticians are mostly
interested in multivariate data
regression analysis is used to predict
the value of one variable the dependent
variable on the basis of other variables
the independent variables in the
simplest form of regression linear
regression you work with one independent
variable the formula for for simple
linear regression is shown on the screen
in the next screen we'll look at a few
examples of regression
analysis regression analysis is used in
several situations such as those
described on the screen in example one
using the data given on the screen you
have to analyze the relation between the
size of a house and its selling price
for a realator in example two you need
to predict the exam scores of students
who study for 7.2 hours with the help of
the data shown on the slide
a couple more examples are given on the
screen in example three based on the
expected number of customers and the
previous day's data given you need to
predict the number of burgers that will
be sold by a KFC Outlet in example four
you have to calculate the life
expectancy for a group of people with
the average length of schooling based on
the data
given let's look at the two main types
of regression analysis simple linear
regression and multiple linear
regression both both of these
statistical methods use a linear
equation to model the relationship
between two or more variables simple
linear regression considers one
quantitative and independent variable X
to predict the other quantitative but
dependent variable y multiple linear
regression considers more than one
quantitative and qualitative variable to
predict a quantitative and dependent
variable y we'll look at the two types
of analyses in more detail in the slides
that follow
in simple linear regression the
predictions of the explained variable y
when plotted as a function of the
explanatory variable X from a straight
line the best fitting line is called the
regression line the output of this model
is a function to predict the dependent
variable on the basis of the values of
the independent variable the dependent
variable is continuous and the
independent variable can be continuous
or discreet let's look at the different
kinds of linear and nonlinear analyses
list of linear techniques are simple
method of least squares coefficient of
multiple determination standard error of
the estimate dummy variable and
interaction similarly there are many
nonlinear techniques available such as
polinomial logarithmic square root
reciprocal and exponential to understand
this model we'll first look at a few
assumptions the simple linear regression
model depicts the relationship between
one dependent and two or more
independent variables the assumptions
which justify the use of this model are
as follows linear and additive
relationship between the dependent and
independent variables multivariate
normality little or no collinearity in
the data little or no autocorrelation in
the data homocedasticity that is
variance of Errors same across all
values of X the equation for this model
is shown on the screen a more
descriptive graphical representation of
simple linear regression is given on the
screen beta KN represents the slope a
slope with two variables implies that
one unit changes in X result in a two
unit change in y beta 1 represents the
estimated change in the average value of
y as a result of one unit change in X
Epsilon represents the estimated average
value of y when the value of x is
zero this demo will show the steps to do
simple linear regression in
r in this demo you'll learn how to do
simple linear
regression let's use X and Y vectors
that we have created in the previous
demo we also ensured there exists a
relationship between X and Y visually by
plotting a
graph to build simple linear regression
model let's use the LM
function to see how the linear model
fits into X and Y let's plot the linear
line by the AB line
function let's use the predict function
to test or predict the linear model we
can pass a known variable to predict the
unknown
variables e
let's look at an example of a common use
for linear regression profit estimation
of a company if I was going to invest in
a company I would like to know how much
money I could expect to make so we'll
take a look at a venture capitalist firm
and try to understand which companies
they should invest in so we'll take the
idea that we need to decide the
companies to invest in we need to
predict The Profit the company makes and
we're going to do it based on the
company's expenses and even just a
specific expense in this case we have
our company we have the different
expenses so we have our R&D which is
your research and development we have
our marketing uh we might have the
location we might have what kind of
administration is going through based on
all this different information we would
like to calculate the profit now in
actuality there's usually about 23 to 27
different markers that they look at if
they're a heavy Dy investor we're only
going to take a look at one basic one
we're going to come in and for
Simplicity let's consider a single
variable R&D and find out which
companies to invest in based on that so
when we take a R&D and we're plotting
The Profit based on the R&D expenditure
how much money they put into the
research and development and then we
look at the profit that goes with that
we can predict a line to estimate the
profit so we can draw a line right
through the data when you look at that
you can see how much they invest in the
R&D is a good marker as to how much
profit they're going to have we can also
note that companies spending more on R&D
make good profit so let's invest in the
ones that spend a higher rate in their
R&D what's in it for you first we'll
have an introduction to machine learning
followed by Machine learning algorithms
these will be specific to linear
regression and where it fits into the
larger model then we'll take a look at
applications of linear regression
understanding linear regression and
multiple linear regression finally we'll
roll up our sleeves and do a little
programming in use case profit
estimation of companies let's go ahead
and jump in let's start with our
introduction to machine learning along
with some machine learning algorithms
and where that fits in with linear
regression let's look at another example
of machine learning based on the amount
of rainfall how much would be the crop
yield so here we have our crops we have
our rainfall and we want to know how
much we're going to get from our crops
this year so we're going to introduce
two variables independent and dependent
the independent variable is a variable
whose value does not change by the
effect of other variables and is used to
manipulate the dependent variable it is
often denoted as X in our example
rainfall is the independent variable
this is a wonderful example because you
can easily see that we can't control the
rain but the rain does control the crop
so we talk about the independent
variable controlling the dependent
variable let's Define dependent variable
as a variable whose value change when
there is any manipulation the values of
the independent variables it is often
denoted as why and you can see here our
crop yield is dependent variable and it
is dependent on the amount of rainfall
received now that we've taken a look at
a real life example let's go a little
bit into the theory and some definitions
on machine learning and see how that
fits together with linear regression
numerical and categorical values let's
take our data coming in and this is kind
of random data from any kind of project
we want to divide it up into numerical
and categorical so numerical is numbers
age salary height where categorical
would be a description the color a dog's
breed gender categorical is limited to
very specific items where numerical is a
range of information now that you've
seen the difference between numerical
and categorical data let's take a look
at some different machine learning
definitions when we look at our
different machine learning algorithms we
can divide them into three areas
supervised
unsupervised reinforcement we we're only
going to look at supervised today
unsupervised means we don't have the
answers and we're just grouping things
reinforcement is where we give positive
and negative feedback to our algorithm
to program it and it doesn't have the
information till after the fact but
today we're just looking at supervised
because that's where linear regression
fits in in supervised data we have our
data already there and our answers for a
group and then we use that to program
our model and come up with an answer the
two most common uses for that is through
the regression and classif ification now
we're doing linear regression so we're
just going to focus on the regression
side and in the regression we have
SIMPLE linear regression we have
multiple linear regression and we have
polom linear regression now on these
three simple linear regression is the
examples we've looked at so far where we
have a lot of data and we draw a
straight line through it multiple linear
regression means we have multiple
variables remember where we had the
rainfall and the crops we might add
additional variables in there like how
much food do we give our crops when do
we Harvest them those would be
additional information add into our
model and that's why it' be multiple
linear regression and finally we have
polinomial linear regression that is
instead of drawing a line we can draw a
curved line through it now that you see
where regression model fits into the
machine learning algorithms and we're
specifically looking at linear
regression let's go ahead and take a
look at applications for linear
regression let's look at a few
applications of linear regression
economic growth used to determine the
economic growth of a country or a state
in the coming quarter can also be used
to predict the GDP of a country product
price can be used to predict what would
be the price of a product in the future
we can guess whether it's going to go up
or down or should I buy today housing
sales to estimate the number of houses a
builder would sell and what price in the
coming months score predictions Cricut
fever to predict the number of runs a
player would score in the coming matches
based on the previous performance I'm
I'm sure you can figure out other
applications you could use linear
regression for so let's jump in and
let's understand linear regression and
dig into the theory understanding linear
regression linear regression is the
statistical model used to predict the
relationship between independent and
dependent variables by examining two
factors the first important one is which
variables in particular are significant
predictors of the outcome variable and
the second one that we need to look at
close is how significant is the
regression line to make predictions with
the highest possible accuracy if it's
inaccurate we can't use it so it's very
important we find out the most accurate
line we can get since linear regression
is based on drawing a line through data
we're going to jump back and take a look
at some ukian geometry the simplest form
of a simple linear regression equation
with one dependent and one independent
variable is represented by y = m * x + C
and if you look at our model here we
plotted two points on here uh X1 and y1
X2 and Y2 y being the dependent variable
remember that from before and X being
the independent variable so y depends on
whatever X is m in this case is the
slope of the line where m equals the
difference in the Y 2 minus y1 and X2 -
X1 and finally we have C which is the
coefficient of the line or where happens
to cross the zero axis let's go back and
look at an example we used earlier of
linear regression we're going to go back
to plotting the amount of crop yield
based on the amount of rainfall and here
we have our rainfall remember we cannot
change rainfall and we have our crop
yield which is dependent on the rainfall
so we have our independent and our
dependent variables we're going to take
this and draw a line through it as best
we can through the middle of the data
and then we look at that that we put the
red point on the y axis is the amount of
crop yield you can expect for the amount
of rainfall represented by the Green Dot
so if we have an idea what the rainfall
is for this year and what's going on we
can guess how good our crops are going
to be and we've created a nice line
right through the middle to give us a
nice mathematical formula let's take a
look and see what the math looks like
behind this let's look at the intuition
behind the regression line now before we
dive into the math and the formulas that
go behind this and what's going on
behind the scenes I want you to note
that when we get into the case study and
we actually apply some python script
that this math that you're going to see
here is already done automatically for
you you don't have to have it memorized
it is however good to have an idea
what's going on so if people reference
the different terms you'll know what
they're talking about let's consider a
sample data set with five rows and find
out how to draw the regression line
we're only going to do five rows because
if we did like the rainfall with
hundreds of points of data that would be
very hard to see what's going on with
the mathematics so we'll go ahead and
create our own two sets of data and we
have our independent variable X and our
dependent variable Y and when X was one
we got Y = 2 when X was uh 2 y was 4 and
so on and so on if we go ahead and plot
this data on a graph we can see how it
forms a nice line through the middle you
can see where it's kind of grouped going
going upwards to the right the next
thing we want to know is what the means
is of each of the data coming in the X
and the Y the means doesn't mean
anything other than the average so we
add up all the numbers and divide by the
total so 1 plus 2 plus 3 + 4 + 5 over 5
= 3 and the same for y we get four if we
go ahead and plot the means on the graph
we'll see we get 3 comma 4 which draws a
nice line down the middle a good
estimate here we're going to dig deeper
into to the math behind the regression
line now remember before I said you
don't have to have all these formulas
memorized or fully understand them even
though we're going to go into a little
more detail of how it works and if
you're not a math whz and you don't know
if you've never seen the sigma character
before which looks a little bit like an
e that's opened up that just means
summation that's all that is so when you
see the sigma character just means we're
adding everything in that row and for
computers this is great because as a
programmer you can easily iterate
through each of the XY points and create
all the information you need so in the
top half you can see where we've broken
that down into pieces and as it goes
through the first two points it computes
the squared value of x the squared value
of y and x * Y and then it takes all of
X and adds them up all of Y adds them up
all of X squ adds them up and so on and
so on and you can see we have the sum of
equal to 15 the sum is equal to 20 all
the way up to x * Y where the sum equals
66 this all comes from our formula for
calculating a straight line where y
equal the slope * X plus the coefficient
C so we go down below and we're going to
compute more like the averages of these
and we're going to explain exactly what
that is in just a minute and where that
information comes from is called the
square means error but we'll go into
that in detail in a few minutes all you
need to do is look at the formula and
see how we've gone about Computing it
line by line instead of trying to have a
huge set of numbers pushed into it and
down here you'll see where the slope m
equals and on the top part if you read
through the brackets you have the number
of data points times the sum of x * Y
which we computed one line at a time
there and that's just the 66 and take
all that and you subtract it from the
sum of x times the sum of Y and those
have both been computed so you have 15 *
20 and on the bottom we have the number
of lines times the sum of X2 easily
computed as 86 for the sum minus I'll
take all that and subtract the sum of
x^2 and we end up as we come across with
our formula you can plug in all those
numbers which is very easy to do on the
computer you don't have to do the math
on a piece of paper or calculator and
you'll get a slope of 6 and you'll get
your C coefficient if you continue to
follow through that formula you'll see
it comes out as equal to 2.2 continuing
deeper into what's going behind the
scenes let's find out the predicted
values of Y for corresponding values of
X using the linear equation where M = 6
and C = 2.2 we're going to take these
values and we're going to go ahead and
plot them we're going to predict them so
y = 6 * where x = 1 + 2.2 equal 2.8 so
on and so on and here the Blue Points
represent the actual y values and the
brown points represent the predicted y
values based on the model we created the
distance between the actual and
predicted values is known as residuals
or errors the best fit line should have
the least sum of squares of these errors
also known as e squ if we put these into
a nice chart where you can see X and you
can see Y where the actual values were
and you can see y predicted you can
easily see where we take Yus y predicted
and we get an answer what is the
difference between those two and if we
square that Yus y prediction squared we
can then sum those squared values that's
where we get the 64 plus the 36 + 1 all
the way down until we have a summation
equals 2.4 so the sum of squared errors
for this regression line is 2.4 we check
this error for each line and conclude
the best fit line having the least e
Square value in a nice graphical
representation we can see here where we
keep moving this line through the data
points to make sure the best fit line
has the least Square distance between
the data points and the regression line
now we only looked at the most common
commonly used formula for minimizing the
distance there are lots of ways to
minimize a distance between the line and
the data points like sum of squared
errors sum of absolute errors root mean
square error Etc what you want to take
away from this is whatever formula is
being used you can easily using a
computer programming and iterating
through the data calculate the different
parts of it that way these complicated
formulas you see with the different
summations and absolute values are
easily computed one piece at a time up
until this point we've only been looking
at two values X and Y well in the real
world it's very rare that you only have
two values when you're figuring out a
solution so let's move on to the next
topic multiple linear regression let's
take a brief look at what happens when
you have multiple inputs so in multiple
linear regression we have uh well we'll
start with the simple linear regression
where we had y = m + x + C and we're
trying to find the value of y now with
multiple linear regression we have
multiple variables coming in so instead
of having just X we have X1 X2 X3 and
instead of having just one slope each
variable has its own slope attached to
it as you can see here we have M1 M2 M3
and we still just have the single
coefficient so when you're dealing with
multiple linear regression you basically
take your single linear regression and
you spread it out so you have y = M1 *
X1 + M2 * X to so on all the way to m to
the nth x to the nth and then you add
your coefficient on there implementation
of linear regression now we get into my
favorite part let's understand how
multiple linear regression works by
implementing it in Python if you
remember before we were looking at a
company and just based on its R&D trying
to figure out its profit we're going to
start looking at the expenditure of the
company we're going to go back to that
we're going to predict his profit but
instead of predicting it just on the R&D
we're going to look at other factors
like Administration costs marketing
costs and so on and from there we're
going to see if we can figure out what
the profit of that company is going to
be to start our coding we're going to
begin by importing some basic libraries
and we're going to be looking through
the data before we do any kind of linear
regression we're going to take a look at
the data see what we're playing with
then we'll go ahead and format the data
to the format we need to be able to run
it in the linear regression model and
then from there we'll go ahead and solve
it and just see how valid our solution
is so let's start with importing the
basic libraries now I'm going to be
doing this in Anaconda Jupiter notebook
a very popular IDE I enjoy it CU it's
such a visual to look at and so easy to
use um just any ID for python will work
just fine for this so break out your
favorite python IDE so here we are in
our Jupiter notebook let me go ahead and
paste our first piece of code in there
and let's walk through what libraries
we're importing first we're going to
import numpy as NP and then I want you
should to skip one line and look at
import pandas as PD these are very
common tools that you need with most of
your linear regression the numpy which
stands for number python is usually
denoted as NP and you have to almost
have that for your sklearn toolbox you
always import that right off the
beginning pandas although you don't have
to have it for your sklearn libraries it
does such a wonderful job of importing
data setting it up into a data frame so
we can manipulate it rather easily and
it has a lot of tools also in addition
to that so we usually like to use the
pandas when we can and I'll show you
what that looks like the other three
lines are for us to get a visual of this
data and take a look at it so we're
going to import matplot library. pyplot
as PLT and then caborn as SNS caborn
works with the matplot library so you
have to always import matplot library
and then caborn sits on top of it and
we'll take a look at what that looks
like you could use any of your own
plotting libraries you want there's all
kinds of ways to look at the data these
are just very common ones and the caborn
is so easy to use it just looks
beautiful it's a nice representation
that you can actually take and show
somebody and the final line is the Amber
sign matap plot Library inline that is
only because I'm doing an inline IDE my
interface in the Anaconda Jupiter
notebook requires I put that in there or
you're not going to see the graph when
it comes up let's go ahead and run this
it's not going to be that interesting so
we're just setting up variables in fact
it's not going to do anything that we
can see but it is importing these
different libraries and setup the the
next step is load the data set and
extract independent and dependent
variables now here in the slide you'll
see companies equals pd. read CSV and it
has a long line there with the file at
the end 1,000 companies. CSV you're
going to have to changes to fit whatever
setup you have and the file itself you
can request just go down to the
commentary below this video and put a
note in there and simply learn we'll try
to get in contact with you and Supply
you with that file so you can try this
coding yourself so we're going to add
this code in here and we're going to see
that I have companies equals pd. reader
CSV and I've changed this path to match
my computer c/s simplylearn
1000 companies. CSV and then below there
we're going to set the x equals to
companies under the iocation and because
this is companies is a PD data set I can
use this nice notation that says take
every row that's what the colon the
first colon is comma except for the last
column that's what the second part is
where we have a colon minus one and we
want the values set into there so X is
no longer a data set a pandis data set
but we can easily extract the data from
our pandis data set with this notation
and then y we're going to set equal to
the last row well the question is going
to be what are we actually looking at so
let's go ahead and take a look at that
and we're going to look at the
companies. head which lists the first
five rows of data and I'll open up the
file in just a second so you can see
where that's coming from but let's look
at the data in here as far as the way
the panda sees it when I hit run you'll
see it breaks it out into a nice setup
this is what pandas one of the things
pandas is really good about is it looks
just like an Excel spreadsheet you have
your rows and remember when we're
programming we always start with zero we
don't start with one so it shows the
first five rows 0 1 2 3 4 and then it
shows your different columns R&D spin
Administration marketing spend State
profit it even notes that the top are
column Nam it was never told that but
pandas is able to recognize a lot of
things that they're not the same as the
data rows why don't we go ahead and open
this file up in a CSV so you can
actually see the raw data so here I've
opened it up as a text editor and you
can see at the top we have R&D spin
comma Administration comma marketing
spin comma State comma profit carriage
return I don't know about you but I'd go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an Excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is where I can see
the data to me it doesn't mean a whole
lot maybe you're an expert in business
and Investments and you understand what
$165,300 compared to the administration
cost of
$136,800 so on so on helps to create the
profit of
92261 83 that makes no sense to me
whatsoever no pun intended so let's flip
back here and take a look at our next
set of code where we're going to graph
it so we can get a better understanding
of our data and what it means so at this
point we're going to use a single line
of code to get a lot of information so
we can see where we're going with this
let's go ahead and paste that into our
uh notebook and see what we got going
and so we have the visualization and
again we're using SNS which is pandas as
you you can see we imported the map plot
library. pyplot as PLT which then the
caborn uses and we imported the caborn
as SNS and then that final line of code
helps us show this in our um inline
coding without this it wouldn't display
and you can display it to a file in
other means and that's the matap plot
library in line with the Amber sign at
the beginning so here we come down to
the single line of code caborn is great
because it actually recognizes the panda
data frame so I can just take the
companies. four for coordinates and I
can put that right into the Seaborn and
when we run this we get this beautiful
plot and let's just take a look at what
this plot means if you look at this plot
on mine the colors are probably a little
bit more purplish and blue than the
original one uh we have the columns in
the rows we have R and D spending we
have Administration we have marketing
spending and profit and if you cross
index any two of these since we're
interested in profit if you cross index
profit with profit it's going to show up
if you look at the scale on the right
way up in the dark why because those are
the same data they have an exact
correspondence so r& D spending is going
to be the same as R&D spending and the
same thing with Administration cost so
right down the middle you get this dark
row or dark um diagonal row that shows
that this is the highest corresponding
data that's exactly the same and as it
becomes lighter there's less connections
between the data so we can see with
profit obviously profit is the same as
profit and next it has a very high
correlation with R&D spending which we
looked at earlier and it has a slightly
less connection to marketing spending
and even less to how much money we put
into the administration so now that we
have a nice look at the data let's go
ahead and dig in and create some actual
useful linear regression models so that
we can predict values and have a better
profit now that we've taken a look at
the visualization of this data we're
going to move on to the next step
instead of just having a pretty picture
we need to generate some hard data some
hard values so let's see what that looks
like we're going to set up our linear
regression model in two steps the first
one is we need to prepare some of our
data so it fits correctly and let's go
ahead and paste this code into our
Jupiter notebook and what we're bringing
in is we're going to bring in the
sklearn pre-processing where we're going
to import the label encoder and the one
hot encoder to use the label encoder
we're going to create a variable called
label encoder and set it equal to
capital L label capital E encoder this
this creates a class that we can reuse
for transferring the labels back and
forth now about now you should ask what
labels are we talking about let's go
take a look at the data we processed
before and see what I'm talking about
here if you remember when we did the
companies. head and we printed the top
five rows of data we have our columns
going across we have column zero which
is R&D spending column one which is
Administration column two which is
marketing spending and column three is
State and you'll see under State we have
New York York California Florida now to
do a linear regression model it doesn't
know how to process New York it knows
how to process a number so the first
thing we're going to do is we're going
to change that New York California and
Florida and we're going to change those
to numbers that's what this line of code
does here x equals and then it has the
colon comma 3 in Brackets the first part
the colon comma means that we're going
to look at all the different rows so
we're going to keep them all together
but the only row we're going to edit is
the third row and in there we're going
take the label coder and we're going to
fit and transform the X also the third
row so we're going to take that third
row we're going to set it equal to a
transformation and that transformation
basically tells it that instead of
having a uh New York it has a zero or
one or a two and then finally we need to
do a one hot encoder which equals one
hot encoder categorical features equals
three and then we take the X and we go
ahead and do that equal to one hot
encoder fit transform X to array this
final transformation preps our data for
us so it's completely set the way we
need it as just a row of numbers even
though it's not in here let's go ahead
and print X and just take a look what
this data is doing you'll see you have
an array of arrays and then each array
is a row of numbers and if I go ahead
and just do row zero you'll see I have a
nice organized row of numbers that the
computer now understands we'll go ahead
and take this out there because it
doesn't mean a whole lot to us it's just
a row of numbers next on setting up our
data we have avoiding dummy variable
trap this is very important why because
the computer is automatically
transformed our header into the setup
and it's automatically transformed all
these different variables so when we did
the encoder the encoder created two
columns and what we need to do is just
have the one because it has both the
variable and the name that's what this
piece of code does here let's go ahead
and paste this in here and we have xal X
colon comma 1 colon all this is doing is
removing that one extra column we put in
there when we did our one hot encoder
and our label encoding let's go ahead
and run that and now we get to create
our linear regression model and let's
see what that looks like here and we're
going to do that in two steps the first
step is going to be in splitting the
data now whenever we create a uh
predictive model of data we always want
to split it up so we have a training set
and we have a testing set that's very
important otherwise we'd be very ethical
without testing it to see how good our
fit is and then we'll go ahead and
create our multiple linear regression
model and train it and set it up let's
go ahead and paste this next piece of
code in here and I'll go ahead and
shrink it down a Siz or two so it all
fits on one line so from the sklearn
module selection we're going to import
train test split and you'll see that
we've created four completely different
variables we have capital x train
capital X test smaller case y train
smaller case y test
that is the standard way that they
usually reference these when we're doing
different uh models usually see that a
capital x and you see the train and the
test and the lowercase Y what this is is
X is our data going in that's our R&D
spin our Administration our marketing
and then Y which we're training is the
answer that's the profit because we want
to know the profit of an unknown entity
so that's what we're going to shoot for
in this tutorial the next part train
test split we take X and we take y we've
already created those X has the columns
with the data in it and Y has a column
with profit in it and then we're going
to set the test size equals 0.2 that
basically means 20% So 20% of the rows
are going to be tested we're going to
put them off to the side so since we're
using a th000 lines of data that means
that 200 of those lines we're going to
hold off to the side to test for later
and then the random State equals zero
we're going to randomize which ones it
picks to hold off to the side we'll go
ahead and run this it's not overly
exciting it's setting up our variables
but the next step is the next step we
actually create our linear regression
model now that we got to the linear
regression model we get that next piece
of the puzzle let's go a and put that
code in there and walk through it so
here we go we're going to paste it in
there and let's go ahead and since this
is a shorter line of code let's zoom up
there so we can get a good look and we
have from the SK learn. linear model
we're going to import linear regression
now I don't know if you recall from
earlier when we were doing all the math
let's go ahead and flip back there and
take a look at that do you remember this
where we had this long formula on the
bottom and we were doing all this suiz
and then we also looked at setting it up
with the different lines and then we
also looked all the way down to multiple
linear regression where we're adding all
those formulas together all of that is
wrapped up in this one section so what's
going on here is I'm going to create a
variable called regressor and the
regressor equals the linear regression
that's a linear regression model that
has all that math built in so we don't
have to have it all memorized or have to
compute it individually and then we do
the regressor do fet in this case we do
X train and Y train because we're using
the training data X being the data in
and Y being profit what we're looking at
and this does all that math for us so
within one click in one line we've
created the whole linear regression
model and we fit the data to the linear
regression model and you can see that
when I run the regressor it gives an
output linear regression it says copy
xals True Fit intercept equals true true
in jobs equal one normalize equals false
it's just giving you some general
information on what's going on with that
regressor model now that we've created
our linear regression model let's go
ahead and use it and if you remember we
kept a bunch of data aside so we're
going to do a y predict variable and
we're going to put in the X test and
let's see what that looks like scroll up
a little bit paste that in here
predicting the test set results so here
we have y predict equals regressor do
predict X test going in and this gives
us y predict now because I'm in Jupiter
in line I can just put the variable up
there and when I hit the Run button
it'll print that array out I could have
just as easily done print y predict so
if you're in a different IDE that's not
an inline setup like the Jupiter
notebook you can do it this way print y
predict and you'll see that for the 200
different test variables we kept off to
the side it's going to produce 200
answers this is what it says the profit
are for those 200 predictions but let's
don't stop there let's keep going and
take a couple look we're going to take
just a short detail here and calculating
the coefficients and the intercepts this
gives us a quick flash at what's going
on behind the line we're going to take a
short detour here and we're going to be
calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has a
coefficients for us and we can simply
just print regressor do coefficient
uncore when I run this you'll see see
our coefficients here and if we can do
the regressor coefficient we can also do
The regressor Intercept let's run that
and take a look at that this all came
from the multiple regression model and
we'll flip over so you can remember
where this is going into and where it's
coming from you can see the formula down
here where y = M1 * X1 + M2 * X2 and so
on and so on plus C the coefficient so
these variables fit right into this
formula y = slope 1 * column 1 variable
plus slope 2 * column 2 variable all the
way to the m into the n and x to the N
plus C the coefficient or in this case
you have -
8.89 to the power of 2 etc etc times the
First Column and the second column and
the third column and then our intercept
is the minus
10309 Point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure it's a valid model that this
model works and understand how good it
works so calculating the r s value
that's what we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in code and so we're going to use
this from SK learn. metrics we're going
to import R2 score that's the R squar
value we're looking at the error so in
the R2 score we take our y test versus
our y predict y test is the actual
values we're testing that was the one
that was given to us that we know are
true the Y predict of those 200 values
is what we think it was true and when we
go ahead and run this we see we get a
9352 that's the R2 score now it's not
exactly a straight percentage so it's
not saying it's 93% correct but you do
want that in the upper 90s oh and higher
shows that this is a very valid
prediction based on the R2 score and if
r squ value of 91 or 92 as we got on our
model remember it does have a random
generation involved this proves the
model is a good model which means
success yay we successfully trained our
model with certain predictors and
estimated the profit of the companies
using linear regression so now that we
have a successful linear regression
model let's take a look at what we went
over today and take a look at our key
take ways first if you are an aspiring
data scientist who's looking out for
online training and certification in
data science from the best universities
and Industry experts then search no more
simply learns postgraduate program in
data science from calch University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below what is logistic
regression let's say we have to build a
predictive model or a machine learning
model to predict whether the passengers
of the Titanic ship have survived or not
the ship R so how do we do that so we
use logistic regression to build a model
for this how do we use logistic
regression so we have the information
about the passengers their ID whether
they have survived or not their class
and name and so on and so forth and we
use this information where we already
know whether the person has survived or
not that is the labeled information and
we help the system to train based on
this information based on this labeled
data this is known as labeled data and
during the process of building the model
we probably will remove some of the
non-essential parameters or attributes
here we only take those attributes which
are really required to make these
predictions and once we train the model
we run new data through it whereby the
model will predict whether the passenger
has survived or not so let's see what we
will learn in this video we will talk
about what is supervised learning and we
will go into details about
classification which is one of the
techniques for supervised learning and
then we will further focus on logistic
regression is which is one of the
algorithms for performing classification
especially binary classification then we
will compare linear and logistic
regression and what are some of the
logistic regression applications and
finally we will end with the use case or
a demo of actual python code for doing
logistic regression in Jupiter no all
right so let's start with what is
supervised learning supervised learning
is one of the two main types of machine
learning methods here we use what is
known as labeled data to help the system
learn this is very similar to how we
human beings learn so let's say you want
to teach a child to recognize an apple
how do we do that we never tell the
child okay this is an apple has a
certain diameter on the top certain
diameter at the bottom and this has a
certain RGB color no we just show an
apple to the child and tell the child
this is Apple and then next time when we
show an apple child immediately
recognizes yes this is an apple
supervised learning works very similar
on the similar lines so where does
logistic regression fit into the overall
machine learning process machine Lear
learning is divided into two types
mainly two types there is a third one
called reinforcement learning but we
will not talk about that right now so
one is supervised learning and the other
is unsupervised learning unsupervised
learning uses techniques like clustering
and Association and supervised learning
uses techniques like classification and
regression now supervised learning is
used when you have labeled data you have
historical data then you use supervised
learning when you don't have labeled
data than you used unsupervised learning
it's in supervised learning there are
two types of techniques that are used
classification and regression based on
what is the kind of problem we are
solved let's say we want to take the
data and classify it it could be binary
classification like a zero or a one an
example of classification we have just
seen whether the passenger has survived
or not survived like a zero or one that
is known as binary classification
regression on the other hand is you need
to predict a value what is known as a
continuous value classification is for
discrete values regression is for
continuous values let's say you want to
predict the share price or you want to
predict the temperature that will be
there what will be the temperature
tomorrow that is where you use
regression whereas classification are
discrete values is will the customer buy
the product or will not buy the product
will you get a promotion or you will not
get a promotion I hope you're getting
the idea or it could be multiclass
classification as well let's say you
want to build an image classification
model so the image classification model
would take an image as an inut and
classify into multiple classes whether
this image is of a cat or a dog or an
elephant or a tiger so there are
multiple classes so not necessarily
binary classification so that is known
as multiclass classification so we are
going to focus on classification because
logistic regression is one one of the
algorithms used for classification now
the name may be a little confusing in
fact whenever people come across
logistic regression it always causes
confusion because the name has
regression in it but we are actually
using this for performing classification
okay so yes it is logistic regression
but it is used for classification and in
case you wondering is there something
similar for regression yes for
regression we have linear regression
keep that in mind so linear regression
is used for regression logistic
regression is used for classification so
in this video we are going to focus on
supervised learning and within
supervised learning we're going to focus
on classification and then within
classification we are going to focus on
logistic regression algorithm so first
of all classification so what are the
various algorithms available for
performing classification the first one
is decision tree there are of course
multiple algorithms but here we will
talk about a few decision trees are
quite popular and very easy toand and
therefore they used for classification
then we have K nearest neighbors this is
another algorithm for performing
classification and then there is
logistic regression and this is what we
are going to focus on in this video and
we are going to go into a little bit of
details about logistic regression all
right what is logistic regression as I
mentioned earlier logistic regression is
an algorithm for performing binary
classification so let's take an example
and see how this works let's say your
car has not been serviced for quite a
few years and now you want to find out
if it it's going to break down in the
near future so this is like a
classification problem find out whether
your car will break down or not so how
are we going to perform this
classification so here's how it looks if
we plot the information along the X and
Y access X is the number of years since
the last service was performed and Y is
the probability of your car breaking
down and let's say this information was
this data rather was collected from
several car users it's not just your car
but several car users so that is our
labeled data so the data has been
collected and um for for the number of
years and when the car broke down and
what was the probability and that has
been plotted along X and Y AIS so this
provides an idea or from this graph we
can find out whether your car will break
down or not we'll see how so first of
all the probability can go from 0 to one
as you all aware probability can be
between 0er and one and as we can
imagine it is intuitive as well as the
number of years are on the Lower Side
maybe one year 2 years or 3 years till
after the service the chances of your
car breaking down are very limited right
so for example chances of your car
breaking down on the probability of your
car breaking down within 2 years of your
last service are 0.1 probability
similarly 3 years is maybe3 and so on
but as the number of years increases
let's say if it was six or S years there
is almost a certainty that your car is
going to break down that is what this
graph shows so this is an example of a
application of the classification
algorithm and we will see in little
details how exactly logistic regression
is applied here one more thing needs to
be added here is that the dependent
variables outcome is discrete so if we
are talking about whether the car is
going to break down or not so that is a
discrete value the Y that we are talking
about the dependent variable that we are
talking about what we are looking at is
whether the car is going to break down
or not yes or no that is what we are
talking about so here the outcome is
discrete and not a continuous value so
this is how the logistic regression
curve looks let me explain a little bit
what exactly and how exactly we are
going to uh determine the class at the
outcome rather so for a logistic
regression curve a threshold has to be
set saying that because this is a
probability calculation remember this is
a probability calculation and the
probability itself will not be Zer or 1
but based on the probability we need to
decide what the outcome should be so
there has to be a threshold like for
example 0.5 can be the threshold let's
say in this case so any value of the
probability below5 is considered to be
zero and any value above 0.5 is
considered to be one so an output of
let's
say8 will mean that the car will break
down so that is considered as an output
of one and let's say an output of 0. 29
is considered as zero which means that
the car will not break down so that's
the way logistic regression works now
let's do a quick comparison between
logistic regression and linear
regression because they both have the
term regression in them so it can cause
confusion so let's try to remove that
confusion so what is linear regression
linear regression is a process is once
again an algorithm for wise learning
however here you're going to find a
continuous value you're going to
determine a continuous value it could be
the price of a real estate property it
could be your hike how much hike you're
going to get or it could be a stock
price these are all continuous values
these are not discrete compared to a yes
or a no kind of a response that we are
looking for in logistic regression so
this is one example of a linear
regression let's say at the HR team of a
company tries to find out what should be
the salary hike of an employee so they
collect all the details of their
existing employees their ratings and
their salary hikes what has been given
and that is the labeled information that
is available and the system learns from
this it is trained and it learns from
this labeled information so that when a
new employees information is fed based
on the rating it will determine what
should be the height so this is a linear
regression problem and a linear
regression example now salary is a
continous value you can get 5,000
5,500 5,600 it is not discrete like a
cat or a dog or an apple or a banana
these are discrete or a yes or a no
these are discrete values right so this
where you are trying to find continuous
values is where we use linear regression
so let's say just to extend on the
scenario we now want to find out whether
this employee is going to get a
promotion or not so we want to find out
that is a discrete problem right a yes
or no kind of a problem in this case we
actually cannot use linear regression
even though we may have labeled data so
this is the labeled data So based on the
employee rating these are the ratings
and then some people got the promotion
and this is the ratings for which people
did not get promotion that is a no and
this is the rating for which people got
promotion we just plotted the the data
about whether a person has got an
employee has got promotion or not yes no
right so there is nothing in between and
what is the employees rating okay and
ratings can be continuous that is not an
issue but the output is discrete in this
case whether employee got promotion yes
no okay so if we try to plot that and we
try to find a straight line this is how
it would look and as you can see doesn't
look very right because looks like there
will be lot of Errors do root mean
square error if you remember for linear
regression would be very very high and
also the the values cannot go beyond
zero or Beyond one so the graph should
probably look somewhat like this clipped
at 0o and one but still the straight
line doesn't look right therefore
instead of using a linear equation we
need to come up with something different
and therefore the logistic regression
model looks somewhat like this so we
calculate the probability and if we plot
that probability not in the form of a
straight line but we need to use some
other equation we will see very soon
what that equation is then it is a
gradual process right so you see here
people with some of these ratings are
not getting any promotions and then
slowly uh at certain rating they get
promotion so that is a gradual process
and uh this is how the math behind
logistic regression looks so we are
trying to find find the odds for a
particular event happening and this is
the formula for finding the odds so the
probability of an event happening
divided by the probability of the event
not happening so P if it is the
probability of the event happening
probability of the person getting a
promotion and divided by the probability
of the person not getting a promotion
that is 1 minus
P so this is how you measure the odds
now the values of the odds range from 0
to Infinity so when this probability is
zero then the odds will the value of the
odds is equal to zero and when the
probability becomes one then the value
of the odds is 1 by 0 that will be
Infinity but the probability itself
remains between 0 and 1 now this is how
an equation of a straight line Looks So
Y is equal to Beta 0 plus beta 1 x where
beta 0 is the Y intercept and beta 1 is
the slope of the line if we take the
odds equation and take a log of both
sides then this would look somewhat like
this and the term logistic is actually
derived from the fact that we are doing
this we take a log of PX by 1 minus PX
this is an extension of the calculation
of odds that we have seen right and that
is equal to Beta 0 + beta 1 x which is
the equation of the straight line and
now from here if you want to find out
the value of PX you will see we we can
take the exponential on both sides and
then if we solve that equation we will
get the equation of PX like this PX is =
1 by 1 + e^ of- beta 0 + beta 1 x and
recall this is nothing but the equation
of the line which is equal to y y is
equal to Beta 0 + beta 1 x so that this
is the equation also known as the
sigmoid function and this is the
equation of the logistic regression Al
all right and if this is plotted this is
how the sigmoid curve is obtained so
let's compare linear and logistic
regression how they are different from
each other let's go back so linear
regression is solved or used to solve
regression problems and logistic
regression is used to solve
classification problems so both are
called regression but linear regression
is used for solving regression problems
where we predict continuous values
whereas log regression is used for
solving classification problems where we
have had to predict discrete values the
response variables in case of linear
regression are continuous in nature
whereas here they are categorical or
discrete in nature and um linear
regression helps to estimate the
dependent variable when there is a
change in the independent variable
whereas here in case of logistic
regression it helps to calculate the
probability or the possibility of a
particular event happen happening and
linear regression as the name suggests
is a straight line that's why it's
called linear regression whereas
logistic regression is a sigmoid
function and the curve is the shape of
the curve is s it's an s-shaped curve
this is another example of application
of logistic regression in weather
prediction whether it's going to rain or
not rain now keep in mind both are used
in weather prediction if we want to find
the discrete values like whether it's
going to rain or not r brain that is a
classification problem we use logistic
regression but if we want to determine
what is going to be the temperature
tomorrow then we use linear regression
so just keep in mind that in weather
prediction we actually use both but
these are some examples of logistic
regression so we want to find out
whether it's going to be rain or not
it's going to be sunny or not whe it's
going to snow or not these are all
logistic regression examples a few more
examples classification of objects this
is a again another example of logis
regression now here of course one
distinction is that these are multiclass
classification so logistic regression is
not used in its original form but it is
used in a slightly different form so we
say whether it is a dog or not a dog I
hope you understand so instead of saying
is it a dog or a cat or elephant we
convert this into saying so because we
need to keep it to Binary classification
so we say is it a dog or not a dog is it
a cat or not a cat so that's the way
logistic regression can be used for
classifying objects otherwise there are
other techniques which can be used for
performing multiclass classification in
healthcare logistic regression is used
to find the survival rate of a patient
so they take multiple parameters like
trauma score and age and so on and so
forth and they try to predict the rate
of survival all right now finally let's
take an example and see how we can apply
logistic regression to predict the
number that is shown in the image so
this is actually a live demo I will take
you into Jupiter notebook and U show the
code but before that let me take you
through a couple of slides to explain
what we are trying to do so let's say
you have an 8 by8 image and the the
image has a number 1 2 3 4 and you need
to train your model to predict what this
number is so how do we do this so the
first thing is is obviously in any
machine learning process you train your
model so in this case we are using
logistic regression so and then we
provide a training set to train the
model and then we test how accurate our
model is with the test data which means
that like any machine learning process
we split our initial data into two parts
training set and test set with the
training set we train our model and then
with the test set we we test the model
till we get good accuracy and then we
use it for for for inference right so
that is typical methodology of uh uh
training testing and then deploying of
machine learning models so let's uh take
a look at the code and uh see what we
are doing so I'll not go line by line
but just take you through some of the
blocks so first thing we do is import
all the libraries and then we basically
take a look at the images and see what
is the total number of images we can
display using mat plot lip some of the
images a sample of these images and um
then we split the data into training and
test as I mentioned earlier and we can
do some exploratory analysis and uh then
we build our model we train our model
with the training set and then we test
it with our test set and find out how
accurate our model is using the
confusion Matrix the heat map and use
heat map for visualizing this and I will
show you in the code what exactly is the
confusion Matrix and how it can be used
for finding the accuracy in our example
we got we get an accuracy of about 0.94
which is pretty good or 94% which is
pretty good all right so what is a
confusion Matrix this is an example of a
confusion Matrix and uh this is used for
identifying the accuracy of a uh
classification model or like a logistic
regression model so the most important
part in a confusion Matrix is that first
of all this as you can see this is a
matrix and the size of the Matrix
depends on how many outputs we are
expecting right so the most important
part here is that the model will be most
accurate when we have the maximum
numbers in its diagonal like in this
case that's why it has almost 93 94%
because the diagonal should have the
maximum numbers and the others other
than diagonals the cells other than the
diagonals should have very few numbers
so here that's what is happening so
there is a two here there are there's a
one here but most of them are along the
diagonal this what does this mean this
means that the number that has been fed
is zero and the number that has being
detected is also zero so the predicted
value and the actual value are the same
so along the diagonals that is true
which means that let's let's take this
diagonal right if if the maximum number
is here that means that like here in
this case it is 34 which means that 34
of the images that have been fed or
rather actually there are two
misclassifications in there so 36 images
have been fed which have number four and
out of which 34 have been predicted
correctly as number four and one has
been predicted as number eight and
another one has been predicted as number
nine so these are two
misclassifications okay so that is the
meaning of saying that the maximum
number should be in the diagonal so if
you have all of them so for an ideal
model which has let's say 100% accuracy
everything will be only in the diagonal
there will be no numbers other than zero
in all other cells so that is like a
100% accurate model okay so that's uh
gist of how to use this Matrix how to
use this uh confusion Matrix I know the
name uh is a little funny sounding
confusion Matrix but actually it is not
very confusing it's very straightforward
so you are just plotting what has been
predicted and what is the labeled
information or what is the actual data
that's also known as the ground truth
sometimes okay these are some fancy
terms that are used so predicted label
and actual label that's all it is okay
yeah so we are showing a little bit more
information here so 38 have been
predicted and here you will see that all
of them have been predicted correctly
there have been 38 zeros and the the
predicted value and the actual value is
is exactly the same whereas in this case
right it has uh there are I think 37 + 5
yeah 42 have been fed the images 42
images are of Digit three and uh the
accuracy is only 37 of them have been
accurately predicted three of them have
been predicted as number seven and two
of them have been predicted as number
eight and so on and so forth okay all
right so with that let's go into Jupiter
notebook and see how the code looks so
this is the code in in Jupiter notebook
for logistic regression in this
particular demo what we are going to do
is train our model to recognize digits
which are the images which have digits
from let's say 0 to 5 or 0 to 9 and um
and then we will see how well it is
trained and whether it is able to
predict these numbers correctly or not
so let's get started so the first part
is as usual we are importing some
libraries that are required and uh then
the last line in this block is to load
the digits so let's go ahead and run
this code then here we will visualize
the shape of these uh digits so we can
see here if we take a look this is how
the shape is
1797 by 64 these are like 8 by8 images
so that's that's what is reflected in
this shape now from here onwards we are
basically once again importing some of
the libraries that are required like
numpy and map plot and we will take a
look at uh some of the sample images
that we have un loaded so this one for
example creates a figure uh and then we
go ahead and take a few sample images to
see how they look so let me run this
code and so that it becomes easy to
understand so these are about five
images sample images that we are looking
at 0 1 two three four so this is how the
images this is how the data is okay and
uh based on this we will actually train
our logistic regression model and then
we will test it and see how well it is
able to recognize so the way it works is
the pixel information so as you can see
here this is an 8 by 8 pixel kind of a
image and uh the each pixel whether it
is activated or not activated that is
the information available for each pixel
now based on the pattern of this
activation and non-activation of the
various pixels this will be identified
as a zero for example right similarly as
you can see so overall each of these
numbers actually has a different pattern
of the pixel activation and that's
pretty much that our model needs to
learn uh for which number what is the
pattern of the activation of the pixels
right right so that is what we are going
to train our model okay so the first
thing we need to do is to split our data
into training and test data set right so
whenever we perform any training we
split the data into training and test so
that the training data set is used to
train the system so we pass this
probably multiple times uh and then we
test it with the test data set and the
split is usually in the form of there
and there are various ways in which you
can split this data it is up to the
individual preferences in our case here
we are splitting in the form of 23 and
77 so when we say test size as 20 23
that means 23% of the entire data is
used for testing and the remaining 77%
is used for training so there is a
readily available function which is uh
called train test split so we don't have
to write any special code for the
splitting it will automatically split
the data based on the proportion that we
give here which is test size so we just
give the test size automatically
training size will be determined and uh
we pass the data that we want to split
and the the results will be stored in
xcore train and Yore train for the
training data set and what is xcore
train this are these are the features
right which is like the independent
variable
and Yore train is the label right so in
this case what happens is we have the
input value which is or the features
value which is in xcore train and since
this is labeled data for each of them
each of the observations we already have
the label information saying whether
this digit is a zero or a one or a two
so that this this is what will be used
for comparison to find out whether the
the system is able to recognize it
correctly or there is an error for each
observation it will compare with this
right so this is the label so the same
way xcore train Yore train is for the
training data set xcore test Yore test
is for the test data set okay so let me
go ahead and execute this code as well
and then we can go and check quickly
what is the how many entries are there
and in each of this so xcore train the
shape is
1383 by 64 and yore rain has 1383
because there is uh nothing like the
second part is not required here and
then xcore test shape we see is 414 so
actually there are 414 observations in
test and 1383 observations in train so
that's basically what these four lines
of code are are saying okay then we
import the logistic regression library
and uh which is a part of psychic learn
so we we don't have to implement the
logistic regression process itself we
just call these the function and uh let
me go ahead and execute that so that uh
we have the logistic regression Library
imported now we create an instance of
logistic regression right so logistic RR
is a is an instance of logistic
regression and then we use that for
training our model so let me first
execute this code so these two lines so
the first line basically creates an
instance of logistic regression model
and then the second line is where we are
passing our data the training data set
right this is our the the predictors and
uh this is our Target we are passing
this data set to train our model all
right so once we do this in this case
the data is not large but by and large
uh the training is what takes usually a
lot of time so we spend in machine
learning activities in machine learning
projects we spend a lot of time for the
training part of it okay so here the
data set is relatively small so it was
pretty quick so all right so now our
model has been trained using the
training data set and uh we want to see
how accurate this is so what we'll do is
we will test it out in probably faces so
let me first try out how well this is
working for uh one image okay I will
just try it out with one image my the
first entry in my test data set and see
whether it is uh correctly predicting or
not so and in order to test it so for
training purpose we use the fit method
there is a method called fit which is
for training the model and once the
training is done if you want to test for
a particular value new input you use the
predict method okay so let's run the
predict method and we pass this
particular image and uh we see that the
shape is or the prediction is four so
let's try a few more let me see for the
next 10 uh seems to be fine so let me
just go ahead and test the entire data
set okay that's basically what we will
do so now we want to find out how
accurately this has um performed so we
use the score method to find what is the
percentage of accuracy and we see here
that it has performed up to 94% Accurate
okay so that's uh on this part now what
we can also do is we can um also see
this accuracy using what is known as a
confusion Matrix so let us go ahead and
try that as well uh so that we can also
visualize how well uh this model has uh
done so let me execute this piece of
code which will basically import some of
the libraries that are required and um
we we basically create a confusion
Matrix an instance of confusion matrix
by running confusion Matrix and passing
these uh values so we have so this
confusion undor Matrix method takes two
parameters one is the Yore test and the
other is the prediction so what is the
Yore test these are the labeled values
which we already know for the test data
set and predictions are what the system
has predicted for the test data set okay
so this is known to us and this is what
the system has uh the model has
generated so we kind of create the
confusion Matrix and we will print it
and uh this is how the confusion Matrix
looks as the name suggests it is a
matrix and um the key point out here is
that the accuracy of the model is
determined by how many numbers are there
in the diagonal the more the numbers in
the diagonal the better the accuracy is
okay and first of all the total sum of
all the numbers in this whole Matrix is
equal to the number of observations in
the test data set that is the first
thing right so if you add up all these
numbers that will be equal to the number
of observations in the test data set and
then out of that the maximum number of
them should be in the diagonal that
means the accuracy is predic good if the
the numbers in the diagonal are less and
in all other places there are a lot of
numbers which means the accuracy is very
low the diagonal indicates a correct
prediction that the this means that the
actual value is same as the predicted
value here again actual value is same as
the predicted value and so on right so
the moment you see a number here that
means the actual value is something and
the predicted value is something else
right similarly here the actual value is
something and the predicted value is
something else so that is basically how
we read the confusion Matrix now how do
we find the accuracy you can actually
add up the total values in the diagonal
so it's like 38 + 44 + 43 and so on and
divide that by the total number of test
observations that will give you the
percentage accuracy using a confusion
Matrix now let us visualize this
confusion Matrix in a slightly more
sophisticated way uh using a heat map so
we will create a heat map with some
We'll add some colors as well it's uh
it's like a more visually visually more
appealing so that's the whole idea so if
we let me run this piece of code and
this is how the heat map looks uh and as
you can see here the diagonals again are
are all the values are here most of the
values so which means reasonably this
seems to be reasonably accurate and yeah
basically the accuracy score is 94% this
is calculated as I mentioned by adding
all these numbers divided by the total
test values or the total number of
observations in test data set okay so
this is the confusion Matrix for
logistic
regression all right so now that we have
seen the confusion Matrix let's take a
quick sample and see how well uh the
system has classified and we will take a
a few examples of the data so if we see
here we we picked up randomly a few of
them so this is uh number four which is
the actual value and also the predicted
value both are four this is an image of
zero so the predicted value is also zero
actual value is of course zero then this
is the image of nine so this is also
been predicted correctly nine and actual
value is nine and this is a image of one
and again this has been predicted
correctly as like the actual value okay
so this was a quick demo of logistic
regression how to use logistic
regression to identify images
so we put them side to about side we
have our linear regression which is a
predictive number used to predict a
dependent output variable based on
Independent input variable accuracy is a
measured uh using least squares
estimation so that's where you take uh
you could also use absolute value uh the
least squares is more popular there's
reasons for that mathematically and also
for computer runtime
uh but it does give you an an accuracy
based on the the least Square estimation
the best fit line is a straight line and
clearly that's not always used in all
the regression models there's a lot of
variations on that the output is a
predicted integer value again this is
what we're talking about we're talking
about linear regression and we're
talking about regression it means the
number is coming out linear usually
means we're looking for that line versus
a different model and it's used in
business domain forecasting stocks uh
it's used as a basis of of most um uh
predictions with numbers so if you're
looking at a lot of numbers you're
probably looking at a a linear
regression
model uh for instance if you do just the
high lows of the stock exchange and
you're you're going to take a lot more
of that if you want to make money off
the stock you'll find that the linear
regression model fits uh probably better
than almost any of the other models even
you know high-end neural networks and
all these other different machine
learning and AI models because they're
numbers they're just a straight set of
numbers you have a high value low value
volume uh that kind of thing so when
you're looking at something that
straight numbers um and are connected in
that way usually you're talking about a
linear regression model and that's where
you want to start a logistic regression
model used to classify dependent output
variable based on Independent input
variable so just like the linear
regression model and like all of our
machine learning tools you have your
features coming in
uh and so in this case you might have uh
label you know an image or something
like that is is probably the very
popular thing right now labeling
broccoli and vegetables or whatever
accuracy is measured using maximum
likelihood estimation the best fit is
given by a curve and we saw that um
we're talking about linear regression
you definitely are talking about a
straight line although there is other
regression models that don't use
straight lines and usually when you're
looking at a logistic regression the
math as you saw was still kind of a
ukian line but it's now got that sigmoid
activation which turns it into um a
heavily weighted curve and the output is
a binary value between zero and one and
it's used for classification image
processing as I mentioned is is what
people usually think of um although they
use it for classification of um like a
window of things so you could take a
window of stock history and you could
CLA generate classifications based on
that and separate the data that way if
it's going to be that this particular
pattern occurs is going to be upward
trending or downward
trending in fact a number of stock uh uh
Traders use that not to tell them how
much to bid or what to bid uh but they
use it as to whether it's worth looking
at the stock or not whether the Stock's
going to go down or go up and it's just
a01 do I care or do I even want to look
at it so let's do a demo you get a
picture of what this looks like in
Python code let's predict the price at
which insurance should be sold to a
particular customer based on their
medical history we will also classify on
a mushroom data set to find the
poisonous and nonpoisonous
mushrooms and when you look at these two
datas the first one uh we're looking at
the price so the price is a number um so
let's predict the price which the
insurance should be sold to and the
second one is we're looking at either
it's poisonous or it's not poisonous so
first off before we begin the demo I'm
in the Anaconda
Navigator in this one I've loaded the
python
3.6 and using the Jupiter notebook and
you can use jupyter notebook by itself
um you can use the Jupiter lab which
allows multiple tabs it's basically the
notebook with tabs on it uh but the
Jupiter notebook is just fine and it'll
go into uh Google Chrome which is what
I'm using for my Internet Explorer and
from here we open up new and you'll see
Python 3 and again this is loaded with
python
3.6 and we're doing the linear versus
logic uh regression or logit you'll see
L git T um is one of the one of the
names that kind of pops up when you do a
search on here uh but it is a logic
we're looking at the logistic regression
models and we'll start with the linear
regression uh because it's easy to
understand draw a line through stuff um
and so in programming uh we got a lot of
stuff to unfold here in our in our uh
startup as we preload all of our
different
parts and let's go ahead and break this
up we have at the beginning import uh
pandas so this is our data frame uh it's
just a way of storing the data think of
a uh when you talk about a data frame
think of a spreadsheet you rows and
columns it's a nice way of viewing the
data and then we have uh we're going to
be bringing in our pre-processing label
en Co coder I'll show you what that is
um when we get down to it it's easier to
see in the data uh but there's some data
in here like um sex it's male or female
so it's not like an actual number it's
either your one or the other that kind
of stuff ends up being encoded that's
what this label encoder is right here we
have our test spit
model if you're going to build a model
uh you do not want to use all the data
you want to use some of the data and
then test it to see how good it is and
if it can't have seen the data you're
testing on until you're ready to test it
on there and see how good it is and then
we have our uh logistic regression model
our categorical one and then we have our
linear regression model these are the
two these right here let me just um um
clear all that there we go uh these two
right here are what this is all about
logistic versus uh linear is it
categorical are we looking for a true
false or are we looking for um a
specific
number and then finally um usually at
the very end we have to take and just
ask how accurate is our model did it
work um if you're trying to predict
something in this case we're going to be
doing um uh Insurance costs uh how close
to the insurance cost does it measure
that we expect it to be you know if
you're an insurance company you don't
want to promise to pay everybody's
medical bill and not be able
to and in the case of the mushrooms you
probably want to know just how much at
risk you are for following this model uh
as to far as whether you're going to get
eat a poisonous mushroom and die or
not um so we'll look at both of those
and we'll get talk a little bit more
about the shortcomings and the um uh
value of these different processes so
let's go ahead and run this this is
loaded the data set on here and then
because we're in Jupiter notebook I
don't have to put the print on there we
just do data set and by and it prints
out all the different data on here and
you can see here for our insurance
because that's what we're starting with
uh we're loading that with our pandas
and it prints it in a nice format where
you can see the age sex uh body mass
index number of children smoker so this
might be something that the insurance
company gets from the doctor it says hey
we're going to this is what we need to
know
to give you a quote for what we're going
to charge you for your
insurance and you can see that it has uh
1,338 rows and seven columns you can
count the columns 1 2 3 4 five six seven
so there's seven columns on
here and the column we're really
interested in is charges um I want to
know what the charges are going to be
what can I expect not a very good Arrow
drawn um what to expect them to charge
on there uh so is this going to be you
know is this person going to cost me uh
$1,884 or is this person only going to
cost me uh
3,866 how do we guess that so that we
can guess what the minimal charge is for
their
insurance and then there's one other
thing you really need to notice on this
data um and I mentioned it before but
I'm going to mention it again because
pre-processing data is so much of the
work in data science um sex well how do
you how do you deal with female versus
male um are you a smoker yes or no what
does that mean region how do you look at
Region it's not a number how do you draw
a line between Southwest and
Northwest um you know they they're
objects it's either your Southwest or
you're Northwest it's not like I'm
southwest I guess you could do longitude
and latitude but the data doesn't come
in like that it comes in as true false
or whatever you know it's either your
Southwest or your
Northwest so we need to do a little bit
of preprocessing of the data on here to
make this
work oops there we go okay so let's take
a look and see what we're doing with
pre-processing and again this is really
where you spend a lot of time with data
Sciences trying to understand how and
why you need to do that and so we're
going to do uh you'll see right up here
label uh and then we're going to do the
do a label encoder one of the modules we
brought in so this is SK learns uh label
encoder I like the fact that it's all
pretty much automated uh but if you're
doing a lot of work with the label
encoder you should start to understand
how that
fits um and then we have uh label. fit
right here where we're going to go ahead
and do the data set uh. sex. drop
duplicates and then for data set sex
we're going to do the label transform
the data sex and so we're looking right
here at um male or female and so it
usually just converts it to a zero1
because there's only two choices on
here same thing with the smoker it's
zero or one so we're going to transfer
the trans change the smoker uh 01 on
this and then finally we did region down
here region does it a little bit
different we'll take a look at that and
um it it's I think in this case it's
probably going to do it because we did
it on this label
transform um with this particular setup
it gives each region a number like 0 1
two three so let's go a and take a look
and see what that looks like go and run
this and you can see that our new data
set um has age that's still a number uh
Sex Is Zero or one uh so zero is female
one is male number number of children we
left that alone uh smoker one or zero it
says no or yes on there we actually just
do one for no zero or no yeah one for no
I'm not sure how it organized them but
it turns the smoker into zero or one yes
or no uh and then region it did this as
0 1 2 three so it's three
regions now a lot of times in in when
you're working with data science and
you're dealing with uh regions or even
word
analysis um instead in of doing one
column and labeling it 0 one 2 3 A lot
of times you increase your features and
so you would have region Northwest would
be one column yes or no region Southwest
would be one column yes or no true
01 uh but for this this this particular
setup this will work just fine on here
now that we spend all that time getting
it set up uh here's the fun part uh
here's the part where we're actually
using our setup on this and you'll see
right here we have our um y linear
regression uh data set drop the charges
because that's what we want to
predict and so our X I'm sorry our X
linear data set dro the charges because
that's where we're going to predict
we're predicting charges right here so
we don't want that as our input for our
features and our y output is charges
that's what we want to guess we want to
guess what the charges
are and then what we talked about
earlier is we don't want to do all the
data at once so we're going to going to
take
um3 means 30% we're going to take 30% of
our data and it's going to be as the
train as the testing site so here's our
y test and our X test down there um and
so that part our model will never see it
until we're ready to test to see how
good it is and then of course right here
you'll see our um training set and this
is what we're going to train it we're
going to trade it on 70% of the data and
then finally the big ones uh this is
where all the magic happens this is
where we're going to create our magic
setup and that is right here our linear
model we're going to set it equal to the
linear regression model and then we're
going to fit the data on
here and then at this point I always
like to pull up um if you if you if
you're working with a new models good to
see where it comes from and this comes
from the sidekit uh learn and this is
the SK learn linear model linear
regression that we imported earlier and
you can see they have different
parameters the basic parameter works
great if you're dealing with just
numbers uh mentioned that earlier with
stock high lows this model will do as
good as any other model out there for do
if you're doing just the very basic high
lows and looking for a linear fit a
regression model fit um and what you one
of the things when I am looking at this
is I look for
methods and you'll see here's our fit
that we're using right now and here's
our
predict and we'll actually do a little
bit in the middle here as far as looking
at some of the parameters hidden behind
it the math that we talked about
earlier and so we go in this we go ahead
and run this you'll see it loads the
linear regression model and just has a
nice output that says hey I loaded the
linear regression model and then the
second part is we did the fit and so
this model is now trained our linear
model is now trained on the training day
data and so one of the things we can
look at is the um um for idx and colon
name and enumerate X linear train
columns kind of an interesting thing
this prints out the coefficients uh so
when you're looking at the back end of
the data you remember we had that
formula uh BX X1 plus bxx2 plus the plus
the uh intercept uh and so forth these
are the actual coefficients that are in
here this is what it's actually
multiplying these numbers
by and you can see like region gets a
minus value so when it adds it up I
guess a region you can read a lot into
these numbers uh it gets very
complicated there's ways to mess with
them if you're doing a basic linear
regression model you usually don't look
at them too closely uh but you might
start looking in these and saying hey
you know what uh smoker look how smoker
impacts the cost um it's just massive uh
so this is a flag that hey the value of
the smoker really affects this model and
then you can see here where the body
mass index uh so somebody who is
overweight is probably less healthy and
more likely to have cost money and then
of course age is a factor um and then
you can see down here we have uh sex is
than a factor also and it just it
changes as you go in there negative
number it probably has its own meaning
on there again it gets really
complicated when you dig into the um
workings and how the linear model works
on that and so um we can also look at
the intercept this is just kind of fun
um so it starts at this negative number
and then adds all these numbers to it
that's all that means that's our
intercept on there and that fits the
data we have on that and so you can see
right here we can go back and oops give
me just a second there we go we can go
ahead and predict the unknown data and
we can print that out
and if you're going to create a model to
predict something uh we'll go ahead and
predict it here's our y prediction value
linear model
predict and then we'll go ahead and
create a new data frame in this case
from our X linear test group we'll go
ahead and put the cost back into this
data frame and then the predicted cost
we're going to make that equal to our y
prediction and so when we pull this up
uh you can see here that we have
uh the actual cost and what we predicted
the cost is going to
be there's a lot of ways to measure the
accuracy on there uh but we're going to
go ahead and jump into our mushroom
data and so in this you can see here we
we've run our basic model we built our
coefficients you can see the intercept
the back end you can see how we're
generating a number here uh now with
mushrooms we want to yes or no we want
to know whether we can eat them or not
and so here's our mushroom file we're
going to go and run this take a look at
the data and again you can ask for a
copy of this file uh send a note over to
Simply
learn.com and you can see here that we
have a class um the cap shape cap
surface and so forth so there's a lot of
feature in fact there's 23 different
columns in here going
across and when you look at this um I'm
not even sure what these particular like
PE PE I don't even know what the class
is on
this I'm going to guess by the notes
that the class is uh poisonous or
edible so if you remember before we had
to do a little precoding on our data uh
same thing with here uh we have our cap
shape which is b or X or k um we have
cap color uh these really aren't numbers
so it's really hard to do anything with
just a a single number so we need to go
ahead and turn turn those into a label
encoder which again there's a lot of
different encoders uh with this
particular label encoder it's just
switching it to 0 1 2 3 and giving it an
integer
value in fact if you look at all the
columns all of our columns are labels
and so we're just going to go ahead and
uh loop through all the columns in the
data and we're going to transform it
into a um label encoder and so when we
run this you can see how this get
shifted from uh xbxx K to 012 3 4 5 or
whatever it is class is 01 one being
poisonous zero looks like it's
editable and so forth on here so we're
just encoding it if you were doing this
project depending on the results you
might encode it differently like I
mentioned earlier you might actually
increase the number of features as
opposed to laboring 0 1 2 3 4 five um in
this particular example it's not going
to make that big of a difference how we
encode
it and then of course we're looking for
uh the class whether it's poisonous or
edible so we're going to drop the class
in our x uh Logistics model and we're
going to create our y Logistics model is
based on that class so here's our
XY and just like we did before we're
going to go ahead and split it uh using
30% for test 7 70% to program the model
on
here and that's right here whoops there
we go there's our uh train and
test and then you'll see here on this
next setup um this is where we create
our model all the magic happens right
here uh we go ahead and create a
logistics model I've up the max
iterations if you don't change this for
this particular problem you'll get a
warning that says this has not
converged um because that that's what it
does is it goes through the math and it
goes hey can we minimize the error and
it keeps finding a lower and lower error
and it still is changing that number so
that means it hasn't conversed yet it
hasn't find the lowest amount of error
it can and the default is 100 uh there's
a lot of settings in here so when we go
in here to let me pull that up from the
sklearn uh so we pull that up from the
sklearn
model you can see here we have our
logistic it has our different settings
on here that you can mess with most of
these work pretty solid on this
particular setup so you don't usually
mess a lot usually I find myself
adjusting the um iteration and it'll get
that warning and then increase the
iteration on there and just like the
other model you can go just like you did
with the other model we can scroll down
here and look for our
methods and you can see there's a lot of
methods uh available on here
and certainly there's a lot of different
things you can do with it uh but the
most basic thing we do is we fit our
model make sure it's set right uh and
then we actually predict something with
it so those are the two main things
we're going to be looking at on this
model is fitting and predicting there's
a lot of cool things you can do that are
more advanced uh but for the most part
these are the two which um I use when
I'm going into one of these models and
setting them
up so let's go ahead and close out of
our sklearn setup on there and we'll go
ahead and run this and you can see here
it's now loaded this up there we now
have a uh uh logistic model and we've
gone ahead and done a predict here also
just like I was showing you earlier uh
so here is where we're actually
predicting the data so we we've done our
first two lines of code as we create the
model we fit the model to our training
data and then we go ahead and predict
for our test data now in the previous
model we didn't dive into the test score
um I think I just showed you a graph and
we can go in there and there's a lot of
tools to do this we're going to look at
the uh model score on this one and let
me just go ahead and run the model
score and it says that it's pretty
accurate we're getting a roughly 95%
accuracy well that's good one 95%
accuracy 95% accuracy might be good for
a lot of
things but when you look at something as
far as whether you're going to pick a
mushroom on the side of the trail and
eat it we might want to look at the
confusion Matrix and for that we're
going to put in our y listic test the
actual values of edible and unedible and
we're going to put in our prediction
value and if you remember on here um
let's see I believe it's poisonous was
one uh zero is edible so let's go ahead
and run that 01 zero is good so here is
um a confusion Matrix and this is if
you're not familiar with these we have
true true true
false true false false false so it says
out of the edible mushrooms we correctly
labeled 121 mushrooms edible that were
edible and we correctly measured
1,113 poisonous mushrooms as
poisonous but here's the
kicker I labeled uh 56 edible mushrooms
as as being um poisonous well that's not
too big of a deal we just don't eat them
but I measured 68 mushrooms as being
edible that were poisonous so probably
not the best choice to use this model to
predict whether you're going to eat a
mushroom or not and you'd want to dig a
Little Deeper before you uh start e
picking mushrooms off the side of the
trail so a little warning there when
you're looking at any of these data
models looking at the error and how that
error fits in with what domain you're in
domain in this case being edible
mushrooms uh be a little careful make
sure that you're looking at them
correctly so we've looked at uh edible
or not edible we've looked at uh
regression model as far as uh the end
values what's going to be the cost and
what our predicted cost is so we can
start figuring out how much to charge
these people for their
insurance and so these really are the
fundamentals of data science when you
pull together uh when I say data science
talking about your machine learning
code classification is probably one of
the most widely used tools in machine
learning in today's world it is also one
of the simpler versions to start
understanding how a lot of machine
Learning Works we're going to start by
taking a look at what exactly is
classification the important
terminologies around classification
we'll look at some real world
applications my favorite popular
classification algorithms and there are
a lot out there so we're only going to
touch briefly on a variety of them so
you can see how they the different
flavors work and we'll have some
Hands-On Demos in Python embedded
throughout the tutorial classification
classification is a task that requires
the use of machine learning algorithms
to learn how to assign a class label to
a given data you can see in this diagram
we have our unclassified data it goes
through a classification algorithms and
then you have classified data it's hard
to just see it as data and that really
is where you kind of start and where you
end when you start running these um
machine learning algorithms and
classification and the classification
algorithms is a little black box in a
lot of respects and we'll look into that
you can see what I'm talking about when
we start swapping in and out different
models let's say we are given the task
of classifying a given bunch of fruits
and vegetables on the basis of their
category I.E fruits are to be grouped
together and vegetables are to be
grouped together and so we have a data
set we'll call it the bunch is divided
into clusters one of which consists of
the fruits while the other has the
vegetables you can actually look at this
as any kind of data when we talk about
breast cancer can we sort out in images
to see what is malignant what is benign
very popular one can you classify
flowers the iris uh data set uh
certainly in Wildlife can you classify
different animals and track where
they're going classification is really
the bottom starting point or the
Baseline for a lot of machine learning
and setting it up and trying to figure
out how we're going to break the data up
so we can use it in a way that is
beneficial so here the fruits and the
vegetables are grouped into clusters and
each clusters has a specific
characteristic I whether they are a
fruit or a vegetable and you can see we
have a pile of fruits and vegetables we
feed it into the algorithm and the
algorithm separates them out and you
have fruits and vegetables so some
important terminologies before we dig
into how it sorts them out and what that
all means uh when we look at the
terminologies you have a classifier
that's the algorithm that is used to map
the input data to a specific category
the classification model the model that
predicts or draws a class to the input
data given for training feature it is an
individual measurable property of the
phenomena being observed and labels the
characteristics on which the data points
of a data set are categorized the
classifier and the classification model
go together a lot of times the
classifier is part of the classification
model um and then you choose which
classifier use after you choose which
model you're using where features are
what goes in labels are what comes out
so your classifier model is right in the
middle of that that's that little black
box we were just talking about clusters
they are a group of data points which
have some common characteristics binary
classification it is a classification
condition with two outcomes which are
either true true or false multi-label
classification this is a classification
condition where each sample is assigned
to a set of labels or targets multiclass
classification the classification with
more than two classes here each sample
is assigned to one and only one label
when we look at this group of
terminologies a few important things to
notice uh going from the top clusters
when we cluster data together we don't
necessarily have to have an end goal we
just want to know what features cluster
together these features then are mapped
to the outcome we want in many cases the
first step might not even care about the
outcome only about what data connects
with other data and there's a lot of
clustering algorithms out there that do
just the clustering part binary
classification it is a classification
condition with two outcomes which are
either true or false we're talking
usually um it's a uh it's either a cat
or it's not a cat it's either a dog or
it's not a dog um that's the kind of
thing we talk about binary
classification and then that goes into
multi-label classification think of
label as you can have an object that is
brown you can have an object that is
labeled as a dog so it has a number of
different labels that's very different
than a multiclass classification where
each one's a binary uh you can either be
a cat or a dog you can't be both a cat
and a dog real world applications so to
make sense of this uh of course the
challenge is always in the details is to
understand how we apply this in the real
world so in real world applications we
use this all the time we have email spam
classifier so you have your uh email
inbox coming in uh it goes through the
email filter that we usually don't see
in the background and it goes this is
either valid email or it's a Spam and it
puts it in the spam filter if that's
what it thinks it is uh Alexa's voice
classifier Google Voice any of the voice
class classifiers they're looking for
points so they try to group words
together and then they try to find those
groups of words trigger a CL classifier
so it might be that the classifier is to
open your task program or open your text
program so that you can start sending a
text sentimental sentiment analysis is
really big uh when we're tracking
products we're tracking marketing trying
to understand uh whether something is
liked or disliked is huge uh that's like
one of the biggest driving forces in SS
nowadays and you almost have to have
these different filters going on if
you're running a large business of any
kind fraud detection uh you can think of
banks uh they find different things on
your bank statement and they detect that
there's something going on there they
have algorithms for tracking the logs on
computers they start finding weird logs
on computers they might find a hacker I
mentioned the cat and dog so here's our
image classification we have a neighbor
who runs an outdoor webcam and we like
to have it come up with a classification
when the wild animals in our area are
out like foxes we actually have a
mountain lion that lives in the area so
it's nice to know when he's here
handwriting prediction uh classifying A
B C D and then classifying words to go
with that so let's go ahead and uh roll
our sleeves up and take a look at some
popular classification algorithms before
we look at the algorithms uh let's go
back and take a look at our definitions
we have a classifier and a
classification model so we're looking at
the classifier an algorithm that that is
used to map the input data to a specific
category one of those algorithms is a
logistic regression the logistic
regression is a classification algorithm
used to model the probability of a
certain class or event existing such as
pass fail or when lose Etc it provides
its output using the logistic function
or sigmoid function to return the
probability value that can then be
mapped to two or more discret classes a
sigmoid function is an activation
function that fit the variable and limit
the output to a range between 0 and one
a standard sigmoid function or logistic
function is represented by the formula
FX = 1/ 1 + e- x where X is the equation
of the line and E is the exponential
just taking a quick look at this um you
can think of this as being a point of
uncertainty and so as we get closer and
closer to the middle of the line it's
either um activated or not
and we want to make that just shoot way
up uh so you'll see a lot of the
activation formulas kind of have this
nice s curb where it approaches one and
approaches zero and based on that
there's only a small region of error and
so you can see in the sigmoid logistic
function uh the 1 over 1+ e- x to the
minus X you can see it fam that that
nice s curb uh we also can use a tangent
variation there's a lot of other
different uh models here as far as the
actual algorithm this this is the most
commonly used one let's go ahead and
roll up our sleeves and take a look at a
demo that is going to use a logistic
regression so we're going to have the
activation formula and the model because
you have to have you have to have both
for this we will go into our Jupiter
notebook now I personally use the
Anaconda navigator to open up the
Jupiter notebook to set up my ID as a
web based it's got some advantages that
it's very easy to display in uh but it
also has some disadvantages in that if
you're trying to do multi- threads and
multi-processing you start running into
a single git issues with python and then
I jump to uh py charm really depends on
whatever ID you want just make sure
you've installed uh numpy and the
sklearn modules into your Python and
whatever environment you're working in
so that you'll have access to that for
this demo now the team in the back has
prepared my code for me which I'll start
bringing in one section at a time so we
can go through it before we do that it's
always nice to actually see where this
information is coming from and what
we're working with uh so the first part
is we're going to import our packages
which you need to install into your
Python and that's going to be your numpy
um we usually use numpy as NP and then
from sklearn the learn model we're going
to use a logistic regression and from
sklearn metrics we're going to import
the classification report confusion
Matrix and if we go ahead and open up S
uh the scikit-learn dorg and go under
their API you can see all the different
um features and models they have and
we're looking at the linear model uh
logistic regression one of the more
common classifiers out there and if we
go ahead and go into that and dig a
Little Deeper you'll see here where they
have the different settings and it even
says right here note the regularization
is applied by default so by default that
is the activation formula being used now
we're not going to spend we might come
back to this look at some of the other
models CU it's always good to to see
what you're working with but let's go
ahead and jump back in here and we have
our Imports we're going to go ah and run
those uh so these are now available to
us as we go through our Jupiter notebook
script and they put together a little
piece of data for us this is simply um
going through uh 021 actually let's go
ahead and print this out over here we'll
go ahead and print X just so you can see
what we're actually looking for and when
we run this you can see that we have our
X is 01 2 through 9 we reshaped it the
reason for this is looking for a row of
data usually we have multiple features
we just have the one feature which
happens to be 0o through 9 and then we
have our 10 answers right down here uh 0
1 0 00111111 you can bring in a lot of
different data depending on what you're
working with uh you can make your own
you can instead of having this as just a
single you could actually have like
multiple features in here but we just
have the one feature for uh this
particular demo and this is really where
all the magic happens right here uh and
I told you is like a black box that's
the part that is is is kind of hard to
follow and so if you look right here we
have our model we talked about the model
right there and then we went ahead and
set it for Library linear as I showed
you earlier that's actually default so
it's not that important uh random State
equals zero this stuff you don't worry
too much about and then with the S kit
learn you'll see the model fit this is
very common to S kit they use similar
stuff in a lot of other different
packages but you'll you'll see that
that's very common you have to fit your
data and that means we're just taking
the data and we're fitting our X right
here which is our features that's our X
and here's y these are the labels we're
looking for so before we were looking at
is it fraud is it not is it cat is it
not U that kind of thing and this is
looking at 01 so we want to have a
binary setup on this and we'll go ahead
and run this uh you can see right here
it just tells us what we loaded it with
as our defaults and that this model has
now been created and we've now fit our
data to it and then comes the fun part
you work really hard to clean your data
to um bake it and cook it there's all
kinds of I don't know why they go with
the cooking terms as far as how we get
this data formatted then you go through
and you pick your model you pick your
solver and you have to test it to see
hey which one's going to be best and so
we want to go ahead and evaluate the
model and you do this is that once
you've figured out which one is going to
work the best for you you want to
evaluate it so you can compare it to
your last Model and you can either
update it to create a new one or maybe
change the um solver to something else I
mentioned tangent that's one of the
other common ones that's commonly used
with language for some reason the
tangent even though it looks almost to
me identical to the one we're using with
the sigmoid function uh it for some
reason it activates better with language
even though it's a very small shift in
the actual math behind it we already
looked at the um data early but we'll go
and look at it again just you can see we
look at we have our rows of Zer one row
it only has one entity and we have our
output that matches these rows and these
do have to match you'll get an error if
you put in something with a different
shape so if you have um 10 rows of data
and nine answers it's going to give you
an error because you need to have 10
answers for it a lot of times you
separate this too when you're doing
larger models uh but for this we're just
going to take a quick look at that the
first thing we want to start looking at
is uh The Intercept one of the features
inside our linear regression model model
we'll go ahead and run that and print it
uh you'll see here we have an intercept
of minus 1.51 6 and if we're going to
look at the uh intercept we should also
look at our um coefficients and if you
run that you'll see that we get a we get
the um our coefficient is the 7035 you
can just think of this as your eidan
geometry for very basic model like this
where it intercepts the Y at some point
and we have a coefficient multiplying by
it little more complicated in the back
end but that is the just a this simple
model with just the one feature in there
and we'll go ahead and uh we'll reprint
the Y cuz I want to put them on top of
each other with the Y predict and so
these were the Y values we put in and
this is the Y predict we had coming out
and you can see yeah here we go uh
there's the Y actual and there's what
the prediction comes in uh now keep in
mind that we used the actual complete
data as part of our training uh that is
is if you're doing a real model a big
stopper right there because you can't
really see how good it did unless you
split some data off to test it on this
is the first step is you want to see how
your model actually test on the data you
trained it with and you can see here
there is this point right here where it
has it wrong and this point right here
where it also has it wrong and it makes
sense because we're going our input is
01 0 through 9 and it has to break it
somewhere and this is where the break is
uh so it says this half the data is
going to be zero because that's what it
looked like to me if I was looking at it
without an algorithm and this data is
probably going to be one and I I didn't
I forgot to point this out so let's go
back up here I just kind of glanced over
this window here where we did a lot of
stuff let's go back and and just take a
look at that what was done here is we
ran a prediction uh so this is where our
predict comes in is our model. predict
so we had a model fit we created the
model we programmed it to give us the
right answer uh now we go ahead and
predict what we think it's going to be
there's our model. predict probability
of X and then we have our y predict
which is very similar but this is has to
do more with the probability numbers so
if you remember down below we had the
setup where we're looking at uh that
sigmoid function that's what this is
returning and the Y predict is returning
a zero or a one and then we have our um
confusion Matrix we'll look at that and
we have our report which just basically
Compares our y to our y predict which we
just did it's kind of nice as simple
data so it's really easy to see what
we're doing that's why we do use a
simple data this can get really
complicated when you have a lot of
different features and things going on
and splits uh so here we go we've had
our I printed out our actual and our
prediction so this is the actual data
this is what the predict ran um and then
we'll go ahead and do we're going to
print out the confusion Matrix we were
just talking about that uh this is great
if you have a lot of data to look at but
you can see right here our confusion
Matrix says uh if you remember from the
confusion Matrix we have the two this is
two correct one two and uh it's been a
while since I looked at a a confusion
Matrix there's the two and then we have
this one which is our six that's where
the six comes from and then we have this
one which is the um one false this is
the two on So we have this one here and
this one here which is
misclassified this really depends on
what data you're working with as to what
you're is important um you might be
looking at this model and if this model
this confusion Matrix comes up and says
that uh You'
misclassified even one person as being
non malignant cancer that's a bad model
uh I wouldn't want that classification
I'd want this number to be zero I
wouldn't care if this false positive was
off by a little bit more long as I knew
that I was correct on the important
factor that I don't have cancer so you
can see that this confusion Matrix
really aims you in the right direction
of what you need to change in your model
how you need to adjust it uh and then
there's of course a report reports are
always nice um if you notice we
generated a report earlier we'll go and
just print the report up and you can
remember this is our report it's a
classification report y comma y predict
so we're just putting in our two values
basically what we did here visually with
our actual and our predicted value and
we'll go ahead and run the report and
you can see it has the Precision uh the
recall your F1 score your support uh
translated into a accuracy macro average
and weighted average so it has all the
numbers a lot of times when working with
um clients or with the shareholders in
the company this is really where you
start because it has a lot of data and
they can just kind of stare at it and
try to figure it out and then you start
bringing in like the confusion Matrix I
almost do this in reverse verse as to
what they show I would never show your
shareholders The Intercept of the
coefficient that's for your internal
team only working on machine language uh
but the confusion Matrix and the report
are very important those are the two
things you really want to be able to
show on these uh and you can see here we
did a decent job of um classifying the
data managed to get a significant
portion of it correct uh we had our was
it accuracy here is a 080 F1 score uh
that kind of thing so you know it's a
pretty accurate model of course this is
pretty goofy it's very simple model and
it's just splitting the model between
ones and zeros so that was our demo of
the um on logistic regression on there
let's go take a look at K nearest
neighbors uh this one is another very
highly used and important algorithm to
understand K nearest neighbors is a
simple algorithm which stores all
available cases and classifies new cases
based on the similarity measure the K
nearest neighbor finds out the class of
the new data point by finding its
nearest neighbors if there are three
data points of Class A and two data
points of Class B near to the new data
point then the KNN classifies the new
data point as Class A the K and K
nearest neighbors is the number of
nearest neighbors we are looking for I
I.E if we say k equals three this means
that we are looking for nearest three
neighbors of unclassified data point
usually we take the the K value between
3 to 10 as it leads to a better result a
smaller value of K means that noise will
have a bigger influence on the result
and a larger value of K makes it
computationally expensive hence the data
scientists prefer the range of K between
three and 10 when we talk about noise
you remember the data we just looked at
was 0 one 1 0 0 it had some some values
where cut it and said everything to the
right is a one everything to the left is
a zero but it had some ones and zer
mixed in there that's called noise
that's what they're talking about is
there's some things that are right in
the middle in the classification which
makes it very hard to classify so
suppose we're trying to find the class
for a new Point indicated by the red
color and you can see it's kind of right
between the cat right between the dogs
let k equal three so we are finding the
three in in for the red data point by
looking at the plot on the right we can
see that the red data point belongs to
the class dogs as it has two votes for
class dog and one vote for class cat and
if you ask the question well what are
you measuring the distance what is that
distance um it could be the measurements
of the ears whether they're pointed or
floppy that might be one of the features
you're looking at is how floppy the ears
are um another one might be the whiskers
versus the nose um and then you take
those measurements and using uh one of
the most common things in K means
measurement is the ukian geometry you
can figure out the distance between
those points there's a lot of different
algorithms for that but you can think
about it that you do have to have some
kind of solid data to measure and so we
can conclude that the new data point
belongs to the class dog so let's go
ahead and see what this looks like in
code and do a demo on the K nearest
Neighbors in here and we'll go right
back into our Jupiter notebook and open
up a new um Python Programming script
page of course once we're in here we'll
want to look at the uh ssy kit learn uh
I did just a quick search for SK
neighbors Ken neighbors classifier um
this actually is the older version .0 no
023 is a one we want and you'll see here
that we have all their defaults in
Neighbors equals 5 at defaults we were
talking about that between three and 10
there's different ways to weigh it
there's an algorithm based on it I
mentioned ukian geometry finding the
distance there's other algorithms for
figuring out what that distance is and
how to weight those uh and there's a lot
of other parameters you can adjust for
the most part the K means uh basic setup
is a good place to start and just let
the defaults go uh we might play with
some of those we'll see what the guys in
the back did and from here we're going
to import numpy we're going to use
pandas if you haven't been running
pandas pandas is our data frame which
sits on top of numpy uh data frames are
you know numes is our number array
pandas is our data frame mat plot
Library cuz we're going to plot some
graphs everybody likes some pretty
pictures it makes it a lot easier to see
what's going on when you have a nice
display and that's also what the Seaborn
is in here and the uh setup that sits on
top of the map plot Library the ones we
really want to look at right here are
the what we're bringing in from sklearn
these ones right here uh so from sklearn
we're going to load I mentioned the
breast cancer that's a very popular one
because it has I believe it's 36
measurements so there's 36 features and
unless you're a expert you're not going
to know what any of those features
really mean you can sort of guess but
there special measurements they take of
when they take a image uh and of course
our confusion Matrix so that we can take
a look and see what the data looks like
and how good we did uh and then we have
our knen neighbors classifier on here uh
and then I mentioned that uh whenever
you do training and testing you want to
split the data up you don't want to
train the data and then test it on the
same data that just tells you how good
your training model is it doesn't tell
you whether it actually works on unknown
data and so this just splits it off so
that we can train it and then we can
take a look at data we don't have in
there and see how good it did and we'll
go ahead and load our uh data up so
here's our our setup on that oops there
we go so we're going to go ahead and
load the data up uh we have our x value
and that's going to come from our breast
cancer. data and column breast cancer
feature names so there's our actual um
all our different features we'll print
that out here in a second and then we
have our um mean area mean compactness
so I guess we're going to take the data
and we're only going to use a couple of
the columns this just makes it easier to
read um of course when you actually were
going to do this you'd want to use all
your columns and then we have our Y and
this is simply um whether it's either
malignant or B9 and then um we want to
go ahead and drop the first line because
that's how it came in on there and we'll
go ahead let's just take a look at this
a little closer here let's go and run
this real quick and just because I like
to see my data before I run it we can
look at this and we can look at the
original features remember we're only
going to use two features off of here
just to make it a little easier to
follow and here's the actual data and
you can see that this is just this
massive stream of data coming in here
it's going to just skip around CU
there's so much in there to set up I
think there's like 500 if I remember
correctly and you can see here's all the
different measurements they take but we
don't we don't really need to see that
on here we're just going to take a look
at just the two columns and then also
our solution we'll go and just do a
quick uh print y on here so you can see
what the Y looks like and it is simply
just 0 0 you know B9 0000 01 so a one
means it's B9 a zero means it's
malignant uh is what we're looking at on
that go and cut that out of there the
next stage is to go ahead and split our
data I metion mentioned that earlier uh
we'll just go ahead and let them do the
splitting forest for us uh we have XT
train X test y train y test and so we go
ahead and train test split XY random
State equals 1 makes it nice and easy
for us we'll go and run that and so now
we have our training and our testing
train means we're going to use that to
train the model and then we're going to
use the test to test to see how good our
model does and then we'll go ahead and
create our model here's our KNN model
the K Neighbors classifier n neighbors
equals 5 the metrics is ukian remember I
talked about ukian uh this is simply
your c^2 equals A2 + B2 + u u A2 = B2 +
c2+ C2 + d^2 and then you take the
square root of all that that's what
they're talking about here it's just the
length of the hypotenuse of a triangle
but you can actually do that in multiple
Dimensions just like you do in two
Dimensions with a regular triangle and
here we have our fit this should start
to look familiar since we already did
that in our last example that's very
standard for S uh s kit and any other
one although sometimes the fit
algorithms look a little bit more
complicated because they're doing more
things on there especially when you get
into neural networks uh and then you
have your K neighbors it just tells you
we created a um a k neighbor setup they
kind of wanted us to reformat the Y but
it's not that big of a deal for this uh
and it comes out and shows you that
we're using the ukian uh metric for our
measurement so now we've created a model
here's our live model we fitted the data
to it we say hey here's our training
data uh let's go ahead and predict it so
we're going to take our y predict equals
KNN predict y test so this is data we
have not this model has not seen this
data and so we're going to create a
whole new set of data off of there now
before we look at our prediction in fact
let's um I'm going to bring this down
and and put it back in here later let's
take a look at our X test data versus
the Y test what does it look like and so
we have our mean area we're going to
compare it to our mean compactness we're
going to go ahead and run that and we
can see here the data if you look at
this just eyeballing it we put it in
here we have a lot of blue here and we
have a lot of orange here and so these
dots in the middle especially like this
one here and these here these are the
ones that are going to give us false
negatives and so we should expect this
is your noise this is where we're not
sure what it is and then B9 is in this
case is done in blue and malignant is
done in one uh so if you look at it
there's two points based on these
features which makes it really hard to
have 100% where the 100% is down here um
or up here that's kind of the thing I'd
be looking for when we're talking about
cancer and stuff like that where you
really don't want any uh false negatives
you want everything you false positive
great you're going to go in there and
have another setup in there where you
might get a get a op Toops or something
like that done on it again that's very
data specific specific on here uh so now
let's go ahead and pull in and and get
our um our prediction in here and we'll
create our y prediction we'll go and run
that so now this is loaded with what we
think the unknown data is going to be
and we can go ahead and take that and go
ahead and plot it because it's always
nice to have some pretty pictures and
when we plot it we're going to do the
mean area versus mean compactness again
you look at this map and you can see
that there's some clear division here we
can clearly say on some of the stuff
that are why prediction if we look at
this map up here and this map down here
we probably got some pretty good deal it
looks pretty good like they match a lot
this is of course just eyeballing it
really you don't want to eyeball these
things you want to show people the
pictures so that they can see it and you
can say hey this is what it looks like
uh but we really want the confusion
Matrix and when we do the Y test and the
Y predict we can see in the confusion
Matrix here it did pretty good um and
we'll just go ahead and point that out
real quick uh here's our 42 which is
positive and our
79 and if I remember correctly I'd have
to look at the data which one of these
is a false negative I believe it's the
nine that's scary I would not want to be
one of those nine people told that I
don't have cancer and then suddenly find
out I do uh so we would need to find a
way to sort this out and there is
different ways to do that uh a little
bit past this but you can start messing
with the actual ukian geometry and the
activation
uh measurements and start changing those
and how they interact but that's very
Advanced there's also other ways to
classify them or to create a whole
another class right here of we don't
knows those are just a couple of the
solutions you might use for that but for
a lot of things this works out great uh
you can see here you know maybe you're
trying to sell something well if this
was not uh life dependent and this was
if I display this ad 42 of these people
are going to buy it and if I display
this other AD if I don't display it 79
people are going to go a different
direction or whatever it is so maybe
you're trying to display whether they're
going to buy something if you add it on
to the website in which case that's
really a good numbers you've just added
a huge number of sales to your company
so that was our K nearest neighbors uh
let's go ahead and take a look at
support Vector machines so support
Vector machines uh is the main objective
of a support Vector machine algorithm is
to find a hyperplane in an n-dimensional
space in a a number of features that
distinctly classifies the data points
and if you remember we were just looking
at those nice graphs we had earlier in
fact let me go aead and flip back on
over there if we were looking at this
data here we might want to try to find a
nice line through the data and that's
what we're talking about with uh this
next setup so the main objective of
support Vector machine algorithm is to
find a hyperplane in an in dimensional
space in is a number of features that
distinctly classifies the data points to
separate the two classes of data there
are many hyperplanes that can be chosen
our objective is to find the plane that
has the maximum margin I.E the maximum
distance between data points of both
classes the dimensions of the hyper
plane depends on the number of features
if there are two input features then the
hyperplane is just a line if there are
three features then the hyper plane
becomes a two-dimensional plane the line
that separates the data into two classes
is called as support Port vector
classifier or hard margin and that's why
I just showed you on the other data you
can see here we look for a line that
splits the data evenly the problem with
hard margin is that it doesn't allow
outliers and doesn't work with
non-linearly separable data and we just
were looking at that let me flip back on
over here and when we look at this setup
in here and we look at this data here we
go look how many outliers are in here
these are all with all these blue dots
on the wrong side of the line would be
considered an outlier and the same with
the red line and so it becomes very
difficult to divide this data unless
there's a clear space there we go
therefore we introduce soft margins
which accept the new data point and
optimize a model for nonlinear data
points soft margins pass through the
data points at the border of the classes
the support Vector machine can be used
to separate the two classes of shapes
here we can see that although triangles
and diamond shapes have pointy edges but
we are able to CL classify them in two
categories using a support Vector
machine let's go ahead and see what that
looks like in a demo we flip back on
over to our Jupiter notebook we always
want to start with taking a look at the
sklearn uh API in this case the svm SVC
there is a significant number of
parameters because SK the um svm has
been a lot of development in the recent
years and it's become very popular if we
scroll all the way down to methods
you'll see right here is our fit and our
predict that's what you should see in
most of the um sidekit learn packages so
this should look very familiar to what
we've already been working on and we'll
go ahead and bring in our import and run
that uh we should already have pandas
numpy our map plot Library which we're
going to be using to graph some of the
things a little bit different right here
you'll see that we're going to go ahead
and bring in um one of the fun things
you can do with test data is uh make
circles circles are really hard to
classify you have a ring on the inside
and a ring on the outside and you can
see where that can cause some issues and
we'll take a look at that a little
closer in a minute here's our svm uh
setup on there and then of course our
metrics because we're going to use that
to take a closer look at things so we go
and run this whoops already did once
we've gone ahead and done that we're
going to go ahead and start making
ourselves some data and this part
probably a little bit more complicated
uh than we need I'm not going to go too
much in detail on it uh we're going to
make a mesh grid and we'll see what that
looks like in a minute where we're
defining the minimums you can see in
here create a mesh grid of points here's
our parameters of x y and H it's going
to return XX y y you can also send a
note to us make sure you get a copy of
this if you want to dig deeper into this
particular code especially and we have a
y men y Max uh y men y Max plus one
here's our mesh grid we're actually
going to make XX and y y plot the
Contours all the way through and this is
just uh way of creating data it's kind
of a fun way to create some data we go
plot Contours ax LF XX y y and uh return
it out add some perimeters here so that
when we're doing it we have our setup on
there train property and then we'll go
ahead and make our data just throw that
right in there too in the same setup and
run that uh so now we have X and Y we're
going to make our circles we have n
samples equals samples in this case
we're going to have 500 samples we went
ahead and gave it uh noise of 005 random
State 123 these are all going into oops
let's go back up here we go make our
mesh grid make circles there it is uh so
this is going into our make circles up
here and this is part of that setup on
this and then once we've gone ahead and
make the circle let's go ahead and plot
it uh that way you can see we're talking
about right now what I'm saying is
really confusing because without a
visual it doesn't really mean very much
what we're actually doing so I'm going
go ahead and run this with the plot and
let's go back up here and just take a
look we've made our Circle we have our
in samples equals sample so we're going
to have 500 we're going to have training
property point8 here's our data frame we
go and load it into the data frame so we
can plot it groups DF Group by label uh
this is just kind of a fun way if you
have multiple columns you can really
quickly pull whatever setup is in there
and then we go ahead and plot it and you
can see here we have two rings that are
formed and that's all this is doing is
just making this data for us this is
really hard data to figure out um a lot
of programs get confused in this because
there's no straight line or anything
like that but we can add planes and
different setups on here and so you can
see we have some very interesting data
we have our zero is in blue and our
one's in the yellow in the middle and
the data points are um an XY coordinate
plot on this one of the things we might
want to do on here is go ahead and find
the Min to Max r ratio uh set up in
there and we can even do let's just do a
print X so you can see what the data
looks like that we're producing on this
there we go so uh xal x - Xmen over x
max - x Min all we're doing is putting
this between zero and one whatever this
data is we want um a zero to one setup
on here if you look at this all our data
is
8.5 that's what this particular line is
doing that's a pretty common thing to do
in processing data especially in neural
networks uh neural networks don't like
big numbers they create huge biases and
cause all kinds of problems and that's
true in a lot of our models some of the
models it doesn't really matter but when
you're doing enough of these you just
start doing them uh you just start
putting everything between zero and one
there's even some algorithms to do that
in the SK learn although it's pretty as
you can see it's pretty easy to do it
here so uh let's go ahead and jump into
the next thing which is a linear kind of
setup Cals 1.0 this is the SPM alization
parameter we're going to use models
here's our svm and let's go and scroll
up just a notch there we go now we so
here we are with our model we're going
to create the the setup with the SVC
kernel is linear and we'll come back to
that CU that's an important uh setup in
there as far as what our kernel is and
you'll you'll see why that's so
important here in a minute because we're
going to look at a couple of these and
so this one is we're actually going to
be changing how that processes it uh and
then our c here's our one our 0.0 and
then the rest of this is uh plotting
we're just adding titles making the
Contours so it's a pretty graph you can
actually spend this would be a whole
class just to do all the cool things you
can do with Scatter Plots and regular
plots and colors and things like that in
this case we're going to create a graph
with a nice white line down the middle
so that you can see what's going on here
and when we do this you can see that as
it as it split the data uh the linear
did just that it drew a line through the
the data and it says this is supposed to
be blue and this is supposed to be red
and it doesn't really fit very well
that's because we used a linear division
of the data and it's not very linear
data on it's anything but linear so when
we when we look at that it's like well
okay that didn't work what's the next
option well there's a lot of choices in
here one of them is just simply we can
change this from the kernel being linear
to poly and we'll just go back up here
and use the same chart um oops here we
go uh so here we go uh linear kernel
we'll change this to poly and then when
we come in here and create our model
here's our model up here linear we can
actually just go right in and change
this to the poly model and if you
remember when we go back over here to
the SVC and let's scroll back up here
there's a lot of different options oops
even further okay so when we come up
here we start talking about the kernel
here's the kernel there's linear there's
poly RBF sigmoid precomputed there's a
lot of different ways to do this setup
um and their actual default is RBF very
important to note that uh so when you're
running these models understanding which
parameter is really has a huge effect on
what's going on in this particular one
with the uh spvm the kernel is so
important you really need to know that
and we switched our kernel to poly and
when we run this uh you can see it's
changed a little bit we now have quite
an interesting looking diagram and you
can see on here it now has these
classifications correct but it messes up
in this blue up here and it messes up on
this blue is correct and this blue is
supposed to be red you can see that it
still isn't quite fitting on there and
so that is uh we do a a polyfit uh you
can see if you have a split in data
where there's a group in the middle this
one kind of data and the groups on the
outside are different the polyfit or the
poly kernel is what's going to be fit
for that so uh if that doesn't work then
what are we going to use well uh they
have the RBF kernel and let's go ahead
and take a look and see what the RBF
looks like and uh let me go and turn
there we go turn my drawing off and the
RBF kernel oops there we go RBF and then
of course for our title it's always nice
to have it match with the RBF kernel and
we go aad and run this and you can see
that the RBF kernel does a really good
job uh it actually has divided the DAT
app on here and this is the kind of what
you expect here was that ring here's our
inner ring and an outer ring of data and
so the RBF fits this data package quite
nicely um and that when we talk about
svm it really is powerful in that it has
this kind of sorting feature to it in
its algorithms this is something that is
really hard to get the SK uh means to do
or the K means uh setup and so when you
start looking at these different machine
learning algorithms understanding your
data and how it's grouped is really
important it makes makes a huge
difference as far as what you're
Computing and what you're doing with it
so that was a demo on the support Vector
uh certainly you could have done
continued on that and done like a
confusion Matrix and all that kind of
fun stuff to see how good it was and
split the data up to see how it uh uh
vectorizes the visual on that so
important it makes a big difference just
to see what it looks like and that giant
donut and why it it does Circle so well
or your poly version or your linear
version so we've looked at some very
numerical kind of uh setups where
there's a lot of math involved ukian
geometry um that kind of thing a totally
different machine learning algorithm for
approaching this is the decision trees
and there's also Forest that go with the
decision trees they're based on multiple
trees combined the decision tree is a
supervised learning algorithm used for
classification it creates a model that
predicts the value of a Target variable
by learning simple decision rules
inferred from the data features a
decision tree is a hierarchal tree
structure where an internal node
represents features or tribute the
branch represents a decision Rule and
each Leaf node represents the outcome
and you can see here where they have the
first one uh yes or no and then you go
either left or right and so forth one of
the coolest things about decision trees
um is and I'll see people actually run a
decision tree even though their final
model is different because the decision
tree allows you to see what's going
going on you can actually look at it and
say why did you go right or left what
was the choice where's that break uh and
that is really nice if you're trying to
share that information with somebody
else as to why when you start getting
into the why this is happening decision
trees are very powerful so the topmost
note of a decision tree is known as the
root node it learns to partition on the
basis of the attribute value it
partitions the tree in a recursive
manner so you have your decision node if
you get yes you go down to the next node
that's a decision Noe and either yes you
go to if it ends on a leaf node then you
know your answer uh which is yes or no
so there's your there's your end
classification set up on there here's an
example of a decision tree that tells
whether I'll sleep or not at a
particular evening mine would be
depending on whether I have the news on
or not do I need to sleep no okay I'll
work uh yes is it raining outside yes
I'll sleep no all work so I guess if
it's uh not raining outside it's harder
to fall asleep where you have that nice
uh rain coming in and again this is
really cool about a decision tree is I
can actually look at it and go oh I like
to sleep when it rains outside so when
you're looking at all the data you can
say oh this is where the switch comes in
when it rains outside I'll sleep really
good if it's not raining or if I don't
need sleep then I'm not going to sleep
I'm going to go work so let's go ahead
and take a look at that that looks like
in the code just like we did before we
go ahead and open up the S kit set up
just to tell you what the decision tree
classifier has you have your parameters
which we'll look a little bit more in
depth at as we write the code but it has
uh different ways of splitting it the
strategy used to choose a split at each
node uh Criterion max depth remember the
tree how far down do you want it do you
want it to take up the space of your
whole computer with and and map every
piece of data or you know the smaller
that number is the smaller the level of
the tree is and the less processing it
takes but is also more General so you're
less likely to get as in-depth an answer
um and then of course minimal samples
you need for it to split samples for the
leaf there's a lot of things in here as
far as what how big the tree is and how
to define it and when do you define it
and how to weight it and they have their
different attributes which you can dig
deeper into uh that can be very
important if you want to know the why of
things uh and then we go down here to
our methods and you'll see just like
everything else we have our fit method
very important uh and our predict uh the
two main things that we use what is what
we're going to predict our X to be equal
to and we'll go ahead and go up here and
start putting together the code uh we're
going to import our numpy our pandas
there's our confusion Matrix our train
test Split Decision tree classifier
that's the big one that we're actually
working with uh that's the line right
here where we're going to be oops
decision tree there it is decision tree
classifier that's the one I was looking
for and of course we want to know the
accuracy and the classification report
on here and we're going to do a little
different than we did in the other
examples and there's a reason for this
let me go and run this and load this up
here uh we're going to go ahead and
build things on functions and this is
when you start splitting up into a team
this is the kind of thing you start
seeing a lot more both in teams and for
yourself because you might want to swap
one data to test it on a different data
depending on what's going on uh so we're
going to have our import data here um
the data set length the balance and so
forth um this just returns balance data
let me just go ahead and print cuz I'm
curious as to what this looks like
import data and it's going to return the
balance data so if I run that uh if we
go ahead and print this out here and run
that you can see that we have a whole
bunch of data that comes in there and
some interesting setup on here has uh
let's see BR RR I'm not sure exactly
what that represents on here uh 111 112
and so forth so we have a different set
of data here the shape is uh five
columns 1 two 3 four five uh seems to
have a number at the beginning which I'm
going to guess uh B RL a letter I mean
and then a bunch of numbers in there one
one one one let's see down here we got
555 uh set up on this and let's see
balance data and since it said balance
data I'm going to guess that uh b means
balanced R means you need to move it
right and L means it's um needs to be
moved left or skewed to left I'm not
sure which one uh let's go and close
that out and we'll go ahead and create a
function to split the data set uh X
balance data equals data values y
balance equals data values of zero
there's that letter remember left right
and balance then we're looking for the
values of 1 through five and we go ahead
and split it just like you would X train
y train set random State 100 test size
is.3 so we're taking 30% of the data and
it's going to return your X your y your
y train your uh your X train your X test
your y train in your um y test again we
do this because if you're running a lot
of these you might want to switch how
you split the data and how you train it
I tend to use a bfold method I'll take a
third of the data and I'll train it on
the other twoth thirds and test it on
that third and then I'll switch it I'll
switch which third is a test data and
then I can actually take that
information and correlate it and it
gives me a really uh robust package for
figuring out what the complete accuracy
is uh but in this case we're just going
to go ahead this is our function for
splitting data and this is where it kind
of gets interesting because remember we
were talking a little bit about uh the
different settings in our model and so
uh in here we're going to create a
decision tree but we're going to use the
Gen Genie setup and where did that come
from uh what's the genie on here uh so
if we go back to the top of their page
and we have what uh Criterion are we
going to use we're going to use Genie
they have Genie and entropy those are
the two main ones that they use for the
decision tree uh so this one's going to
be Genie and if we're going to have a
function that creates the Genie model
and it even goes down here and here's
our fit train of the Genie model uh
we'll probably also want to create one
for entropy sometimes I even just um I
might even make this just one function
with a different setups and I know one
of my one of the things I worked on
recently I had to create a one that test
it across multiple models and so I would
send the parameters to the models or I
would send this part right here where
says decision tree classifier that whole
thing might be what I send to create the
model and I know it's going to fit we're
going to have our xtrain and we're going
to have a predict and all that stuff is
the same so you can just send that model
to your function uh for testing
different models again this just gives
you one of the ways to do it and you can
see here we're going to chain train with
the genie and we're also going to change
train with the entropy to see how that
works and if you're going to have your
models going two separate models you're
sending there we'll go ahead and create
a prediction this simply is our y
predict equals our uh whatever object we
sent whatever model we sent here the CL
LF object and predict against our X test
and you can see here print y predict and
return y predict set up on here we'll
load that definition up and then if
you're going to have a function that
runs a predict and print some things out
uh we should also have our accuracy
function so here's our calculate the
accuracy what are we sending we're
sending our y test data this could also
be y actual and Y predict and then we'll
print out a confusion Matrix uh then
we'll print out the accuracy of the um
score on here and print a report
classification report bundle it all
together there so if we bring this all
together we have um all the steps we've
been working towards which is importing
our data by the ways you'll spend 80% of
your time importing data in most machine
learning setups and cooking it and
burning it and getting it formatted so
that it it uh works with whatever models
you're working with the decision tree
has some cool features in that if you're
missing data it can actually pick that
up and just skip that and says I don't
know how to split this there's no way of
knowing whether it rained or didn't rain
last night so I'll look at something
else like whether you watched uh TV
after 8:00 you know that blue screen
thing uh so we have our function
importing our data set we bring in the
data we split the data so we have our X
test test and Y train and then we have
our different models our clf Genie so
it's a decision tree classifier using
the genie setup and then we can also
create the model using entropy uh and
then once we have that we have our
function for making the prediction and
we have our function for calculating the
accuracy uh and then if we're going to
have that we should probably have our
main code involved here this probably
looks more familiar if you're you're
depending on what you're working on if
you're working on like a pie charm then
you would see this in throwing something
up real quick in jupyter Notebook uh so
here's our our main data import which
we've already defined uh we get our
split data we create our Genie we create
our entropy so there's our two models
going on here there's our two models so
these are two separate data models we've
already sent them to be trained then
we're going to go ahead and print the
results using Genie index so we'll start
with the genie
and we want to go ahead with the genie
and print our um our predictions YX test
to the genie and calculate the accuracy
on here and then we want to print the
results using entropy so this is just
the same thing coming down like we did
with the genie we're going to put put
out our y predict entropy and our
calculations so let's go ahead and run
that and just see what this uh piece of
code does uh we do have like one of our
data needs to be is getting a warning on
there that's nothing major because it's
just a simple warning an update of a new
versions coming out uh and so here we
are we have our data set it's got 625
you can actually see an example of the
data set B meaning balanced I guess and
here's our five data points 1111 means
it's balanced it's skewed to the right
with
1112 uh and so forth on here and then
we're going to go ahead and predict from
our prediction whether it's to the right
or to the left you can think of a
washing machine that's SK that's banging
on one side of the thing or maybe it's
an automated car where we're down the
middle of the road that's IM balance and
it starts going veering to the right so
we need to correct for it uh and when we
print out the confusion Matrix we have
three different variables r l and B so
we should three the three different
variables on here and you have as far as
whether it predicts in this case the
balance there's not a lot of balance
loads on here and didn't do a good job
guessing whether it's balanced or not
that's what I took from this first one
uh the second one I'm guessing is the
right so did pretty good job guessing
the right balance you can see that a
bunch of them came up left unbalanced um
probably not good for an automated car
as it tells you 18 out of the uh 18 miss
things and tells you to go the wrong
direction and here we are going the
other way uh 19 to 71 and of course we
can back that up with an accuracy report
on here and you can see the Precision
how well the left and right balance is
79% 79% precision and so forth and then
we went and used the entropy and let me
just see if we can get so we can get
them both next to each other here's our
entropy of our um the first setup our
first model which is the Genie model
6718
1971
6322 2070 pretty close the two models
you know that's not a huge difference in
numbers this second one of entropy did
slightly it looks like slightly worse
cuz it did one better as far as the
right balance and and did what is this
uh four worse on the left balance or
whatever uh so slightly worse if I was
guessing between these I'd probably use
the first one they're so close though
that wouldn't be it wouldn't be a Clear
Choice as to which one worked better and
there's a lot of numbers you can play
with here which might give better
results depending on what the data is
going in now uh one of the takeaways you
should have from the different category
routines we ran is that they run very
similar you you certainly change the
perimeters in them as to whether you're
using what model you're using and how
you're using it and what data they get
applied to but when you're talking about
the scikit learn package it does such an
awesome job of making it easy uh you
split your data up you train your data
and you run the prediction and then you
see what kind of accuracy what kind of
confusion confusion Matrix It generates
so um we talk about algorithm selection
logistic regression K nearest neighbors
uh logistic regression is used when we
have a binomial outcome for example to
predict whether an email is Spam or not
whether the tumor is malignant or not
the logistic regression works really
good on that you can do it in a k
nearest neighbors also the question is
which one will it work better in um I
find the logistic regression models work
really good in a lot of raw numbers so
if you're working with say the stock
market is this a good investment or a
bad investment um so that's one of the
things it handles the numbers better K
near neighbors are used in scenarios
where
nonparametric no fixed number of
perimeters algorithms are required it is
used in pattern recognition Data Mining
and intrusion detection uh so K means
really good in finding the patterns um
I've seen that as a pre-processor to a
lot of other processors where you use
the K nearest neighbors to figure out
what data groups together very powerful
package support Vector machines uh
support Vector machines are used
whenever the data has higher Dimensions
the human genome microarray svms are
extensively used in the hard handwriting
recognition models and you can see that
we were able to switch between the
parabolic and the circular setup on
there where you can now have that dnut
kind of data and be able to filter that
out with the support Vector machine and
then decision trees are mostly used in
operational researches specifically
decision analysis to help identify a
strategy most likely to reach any goal
they are pre preferred where the model
is easy to understand I like that last
one it's a good description is it easy
to understand so you have data coming in
when am I going to go to bed you know is
it raining outside you can go back and
actually look at the pieces and see
those different decision modes takes a
little bit more to dig in there and
figure out what they're doing uh but you
can do that and you can actually help
you figure out why um people love it for
the Y Factor so uh strengths and
limitations
big one on all of these the strengths
and limitations we talk about logistic
regressions uh the strengths are it is
easy to implement and efficient to train
it is relatively easy to regularize the
data points remember how we put
everything between zero and one when you
look at logistic regression models uh
you don't have to worry about that as
much limitations as a high Reliance on
proper representation of data it can
only predict a categorical outcome with
the K nearest neighbor it doesn't need a
separate training period new data can be
added seamlessly without affecting the
accuracy of the model uh kind of an
interesting thing because you can do
partial training uh that can become huge
if you're running across really large
data sets or the data is coming in it
can continually uh do a partial fit on
the data with the K nearest neighbors
and continue to adjust that data uh it
doesn't doesn't work on high dimensional
and large data sets we were looking at
the breast cancer uh 36 different
features what happens when you have 127
features or a million features and you
say well what do you have a million
features in well if I was analyzing uh
log um the legal documents I might have
a tokenizer that splits a words up to be
analyzed and that tokenizer might create
1 million different words available that
might be in the document for doing
weights uh sensitive to noisy data
outliers and missing values that's a
huge one with K nearest neighbors they
really don't know what to do with a
missing value how do you compute the the
distance if you don't know what the
value is uh the svm uh Works more
efficiently on high dimensional data it
is relatively memory efficient so it's
able to create those planes with only a
few different variables in there as
opposed to having to store a lot of data
for different uh features and things
like that it's not suitable for a large
data sets uh the svm you start running
this over gigabytes of data causes some
huge issues under performs if the data
has noise or overlapping that's a big
one we were looking at that where the
spvm splits it and it creates a soft
buffer but what happens when you have a
lot of stuff in the middle uh that's
hard to sort out it doesn't know what to
do with that causes SPM to start
crashing or not perform as well decision
trees handles nonlinear perimeters and
missing values efficiently the missing
values is huge uh I've seen this in uh
was it the wine tasting data sets where
they have three different data sets and
they share certain features uh but then
each one has some features that aren't
in the other ones and it has to figure
out how to handle those well the
decision tree does that automatically
instead of having to figure a way to
fill that data in before processing like
you would with the other models uh it's
easy to understand and has less training
period so it trains pretty quickly uh
comes up there and just keeps forking
the tree down and moving the parts
around and so it it doesn't have to go
through the data multiple times guessing
and adjusting it just creates the tree
as it goes overfitting and high variants
are the most annoying part of it that's
that's an understatement uh that has to
do with how many leavs and how many
decisions you have a due the more you
have the more overfit it is to the data
and also uh just in making the choices
and how the choices come in it might
overfit to a specific feature because
that's where it started at and that's
what it knows and it really um is
challenged with large data sets they've
been working on that with the data
Forest but it's not suitable for large
data sets it's really something you'd
probably run on a single machine and not
across um not across a uh uh data pool
or anything if you are an aspiring data
scientist who's looking out for online
training and certification in data
science from the best universities and
Industry experts then search no more
simply learns postgraduate program in
data science from calc University in
collaboration with IBM should be the
right choice for more details on this
program
please use the link in the description
box below so the decision tree one of
the many powerful tools in the machine
learning library begins with a problem I
think I have to buy a car so in making
this question you want to know how do I
decide which one to buy and you're going
to start asking questions is a mileage
greater than 20 is a price less than 15
will it be sufficient for six people
does it have enough airbag antiock
brakes all these questions come up then
as we feed all this data in we make a
decision and that decision comes up oh
hey this seems like a good idea here's a
car so as we go through this decision
process using a decision tree we're
going to explore this maybe not in
buying a car but in how to process data
what's in it for you let's start by
finding out what is machine learning and
why we even want to know about it for
processing our data and we'll go into
the three basic types of machine
learning and the problems that are used
by Machine learning to solve finally
we'll get into what is a decision tree
what are the problems a decision tree
solves what are the advantages and
disadvantages of using a decision tree
and then we want to dig in a little deep
into the mechanics how does the decision
tree work and then we'll go in and do a
case loan repayment prediction where we
actually going to put together some
python code and show you the basic
python code for generating a decision
tree what is machine learning there are
so many different ways to describe what
is machine learning in today's world and
illustrate it we're going to take a
graphic here and uh making decisions or
trying to understand what's going on and
really underlying machine learning is
people want it wish they were smarter
wish we could understand the world
better so you can see a guy here who's
uh saying hey how can I understand the
world better and someone comes up and
says let's use artificial intelligence
machine learning is a part of artificial
intelligence and that way he gets a big
smile on his face because now he has
artificial intelligence to help and make
his decisions uh and they can think in
new ways so this brings in new ideas so
what is machine learning this is a
wonderful graph here you can see where
we have learn predict decide these are
the most basic three premises of machine
learning in learning we can describe the
data in new ways and able to learn new
aspects about what we're looking at and
then we can use that to predict things
and we can use that to make decisions so
maybe it's something that's never
happened before but we can make a good
guess whether it's going to be a good
investment or not it also helps us
categorize stuff so we can remember it
better so it's easier to pull it out of
the catalog we can analyze data in new
ways we never thought possible and then
of course there's the uh very large
growing industry of recognize we can do
facial recognition driver recognition
automated car recognition all these are
part of machine learning going back to
our guy here who's in his ordinary
system and would like to be smarter make
better choices what happens with machine
learning is an application of artificial
intelligence wherein the system gets the
ability to automatically learn and
improved based on experience so this is
exciting because you have your ordinary
guy who now has another form of
information coming in and this is with
the artificial intelligence helps him
see things he never saw or track things
he can't track so instead of having to
read all the news feeds he can now have
an artificial intelligence sorted out so
he's only looking at the information he
needs to make a choice with and of
course we use all those machine learning
tools back in there and he's now making
smarter choices with less work types of
machine learning let's break it into
three primary types of learning first is
supervised learning where you already
have the data and the answers so if you
worked at a bank you'd already have a
list of all the previous loans and who
defaulted on them and who made good
payments on them you then program your
machine learning tool and that lets you
predict on the next person whether
they're going to be able to make their
payments or not on their loan if you
have one category where you already know
the answers the next one would be you
don't know the answers you just have a
lot of information coming in
unsupervised learning allows you to
group liked information together so if
you're analyzing photos it might group
all the images of trees together and all
the images of houses together without
ever knowing what a house or a tree is
which leads us to the third type of
machine learning the third type of
machine learning is reinforcement
learning unlike supervised or
unsupervised learning you don't have the
data prior to starting so you get the
data one line at a time and then whether
you make a good choice or a bad choice
the machine learning tool has to then
adjust accordingly so you get a plus or
minus feedback you can liken this to the
way a human learns we experience life
one minute at a time and we learn from
that and either our memories as good or
we learn to avoid some problems in
machine learning to understand where the
decision tree fits into our machine
learning tools we have to understand the
basics of some of the machine learning
problems and three of the primary ones
fall underneath classification problems
with categorical Solutions like yes or
no true or false one or zero this might
be does it belong to a particular group
yes or no then we have regression
problems where there's a continuous
value needs to be predicted like product
prices profit and you can see here this
is a very simple linear graph uh you can
guess what the next value is based on
the first four it kind of follows a
straight line going up and clustering
this is problems whereing the data needs
to be organized to find specific
patterns like in the case of product
recommendation they group all the
different products that people just like
you viewed on a shopping site and say
people who bought this also bought this
the most commonly used for the decision
tree is for classification for figuring
out is it b or is it not is it a fruit
or is it a vegetable yes or no true
false left or right 01 and so we talk
about classification we're going to look
at the basic machine learning these are
the four main tools used in
classification there's the Nave Bay
logistic regression decision tree and
random Forest the first two are for
simpler data so if your data is not very
complex you can usually use these to do
a fairly good representation by drawing
a line through the data or curve through
the data they work Wonderful in a lot of
problems but as things get more
complicated the decision tree comes in
and then if you have a very large amount
of data you start getting into the
random Forest so the decision tree is
actually a part of the random Forest but
today we're just going to focus on the
decision
tree what is a decision tree let's go
through a very simple example before we
dig in deep decision tree is a
tree-shaped diagram used to determine a
course of action each branch of the tree
represents a possible decision
occurrence or reaction let's start with
a simple question how do I identify a
random vegetable from a shopping bag so
we have this group of vegetables in here
and we can start off by asking a simple
question is it red and if it's not then
it's going to be the purple fruit to the
left probably an eggplant if it's true
it's going to be one of the red fruits
is a diameter greater than two if false
it's going to be a what looks to be a
red chili and if it's true it's going to
be a bell pepper from the capsicum
family so it's a
capsicum problems that decision tree can
solve so let's look at the two different
categories the decision tree can be used
on it can be used on the classification
the true false yes no and it can be used
on regression where we figure out what
the next value is in a series of numbers
or a group of data in classification the
classification tree will determine a set
of logical if then conditions to
classify problems for example
discriminating between three types of
flowers based on certain features in
regression a regression tree is used
when the target variable is numerical or
continuous in nature we fit the
regression model to the Target variable
using each of the independent variables
each split is made based on the sum of
squared error before we dig deeper into
the mechanics of the decision tree let's
take a look at the advantages of using a
decision tree and we'll also take a
glimpse at the disadvantages the first
thing you'll notice is that it's simple
to understand interpret and visualize it
really shines here because you can see
exactly what's going on in a decision
tree little effort is required for data
preparation so you don't have to do
special scaling there's a lot of things
you don't have to worry about when using
a decision tree it can handle both
numerical and categorical data as we
discovered earlier and nonlinear
parameters don't affect its performance
so even if the data doesn't fit an easy
curved graph you can still use it to
create an effective decision or
prediction so we're going to look at the
advantages of a decision tree we also
need to understand the disadvantages of
a decision tree the first disadvantage
is overfitting overfitting occurs when
the algorithm captures noise in the data
that means you're solving for one
specific instance instead of a general
solution for all the data High variant
the model can get unstable due to small
variation in data low bias tree a highly
complicated decision tree tends to have
a low bias which makes it difficult for
the model to work with new data decision
tree important terms before we dive in
further we need to look at some basic
terms we need to have some definitions
to go with our decision tree in the
different parts we're going to be using
we'll start with entropy entropy is a
measure of Randomness or
unpredictability in the data set for
example we have a group of animals in
this picture there's four different
kinds of animals and this data set is
considered to have a high entropy you
really can't pick out what kind of
animal it is based on looking at just
the four an animals as a big clump of of
uh entities so as we start splitting it
into subgroups we come up with our
second definition which is Information
Gain Information Gain it is the measure
of decrease in entropy after the data
set is split so in this case based on
the color yellow we've split one group
of animals on one side as true and those
who aren't yellow as false as we
continue down the yellow side we split
based on the height true or false equals
10 and on the other side height is less
than 10 true or false and as you see as
we split it the entropy continues to be
less and less and less and so our
Information Gain is simply the entropy
E1 from the top and how it's changed to
E2 in the bottom and we'll look at the
uh deeper math although you really don't
need to know a huge amount of math when
you actually do the programming in
Python because they'll do it for you but
we'll look on the actual math of how
they compute entropy finally we went
under the different parts of our tree
and they call the leaf node Leaf node
carries a classification or the decision
so it's the final end at the bottom the
decision node has two or more branches
this is where we're breaking the group
up into different parts and finally you
have the root node the topmost decision
node is known as the root
node how does a decision tree work
wonder what kind of animals I'll get the
jungle today maybe you're the hunter
with the gun or if you're more into
photography you're a photographer with a
camera so let's look at this group of
animals and let's try to classify
different types of animals based on
their features using a decision tree so
the problem statement is to classify the
different types of animals based on
their features using a decision tree the
data set is looking quite messy and the
entropy is high in this case so let's
look at a training set or a training
data set and we're looking at color
we're looking at height and then we have
our different animals we have our
elephants our giraffes our monkeys and
our tigers and they're of different
colors and shapes let's see what that
looks like and how do we split the data
we have to frame the conditions that
split the data in such a way that the
Information Gain is the highest note
gain is the measure of decrease in
entropy after splitting so the formula
for entropy is the sum that's what this
symbol looks like that looks like kind
of like a uh e funky e of K where I
equals 1 to k k would represent the
number of animal the different animals
in there where value or P value of I
would be the percentage of that animal
times the log base 2 of the same the
percentage of that animal let's try to
calculate the entropy for the current
data set and take a look at what that
looks like and don't be afraid of the
math you don't really have to memorize
this math just be aware that it's there
and this is what's going on in the
background and so we have three giraffes
two tigers one monkey two elephants a
total of eight animals gathered and if
we plug that into the formula we get an
entropy that equals 3 over8 so we have
three drafts a total of eight times the
log usually they use base two on the log
so log base 2 of 3 over 8 plus in this
case let's say it's the elephants 2 over
8 2 elephants over total of 8 * log base
2 2 over 8 plus one monkey over total of
8 log base 2 1 over 8 and plus 2 over 8
of the Tigers log base 2 over 8 and if
we plug that into our computer our
calculator I obviously can't do logs in
my head we get an entropy equal to
.571 the program will actually calculate
the entropy of the data set similarly
after every split to calculate the gain
now we're not going to go through each
set one at a time to see what those
numbers are we just want you to be aware
that this is a Formula or the
mathematics behind it gain can be
calculated by finding the difference of
the subsequent entropy values after a
split now we will try to choose a
condition that gives us the highest gain
we will do that by splitting the data
using each condition and checking that
the gain we get out of them the
condition that gives us the highest gain
will be used to make the first split can
you guess what that first split will be
just by looking at this image as a human
it's probably pretty easy to split it
let's see if you're right if you guessed
the color yellow you're correct let's
say the condition that gives us the
maximum gain is yellow so we will split
the data based on the color yellow if
it's true that group of animals goes to
the left if it's false it goes to the
right the entropy after the splitting
has decreased considerably however we
still need some splitting at both the
branches to attain an entropy value
equal to zero so we decided to split
both the nodes using height as a
condition since every Branch now
contains single label type we can say
that entropy in this case has reached
the least value and here you see we have
the giraffes the Tigers the monkey and
the elephants all separated into their
own groups this tree can now predict all
the classes of animals present in the
data set with 100% accuracy that was
easy use case loan repayment prediction
let's get into my favorite part and open
up some Python and see what the
programming code in the scripting looks
like in here we're going to want to do a
prediction and we start with this
individual here who's requesting to find
out how good his customers are going to
be whether they're going to repay their
loan or not for his bank and from that
we want to generate a problem statement
to predict if a customer will repay loan
amount or not and then we're going to be
using the decision tree algorithm in
Python let's see what that looks like
and let's dive into the code in our
first few steps of implementation we're
going to start by importing the
necessary packages that we need from
Python and we're going to load up our
data and take a look at what the data
looks like so the first thing I need is
I need something to edit my Python and
run it in so let's flip on over and here
I'm using the Anaconda Jupiter notebook
now you can use any python IDE you like
to run it in but I find the jupyter
notebooks really nice for doing things
on the Fly and let's go ahead and just
paste that code in the beginning and
before we start let's talk a little bit
about what we're bringing in and then
we're going to do a couple things in
here we have to make a couple changes as
we go through this first part of the
import the first thing we bring in is
numpy as NP that's very standard when
we're dealing with uh mathematics
especially with uh very complicated
machine learning tools you almost always
see the numpy come in for your num your
numberers it's called number python it
has your mathematics in there in this
case we actually could take it out but
generally you'll need it for most of
your different things you work with and
then we're going to use pandas as PD
that's also a standard the pandas is a
data frame setup and you can liken this
to uh take your basic data and storing
it in a way that looks like an Excel
spreadsheet so as we come back to this
when you see NP or PD those are very
standard uses you'll know that that's
the pandas and I'll show you a little
bit more when we explore the data in
just a minute then we're going to need
to split the data so I'm going to bring
in our train test and split and this is
coming from the sklearn package cross
validation in just a minute we're going
to change that and we'll go over that
too and then there's also the sk. tree
import decision tree classifier that's
the actual tool we're using remember I
told you don't be afraid of the
mathematics it's going to be done for
you well the decision tree classifier
has all that mathematics in there for
you so you don't have to figure it back
out again and then we have SK learn.
metrics for accuracy score we need to
score our our setup that's the whole
reason we're splitting it between the
training and testing data and finally we
still need the sklearn import tree and
that's just the basic tree function
that's needed for the decision tree
classifier and finally we're going to
load our data down here and I'm going to
run this and we're going to get two
things on here one we're going to get an
error and two we're going to get a
warning let's see what that looks like
so the first thing we had is we have an
error why is this error here well it's
looking at this it says I need to read a
file and when this was written the
person who wrote it this is their path
where they stored the file so let's go
ahead and fix
that and I'm going to put in here my
file path I'm just going to call it full
file name and you'll see it's on my C
drive and this is very lengthy setup on
here where I stored the data 2. CSV
file don't worry too much about the full
path because on your computer it'll be
different the data. 2 CSV file was
generated by simply learn if you want a
copy of that you can comment down below
and request it here in the
YouTube and then if I'm going to give it
a name full file name I'm going to go
ahead and change it here to
full file name so let's go ahead and run
it now and see what
happens and we get a
warning when you're coding understanding
these different warnings and these
different errors that come up is
probably the hardest lesson to learn so
let's just go ahead and take a look at
this and use this as a uh opportunity to
understand what's going on here if you
read the warning it says the cross
validation is depreciated app it so it's
a warning on it's being removed and it's
going to be moved in favor of the model
selection so if we go up here we have
sklearn Doc crossvalidation and if you
research this and go to sklearn site
you'll find out that you can actually
just swap it right in there with model
selection and so when I come in here and
I run it again that removes a warning
what they've done is they've had two
different developers develop it in two
different branches and then they decided
to keep one of those and eventually get
rid of the other one that's all that is
and very easy and quick to
fix before we go any further I went
ahead and opened up the data from this
file remember the the data file we just
loaded on here the dataor 2. CSV let's
talk a little bit more about that and
see what that looks like both as a text
file because it's a comma separated
variable file and in a spreadsheet this
is what it looks like as a basic text
file you can you see at the top they've
created a header and it's got 1 2 3 4
five columns and each column has data in
it and let me flip this over cuz we're
also going to look at this uh in an
actual spreadsheet so you can see what
that looks like and here I've opened it
up in the open Office calc which is
pretty much the same as um Excel and
zoomed in and you can see we've got our
columns and our rows of data little
easier to read in here we have a result
yes yes no we have initial payment last
payment credit score house number if we
scroll way
down we'll see that this occupies a,1
lines of code or lines of data with uh
the first one being a column and then
1,000 lines of
data now as a
programmer if you're looking at a small
amount of data I usually start by
pulling it up in different sources so I
can see what I'm working
with but in larger data you won't have
that option it'll just be um too too
large so you need either bring in a
small amount that you can look at it
like we're doing right now or we can
start looking at it through the python
code so let's go ahead and move on and
take the next couple steps to explore
the data using python let's go ahead and
see what it looks like in Python to
print the length and the shape of the
data so let's start by printing the
length of the database we can use a
simple Lin function from Python and when
I run this you'll see that it's a th
long and that's what we expected there's
a th lines of data in there if you
subtract the column head and this is one
of the nice things when we did the uh
balance data from the panda read CSV
you'll see that the header is row zero
so it automatically removes a
row and then shows the data separate it
does a good job sorting that data out
for us and then we can use a different
function and let's take a look at that
and again we're going to utilize the
tools in
Panda and since the balance uncore data
was loaded at as a panda data
frame we can do a shape on it and let's
go ahead and run the shape and see what
that looks
like what's nice about this shape is not
only does it give me the length of the
data we have a thousand lines it also
tells me there's five columns so we were
looking at the data we had five columns
of data and then let's take one more
step to explore the data using Python
and now that we've taken a look at the
length and the shape let's go ahead and
use the uh pandas module for head
another beautiful thing in the data set
that we can utilize so let's put that on
our sheet here and we have print data
set and balance data. head and this is a
panda's print statement of its own so it
has its own print feature in there and
then we went ahead and gave a label for
a print job here of data set just a
simple print statement and when we run
that and let's just take a closer look
at that let me zoom in
here there we
go pandas does such a wonderful job of
making this a very clean readable data
set so you can look at the data you can
look at the column headers you can have
it uh when you put it as the head it
prints the first five lines of the data
and we always start with zero so we have
five lines we have 0 1 2 3 4 instead of
1 2 3 4 5 that's a standard scripting
and programming set is you want to start
with the zero position and that is what
the data head does it pulls the first
five rows of data puts in a nice format
that you can look at and view very
powerful tool to view the data so
instead of having to flip and open up an
Excel spreadsheet or open Office Cal or
trying to look at a word dock where it's
all scrunched together and hard to read
you can now get a nice open view of what
you're working with we're working with a
shape of a th long five wide so we have
five columns and we do the full data
head you can actually see what this data
looks like the initial payment last
payment credit scores house number so
let's take this now that we've explored
the data and let's start digging into
the decision tree so in our next step
we're going to train and build our data
tree and to do that we need to First
separate the data out we're going to
separate into two groups so that we have
something to actually train the data
with and then we have some data on the
side to test it to see how good our
model is remember with any of the
machine learning you always want to have
some kind of test set to to weigh it
against so you know how good your model
is when you distribute it let's go ahead
and break this code down and look at it
in pieces so first we have our X and
Y where do X and Y come from well X is
going to be our data and Y is going to
be the answer or the target you can look
at it source and Target in this case
we're using X and Y to denote the data
in and the data that we're actually
trying to guess what the answer is going
to be and so to separate it we can
simply put in x equals the balance of
the data. values the first brackets
means that we're going to select all the
lines
in the database so it's all the data and
the second one says we're only going to
look at columns 1 through five remember
we always start with zero zero is a yes
or no and that's whether the loan went
default or not so we want to start with
one if we go back up here that's the
initial payment and it goes all the way
through the house
number well if we want to look at uh 1
through five we can do the same thing
for Y which is the answers and we're
going to set that just equal to the zero
row so so it's just the zero row and
then it's all rows going in there so now
we've divided this into two different
data sets one of them with the data
going in and one with the
answers next we need to split the
data and here you'll see that we have it
split into four different parts the
first one is your X training your X test
your y train your y test
simply put we have X going in where
we're going to train it and we have to
know the answer to train it with and
then we have X test where we're going to
test that data and we have to know in
the end what the Y was supposed to be
and that's where this train test split
comes in that we loaded earlier in the
modules this does it all for us and you
can see they set the test size equal to3
so it's roughly 30% will be used in the
test and then we use a random state so
it's completely random which rows it
takes takes out of there and then
finally we get to actually build our
decision tree and they've called it here
clf entropy that's the actual decision
tree or decision tree classifier and in
here they've added a couple variables
which we'll explore in just a minute and
then finally we need to fit the data to
that so we take our clf entropy that we
created and we fit the XT train and
since we know the answers for X train or
the Y train we go aad and put those in
and let's go ahead and run this and what
most of these SK learn modules do is
when you set up the variable in this
case when we set the clf entrop equal
decision tree classifier it
automatically prints out what's in that
decision tree there's a lot of variables
you can play with in here and it's quite
beyond the scope of this tutorial to go
through all of these and how they work
but we're working on entropy that's one
of the options we've added that it's
completely a random state of 100 so 100%
And we have a max depth of three now the
max depth if you remember above when we
were doing the different graphs of
animals means it's only going to go down
three layers before it stops and then we
have minimal samples of leaves is five
so it's going to have at least five
leavs at the end so I'll have at least
three splits I'll have no more than
three layers and at least five end
leaves with the final result at the
bottom now that we've created our
decision tree classifier not only
created it but trained it let's go ahead
and apply it and see what that looks
like so let's go ahead and make a
prediction and see what that looks like
like we're going to paste our predict
code in here and before we run it let's
just take a quick look at what it's
doing here we have a variable y predict
that we're going to do and we're going
to use our variable clf entropy that we
created and then you'll see do predict
and that's very common in the SK learn
modules that their different tools have
the predict when you're actually running
a prediction in this case we're going to
put our X test data in here
now if you delivered this for use an
actual commercial use and distributed it
this would be the new loans you're
putting in here to guess whether the
person's going to be uh pay them back or
not in this case so we need to test out
the data and just see how good our
sample is how good of our tree does at
predicting the loan payments and finally
since Anaconda Jupiter notebook is works
as a command line for python we can
simply put the Y predict e in to print
it I could just as easily have put the
print and put brackets around y predict
e in to print it out we'll go ahead and
do that it doesn't matter which way you
do
it and you'll see right here that it
runs a prediction this is roughly 300 in
here remember it's 30% of a th000 so you
should have about 300 answers in here
and this tells you which each one of
those lines of our test went in there
and this is what our y predict came out
so let's move on to the next step we're
going to take the this data and try to
figure out just how good a model we have
so here we go since sklearn does all the
heavy lifting for you and all the math
we have a simple line of code to let us
know what the accuracy is and let's go
ahead and go through that and see what
that means and what that looks like
let's go ahead and paste this in and let
me zoom in a little bit there we go so
you have a nice full picture and we'll
see here we're just going to do a print
accuracy is and then we do the accuracy
score and this was something we imported
um earlier if you remember at the very
beginning let me just scroll up there
real quick so you can see where that's
coming from that's coming from here down
here from sklearn docs import accuracy
score and you could probably run a
script make your own script to do this
very easily how accurate is it how many
out of 300 do we get right and so we put
in our y test that's the one we ran the
predict on and then we put in our y
predict in that's the answers we got and
we're just going to mulp mply that by
100 because this is just going to give
us an answer as a decimal and we want to
see it as a percentage and let's run
that and see what it looks like and if
you see here we got an accuracy of 93.
66667 so when we look at the number of
loans and we look at how good our model
fit we can tell people it has about a
93.6 fitting to it so just a quick recap
on that we now have accuracy setup on
here and so we have created a model that
uses the decision algorithm to predict
whether a customer will repay the loan
or not the accuracy of the model is
about
94.6% the bank can now use this model to
decide whether it should approve the
loan request from a particular customer
or not and so this information is really
powerful we might not be able to as
individuals understand all these numbers
because they have thousands of numbers
that come in but you can see that this
is a smart decision for the bank to use
a tool like this to help them to predict
how good their uh profits going to be
off of the loan balances and how many
are going to default or not so we've had
a lot of fun learning about decision
trees so let's take a look at the key
takeaways that we've covered today what
is machine learning we covered up some
different aspects of machine learning
and what that is utilized in your
everyday life and what you can use it
for for predicting for describing for
guessing what the next outcome is for
storing information we looked at the
three main types of machine learning
supervised learning unsupervised
learning and reinforced learning we
looked at problems in machine learning
and what it solves classification
regression and clustering finally we
went through uh how does the decision
tree work where we looked at the hunter
he's trying to sort out the different
animals and what kind of animals they
are and then we rolled up our sleeves
and did our python coding and actually
applied it to a data set with that we
have come to an end of this session on
data science full cuse if you have any
queries regarding any of the topics
covered in this session or if you
require the resources that we used in
the session like the pp code
demonstrated all the data sets used then
do let us know in the comment section
below and our team of experts will be
more than happy to resolve all your
queries at the earliest until next time
thank you stay safe and keep learning
staying ahead in your career requires
continuous learning and upscaling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in cuttingedge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know
more hi there if you like this video
subscribe subcribe to the simply learn
YouTube channel and click here to watch
similar videos to nerd up and get
certified click here