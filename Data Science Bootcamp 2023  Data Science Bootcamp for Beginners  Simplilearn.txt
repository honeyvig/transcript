foreign
boot camp this internship program is
designed to equip you with the skills
and knowledge needed to become a
professional data scientist whether you
are a beginner or have some prior
experience in data science this boot
camp will provide you with an extensive
training program and assist you in
developing your expertise
did you know that between the dawn of
time in 2003 five exabytes of data had
been created at Google by 2010 this
amount of data was being created every
two days and by 2021 it was being
created every 40 minutes and data
science was identified as a skill with
the high estimate a professional data
scientist will get an average salary of
121 000 in the US and 10 lakh in India
are you looking to Kickstart your career
in data science then take up our Caltech
data science bootcamp by simply learn
and upskill yourself to a professional
level this certification program will
take you through the data science
Concepts from basic to advanced level
with its essential tools like python
numpy C bone etc for more information
about the program check out the link in
the description box
hi guys welcome to this data science
bootcamp by simplylearn in this boot
camp you will learn about the various
factors of data science
including data science Basics and top 5
data science lab Diaries then we will
discuss linear and logistic regression
followed by some of the algorithms like
decision tree random tree and kn
moving forward we will understand
clustering and deep learning and at last
we'll discuss the most frequently Asked
data science interview questions now
over to our training experts
are you one of the many who dreams of
becoming a data scientist
watching this video if you're passionate
about data science because we will tell
you how does it really work under the
hood Mi is a data scientist let's see
how a day in a life goes while she's
working on data science project well it
is very important to understand the
business problem first in our meeting
with the clients Emma asks relevant
questions understands and defines
objectives for the problem that needs to
be tackled she is a curious Soul who
asks a lot of eyes one of the many
traits of a good data scientist now she
gears up for data acquisition to gather
and scrape data from multiple sources
like web servers logs databases apis and
online repositories oh it seems like
finding the right data takes both time
and effort after the data is gathered
comes data preparation this step
involves data cleaning and data
transformation data
close time consuming process as it
involves handling many complex scenarios
here Mr deals with inconsistent data
types misspelled attributes missing
values duplicate values and whatnot then
in data transformation she modifies the
data based on defined mapping rules in a
project
ETL tools like talent and Informatica
are used to perform complex
Transformations that helps the team to
understand the data structure better
then understanding what you actually can
do with your data is very crucial for
that Emma does exploratory data analysis
with the help of Eda she defines and
refines the selection of feature
variables that will be used in the model
development but what if Emma skips this
step she might end up choosing the wrong
way tables which will produce an
inaccurate model thus exploratory data
analysis becomes the most important step
now she proceeds to the core activity of
a data science project which is data
modeling she repetitively applies
diverse Gene learning techniques like k
n decision tree knife base to the data
to identify the model that best fits the
business requirement she trains the
models on the training data set and
tests them to select the best performing
model Emma prefers python for modeling
the data however it can also be done
using R and SAS well the trickiest part
is not yet over visualization and
communication Emma meets the clients
again to communicate the business
findings in a simple and effective
manner to convince the stakeholders she
uses tools like Tableau power bi and
qlik view that can help her in creating
powerful reports and
finally maintains the model she tests
the selected model in a pre-production
environment before deploying it in the
production environment which is the best
practice right after successfully
deploying it she uses reports and
dashboards to get real-time analytics
further she also monitors and maintains
the Project's performance well that's
how Emma completes the data science
project we have seen the daily routine
of a data scientist is a whole lot of
fun has a lot of interesting aspects and
comes with its own share of challenges
now let's see how data science is
changing the world data science
techniques along with genomic data
provides a deeper understanding of
genetic issues in reaction to particular
drugs and diseases logistic companies
like DHL FedEx have discovered the best
routes to ship the best suited time to
deliver the best mode of transport to
choose thus leading to cost efficiency
with data science it is possible to not
only predict employee attrition but to
also understand the key variables that
influence employee turnover also the
airline companies can now easily predict
flight delay and notify the passengers
beforehand to enhance their travel
experience well if you're wondering
there are various roles offered to a
data scientist like data analyst machine
learning engineer deep learning engineer
data engineer and of course data
scientist the median base salaries of a
data scientist can range from 95 000 to
165 000 so that was about the data
science are you ready to be a data
scientist if yes then start today the
world of data needs you that's all from
my side today thank you for watching
comment below the next topic that you
want to learn and subscribe to Simply
learn to get the latest updates on more
such interesting videos thank you and
keep learning
so what are the prerequisites for day
there are three essential traits
required for to be a data scientist one
is curiosity you need to be able to ask
questions the first step in data science
is asking questions what is the problem
we are trying to solve if you ask the
right question only then you'll get the
right answer very often this is a very
crucial step where a lot of data science
projects fail because you're you may be
asking the wrong question and then
obviously when you get the answer that's
not the answer you're looking for so it
is very important that you ask the right
question needless to say then the second
part or the second trait is common sense
so you need to be creative you need to
come up with ways to use the data that
you have and try to solve the business
problem on hand in many cases you may
not have all the data that you need in
many cases the data may be incomplete so
that is where you need to come up with
ways what are the best ways to fill
these gaps wherever this is missing and
that's where Common Sense comes into
play last but not least after doing all
this analysis if you are unable to
communicate the results in the right way
the whole exercise will fail so
communication is a key trait for a data
scientist maybe technically you may be a
genius but then if you are unable to
communicate those results in a proper
way once again that will not help so
these are the three main traits
curiosity common sense and community
skills in a way you can say these are
the three C's okay so what are the other
prerequisites the first one so machine
learning machine learning is the
backbone of data science
data science involves quite a bit of
machine learning in addition to the
basic statistics that we do so a data
scientist needs to have a good hang or
need to be very good at data science
second part is modeling so modeling is
also a part of machine learning in a way
but you need to be good at identifying
what are the algorithms that are more
suitable to solve a given problem what
models can we use and how do we train
these models and so on and so forth so
that is the second component then
statistics statistics is like the core
Foundation of data science so you need
to understand statistics and you need to
have a good hang of Statistics in order
to be a good data scientist and this
will also help in getting good results
programming is to some extent required
at least some program or the other would
be required as a part of executing a
data science project the most common
programming languages are Python and R
python specially is becoming a very
popular programming language in data
science because of its ease of learning
because of the multiple live libraries
that it supports for performing data
science and machine learning and so on
so python is by far one of the most
popular languages in data science if any
one of you is wanting to learn a new
language that should be Python and then
of course you need to understand
databases how databases work and how to
handle databases how to get data out of
databases and so on so these are some of
the key components of data science now
coming to the tools and skills that are
used in data science these are some of
the skills from a language perspective
it is python or R and from a skills
perspective in addition to some of the
programming languages it would help if
you have a good knowledge or good
understanding of statistics and what are
the tools that are used in data analysis
SAS is one of the most popular tools
it's been there for very long time and
that's the reason it is very popular and
however this is compared to most of the
other tools it is a proprietary software
whereas Python and R are mostly open
source the other tools are like Jupiter
Jupiter notebooks you have rstudio these
are more development environments and
development tools so Jupiter notebooks
is a interactive development environment
similarly our studio is for performing
or writing our code and Performing
analytics and Performing data analysis
and machine learning activities you can
perform
our studio it has a very nice UI and
initially R was not so popular primarily
because it did not have user interface
and our studio is a relatively new
edition and after the Advent of our
studio R became extremely popular and
there are other tools like Matlab and of
course some people do with Excel as well
as far as data warehousing is concerned
some of the skills that are required are
ETL so in order to extract data and
transform low dtl stands for extract
transform load so you have data in the
databases like your Erp system or a CRM
system you need to extract that and then
do some Transformations and then load it
into your Warehouse so that all the data
from various sources looks uniform then
you need some SQL skills which is
basically querying the data writing SQL
queries Hadoop is another important
skill especially if you are handling
large amounts of data
also one of the Specialties of Hadoop is
it can be used for handling the
unstructured data as well so it can be
used for large amounts of structured and
unstructured data then spark is a
excellent Computing engine for
performing data analysis or machine
learning in a distributed mode so if you
have large amount of data the
combination of spark and Hadoop can be
extremely powerful so you store your
data in Hadoop hdfs and use spark as
your computation engine it works in a
distributed mode similar to Hadoop if
so that those are excellent skills for
data warehousing and there are some
standard tools that are available like
Informatica data stage talent and also
AWS redshift if you want to do some on
the cloud I think AWS redshift is again
a good tool data visualization tools for
data visualization some of the skills
that would be required are let's say are
you R provides some
visualization capabilities especially
for developing during development and
then you have python libraries
matplotlib and so on which provides very
powerful visualization capabilities and
that is from skills perspective whereas
tools that can be used are Tableau is a
very very popular visualization tool
again that's a proprietary tool so it's
a little expensive maybe but excellent
capabilities from a visualization
perspective then there are tools like
cognos which is an IBM product which
provides very good visualization
capabilities as well and then coming to
the machine learning part of it the
skills required there are python which
is more for programming part and then
you will need some mathematical skills
like algebra linearly
minerals and then statistics and maybe a
little bit of calculus and so on and the
tools that are used for machine learning
are spark mlib and Apache mahot and on
cloud if you want to do something you
can use Microsoft Azure ml Studio as
well so these are by no means an
exhaustive list there are actually many
many tools and probably a few more
skills also maybe there but this is this
gives a quick overview like a
summarizing of summarization of the
tools and skills now moving on to the
life of a data scientist what does a
data scientist do during the course of
his work so let's see so typically a
data scientist is given a problem a
business problem that he needs to solve
and in order to do that if you remember
from the previous slide he basically
asks the question as to what is the
problem that he needs to solve so that
is the first thing he has got the
problem then the next thing is to gather
the data that is required
solve this problem so he goes about
looking for data from anywhere it could
be the Enterprise very often the data is
not provided in the nice format that he
would like to have it or we would like
to have it so first step is to get
whatever data that is possible what is
known as raw data in whatever format so
it could be Enterprise data it could be
it is a probably a requirement to go and
get some public data in some cases so
all that raw data is collected and then
that is processed and analyzed and in
prepared into a format in which it can
be used and then it is fed into the
analytics system be it machine learning
algorithm or statistical model and we
get the output and then he puts these
output in a proper format for presenting
it to the stakeholders and communicating
those insights or the results to the
stakeholder so this is a very high level
View of like a a day in the life of a
data scientist so Gathering data raw
data performing some quick analysis on
that and maybe processing or
manipulating this data to bring it into
a certain good format so that it can be
used for the analysis feeding this into
that analysis system that has been
designed be it mathematical models
machine learning models and then get the
results the insights and then present it
in a nice way so that the stakeholders
can understand how about machine
learning algorithms so let's see what
are the various machine learning
algorithms that would be required for a
data scientist so these are a few of the
algorithms again there's not an
exhaustive list we have regression is
one of the supervised learning models or
techniques so in case of regression you
try to let's say come up with a
continuous number so the difference
between regression and let's say a
classification is that in case of
classification those are discrete values
whereas here we are talking about
regression where you let's say you are
trying to predict the temperature which
is a continuous value or the share price
which is a continuous value so that is
regression so you need to know what is
regression how to perform regression and
we need to understand clustering so
clustering is one of the unsupervised
learning techniques in this case there
is no label data is available and you
get some data and then you want to put
this into some shape so that you can
analyze it and you try to make sense out
of it let's say you have one example is
you have a list of cricketers and they
have not been marked as Bowlers and
batsmen or roll renders or whatever
right so you just have their names and
maybe how many runs they scored how many
wickets they have taken and so on but
there is no readily available
information saying that okay this person
is a batsman this person is a bowler and
so on so I'm talking about Cricket
hopefully most of you are familiar with
the game of cricket so how do we find
out so then we put this into a
clustering mechanism and then the system
will say that okay these are the people
who are all who have all scored good
amount of runs so they belong to one
cluster these are all the people who
have taken good amount of wickets so
they belong to one cluster and maybe
here are some people who have taken good
amount of wickets and they have made
good amount of runs so they may be
belonging to one group and then we take
a look at it and then we label them as
okay people who have all together and
those are you know scored many runs they
are we label them as batsmen people who
have taken a lot of wickets we label
them as Bowlers and people who have
taken good amount of wickets and also
made some good runs we label them as
all-rounders but the system will just
say okay this is cluster one cluster two
cluster three the names we give we human
beings have to give the names now
decision tree is used for what is known
as classification primarily it can also
be used for regression but by and large
it is used for classification and here
again it's a very logical way in which
the algorithm goes about classifying the
various inputs one of the biggest
advantages of decision tree is that it's
very easy to understand and it's very
easy to explain why a certain object has
been classified in a certain way
compared to maybe some of the other
mechanisms like say support Vector
machines or logistic regression and so
on so that's the advantage of dysentery
but that is also very popular algorithm
then we have support Vector machines
primarily for classification purpose and
then we have knife base this is again a
statistical probability based
classification method so these are a few
algorithms there are a few more that are
not listed here but there are some more
algorithms as well and by the way there
are more detailed or there are detailed
videos about each of these algorithms
available you can check in the playlist
so now let's talk about the life cycle
of a data science project okay the first
step is the concept study in this step
it involves understanding the business
problem asking questions get a good
understanding of the business model meet
up with all the stakeholders understand
what kind of data is available and all
that is a part of the first step so here
are a few examples we want to see what
are the various specifications and then
what is the end goal what is the budget
is there an example of this kind of a
problem that has been maybe solved
earlier so all this is a part of the
concept study and another example could
be a very specific one to predict the
price of a 1.35 carat diamond and there
may be Rel event information inputs that
are available and we want to predict the
price the next step in this process is
data preparation data Gathering and data
preparation also known as data munging
or sometimes it is also known as data
manipulation so what happens here is the
raw data that is available may not be
usable in its current format for various
reasons so that is why in this step a
data scientist would explore the data he
will take a look at some sample data
maybe there are millions of Records pick
a few thousand records and see how the
data is looking are there any gaps is
the structure appropriate to be fed into
the system are there some columns which
are probably not adding value may not be
required for the analysis very often
these are like names of the customers
they will probably not add any value or
much value from an analysis perspective
the structure of the data Maybe the data
is coming from multiple data sources and
the structures may not be matching what
are the other problems there may be gaps
in the data so the data all the columns
all the cells are not filled if you're
talking about structured data there are
several blank records or blank columns
so if you use that data directly you'll
get errors or you will get inaccurate
results so how do you either get rid of
that data or how do you fill this gaps
with something meaningful so all that is
a part of data munging or data
manipulation so these are some
additional sub topics within that so
data integration is one of them there
are any conflicts in the data there may
be data may be redundant yeah data
resonant redundancy is another issue
then maybe you have let's say data
coming from two different systems and
both of them have customer table for
example customer information so when you
merge them there is a duplication issue
so how do we resolve that so that is
what data transformation as I said there
will be situations where data is coming
from multiple sources and then when we
merge them together they may not be
matching so we need to do some
transformations to make sure everything
is similar we may have to do some data
reduction if the data size is too big
you may have to come up with ways to
reduce it meaningfully without losing
information then data cleaning so there
will be either wrong values or your null
values or there are missing values so
how do you handle all of that few
examples of very specific stuff so there
are missing values how do you handle
missing values or null values here in
this particular slide we are seeing
three types of issues one is missing
value then you have null value you see
the difference between the two right so
in a missing value there is nothing
blank null value it says null now the
system cannot handle if there are null
values similarly there is improper data
so it's supposed to be numeric value but
there is a string or a non-numeric value
so how do we clean and prepare the data
so that our system can work flawlessly
so there are multiple ways and there is
no one common way of doing this it can
vary from Project to project it can vary
from what exactly is the problem we are
trying to solve it can vary from data
scientist to data scientists
organization to organization so these
are like some standard practices people
come up with and and of course there
will be a lot of trial and errors
somebody would have tried out something
and it worked and
that mechanics mechanism so that's how
we need to take care of data cleaning
now what are the various ways of doing
you know if values are missing how do
you take care of that now if the data is
too large and only a few records have
some missing values then it is okay to
just get rid of those entire rows for
example so if you have a million records
and out of which 100 records don't have
full data so there are some missing
values in about 100 cards so it's
absolutely fine because it's a small
percentage of the data so you can get
rid of the entire records which are
missing values but that's not a very
common situation very often you will
have multiple or at least you know large
number of a data set for example a lot
of million records you may have 50 000
records which are like having missing
values now that's a significant amount
you cannot get rid of all those records
your analysis will be inaccurate so how
do you handle such situations so there
are again multiple ways of doing it one
is you can probably if a particular
values are missing in a particular
column you can probably take the mean
value for that particular column and
fill all the missing values with the
mean value so that first of all you
don't get errors because of missing
values and second you don't get results
that are way off because these values
are completely different from what is
there so that is one way then a few
other could be either taking the median
value or depending on what kind of data
we are talking about so something
meaningful we will have put in there if
we are doing some machine learning
activity then obviously as a part of
data preparation you need to split the
data into training and test data set the
reason being if you try to test with a
data set which the system has already
seen as a part of training then it will
tend to give reasonably accurate results
because it has already seen that data
and that is not a good measure of the
accuracy of the system so typically you
take the entire data set the input data
set and split it into two parts and
again the ratio can vary from person to
person individual preferences some
people like to split it into 50 50 some
people like it has
63.33 and 33.3 is basically two third
and one third and some people do it as
80 20 80 for training and 24 testing so
you split the data perform the training
with the 80 percent and then use the
remaining 20 for testing all right so
that is one more data preparation
activity that needs to be done before
you start analyzing or applying the data
or putting the data through the model
then the next step is model planning now
this models can be statistical models
this could be machine learning models so
you need to decide what kind of models
you're going to use again it depends on
what is the problem you're trying to
solve if it is a regression problem you
need to think of a Direction algorithm
and come up with a regression model so
it could be linear regression or if you
are talking about classification then
you need to pick up an appropriate
classification algorithm like logistic
regression or decision tree or svm and
then you need to train that particular
model so that is the model building or
model planning process and the cleaned
up data has to be fed into the model and
apart from cleaning you may also have to
in order to determine what kind of model
you will use you have to perform some
exploratory data analysis to understand
the relationship between the various
variables and save the data is
appropriate and so on right so that is
the additional preparatory step that
needs to be done so little bit of
details about exploratory data analysis
so what exactly is exploratory data
analysis basically to as the name
suggests you're just exploring you just
receive the data and you're trying to
explore and find out what are the data
types and what is the is the data clean
in in each of the columns what is the
maximum minimum value so for example
there are out of the box functionality
available in tools like R so if you just
ask for a summary of the table it will
tell you for each column it will give
some details as to what is the mean
value what is the maximum value and so
before so this exercise or this
exploratory analysis is to get an
understanding of your data and then you
can take steps to during this process
you find there are a lot of missing
values you need to take step fix those
you will also get an idea about what
kind of model to be used and so on and
so forth what are the various techniques
used for exploratory data analysis
typically these would be visualization
techniques like you use histograms you
can use box plots you can use Scatter
Plots so these are very quick ways of
identifying the patterns or a few of the
trends of the data and so on and then
once your data is ready you you've
decided on the model what kind of model
what kind of algorithm you're going to
use if you're trying to do machine
learning you need to pass your 80 the
training data or rather you use the
training data to train your model and
the training process itself is iterative
so the training process you may have to
perform multiple times and once the
training is done and you feel it is
giving good accuracy then you move on to
test so you take the remaining 20 of the
data remember we split the data into
training and test so the test data is
now used to check the accuracy or how
well our model is performing and if
there are further issues let's say a
model is still during testing in the
accuracy is not good then you may want
to retrain your model or use a different
model so this whole thing again can be
iterative but if the test process is
passed or if the model passes the test
then it can go into production and it
will be deployed all right so what are
the various tools that we use for model
planning R is an excellent tool in a lot
of ways whether you're doing regular
statistical analysis or machine learning
or any of these activities are in along
with our studio provides a very powerful
environment to do data analysis
including visualization it has a very
good integrated visualization or plot
mechanism which can be used for doing
exploratory data analysis and then later
on to do analysis detailed analysis and
machine learning and so phone then of
course you can write python programs
python offers a rich library for
performing data analysis and machine
learning and so on Matlab is a very
popular tool as well especially during
education so this is a very easy to
learn tool so Matlab is another tool
that can be used and then last but not
least SAS SAS is again very powerful it
is proprietary tool and it has all the
components that are required to perform
very good statistical analysis or
perform data science so those are the
various tools that would be required for
or that that can be used for model
building and so the next step is model
building so we have done the planning
part we said okay what is algorithm we
are going to use what kind of model we
are going to use now we need to actually
train this model or build the model
rather so that it can then be deployed
so what are the various ways or where
what are the various types of model
building activities so it could be let's
say in this particular example that we
have taken you want to find out the
price of 1.35 carat diamond so this is
let's say a linear regression problem
you have data for various carrots of
diamond and you use that information you
pass it through a linear regression
model or you create a real linear
regression model which can then predict
your price for 1.35 carat so this is one
example of model building and then a
little bit details of how linear
regression works so linear regression is
basically coming up with a relation
between an independent variable and a
dependent variable so it is pretty much
like coming up with equation of a
straight line which is the best fit for
the given data so like for example here
Y is equal to MX plus C so Y is is the
dependent variable and X is the
independent variable we need to
determine the values of M and C for our
given data so that is what the training
process of this model does at the end of
the training process you have a certain
value of M and c and that is used for
predicting the values of any new data
that comes all right so the way it works
is we use the training and the test data
set to train the model and then validate
whether the model is working fine or not
using test data and if it is working
fine then it is taken to the next level
which is put in production if not the
model has to be retrained if the quracy
is not good enough then the model is
retrained maybe with more data or you
come up with a newer model or algorithm
and then repeat that process so it is an
iterative process once the training is
completed training and test and this
model is deployed and we can use this
particular model to determine what is
the price of 1.35 carat diamond remember
that was our problem statement so now
that we have the best fit for this given
data we have the price of 1.35 carat
diamond which is 10 000. so this is one
example of how this whole process works
now how do we build the model there are
multiple ways you can use Python for
example and use libraries like pandas or
numpy to build a model and implement it
this will be available as a separate
tutorial a separate video in this
playlist so stay tuned for that moving
on once we have the results the next
step is to communicate this results to
the appropriate stakeholders so which is
basically taking this results and
preparing like a presentation or or a
dashboard and communicating these
results to the concerned people so
finishing or getting the results of the
analysis is not the last step but you
need to as a data scientist take this
results and present it to the team that
has given you this problem in the first
place and explain your findings explain
the findings of this exercise and
recommend maybe what steps they need to
take in order to overcome this problem
or solve this problem so that is the
pretty much once that is accepted and
the last step is to operationalize so if
everything is fine your data scientists
presentations are accepted then they put
it into factors and thereby they will be
able to improve or solve the problem
that they stated in step one
so quick summary of the life cycle you
have a concept study which is basically
understanding the problem asking the
right questions and trying to see if
there is enough data to solve this
problem and then even maybe gather the
data then data preparation the raw data
needs to be manipulated you need to do
data munging so that you have a data in
a certain proper format to be used by
the model or our Analytics system and
then you need to do the model planning
what kind of a model what algorithm you
will use for a given problem and then
the model building so the exact
execution of that model happens in step
four and you implement and execute that
model and put the data through the
analysis in this step and then you get
the results this results are then
communicated packaged and presented and
communicated to the stakeholders and
once that is accepted that is
operationalized so that is the final
step now in the end let's take a quick
look at the demand for data scientists
data science is an area of great demand
the demand for data scientists is
currently huge and the supply is very
low so there is a huge gap so what are
some of the industries with high demand
for data scientists I think gaming is
definitely one area where it's a
industry which is consumer facing
industry and a lot of people play games
and growing industry and it requires a
lot of data science so that is an area
where data scientists are in demand then
we have health care for example data
science is used for diagnosis and
several other activities within
Healthcare predicting for example heart
disease so Healthcare is definitely
Finance definitely Banks insurance
companies all of these there is a huge
demand for data scientists marketing is
like a horizontal functionality across
all Industries there's a demand for data
scientists there then of course in
technology area so pretty much all of
these areas there is a lot of demand
globally there is a huge demand so this
is a very very critical skill that would
be required currently as well as in the
future
solving problems with data data science
we have to ask possible questions and
the desired algorithms and these really
are kind of an overview of some of the
terminology you'll want to know in data
science so we talk about that how much
or how many we're talking about
regression regression basically means a
number if you've ever seen stock charts
you see the guess at what the next sales
price is or buy price is that's all on
your regression side
is it a or b classification
is that a dog or a cat that my photo
took a picture of how is this organized
clustering this is where we take things
that look alike and put them together so
that we can then make predictions on it
how different is this
just like you have clustering you have
the opposite anomaly detection how do we
find things that don't fit in where are
the outliers what are things we might
need to look at that aren't in our model
and what to do next reinforced learning
this is probably the newest Market is
how do we set something up whereas the
data comes in it learns from the new
data as to what the next action is going
to take
most of these are combined so when we
talk about regression classification
clustering anomaly detection reinforced
learning a lot of times when you put
your model together you might have
multiple parts of this we might look at
the clustering of data and then feed it
into a classification is this a bad loan
to make it the bank or is this a good
loan to give
and from there we might start looking at
when we start clustering these things as
good and bad and what kind of setup is
we then might run it through a
regression model to say hey what is the
amount dollar amount this person should
be allowed to take a loan out
and you can see that all of these start
coming in and then a lot of the models
reinforce learning
we talk about like bank loans and things
like that once a year
they have to rerun those algorithms and
so that's like a human run reinforced
learning now we're starting to automate
that so that as the data comes in real
time the models start updating
themselves they start figuring out a way
to solve new problems as they occur
and of course in robotics reinforced
learning is one of the biggest growing
Fields logarithms used in data science
linear regression
logistic regression when you're looking
at both of these think numbers we're
talking regression models and so linear
regression figures out the best line
through the data Maybe it's looking for
a curved line
logistic regression starts looking at
data on a Continuum so that
it goes to an exponential you have a
spot where it might be one or the other
and then you know what it is as it goes
to one side or the other decision tree
wonderful tool if you used to call it a
hack now it's becoming mainstream to
look at the back end what made the
decision so it's really easy to trace
back to how you arrived at the decision
decision trees are kind of nice that way
nearest neighbors okay nearest neighbors
is your clustering K means clustering
hierarchical clustering so hierarchical
starts dividing them and builds a
hierarchy where the K means and the K
nearest neighbor look for things that
kind of connect each other how close are
they in data format
DB scan another form of clustering your
data principal component analysis so we
start breaking up and looking at the
different components in our data coming
in these are all algorithms used in data
science and there are more these are
kind of the mainline ones and each one
of these has many settings which you can
play with to build a better model to
predict your data and where you want it
to go python is the most widely used
programming language today when it comes
to solving data science tasks and
challenges python never ceases to
surprise its audience most data
scientists out there are already
leveraging the power of python every day
hi I'm a baksha from Simply learn and
well after some thought and a bit more
research I was finally able to narrow
down my choice of top python libraries
for data science what are they let's
find out so let's talk about this
amazing Library tensorflow which is also
one of my favorites so tensorflow is a
library for high performance numerical
computations with around 35
000 top comments and a Vibrant Community
of around 1500 contributors and it's
used across various scientific domains
it's basically a framework where we can
Define and run computations which
involves tensors and tensors we can say
are partially defined computational
objects again where they will eventually
produce a value that was about
tensorflow let's talk about the features
of tensorflow so tensorflow is majorly
used in deep learning models and neural
networks where we have other libraries
like torch and thiano also but
tensorflow has hands down better
computational graphical visualizations
when compared to them also tensorflow
reduces the error largely by 50 to 60
percent in neural machine translations
it's highly parallel in a way where it
can train multiple neural networks and
multiple gpus for highly efficient and
scalable models this parallel Computing
feature of tensorflow is also called
pipelining also tensorflow has the
advantage of seamless performance as
it's backed by Google it has quicker
updates frequent new releases with the
latest of features now let's look at
some applications tensorflow is
extensively used in speech and image
recognition text-based applications time
series analysis and forecasting and
various other applications involving
video detection so favorite thing about
tensorflow that it's already popular
among the machine learning community and
most are open to trying it and some of
us are already using it now let's look
at an example of a tensorflow model in
this example we will not dive deep into
the explanation of the model as it is
beyond the scope of this video so here
we're using amnest dataset which
consists of images of handwritten digits
handwritten digits can be easily
recognized by building a simple
tensorflow model let's see how when we
visualize our data using matplotlib
Library the inputs will look something
like this then we create our tensorflow
model to create a basic tensorflow model
we need to initialize the variables and
start a session then after training the
model we can validate the data and then
predict the accuracy this model has
predicted 92 percent accuracy let's see
which is pretty well for this model so
that's all for tensorflow if you need to
understand this tutorial in detail then
you can go ahead and watch our deep
learning tutorial from Simply learn as
shown in the right corner interesting
right let's move on to the next library
now let's talk about a common yet a very
powerful python Library called numpy
numpy is a fundamental package for
numerical competition in Python it
stands for numerical python as the name
suggests it has around 18 000 comments
on GitHub with an active community of
700 contributors it's a general purpose
array processing package in a way that
it provides high performance
multi-dimensional objects called arrays
and tools for working with them also
numpy addresses the slowness problem
partly by providing these
multi-dimensional arrays that we talked
about and then functions and operators
that operate efficiently on these arrays
interesting right now now let's talk
about features of numbai it's very easy
to work with large arrays and mattresses
using numpy numpy fully supports object
oriented approach for example coming
back to ndre once again it's a class
possessing numerous methods and
attributes ndra provides for larger and
repeated computations numpy offers
vectorization it's more faster and
compact than traditional methods I
always wanted to get rid of loops and
vectorization of numpy clearly helps me
with that now let's talk about the
applications of numpy numpy along with
pandas is extensively used in data
analysis which forms the basis of data
science it helps in creating the
powerful n-dimensional array whenever we
talk about numpy the mention of the
array we cannot do it without the
mention of the powerful n-dimensional
array also number is extensively used in
machine learning when we are creating
machine learning models as in where it
forms the base of other libraries like
sci-fi psychic learn
Etc when you start creating the machine
learning models in data science you will
realize that all the models will have
their basis numpy or pandas also when
numpa is used with sci-fi and matplotlib
it can be used as a replacement of
Matlab now let's look at a simple
example of an array in numpy as you can
see here there are multiple array
manipulation routines like their basic
examples where you can copy the values
from one array to another we can give a
new shape to an array from maybe one
dimensional do we can make it as a two
dimensional array we can return a copy
of the array collapse into one dimension
now let's look at an example where this
is Jupiter notebook and we will just
create a basic array and for detailed
explanation you can watch our other
videos which Targets on these
explanations of each libraries so first
of all whenever we are using any library
in Python we have to import it so now
this NP is the Alias which we will be
using let's create a simple array
let's look what is the type of this
array
so this is an indiary type of array Also
let's look what's the shape of this
array
so this is a shape of the array now here
we saw that we can expand the shape of
the array
so this is where you can change the
shape of the array using all those
functions now let's create another using
arrange functions if I give arrange 12
it will give me a 1D array of 12 numbers
like this now we can reshape this array
to 3 comma 4 or we can write it here
itself
so this is how arrange function and the
reshape function works for numpy now
let's discuss the next Library which is
sci-fi so this is another free and open
source python Library extensively used
in data science for high level
computations so this Library as the name
suggests stands for Scientific Python
and it has around 19 000 commits on
GitHub with an active community of 600
contributors it is extensively used for
scientific and Technical computations
also as it extends numpy it provides
many user-friendly and efficient
routines for scientific calculations now
let's discuss about some features of
scipy so scipy has a collection of
algorithms and functions which is built
on the numpy extension of python
secondly it has various high-level
commands for data manipulation and
visualization also the ndmh function of
scipy is very useful in
multi-dimensional image processing and
it includes built-in functions for
solving differential equations linear
algebra and many more more so that was
about the features of scipy now let's
discuss its applications so saipa is
used in multi-dimensional image
operations it has functions to read
images from disk into numpy arrays to
write arrays to discuss images resize
images
Etc solving differential equations
Fourier transforms then optimization
algorithms linear algebra Etc let's look
at a simple example to learn what kind
of functions are there in sci-fi here
I'm importing the constants package of
scipy Library so in this package it has
all the constants
so here I'm just mentioning C or H or
any and this Library already knows what
it has to fetch like speed of light
Planck's constant Etc so this can be
used in further calculations data
analysis is an integral part of data
science data scientists spend most of
the day in data munching and then
cleaning the data also hence mention of
pandas is a must in data science
lifecycle yes pandas is the most popular
and widely used python library for data
science along with numpy and matplotlib
the name itself stands for python data
analysis with around 17 000 comets on
GitHub and an active community of 1200
contributors it is heavily used for data
analysis in cleaning as it provides fast
flexible data structures like data
frames series which are designed to work
with structured data very easily and
intuitively now let's talk about some
features of pandas so pandas offers this
eloquent syntax and Rich functionalities
like there are various methods in pandas
like Drop n a fill any which gives you
the freedom to deal with missing data
also Partners provides a powerful apply
function which lets you create your own
function and run it across a series of
data now forget about writing those four
Loops while using pandas also this
library's high level abstraction over
low level numpy which is written in pure
C then it also contains these high level
data structures and manipulation tools
which makes it very easy to work with
pandas like their data structures and
series now let's discuss the
applications of pandas so pandas is
extensively used in general data
wrangling and data cleaning then pandas
also Finds Its usage in ETL jobs for
data transformation and data storage as
it has excellent support for loading CSV
files into its data frame format then
pandas is used in a variety of academic
and Commercial domains including
statistics Finance Neuroscience
economics web analytics Etc then pandas
is also very useful in Time series
specific functionality like date range
generation moving window linear
regression date 15 Etc now let's look at
a very simple example of how to create a
data frame so data frame is a very
useful data structure in pandas and it
has very powerful functionalities so
here I'm only enlisting important
libraries in data science you can
explore more of our videos to learn
about these libraries in detail so let's
just go ahead and create a data frame
I'm using Jupiter notebook again and in
this before using pandas here I am
importing the pandas Library
let me go and run this so in data frame
we can import a file a CSV file Excel
files there are many functions doing
these things and we can also create our
own data and put it into Data frame so
here I am taking random data and putting
in a data frame also I'm creating an
index and then also giving the column
names so PD is the Alias we've given for
pandas random data of 6x4 index which is
taking a range 6 numbers and column name
I'm giving as ABCD now let's go ahead
and look at it
so here it has created a data frame with
my column names ABCD my list has six
numbers 0 to 5 and a random data of six
by four so data frame is just another
table with rows and columns where you
can do various functions over it also I
can go ahead and describe this data
frame to see so it's giving me all these
functionalities where count and mean and
standard deviation
Etc okay so that was about pandas now
let's talk about next library and the
last one so matplot live for me is the
most fun Library out of all of them why
because it has such powerful yet
beautiful visualizations we'll see in
the coming slides plot and mac.lib
suggest that it's a plotting library for
python it has around 26 000 comments on
GitHub and a very Vibrant Community of
700 contributors and because of such
graphs and plots that it produces it's
majorly used for data visualization and
also because it provides an object
oriented API which can be used to embed
those plots into our applications let's
talk about the features of matplotlib
the pi plot module of matplotlab
provides Matlab like interface so
matplotlab is designed to be as usable
as Matlab with an advantage of being
free and open source also it supports
dozens of backends and output types
which means you can use it regardless of
which operating system you are using or
which output format you wish pandas
itself can be used as wrappers around
matplotlips API so as to drive Mac
broadly via cleaner and more modern apis
also when you start using this Library
you will realize that it has a very
little memory consumption and a very
good runtime Behavior now let's talk
about the applications of matplotlib
it's important to discover the unknown
relationship between the variables in
your data set so this Library helps to
visualize the correlation analysis of
variables also in machine learning we
can visualize 95 confidence interval of
the model just to communicate how well
our model fits the data then matpatliff
Finds Its application in outlier
detection using scatter plot Etc and to
visualize the distribution of data to
gain instant insights now let's make a
very simple plot to get a basic idea
I've already imported the libraries here
so this function matplotlib inline will
help you show the plots in the Jupiter
notebook this is also called a magic
function I won't be able to display my
plots in the jupyter notebook if I don't
use this function I am using this
function in numpy to fix random state
for reproducibility now I'll take my n
as 30 and will assign random values to
my variables so this function is
generating 30 random numbers here I am
trying to create a scatter plot so I
want to decide the area let's
put this so just multiplying 30 with
random numbers to the power 2 so that we
get the area of the plot which we will
see in just a minute so using the
scatter function and the Alias of
matplotlip as PLT I've created this if I
don't use this then I have very small
circle since my scatter plot it's
colorful it's nice so that's one very
easy plot I suggest that you Explore
More of matplotlib and I'm sure you will
enjoy it let's create a histogram so I'm
using my the style is GG plot and
assigning some values to these variables
any random values
now we are assigning bars and colors and
Alignment to the plot and here we get
the graph so we can create different
type of visualizations and plots and
then work upon them using matplotlib and
it's just that simple so that was about
the leading python libraries in the
field of data science but along with
these libraries data scientists are also
leveraging the power of some other
useful libraries for example like
tensorflow Keras is another popular
Library which is extensively used for
deep learning and neural network modules
Keras drafts both tensorflow and theano
back-ends so it is a good option if you
don't want to dive into details of
tensorflow then scikit-learn is a
machine learning library it provides
almost all the machine learning
algorithms that you need and it is
designed to interpolate with numpy and
sci-fi then we have c bond which is
another library for data visualization
we can say that c-bond is an enhancement
of matplotlib as it introduces
additional plot types
and let's understand linear regression
and dig into the theory understanding
linear regression linear regression is
the statistical model used to predict
the relationship between independent and
dependent variables by examining two
factors the first important one is which
variables in particular are significant
predictors of the outcome variable and
the second one that we need to look at
closely is how significant is the
regression line to make predictions with
the highest possible accuracy if it's
inaccurate we can't use it so it's very
important we find out the most accurate
line we can get since linear regression
is based on drawing a line through data
we're going to jump back and take a look
at some euclidean geometry the simplest
form of a simple linear regression
equation with one dependent and one
independent variable is represented by y
equals m times X plus C and if you look
at our model here we plotted two points
on here X1 and y1 X2 and Y2 y being the
dependent variable remember that from
before and X being the independent
variable so y depends on whatever X is m
in this case is the slope of the line
where m equals the difference in the Y2
minus y1 and X2 minus X1 and finally we
have C which is the coefficient of the
line or where it happens to cross the
zero axes let's go back and look at an
example we used earlier of linear
regression we're going to go back to
plotting the amount of crop yield based
on the amount of rainfall and here we
have our rainfall remember we cannot
change rainfall and we have our crop
yield which is dependent on the rainfall
so we have our independent and our
dependent variables we're going to take
this and draw a line through it as best
we can through the middle of the data
and then we look at that we put the red
point on the y-axis is the amount of
crop yield you can expect for the amount
of Ray rainfall represented by the Green
Dot so if we have an idea what the
rainfall is for this year and what's
going on then we can guess how good our
crops are going to be and we've created
a nice line right through the middle to
give us a nice mathematical formula
let's take a look and see what the math
looks like behind this let's look at the
intuition behind the regression line now
before we dive into the math and the
formulas that go behind this and what's
going on behind the scenes
I want you to note that when we get into
the case study and we actually apply
some python script that this math you're
going to see here is already done
automatically for you you don't have to
have it memorized it is however good to
have an idea what's going on so if
people reference the different terms
you'll know what they're talking about
let's consider a sample data set with
five rows and find out how to draw the
regression line we're only going to do
five rows because if we did like the
rainfall with hundreds of points of data
that would be very hard to see what's
going on with the mathematics so we'll
go ahead and create our own two sets of
data and we have our independent
variable X and our dependent variable Y
and when X was one we got y equals 2
when X was 2 y was 4 and so on and so on
if we go ahead and plot this data on a
graph we can see how it forms a nice
line through the middle you can see
where it's kind of grouped going upwards
to the right the next thing we want to
know is what what the means is of each
of the data coming in the X and the Y
the means doesn't mean anything other
than the average so we add up all the
numbers and divide by the total so one
plus two plus three plus four plus five
over five equals three and the same for
y we get four if we go ahead and plot
the means on the graph we'll see we get
three comma four which draws a nice line
down the middle a good estimate here
we're going to dig deeper into the math
behind the regression line now remember
before I said you don't have to have all
these formulas memorized or fully
understand them even though we're going
to go into a little more detail of how
it works and if you're not a math whiz
and you don't know if you've never seen
the sigma character before which looks a
little bit like an e that's opened up
that just means summation that's all
that is so when you see the sigma
character it just means we're adding
everything in that row and for computers
this is great because as a programmer
you can easily iterate through each of
the X Y points and create all the
information you need so in the top half
you can see where we've broken that down
into pieces and as it goes through the
first two points it computes the squared
value of x the squared value of y and x
times Y and then it takes all of X and
adds them up all of Y adds them up all
of x squared adds them up and so on and
so on and you can see we have the sum of
equal to 15 the sum is equal to 20 all
the way up to x times Y where the sum
equals 66 6. this all comes from our
formula for calculating a straight line
where y equals the slope times X plus
the coefficient C so we go down below
and we're going to compute more like the
averages of these and we'll explain
exactly what that is in just a minute
and where that information comes from is
called the square means error but we'll
go into that in detail in a few minutes
all you need to do is look at the
formula and see how we've gone about
Computing it line by line instead of
trying to you have a huge set of numbers
pushed into it and down here you'll see
where the slope m equals and then the
top part if you read through the
brackets you have the number of data
points times the sum of x times Y which
we computed one line at a time there and
that's just the 66 and take all that and
you subtract it from the sum of x times
the sum of Y and those have both been
computed so you have 15 times 20 and on
the bottom we have the number of lines
times the the sum of x squared easily
computed as 86 for the sum minus I'll
take all that and subtract the sum of x
squared and we end up as we come across
with our formula you can plug in all
those numbers which is very easy to do
on the computer you don't have to do the
math on a piece of paper or calculator
and you'll get a slope of 0.6 and you'll
get your C coefficient if you continue
to follow through that formula you'll
see it comes out as equal to 2.2
continuing deeper into what's going
behind the scenes let's find out the
predicted values of Y for corresponding
values of X using the linear equation
where m equals 0.6 and C equals 2.2
we're going to take these values and
we're going to go ahead and plot them
we're going to predict them so y equals
0.6 times where x equals 1 plus 2.2
equals 2.8 so on and so on and here the
Blue Points represent the actual y
values and the brown points represent
the predicted y values based on the
model we created the distance between
the action show and predicted values is
known as residuals or errors the best
fit line should have the least sum of
squares of these errors also known as
e-square if we put these into a nice
chart we can see X and you can see why
what the actual values were and you can
see why I predicted you can easily see
where we take y minus y predicted and we
get an answer what is the difference
between those two and if we square that
y minus y prediction squared we can then
sum those squared values that's where we
get the 0.64 plus the 0.36 plus 1 all
the way down until we have a summation
equals 2.4 so the sum of squared errors
for this regression line is 2.4 we check
this error for each line and conclude
the best fit line having the least e
Square value in a nice graphical
representation we can see here where we
keep moving this line through the data
points to make sure the best fit line
has the least squared distance between
the data points and the regression line
now we we only looked at the most
commonly used formula for minimizing the
distance there are lots of ways to
minimize the distance between the line
and the data points like sum of squared
errors sum of absolute errors root mean
square error Etc which you want to take
away from this is whatever formula is
being used you can easily using a
computer programming and iterating
through the data calculate the different
parts of it that way these complicated
formulas you see with the different
summations and absolute values are
easily computed one piece at a time up
until this point we've only been looking
at two values X and Y well in the real
world it's very rare that you only have
two values when you're figuring out a
solution so let's move on to the next
topic multiple linear regression let's
take a brief look at what happens when
you have multiple inputs so in multiple
linear regression we have well we'll
start with the simple linear regression
where we had y equals M plus X plus C
and we're trying to find the value of y
now with multiple linear regression we
have multiple variables coming in so
instead of having just X we have X1 X2
X3 and instead of having just one slope
each variable has its own slope attached
to it as you can see here we have M1 M2
M3 and we still just have the single
coefficient so when you're dealing with
multiple linear regression you basically
take your single linear regression and
you spread it out so you have y equals
M1 times X1 plus M2 times X2 so on all
the way to m to the nth x to the nth and
then you add your coefficient on there
implementation of linear regression now
we get into my favorite part let's
understand how multiple linear
regression works by implementing it in
Python if you remember before we were
looking at a company and just based on
its R and D trying to figure out its
profit we're going to start looking at
the expenditure of the company we're
going to go back to that we're going to
predict as profit but instead of
predicting it just on the r d we're
going to look at other factors like
Administration costs marketing costs and
so on and from there we're going to see
if we can figure out what the profit of
that company is going to be to start our
coding we're going to begin by importing
some basic libraries and we're going to
be looking through the data before we do
any kind of linear regression we're
going to take a look at the data to see
what we're playing with then we'll go
ahead and format the data to the format
we need to be able to run it in the
linear regression model and then from
there we'll go ahead and solve it and
just see how valid our solution is so
let's start with importing the basic
libraries now I'm going to be doing this
in Anaconda Jupiter notebook a very
popular IDE I enjoy because it's such a
visual to look at it's so easy to use
just any ID for python will work just
fine for this so break out your favorite
python IDE so here we are in our Jupiter
notebook let me go ahead and paste our
first piece of code in there and let's
walk through what libraries were
importing first we're going to import
numpy as in p and then I want you to
skip one line and look at import pandas
as PD these are very common tools that
you need with most of your linear
regression the numpy which stands for
number python is usually denoted as NP
and you have to almost have that for
your SK learn toolbox so you always
import that right off the beginning
pandas although you don't have to have
it for your sklearn libraries it does
such a wonderful job of importing data
setting it up into a data frame so we
can manipulate it rather easily and it
has a lot of tools also in addition to
that so we usually like to use the
pandas when we can and I'll show you
what that looks like the other three
lines are for us to get a visual of this
data and take a look at it so we're
going to import
matplotlibrary.pi plot as PLT and then
Seaborn as SNS Seabourn works with the
matplot library so you have to always
import matplot library then Seabourn
sits on top of it and we'll take a look
at what that looks like you could use
any of your own plotting libraries you
want there's all kinds of ways to look
at the data these are just very common
ones and the Seabourn is so easy to use
it just looks beautiful it's a nice
representation that you can actually
take and show somebody and the final
line is the Amber signed matplot library
in line that is only because I'm doing
an inline IDE my interface in the
Anaconda Jupiter notebook requires I put
that in there or you're not going to see
the graph when it comes up let's go
ahead and run this it's not going to be
that interesting so we're just setting
up variables in fact it's not going to
do anything that we can see but it is
importing these different libraries and
setup the next step is load the data set
and extract independent and dependent
variables now here in this slide you'll
see companies equals pd.read CSV and it
has a long line there with the file at
the end 1000 companies.csv you're going
to have to change this to fit whatever
setup you have and the file itself you
can request just go down to the
commentary below this video and put a
note in there and simply learn we'll try
to get in contact with you and Supply
you with that file so you can try this
coding yourself so we're going to add
this code in here and we're going to see
that I have companies equals pd.reader
underscore CSV and I've changed this
path to match my computer C colon slash
simply learn slash 1000 underscore
companies.csv and then below there we're
going to set the x equals to companies
under the I location and because this is
companies is a PD data set I can use
this nice notation that says take every
row that's what the colon the first
colon is comma except for the last
column that's what the second part is
where we have a colon minus one and we
want the values set into there so X is
no longer a data set a pandas data set
but we can easily extract the data from
our pandas data set with this notation
and then y we're going to set equal to
the last row well the question is going
to be what are we actually looking at so
let's go ahead and take a look at that
and we're going to look at the companies
dot head which lists the first five rows
of data and I'll open up the file in
just a second so you can see where
that's coming from but let's look at the
data in here as far as the way the
pandas sees it when I hit run you'll see
it breaks it out into a nice setup this
is what pandas one of the things pandas
is really good about is it looks just
like an Excel spreadsheet you have your
rows and remember when we're programming
we always start with zero we don't start
with one so it shows the first five rows
zero one two three four and then it
shows your different columns R and D
spend Administration marketing spend
State profit it even notes that the top
are column names it was never told that
but pandas is able to recognize a lot of
things that they're not the same as the
data rows why don't we go ahead and open
this file up in a CSV so you can
actually see the raw data so here I've
opened it up as a text editor and you
can see at the top we have r d spend
comma Administration comma marketing
spin comma State comma profit carries
return I don't know about you but I'd go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an Excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is where I can see
the data to me it doesn't mean a whole
lot maybe you're an expert in business
and Investments and you understand what
165
349.20 compared to the administration
cost of 136
897.80 so and so on helps to create the
profit of 192 261 and 83 cents that
makes no sense to me whatsoever no pun
intended so let's flip back here and
take a look at our next set of code
where we're going to graph it so we can
get a better understanding of our data
and what it means so at this point we're
going to use a single line of code to
get a lot of information so we can see
where we're going with this let's go
ahead and paste that into our notebook
and see what we got going and so we have
the visualization and again we're using
SNS which is hand does as you can see we
imported the matplot library.pi plot is
PLT which then the Seaborn uses and we
imported the Seaborn as SNS and then
that final line of code helps us show
this in our inline coding without this
it wouldn't display and you can display
it to a file in other means and that's
the matplot library in line with the
Amber sign at the beginning so here we
come down to the single line of code
Seaborn is great because it actually
recognizes the panda data frame so I can
just take the companies dot core for
coordinates and I can put that right
into the Seaborn and when we run this we
get this beautiful plot and let's just
take a look at what this plot means if
you look at this plot on mine the colors
are probably a little bit more purplish
and blue than the original one we have
the columns and the rows we have R and D
spending we have a demonstration we have
marketing spending and profit and if you
cross index any two of these since we're
interested in profit if you cross index
profit with profit it's going to show up
if you look look at the scale on the
right way up in the dark why because
those are the same data they have an
exact correspondence so R and D spending
is going to be the same as r d spending
and the same thing with Administration
costs so right down the middle you get
this dark row or dark diagonal row that
shows that this is the highest
corresponding data that's exactly the
same and as it becomes lighter there's
less connections between the data so we
can see with profit obviously profit is
the same as profit and next it has a
very high correlation with r d spending
which we looked at earlier and it has a
slightly less connection to marketing
spending and even less to how much money
we put into the administration so now
that we have a nice look at the data
let's go ahead and dig in and create
some actual useful linear regression
models so that we can predict values and
have a better profit now that we've
taken a look at the visualization of
this data we're going to move on to the
next step instead of just having a
pretty picture we need to generate some
hard data some hard values so let's see
what that looks like we're going to set
up our linear regression model in two
steps the first one is we need to
prepare some of our data so it fits
correctly and let's go ahead and paste
this code into our jupyter notebook and
what we're bringing in is we're going to
bring in the sklearn pre-processing
where we're going to import the label
encoder and the one hot encoder to use
the label encoder we're going to create
a variable called label encoder and set
it equal to capital L label capital E
encoder this creates a class that we can
reuse for transferring the labels back
and forth now about now you should ask
what labels are we talking about let's
go take a look at the data we processed
before and see what I'm talking about
here if you remember when we did the
companies dot head and we printed the
top five rows of data we have our
columns going across we have column 0
which is R and D spending column one
which is Administration column two which
is marketing spending and column three
is State and you'll see under State we
have New York California Florida now to
do a linear regression model it doesn't
know how to process New York it knows
how to process a number so the first
thing we're going to do is we're going
to change that New York California and
Florida and we're going to change those
to numbers that's what this line of code
does here x equals and then it has the
colon comma 3 in Brackets the first part
the colon comma means that we're going
to look at all the different rows so
we're going to keep them all together
but the only row we're going to edit is
the third row and in there we're going
to take the label coder and we're going
to fit and transform the X also the
third row so we're going to take that
third row we're going to set it equal to
a transformation and that transformation
basically tells it that instead of
having a New York it has a zero or a one
or a two and then finally we need to do
a one hot encoder which equals one hot
in order categorical features equals
three and then we take the X and we go
ahead and do that equal to one hot
encoder fit transform X to array this
final transformation preps our data
Force so it's completely set the way we
need it is just a row of numbers even
though it's not in here let's go ahead
and print X and just take a look at what
this data is doing you'll see you have
an array of arrays and then each array
is a row of numbers and if I go ahead
and just do row 0 you'll see I have a
nice organized row of numbers that the
computer now understands we'll go ahead
and take this out there because it
doesn't mean a whole lot to us it's just
a row of numbers
next on setting up our data we have
avoiding dummy variable trap this is
very important why because the computers
automatically transformed our header
into the setup and it's automatically
transformed all these different
variables so when we did the encoder the
encoder created two columns and what we
need to do is just have the one because
it has both the variable and the name
that's what this piece of code does here
let's go ahead and paste this in here
and we have x equals x colon comma one
colon all this is doing is removing that
one extra column we put in there when we
did our one hot encoder and our label
encoding let's go ahead and run that and
now we get to create our linear
regression model and let's see what that
looks like here and we're going to do
that in two steps the first step is
going to be in splitting the data now
whenever we create a predictive model of
data we always want to split it up so we
have a training set and we have a
testing set that's very important
otherwise we'd be very unethical without
testing it to see how good our fit is
and then we'll go ahead and create our
multiple linear regression model and
train it and set it up let's go ahead
and paste this next piece of code in
here and I'll go ahead and shrink it
down a size or two so it all fits on one
line so from the sklearn module
selection we're going to import train
test split and you'll see that we've
created four completely different
variables we have capital x train
capital X test smaller case y train
smaller case y test that is the standard
way that they usually reference these
when we're doing different models
usually see that a capital x and you see
the train and the test and the lowercase
Y what this is is X is our data going in
that's our RND spin our Administration
our marketing and then Y which we're
training is the answer that's the profit
because we want to know the profit of an
unknown entity so that's what we're
going to shoot for in this tutorial the
next part train test a split we take X
and we take y we've already created
those X has the columns with the data in
it and Y has a column with profit in it
and then we're going to set the test
size equals 0.2 that basically means 20
percent so twenty percent of the rows
are going to be tested we're going to
put them off to the side so since we're
using a thousand lines of data that
means that 200 of those lines we're
going to hold off to the side to test
for later and then the random State
equals zero we're going to randomize
which ones it picks to hold off to the
side we'll go ahead and run this it's
not overly exciting so setting up our
variables but the next step is the next
step we actually create our linear
regression model now that we got to the
linear regression model we get that next
piece of the puzzle let's go ahead and
put that code in there and walk through
it so here we go we're going to paste it
in there and let's go ahead and since
this is a shorter line of code let's
zoom up there so we can get a good look
and we have from the sklearn dot linear
underscore model we're going to import
linear regression now I don't know if
you recall from earlier when we were
doing all the math let's go ahead and
flip back there and take a look at that
do you remember this or we had this long
formula on the bottom and we were doing
all this summarization and then we also
looked at setting it up with the
different lines and then we also looked
all the way down to multiple linear
regression where we're adding all those
formulas together all of that is wrapped
up in this one section so what's going
on here is I'm going to create a
variable called regressor and the
regressor equals the linear regression
that's a linear regression model that
has all that math built in so we don't
have to have it all memorized or have to
compute it individually and then we do
the regressor.fet in this case we do X
train and Y train because we're using
the training data X being the data n and
y being profit what we're looking at and
this does all that math for us so within
one click and one line we've created the
whole linear regression model and we fit
the data to the linear regression model
and you can see that when I run the
regressor it gives an output linear
regression it says copy x equals True
Fit intercept equals true in jobs equal
one normalize equals false it's just
giving you some general information on
what's going on with that regressor
model now that we've created our linear
regression model let's go ahead and use
it and if you remember we kept a bunch
of data aside so we're going to do a why
predict variable and we're going to put
in the X test and let's see what that
looks like scroll up a little bit paste
that in here predicting the test set
results so here we have y predict equals
regressor dot predict X test going in
and this gives us y predict now because
I'm in Jupiter in line I can just put
the variable up there and when I hit the
Run button it'll print that array out I
could have just as easily done print y
predict so if you're in a different IDE
that's not an inline setup like the
jupyter notebook you can do it this way
print y predict and you'll see that for
the 200 different test variables we kept
off to the side is going to produce 200
answers this is what it says the profit
are for those 200 predictions but let's
don't stop there let's keep going and
take a couple look we're going to take
just a short detail here and calculating
the coefficients and the intercepts this
gives us a quick flash at what's going
on behind the line we're going to take a
short detour here and we're going to be
calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has a
coefficients for us we can simply just
print regressor dot coefficient
underscore when I run this you'll see
our coefficients here and if we can do
the regressor coefficient we can also do
the regressor intercept and let's run
that and take a look at that this all
came from the multiple regression model
and we'll flip over so you can remember
where this is going into and where it's
coming from you can see the formula down
here where y equals M1 times X1 plus M2
times X2 and so on and so on plus C the
coefficient so these variables fit right
into this formula y equals slope one
times column one variable plus slope two
times column two variable all the way to
the m into the n and x to the N plus C
the coefficient or in this case you have
minus 8.89 to the power of two etc etc
times the First Column and the second
column and the third column and then our
intercept is the minus one zero three
zero zero nine point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure it's a valid model that this
model works and understand how good it
works so calculating the r squared value
that's what we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in code and so we're going to use
this from
sklearn.metrix we're going to import R2
score that's the r squared value we're
looking at the error so in the R2 score
we take our y test versus our y predict
y test is the actual values we're
testing that was the one that was given
to us so we know are true the Y predict
of those 200 values is what we think it
was true and when we go ahead and run
this we see we get a
0.9352 that's the R2 score now it's not
exactly a straight percentage so it's
not saying it's 93 percent correct but
you do want that in the upper 90s oh and
higher shows that this is a very valid
prediction based on the R2 score and if
r squared value of 0.91 or 92 as we got
on our model could remember it does have
a random generation involved this proves
the model is a good model which means
success yay we successfully trained our
model with certain predictors and
estimated the profit of the companies
using linear regression
so where does logistic regression fit
into the overall machine learning
process machine learning is divided into
two types mainly two types there is a
third one called reinforcement learning
but we will not talk about that right
now so one is supervised learning and
the other is unsupervised learning
unsupervised learning uses techniques
like clustering and Association and
supervised learning users techniques
like classification and regression now
supervised learning is used when you
have labeled data you have historical
data then you use supervised learning
when you don't have labeled data then
you used unsupervised learning it's it's
supervised learning there are two types
of techniques that are used
classification and regression based on
what is the kind of problem we have
solved let's say we want to take the
data and classify it it could be binary
classification like a zero or a one an
example of classification we have just
seen whether the passenger has survived
or not survived like a zero or one that
is known as binary classification
regression on the other hand is you need
to predict a value what is known as a
continuous value classification is for
discrete values regression is a
continuous values let's say you want to
predict a share price or you want to
predict the temperature that will be
there what will be the temperature
tomorrow that is where you use
regression whereas classification are
discrete values is will the customer buy
the product or will not buy the product
will you get a promotion or you will not
get a promotion I hope you're getting
the idea or it could be multi-class
classification as well let's say you
want to build an image classification
model so the image classification model
would take an image as an input and
classify into multiple classes whether
this image is of a cat or a dog or an
elephant or a tiger so there are
multiple classes so not necessarily
blind reclassification so that is known
as multi-class classification we are
going to focus on classification because
logistic regression is one of the
algorithms used for classification now
the name may be a little confusing in
fact whenever people come across
logistic regression it always causes
confusion because the name has
regression in it but we are actually
using this for performing classification
okay so yes it is logistic regression
but it is used for classification and in
case you are wondering is there
something similar for regression yes for
regression we have linear regression
keep that in mind so linear regression
is used for regression logistic
regression is used for classification so
in this video we are going to focus on
supervised learning and within
supervised learning we are going to
focus on classification and then within
classification we are going to focus on
logistic regression algorithm so first
of all classification so what are the
various algorithms available for
performing classification the first one
is decision tree there are of course
multiple algorithms but here we will
talk about a few addition trees are
quite popular and very easy to
understand and therefore they use for
classification then we have K nearest
neighbors this is another algorithm for
performing classification and then there
is logistic regression and this is what
we are going to focus on in this video
and we are going to go into little bit
of details about logistic regression all
right what is logistic regression as I
mentioned earlier positive regression is
an algorithm for performing binary
classification so let's take an example
and see how this works let's say your
car has not been serviced for quite a
few years and now you want to find out
if it is going to break down in the near
future so this is like a classification
problem find out whether your car will
break down or not so how are we going to
perform this classification so here's
how it looks if we plot the information
along the X and Y axis X is the number
of years since the last service was
performed and why is the probability of
your car breaking down and let's say
this information was this data rather
was collected from several car users
it's not just your car but several car
users so that is our labeled data so the
data has been collected and for for the
number of years and when the car broke
down and what was the probability and
that has been plotted along X and Y axis
so this provides an idea or from this
graph we can find find out whether your
car will break down or we'll see how so
first of all the probability can go from
0 to 1 as you all aware probability can
be between 0 and 1. and as we can
imagine it is intuitive as well as the
number of years are on the Lower Side
maybe one year two years or three years
till after the service the chances of
your car breaking down are very limited
right so for example chances of your car
breaking down on the probability of your
car breaking down within two years of
your last service are 0.1 probability
similarly three years is maybe 0.3 and
so on but as the number of years
increases let's say if it was six or
seven years there is almost a certainty
that your car is going to break down
that is what this graph shows so this is
an example of a application of the
classification algorithm and we will see
in little details how exactly logistic
regression is applied here one more
thing needs to be added here is that the
dependent variables outcome is discrete
so if we are talking about whether the
car is going to break down or not so
that is a discrete value the Y that we
are talking about the dependent variable
that we are talking about what we are
looking at is whether the car is going
to break down or not yes or no that is
what we are talking about so here the
outcome is discrete and not a continuous
way so this is how the logistic
regression curve looks let me explain a
little bit what exactly how exactly we
are going to
determine the class or the outcome
rather so for a logistic regression
curve a threshold has to be set saying
that because this is a probability
calculation remember this is a
probability calculation and the
probability itself will not be 0 or 1
but based on the probability we need to
decide what the outcome should be so
there has to be a threshold like for
example 0.5 can be the threshold let's
say in this case so any value of the
probability below 0.5 is considered to
be 0 and any value above 0.5 is
considered to be 1. so an output of
let's say 0.8 will mean that the car
will break down so that is considered as
an output of 1 and let's say an output
of 0.29 is considered as 0 which means
that the car will not break down so
that's the way logistic regression works
now let's do a quick comparison between
logistic regression and linear
regression because they both have the
term regression in them so it can cause
confusion so let's try to remove that
confusion so what is linear regression
linear regression is a process is once
again an algorithm for supervised
learning however here you are going to
find a continuous value you are going to
determine a continuous value it could be
the price of a real estate property it
could be your height how much height
you're going to get or it could be a
stock price these are all continuous
values these are not discrete compared
to a yes or no kind of a response that
we are looking for in logistic
regression so this is one example of a
linear regression let's say at the HR
team of a company tries to find out what
should be the salary hike of an employee
so they collect all the details of their
existing employees their ratings and
their salary hikes what has been given
and that is the labeled information that
is available and the system learns from
this it is string and it learns from
this labeled information so that when a
new employee's information is fed based
on the rating it will determine what
should be dying so this is a linear
regression problem and a linear
regression example now salary is a
continuous value you can get 5000
5500
5600 it is not discrete like a cat or a
dog or an apple or a banana these are
discrete or a yes or a no these are
discrete values right so this way you
are trying to find continuous values is
where we use linear regression so let's
say just to extend on this scenario we
now want to find out whether this
employee is going to get a promotion or
not so we want to find out if that is
the discrete problem right a yes or no
kind of a problem in this case we
actually cannot use linear regression
even though we may have labeled data so
this is the label So based on the
employee rating these are the ratings
and then some people got the promotion
and this is the ratings for which people
did not get promotion that is a no and
this is the rating for which people got
promotion we just plotted the data about
whether a person has got an employer's
got promotion or not yes no right so
there is nothing in between and what is
the employee's rating okay and ratings
can be continuous that is not an issue
but the output is discrete in this case
whether employee got promotion yes no
okay so if we try to plot that and we
try to find a straight line this is how
it would look and as you can see it
doesn't look very right because looks
like there will be lot of errors this
root mean square error if you remember
for linear regression would be very very
high and also the the values cannot go
beyond zero or Beyond one so the graph
should probably look somewhat like this
clipped at 0 and 1. but still the
straight line doesn't look right
therefore instead of using a linear
equation we need to come up with
something different and therefore the
logistic regression model looks somewhat
like this so we calculate the
probability and if we plot that
probability not in the form of a
straight line but we need to use some
other equation we will see very soon
what that equation is then it is a
gradual process right so you see here
people with some of these ratings are
not getting any promotions and then
slowly uh at certain rating they get
promotion so that is a gradual process
and this is how the math behind logistic
regression looks so we are trying to
find the odds for a particular event
happening and this is the formula for
finding the odd so the probability of an
event happening divided by the
probability of the event not happening
so P if it is a probability of the event
happening probability of the person
getting a promotion and divided by the
probability of the person not getting a
promotion that is 1 minus p
so this is how you measure the odds now
the values of the odds range from 0 to
Infinity so when this probability is 0
then the odds will the value of the odds
is equal to zero and when the
probability becomes 1 then the value of
the odds is 1 by 0 that will be Infinity
but the probability itself remains
between 0 and 1. now this is how an
equation of a straight line love so Y is
equal to beta0 plus beta 1 x where beta0
is the y-intercept and beta 1 is the
slope of the line if we take the odds
equation and take a log of both sides
then this would look somewhat like this
and the term logistic is actually
derived from the fact that we are doing
this we take a log of p x by 1 minus p x
this is an extension of the calculation
of odds that we have seen right and that
is equal to beta0 plus beta 1 x which is
the equation of the straight line and
now from here if you want to find out
the value of p x we will see we can take
the exponential on both sides and then
if we solve that equation we will get
the equation of p x like this p x is
equal to 1 by 1 plus e to the power of
minus beta0 plus beta 1 x and recall
this is nothing but the equation of the
line which is equal to y y is equal to
beta0 plus beta 1 X so that this is the
equation also known as the sigmoid
function and this is the equation of the
logistic regression and all right and if
this is plotted this is how the sigmoid
curve is obtained so let's compare
linear and logistic regression how they
are different from each other let's go
back so linear regression is solved or
used to solve regression problems and
logistic regression is used to solve
classification problems so both are
called regression but linear regression
is used for solving regression problems
where we predict continuous values
whereas logistic regression is used for
solving classification problems where we
have had to predict discrete values the
response variables in case of linear
regression are continuous in nature
whereas here they are categorical or
discrete in nature and linear regression
helps to estimate the dependent variable
when there is a change in the
independent variable whereas here in
case of logistic regression it helps to
calculate the probability or the
possibility of a particular event
happening and linear regression as the
name suggests is a straight line that's
why it's called linear regression
whereas logistic regression is a sigmoid
function and the curve is the shape of
the curve is s it's an s-shaped curve
this is another example of application
of logistic regression in weather
prediction that it's going to rain or
not rain now keep in mind both are used
in weather prediction if we want to find
the discrete values like whether it's
going to rain or not rain that is a
classification problem we use logistic
regression but if we want to determine
what is going to be the temperature
tomorrow then we use linear regression
so just keep in mind that in weather
prediction we actually use both but
these are some examples of logistic
regression so we want to find out
whether it's going to be rain or not is
going to be sunny or not it is going to
snow or not these are all logistic
regression examples a few more examples
classification of objects this is a
again another example of logistic
regression now here of course one
distinction is that these are
multi-class classification so logistic
regression is not used in its original
form but it is used in a slightly
different form so we say whether it is a
dog or not a dog I hope you understand
so instead of saying is it a dog or a
cat or a elephant we convert this into
saying so because we need to keep it to
Binary classification so we say is it a
dog or not a dog is it a cat or not a
cat so that's the way logistic
regression can be used for classifying
objects otherwise there are other
techniques which can be used for
performing multi-class classification
Healthcare non-stick regression is used
to find the survival rate of a patient
so they take multiple parameters like
drama score and age and so on and so
forth and they try to predict the rate
of survival all right now finally let's
take an example and see how we can apply
logistic regression to predict the
number that is shown in the image so
this is actually a live demo I will take
you into jupyter notebook and show the
code but before that let me take you
through a couple of slides to explain
what we are trying to do so let's say
you have an eight by eight image and
there the image has a number one two
three four and you need to train your
model to predict what this number is so
how do we do this so the first thing is
obviously in any machine learning
process you train your model so in this
case we are using logistic regression so
and then we provide a training set to
train the model and then we test how
accurate our model is with the test data
which means that like any machine
learning process we split our initial
data into two parts training set and
listen with the training set we train
our model and then with the test set we
test the model didn't we get good
accuracy and then we use it for for
inference right so that is typical
methodology of
training testing and then deploying of
machine learning models so let's take a
look at the code and see what we are
doing so I'll not go line by line but
just take you through some of the blocks
so first thing we do is import all the
libraries and then we basically take a
look at the images and see what is the
total number of images we can display
using matplotlib some of the images or a
sample of these images and then we split
the data into training and test as I
mentioned earlier and we can do some
exploratory analysis and then we build
our model we train our model with a
training set and then we test it with
our test set and find out how accurate
our model is using the confusion Matrix
the heat map and use heat map for
visualizing this and I will show you in
the code what exactly is the confusion
Matrix and how it can be used for
finding the accuracy in our example we
get an accuracy of about 0.94 which is
pretty good or 94 which is pretty good
all right so what is the confusion
Matrix this is an example of a confusion
Matrix and this is used for identifying
the accuracy of a classification model
or like a logistic regression model so
the most important part in a confusion
Matrix is that first of all this as you
can see this is a matrix and the size of
the Matrix depends on how many outputs
we
so the most important part here is that
the model will be most accurate when we
have the maximum numbers in its diagram
like in this case that's why it has
almost 93 94 percent because the
diagonals should have the maximum
numbers and the others other than
diagnose the cells other than the
diagonal should have very few numbers so
here that's what is happening so there
is a two here there are there's a one
here but most of them are along the
diagonal this what does this mean this
means that the number that has been fed
is zero and the number that has been
detected is also zero so the predicted
value and the actual value are the same
so along the diagonals that is true
which means that let's let's take this
diagonal right if the maximum number is
here that means that like here in this
case it is 34 which means that 34 of the
images that have been fed are rather
actually there are two
misclassifications in there so 36 images
have been fed which have number four and
out of which 34 have been predicted
correctly as number four and one has
been predicted as number eight and
another one has been predicted as number
nine so these are two misclassifications
okay so that is the meaning of saying
that the maximum number should be in the
diagonal so if you have all of them so
for an ideal model which has let's say
100 accuracy everything will be only in
the diagonal there will be no numbers
other than zero in all other cells so
that is like a hundred percent accurate
model okay so that's uh just of how to
use this Matrix how to use this
confusion Matrix I know the name uh is a
little funny sounding confusion Matrix
but actually it is not very confusing
it's very straightforward so you are
just plotting what has been predicted
and what is the labeled information on
what is the actual data that's also
known as the ground proof sometimes okay
these are some fancy terms that are used
so predicted label and the actual name
that's on it okay yeah so we are showing
a little bit more information here so 38
have been predicted and here you will
see that all of them have been predicted
correctly there have been 38 zeros and
predicted value and the actual value is
exactly the same whereas in this case
right it has there are I think 37 plus 5
yeah 42 have been fed the images 42
images are of Digit three and uh the
accuracy is only 37 of them have been
accurately predicted three of them have
been predicted as number seven and two
of them have been predicted as number
eight and so on and so forth okay all
right so with that let's go into Jupiter
notebook and see how the code looks so
this is the code in in Jupiter notebook
for logistic regression in this
particular demo what we are going to do
is train our model to recognize digits
which are the images which have digits
from let's say 0 to 5 or 0 to 9 and
um and then we will see how well it is
trained and whether it is able to
predict these numbers correct a or not
so let's get started so the first part
is as usual we are importing some
libraries that are required and then the
last line in this block is to load the
digits so let's go ahead and run this
code then here we will visualize the
shape of these digits so we can see here
if we take a look this is how the shape
is 1797 by 64. these are like eight by
eight images so that's that's what is
reflected in this shape now from here
onwards we are basically once again
importing some of the libraries that are
required like numpy and matplot and we
will take a look at some of the sample
images that we have loaded so this one
for example creates a figure and then we
go ahead and take a few sample images to
see how they look so let me run this
code and so that it becomes easy to
understand so so these are about five
images sample images that we are looking
at zero one two three four so this is
how the images this is how the data is
okay and based on this we will actually
train our logistic regression model and
then we will test it and see how well it
is able to recognize so the way it works
is the pixel information so as you can
see here this is an eight by eight pixel
kind of a image and the each pixel
whether it is activated or not activated
that is the information available for
each pixel now based on the pattern of
this activation and non-activation of
the various pixels this will be
identified as a zero for example right
similarly as you can see so overall each
of these numbers actually has a
different pattern of the pixel
activation and that's pretty much that
our model needs to learn for which a
number what is the pattern of the
activation of the pixels right so that
is what we are going to train our model
okay so the first thing we need to do is
to split our data into training and test
data set right so whenever we perform
any training we split the data into
training and tests so that the training
data set is used to train the system so
we pass this probably multiple times and
then we test it with the test data set
and the split is usually in the form of
there and there are various ways in
which you can split this data it is up
to the individual preferences in our
case here we are splitting in the form
of 23 and 77 so when we say test size as
20 or 0.23 that means 23 percent of that
entire data is used for testing and the
remaining 77 percent is used for
training so there is a readily available
function which is called train test
split so we don't have to write any
special code for the splitting it will
automatically split the data based on
the proportion that we give here which
is test size so we just give the test
size automatically training size will be
determined and we pass the data that we
want to split and the the results will
be stored in X underscore train and Y
underscore train for the training data
set and what is X underscore train these
are these are the features right which
is like the independent variable and why
underscore train is the label right so
in this case what happens is we have the
input value which is or the features
value which is in X underscore train and
since this is a labeled data for each of
them each of the observations we already
have the label information saying
whether this digit is a zero or a one or
a two so that that's this is what will
be used for compare comparison to find
out whether the the system is able to
recognize it correctly or there is an
error for each observation it will
compare with this right so this is the
label so the same way X underscore train
y underscore train is for the training
data set X underscore test y underscore
test is for the test data set okay so
let me go ahead and execute this code as
well and then we can go and check
quickly what is the how many entries are
there and in each of this so X
underscore train the shape is
1383 by 64 and Y underscore train has
1383 because there is nothing like the
second part is not required here and
then X underscore test shape we see is
414 so actually there are 414
observations in test and 1383
observations in train so that's
basically what these four lines of code
are are saying okay then we import the
logistic regression library and which is
a part of scikit-learn so we we don't
have to implement the logistic
regression process itself we just call
these uh the function and let me go
ahead and execute that so that we have
the logistic regression Library imported
now we create an instance of logistic
regression right so logistic regr is a
is an instance of logistic regression
and then we use that for training our
model so let me first execute this code
so these two lines so the first line
basically creates an instance of
logistic regression model and then the
second line way is where we are passing
our data the training data set right
this is our the the predictors and this
is our Target we are passing this data
set to train our model all right so once
we do this in this case the data is not
large but by and large and the training
is what takes usually a lot of time so
we spend in machine learning activities
in machine learning projects we spend a
lot of time for the training part of it
okay so here the data set is relatively
small so it was pretty quick so all
right so now our model has been trained
using the training data set and we want
to see how accurate this is so what
we'll do is we will test it out in
probably faces so let me first try out
how well this is working for one image
okay I will just try it out with one
image my the first entry in my test data
set and see whether it is uh correctly
predicting or not so and in order to
test it so for training purpose we use
the fit method there is a method called
fit which is for training the model and
once the training is done if you want to
test for a particular value new input
you use the predict method okay so let's
run the predict method and we pass this
particular image and we see that the
shape is or the prediction is four so
let's try a few more let me see for the
next 10 seems to be fine so let me just
go ahead and test the entire data set
okay that's basically what we will do so
now we want to find out how accurately
this has is performed so we use the
score method to find what is the
percentages of accuracy and we see here
that it has performed up to 94 percent
accurate okay so that's on this part now
what we can also do is we can
um also see this accuracy using what is
known as a confusion Matrix so let us go
ahead and try that as well so that we
can also visualize how well this model
has done so let me execute this piece of
code which will basically import some of
the libraries that are required and we
we basically create a confusion Matrix
an instance of confusion matrix by
running confusion Matrix and passing
these values so we have so this
confusion underscore Matrix method takes
two parameters one is the Y underscore
test and the other is the prediction so
what is the Y underscore test these are
the labeled values which we already know
for the test data set and predictions
are what the system has predicted for
the test data set okay so this is known
to us and this is what the system has
the model has generated so we kind of
create the confusion Matrix and we will
print it and this is how the confusion
Matrix looks as the name suggests it is
a matrix and the key point out here is
that the accuracy of the model is
determined by how many numbers are there
in the diagonal the more the numbers in
the diagonal the better the accuracy is
okay and first of all the total sum of
all the numbers in this whole Matrix is
equal to the number of observations in
the test data set that is the first
thing right so if you add up all these
numbers that will be equal to the number
of observations in the test data set and
then out of that the maximum number of
of them should be in the diagonal that
means the accuracy is pretty good if the
the numbers in the diagonal are less and
in all other places there are a lot of
numbers which means the accuracy is very
low the diagonal indicates a correct
prediction that so this means that the
actual value is same as the predicted
value here again actual value is same as
the predictive value and so on right so
the moment you see a number here that
means the actual value is something and
the predicted value is something else
right similarly here the actual value is
something and the predicted value is
something else so that is basically how
we read the confusion Matrix now how do
we find the accuracy you can actually
add up the total values in the diagonal
so it's like 38 plus 44 Plus 43 and so
on and divide that by the total number
of test observations that will give you
the percentage accuracy using a
confusion Matrix now let us visualize
this confusion Matrix tricks in a
slightly more sophisticated way using a
heat map so we will create a heat map
with some We'll add some colors as well
it's uh it's like a more visually
visually more appealing so that's the
whole idea so if we let me run this
piece of code and this is how the heat
map looks and as you can see here the
diagonals again are all the values are
here most of the values so which means
reasonably this seems to be reasonably
accurate and yeah basically the accuracy
score is 94 percent this is calculated
as I mentioned by adding all these
numbers divided by the total test value
so the total number of observations in
test data set okay so this is the
confusion Matrix for logistic regression
all right so now that we have seen the
confusion Matrix let's take a quick
sample and see how well the system has
classified and we will take a few
examples of the data so if we see here
we picked up randomly a few of them so
this is uh number four which is the
actual value and also the predicted
value both are four this is an image of
zero so the predicted value is also zero
actual value is of course zero then this
is the image of 9. so this has also been
predicted correctly nine and actual
value is 9 and this is the image of one
and again this has been predicted
correctly as like the actual value okay
so this was a quick demo of logistic
regression how to use logistic
regression to identify images
need for confusion matrixes
classification models have multiple
output categories most error measures
will tell us the total error in our
model but we cannot use it to find out
individual instances of errors in our
models so you have your input coming in
you have your classifier it measures the
error and it says oh 53 of these are
correct
but we don't know which 53 percent are
correct is it 53 percent correct on
guessing on the spam is it 23 guessing
on spam and 27 guessing on what's not
spam
this is where the confusion Matrix comes
in
so during the classification we also
have to overcome the limitations of
accuracy accuracy can be misleading for
classification problems if there is a
significant class imbalance a model
might predict the majority class for all
cases and have a high accuracy score
and so you can see here we have our
email coming in and there's two spams
the classifier comes in and goes hey it
only catches one of those spams and it
misclassifies one that's not spam
so our model predicted 8 out of 10
instances and we'll have an accuracy of
80 percent but is it classifying
correctly
a confusion Matrix represents a table
layout of the different outcomes of
prediction
and results of a classification problem
and helps visualize its outcomes
and so you see here we have our simple
chart predicted and actual the confusion
Matrix helps us identify the correct
predictions of a model for different
individual classes as well as the errors
so you'll see here that the values
predicted by our classifier are along
the rows this is what we're going to
guess it is or our model is guessing
what this is based on its training so
we've already trained the model to guess
whether it's spam or not spam or
whatever it is you're working on
and then the actual values of our data
set are along the columns
so this is the actual value that's
supposed to be
people who can speak English will be
classified as positives so because they
have a remember zero one do you speak
English yes no and you could extend this
that they might have do you speak French
do you speak whatever language is and so
you might have a whole lot of
classifiers that you would look at each
one of these people who cannot speak
English will be classified as negatives
so there'll be a zero so yeah zero ones
the number of times are actual positive
values are equal to predicted positive
values gives us true positive TP the
number of times are actual negative
values are equal to predictive negative
values gives us true negative TN
the number of times our model wrongly
predicts negative values as positives
gives us a false positive
FP
and you'll see when you're working with
these a lot you know memorizing that
it's false positive you can easily
figure out what that is and pretty soon
you're just looking at the FP or the TP
depending on what you're working on and
the numbers times our model wrongly
predicts positive values as negatives
gives us a false negative FP
now I'm going to do a quick step out
here
let's say you're working in the medical
and we're talking about cancer
uh do you really want a bunch of false
negatives
you want zero under false negative
so when we look at this confusion Matrix
if you have five percent false positives
and five percent false negatives it'd be
much better to even have twenty percent
false positives because they go in and
test it in zero false negatives
let's say it might be true if you're
working on uh say a car driving is this
a safe place for the car to go well you
really don't want any false positives
you know yes this is safe right over the
cliff
so again when you're working on the
project or what it is you're working on
this chart suddenly has huge value we
were talking about spam email how many
important emails say from your banking
overdraft charge coming in that you want
to be a a true false negative you don't
want it to go in the spam folder
likewise you want to get as much of the
spam out of there but you don't want to
miss anything really important
confusion Matrix metrics are performance
measures which help us find the accuracy
of our classifier there are four main
metrics accuracy precision recall and F1
score the F1 score is the one I usually
hear the most and accuracy is usually
what you put on your chart when you're
sending in front of the shareholders how
accurate is it people understand
accuracy
F1 score is a little bit more on the
math side and so you got to be a little
careful when you're quoting F1 scores in
the when you're sitting there with all
the shareholders because a lot of them
will just glaze over so confusion Matrix
metrics are performance measures which
help us find the accuracy of our
classifier there are four main metrics
accuracy the accuracy is used to find
the portion of the correctly classified
values it tells us how often our
classifier is right it is the sum of all
True Values divided by the total values
and this makes sense again it's one of
those things
I don't want to you know it depends on
what you're looking for are you looking
for uh not to miss any spam mails are
you looking to drive down the road and
not run anybody over
precision is used to calculate the
model's ability to classify positive
values correctly it answers the question
when the model predicts a positive value
how often is it right it is the true
positive divided by the total number of
predicted positive values again in this
one depends on what project you're
working on whether this is what you're
going to be focusing on
so recall it is used to calculate the
model's ability to predict positive
values how often does the model actually
predict the correct positive values
it is a true positive divided by the
total number of actual positive values
and then your F1 score it is the
harmonic mean of recall and precision it
is useful when you need to take both
precision and recall into account
consider the following two confusion
Matrix derived from two different
classifier
to figure out which one performs better
we can find the confusion Matrix for
both of them
and you can see we're back to uh does it
classify whether they can speak English
or are non-speaker they speak some they
don't I know the English language and so
we put these two uh confusion matrixes
out here we can go ahead and do the math
behind that we can look up the accuracy
that's a tpn plus TN over the TF plus TN
plus FP plus FN and so we get an
accuracy of 0.8125
and we have a Precision if you do the
Precision which is your TP truth
positive over TP plus FP
we get 0.891
and if we do the recall we'll end up
with the 0.825 that's your TP over TP
plus FN
and then of course your F1 score which
is two times Precision times recall over
Precision plus recall
and we get the 0.857
and if we do that with another model
let's say we had two different models
and we're trying to see which one we
want to use for whatever reason we might
go ahead and compute the same things we
have our accuracy our precision and our
recall and our F1 score
and as we're looking at this we might
look at the accuracy because that's
really what we're interested in is uh
how many people are we able to classify
as being able to speak English I really
don't want to know if I'm you know I I
really don't want to know if they're
non-speakers
um I'd rather Miss 10 people speaking
English instead of 15. and so you can
see from these charts we probably go
with the first model because it does a
better job guessing who speaks English
and has a higher accuracy because in
this case that is what we're looking for
so uh with that we'll go ahead and pull
up a demo so you can see what this looks
like in the python setup in in the
actual coding for this we'll go into
Anaconda Navigator if you're not
familiar with Anaconda it's a really
good tool to use as far as doing display
in demos
and for quick development as a data
scientist I just love the package
now if you're going to do something
heavier lifting there's some limitations
with anaconda and with the setup but in
general you can do just about anything
in here with your python
and for this we'll go with Jupiter
notebook Jupiter lab is the same as
Jupiter notebook you'll see they now
have integration with pi charm if you
work in pi charm uh certainly there's a
lot of other Integrations that Anaconda
has and we've opened up
um we simply learned files I work on and
create a new file called confusion
Matrix demo and the first thing we want
to note is the data we're working with
here I've opened it up in a word pad or
notepad or whatever
uh you can see it's got a row of
headers comma separated and then all the
data going down below
and then I save this in the same file so
I don't have to remember what path I'm
working on of course if you have your
data separated and you're working with a
lot of data you probably want to put it
into a different folder or file
depending on what you're doing
and the first thing we want to do is go
ahead and import our tools we're going
to use the pandas that's our data frame
if you haven't had a chance to work with
the data frame please review Panda's
data frame and go into simply learn you
can pull up the pandas data frame
tutorial on there
and then we're going to use uh the
scikit framework which is all denoted as
sklearn and I can just pull this in you
can see here's the scikit dash learn.org
with the stable version that you can
import into your python
and from here we're going to use the
train test split
for splitting our data we're going to do
some pre-processing we're going to do
use the logistic regression model that's
our actual machine learning model we're
using and then with this core this
particular setup is about is we're going
to do the accuracy score the confusion
Matrix and the classifier report
so let me go ahead and run that and
bring all that information in
and just like we open the file we need
to go ahead and load our data in here so
we're going to go ahead and do our
pandas read CSV
and then just because we're in jupyter
Notebook we can just put data to read
the data in here a lot of times we'll
actually let me just do this I prefer to
do the just the head of the data or the
top part
and you can see we have age sex I'm not
sure what CP stands for test BPS
cholesterol so a lot of different
measurements if you were in this domain
you'd want to know what all these
different measurements mean
I don't want to focus on that too much
because when we're talking about data
science a lot of times you have no idea
what the data means if you've ever
looked up the breast cancer measurement
it's just a bunch of measurements and
numbers unless you're a doctor you're
gonna have no idea what those
measurements mean
but if it's your specialty in your
domain you better know them so we're
going to go ahead and create Y and it's
going to we're going to set it equal to
the Target so here's our Target value
here
and there it's either one or zero
so we have a classifier if you're
dealing with one zero true false what do
you have you have a classifier
and then our X is going to be uh
everything except for the Target so
we're going to go ahead and drop the
target axis equals one remember that's
columns versus the index or rows X is
equal zero would would give you an error
but you would drop like row two
and then we'll go ahead and just print
that out so you can see what we're
looking at and uh here we have Y data X
data and you can see from the X data we
have the X head and we can go ahead and
just do print
the Y head data
and run that
so this is all loading the data that
we've done so far if there's a confusion
in there go back and rewind the tape and
review it
and then we need to go ahead and split
our data into our X train X test y train
y test and then keep in mind you always
want to split the data before we do the
scalar and the reason is is that you
want the scalar on the training data to
be set on the training data data or fit
to it but not on the test data think of
this as being out in the field you're
not it could actually alter your results
so it's always important to do make sure
whatever you do to the training data or
whatever fit you're doing is always done
on the training not on the test and then
we want to go ahead and scale the data
now we are working with linear
regression model I'll mention this here
in a minute when we get to the actual
model
so some sometimes you don't need to
scale it when you're working with linear
regression models it's not going to
change your result as much as say a
neural network where it has a huge
impact
uh but we're going to go ahead and take
here's our X train X test y train y test
we create our scalar we go ahead and
scale the scale is going to fit the X
train
and then we're going to go ahead and
take our X train and transform it and
then we also need to take our X test and
transform it based on the scale on here
so that our X is now between that nice
set minus one to one and so this is all
uh our pre data setup
and
hopefully all of that looks fairly
familiar to you if you've done a number
of our other classes and you're up to
the setup on here
and then we want to go ahead and do is
create our model and we're going to use
a logistic regression model
and from the logistic regression model
we're going to go ahead and fit our X
train and Y train and then we'll run our
predicted value on here
and let's just go ahead and run that and
so now we are we actually have like our
X test and our prediction so if you
remember from
our Matrix we're looking for the actual
versus the prediction and how those
compare
and if I take us back up here you're
going to notice that we imported the
accuracy score the confusion Matrix and
the classification report
and there's of course our logistic
regression the model we're using for
this
and I did mention it's going to talk a
little bit about scalar and the
regression model
the scalar on a lot of your regression
models uh your basic Mass standard
regression models and I'd have to look
it up for the logistic regression model
when you're using a standard regression
model you don't need to scale the data
it's already just built in by the way
the model works
in most cases but if you're in a neural
network and you're there's a lot of
other different setups than you really
want to take this and fit that on there
and so we can go in and do the accuracy
and this is if you remember correctly we
were looking at the accuracy
with the English speaking so this is
saying our accuracy as to whether this
person is I believe this is the heart
data set
um
it's going to be accurate about 85
percent of the time as far as whether
it's going to predict the person's going
to have a heart condition
or the one as it comes up with the zero
one on there
which would mean at this point that you
have an 85 percent being correct on
telling someone they're extremely high
risk for a heart attack kind of thing
and so we want to go ahead and create
our confusion Matrix and let me just do
that
of course the software does everything
for us so we'll go ahead and run this
and you can see right here
um here's our 25
uh
prediction uh correct predictions right
here
and if you remember from our slide I'll
just bring this over so it's a nice
visual we have our true positive false
positive
so we had 25 which were true that it
said hey this person is going to be high
risk at heart and we had four that were
still high risk that it said were false
so out of these 25 people or out of
these 29 people and that makes sense
because you have 0.85 out of 29 people
it was correct on 25 of them and so uh
here's our accuracy score we were just
looking at that our accuracy is your
true positive and your true negative
over all of them so how true is it and
there's our accuracy coming up here 0.85
and then we have our nice Matrix
generated from that
and you can see right here is a similar
Matrix we had going for that from the
slide and this starts to this should
start asking questions at this point
um so if you're in a board meeting or
you're working with this you really want
to start looking at this data here and
saying well
is this good enough is uh this number of
people and hopefully you'd have a much
larger data set in my is my confusion
Matrix showing for the true positive and
false positive is that acceptable for
what we're doing uh and of course if
you're going to put together whatever
data you're putting out you might want
to separate the true negative false
positive false negative true positive
and you can simply do that by doing the
confusion Matrix and then of course the
Ravel part lets you set that up so you
can just split that right up into a nice
Tuple and the final thing we want to
show you here in the coding on this part
is the confusion Matrix metrics
and so we can come in here and just use
the Matrix equals classification report
the Y test and the predict
and then we're going to take that
classification report and go ahead and
print that out
and you can see here it does a nice job
uh giving you your accuracy your micro
average your weighted average you have
your Precision your recall your F1 score
and your support all in one window so
you can start looking at this data and
saying oh okay our precision's at 0.83
uh 0.87 for getting a a positive and
0.83 for the negative site for a zero
and we start talking about whether this
is a valid information or not to use and
when we're looking at heart attack
prediction we're only looking at one
aspect what's the chances of this person
having a heart attack or not you might
have something where we went back to the
languages maybe you also want to know
whether they speak English or Hindi or
French and you can see right here that
we can now take our confusion Matrix and
just expand it as big as we need to
depending on how many different
classifiers we're working on
what is a decision tree let's go through
a very simple example before we dig in
deep decision tree is a tree shape
diagram used to determine a course of
action each branch of the tree
represents a possible decision or
currents or reaction let's start with a
simple question how to identify a random
vegetable from a shopping bag so we have
this group of vegetables in here and we
can start off by asking a simple
question is it red and if it's not then
it's going to be the purple fruit to the
left probably an eggplant if it's true
it's going to be one of the red fruits
is a diameter greater than two if false
is going to be a what looks to be a red
chili and if it's true it's going to be
a bell pepper from the capsicum family
so it's a capsicum
problems that decision tree can solve so
let's look at the two different
categories the decision tree can be used
on it can be used on the classification
the true false yes no and it can be used
on regression when we figure out what
the next value is in a series of numbers
or a group of data in classification the
classification tree will determine a set
of logical if-then conditions to
classify problems for example
discriminating between three types of
flowers based on certain features in
regression a regression tree is used
when the target variable is numerical or
continuous in nature we fit the
regression model to the Target variable
using each of the independent variables
each split is made based on the sum of
squared error
before we dig deeper into the mechanics
of the decision tree let's take a look
at the advantages of using a decision
tree and we'll also take a glimpse at
the disadvantages the first thing you'll
notice is that it's simple to understand
interpret and visualize it really shines
here because you can see exactly what's
going on in a decision tree little
effort is required for data preparation
so you don't have to do special scaling
there's a lot of things you don't have
to worry about when using a decision
tree it can handle both numerical and
categorical data as we discovered
earlier and non-linear parameters don't
affect its performance so even if the
data doesn't fit an easy curved graph
you can still use it to create an
effective decision or prediction
if we're going to look at the advantages
of a decision tree we also need to
understand the disadvantages of a
decision tree the first disadvantage is
overfitting overfitting occurs when the
algorithm captures noise in the data
that means you're solving for one
specific instance instead of a general
solution for all the data High variance
the model can get unstable due to small
variation in data low bias tree a highly
complicated decision tree tends to have
a low bias which makes it difficult for
the model to work with new data
decision tree important terms before we
dive in further we need to look at some
basic terms we need to have some
definitions to go with our decision tree
in the different parts we're going to be
using we'll start with entropy entropy
is a measure of Randomness or
unpredictability in the data set for
example we have a group of animals in
this picture there's four different
kinds of animals and this data set is
considered to have a high entropy you
really can't pick out what kind of
animal it is based on looking at just
the four animals as a big clump of
entities so as we start splitting it
into subgroups we come up with our
second definition which is Information
Gain Information Gain it is a measure of
decrease in entropy after the data set
is split so in this case based on the
color yellow we've split one group of
animals on one side as true and those
who aren't yellow as false as we
continue down the yellow side we split
based on the height true or false equals
10 and on the other side height is less
than 10 true or false and as you see as
we split it the entropy continues to be
less and less and less and so our
Information Gain is simply the entropy
E1 from the top and how it's changed to
E2 in the bottom and we'll look at the
deeper math although you really don't
need to know a huge amount of math when
you actually do the programming in
Python because it'll do it for you but
we'll look on the actual math at how
they compute entropy finally we went on
the different parts of our tree and they
call the leaf node Leaf node carries a
classification or the decision so it's a
final end at the bottom the decision
node has two or more branches this is
where we're breaking the group up into
different parts and finally you have the
root node the topmost decision node is
known as the root node
how does a decision tree work wonder
what kind of animals I'll get in the
jungle today maybe you're the hunter
with a gun or if you're more into
photography you're a photographer with a
camera so let's look at this group of
animals and let's try to classify
different types of animals based on
their features using a decision tree so
the problem statement is to classify the
different types of animals based on
their features using a decision tree the
data set is looking quite messy and the
entropy is high in this case so let's
look at a training set or a training
data set and we're looking at color
we're looking at height and then we have
our different animals we have our
elephants our giraffes our monkeys and
our tigers and they're of different
colors and shapes let's see what that
looks like and how do we split the data
we have to frame the conditions that
split the data in such a way that the
Information Gain is the highest note
gain is the measure of decrease in
entropy after splitting so the formula
for entropy is the sum that's what this
symbol looks like that looks like kind
of like a the funky e of K where I
equals 1 to k k would represent the
number of animals the different animals
in there where value or P value of I
would be the percentage of that animal
times the log base 2 of the same the
percentage of that animal let's try to
calculate the entropy for the current
data set and take a look at what that
looks like and don't be afraid of the
math you don't really have to memorize
this math and just be aware that it's
there and this is what's going on in the
background and so we have three drafts
two tigers one monkey two elephants a
total of eight animals gather and if we
plug that into the formula we get an
entropy that equals three over eight so
we have three drafts a total of eight
times the log usually they use base two
on the log so log base 2 of 3 over 8
plus in this case let's say it's the
elephants two over eight two elephants
over a total of eight times log base two
two over eight plus one monkey over
total of eight log base 2 1 over 8 and
plus two 2 over 8 of the Tigers log base
2 over 8. and if we plug that into our
computer or calculator I obviously can't
do logs in my head we get an entropy
equal to
0.571 the program will actually
calculate the entropy of the data set
similarly after every split to calculate
the gain now we're not going to go
through each set one at a time to see
what those numbers are we just want you
to be aware that this is a Formula or
the mathematics behind it gain can be
calculated by finding the difference of
the subsequent entropy values after a
split now we will try to choose a
condition that gives us the highest gain
we will do that by splitting the data
using each condition and checking that
the gain we get out of them the
condition that gives us the highest gain
will be used to make the first split can
you guess what that first split will be
just by looking at this image as a human
is probably pretty easy to split it
let's see if you're right if you guessed
the color yellow you're correct let's
say the condition that gives us the
maximum gain is yellow so we will split
the data based on the color yellow if
it's true that group of animals goes to
the left if it's false it goes to the
right the entropy after the splitting
has to decreased considerably however we
still need some splitting at both the
branches to attain an entropy value
equal to zero so we decide to split both
the nodes using height as the condition
since every Branch now contains single
label type we can say that entropy in
this case has reached the least value
and here you see we have the giraffes of
the Tigers the monkey and the elephants
all separated into their own groups this
tree can now predict all the classes of
animals present in the data set with a
hundred percent accuracy that was easy
use case loan repayment prediction let's
get into my favorite part and open up
some Python and see what the programming
code and the scripting looks like in
here we're going to want to do a
prediction and we start with this
individual here who's requesting to find
out how good his customers are going to
be whether they're going to be pay their
loan or not for this bank and from that
we want to generate a problem statement
to predict if a customer will repay loan
amount or not and then we're going to be
using the decision tree algorithm in
Python let's see what that looks like
and let's dive into the code in our
first few steps of implementation we're
going to start by importing the
necessary packages that we need from
Python and we're going to load up our
data and take a look at what the data
looks like so the first thing I need is
I need something to edit my Python and
run it in so let's flip on over and here
I'm using the Anaconda Jupiter notebook
now you can use any python IDE you like
to run it in but I find the Jupiter
notebooks really nice for doing things
on the Fly and let's go ahead and just
paste that code in the beginning and
before we we start let's talk a little
bit about what we're bringing in and
then we're going to do a couple things
in here we have to make a couple changes
as we go through this first part of the
import the first thing we bring in is
numpy as NP that's very standard when
we're dealing with mathematics
especially with a very complicated
machine learning tools you almost always
see the numpy come in for your num your
number it's called number python it has
your mathematics in there in this case
we actually could take it out but
generally you'll need it for most of
your different things you work with and
then we're going to use pandas as PD
that's also a standard the pandas is a
data frame setup and you can liken this
to taking your basic data and storing it
in a way that looks like an Excel
spreadsheet so as we come back to this
when you see NP or PD those are very
standard uses you'll know that that's
the pandas and I'll show you a little
bit more when we explore the data in
just a minute then we're going to need
to split the data so I'm going to bring
in our train test and split and this is
coming from the sklearn package cross
validation and just a minute we're going
to change that and we'll go over that
too and then there's also the SK dot
tree import decision tree classifier
that's the actual tool we're using
remember I told you don't be afraid of
the mathematics it's going to be done
for you well the decision tree
classifier has all that mathematics in
there for you so you don't have to
figure it back out again and then we
have sklearn.metrix for accuracy score
we need to score our our setup that's a
whole reason we're splitting it between
the training and testing data and
finally we still need the sklearn import
tree and that's just the basic tree
function that's needed for the decision
tree classifier and finally we're going
to load our data down here and I'm going
to run this and we're going to get two
things on here one we're going to get an
error and two we're going to get a
warning let's see what that looks like
so the first thing we had is we have an
error why is this error here well it's
looking at this it says I need to read a
file and when this was written the
person who wrote it this is their path
where they stored the file
so let's go ahead and fix that
and I'm going to put in here my file
path I'm just going to call it full file
name and you'll see it's on my C drive
and this is very lengthy setup on here
where I stored the data2.csv file
don't worry too much about the full path
because on your computer it'll be
different the data.2 CSV file was
generated by simply learn
if you want a copy of that you can
comment down below and request it here
in the YouTube
and then if I'm going to give it a name
full file name
I'm going to go ahead and change it here
to full
file name so let's go ahead and run it
now and see what happens
and we get a warning
when you're coding understanding these
different warnings and these different
errors that come up is probably the
hardest lesson to learn
so let's just go ahead and take a look
at this and use this as a opportunity to
understand what's going on here if you
read the warning it says the cross
validation is depreciated so it's a
warning on it's being removed and it's
going to be moved in favor of the model
selection
so if we go up here we have
sklearn.cross validation and if you
research this and go to the sklearn site
you'll find out that you can actually
just swap it right in there with model
selection
and so when I come in here and I run it
again
that removes a warning what they've done
is they've had two different developers
develop it in two different branches
and then they decided to keep one of
those and eventually get rid of the
other one that's all that is and very
easy and quick to fix
before we go any further I went ahead
and opened up the data
from this file remember the the data
file we just loaded on here the data
underscore 2.csv let's talk a little bit
more about that and see what that looks
like both as a text file because it's a
comma separated variable file and in a
spreadsheet this is what it looks like
as a basic text file you can see at the
top they've created a header and it's
got one two three four five columns and
each column has data in it and let me
flip this over because we're also going
to look at this in an actual spreadsheet
so you can see what that looks like and
here I've opened it up in the open
Office calc which is pretty much the
same as Excel and zoomed in and you can
see we've got our columns and our rows
of data a little easier to read in here
we have a result yes yes no we have
initial payment last payment credit
score house number if we scroll way down
we'll see that this occupies a thousand
and one lines of code or lines of data
with the first one being a column and
then 1000 lines of data
now as a programmer
if you're looking at a small amount of
data I usually start by pulling it up in
different sources I can see what I'm
working with
but in larger data you won't have that
option it'll just be too too large so
you need to either bring in a small
amount that you can look at it like
we're doing right now or we can start
looking at it through the python code so
let's go ahead and move on and take the
next couple steps to explore the data
using python let's go ahead and see what
it looks like in Python to print the
length and the shape of the data so
let's start by printing the length of
the database we can use a simple Lin
function from python
and when I run this you'll see that it's
a thousand long and that's what we
expected there's a thousand lines of
data in there if you subtract the column
head this is one of the nice things when
we did the balance data from the panda
read CSV you'll see that the header is
row zero so it automatically removes a
row
and then shows the data separate it does
a good job sorting that data out for us
and then we can use a different function
and let's take a look at that and again
we're going to utilize the tools in
panda
and since the balance underscore data
was loaded as a panda data frame
we can do a shape on it and let's go
ahead and run the shape and see what
that looks like
what's nice about this shape is not only
does it give me the length of the data
we have a thousand lines it also tells
me there's five columns so we were
looking at the data we had five columns
of data and then let's take one more
step to explore the data using Python
and now that we've taken a look at the
length and the shape let's go ahead and
use the pandas module for head another
beautiful thing in the data set that we
can utilize so let's put that on our
sheet here and we have print data set
and balance data.head and this is a
pandas print statement of its own so it
has its own print feature in there and
then we went ahead and gave a label for
our print job here of dataset just a
simple print statement
and when we run that
and let's just take a closer look at
that let me zoom in here
there we go
pandas does such a wonderful job of
making this a very clean
readable data set so you can look at the
data you can look at the column headers
you can have it when you put it as the
head it prints the first five lines of
the data and we always start with zero
so we have five lines we have zero one
two three four instead of one two three
four five
that's a standard scripting and
programming set as you want to start
with the zero position and that is what
the data head does it pulls the first
five rows of data puts in a nice format
that you can look at and view very
powerful tool to view the data so
instead of having to flip and open up an
Excel spreadsheet or open Office Cal or
trying to look at a word doc where it's
all scrunched together and hard to read
you can now get a nice open view of what
you're working with we're working with a
shape of a thousand long five wide so we
have five columns and we do the full
data head you can actually see what this
data looks like the initial payment last
payment credit scores house number so
let's take this now that we've explored
the data and let's start digging into
the decision tree so in our next step
we're going to train and build our data
tree and to do that we need to First
separate the data out we're going to
separate into two groups so that we have
something to actually train the data
with and then we have some data on the
site to test it to see how good our
model is remember with any of the
machine learning you always want to have
some kind of test set to weigh it
against so you know how good your model
is when you distribute it let's go ahead
and break this code down and look at it
in pieces
so first we have our X and Y
where did X and Y come from well X is
going to be our data
and Y is going to be the answer or the
target you can look at it source and
Target
in this case we're using X and Y to
denote the data in and the data that
we're actually trying to guess what the
answer is going to be and so to separate
it we can simply put in x equals the
balance of the data.values the first
brackets
means that we're going to select all the
lines in the database so it's all the
data and the second one says we're only
going to look at columns one through
five remember always start with zero
zero is a yes or no and that's whether
the loan went default or not so we want
to start with one if we go back up here
that's the initial payment and it goes
all the way through the house number
well if we want to look at one through
five we can do the same thing for Y
which is the answers and we're going to
set that just equal to the zero row so
it's just the zero row and then it's all
rows going in there so now we've divided
this into two different data sets
one of them with the
data going in and one with the answers
next we need to split the data
and here you'll see that we have it
split into four different parts
the first one is your X training your X
test your y train your y test
simply put we have X going in where
we're going to train it and we have to
know the answer to train it with
and then we have X test where we're
going to test that data and we have to
know in the end what the Y was supposed
to be
and that's where this train test split
comes in that we loaded earlier in the
modules this does it all for us and you
can see they set the test size equal to
0.3 so that's roughly 30 percent will be
used in the test and then we use a
random state so it's completely random
which rows it takes out of there and
then finally we get to actually build
our decision tree and they've called it
here clf underscore entropy that's the
actual decision tree or decision tree
classifier and in here they've added a
couple variables which we'll explore in
just a minute and then finally we need
to fit the data to that so we take our
clf entropy that we created and we fit
the X train and since we know the
answers for X trade or the Y train we go
ahead and put those in and let's go
ahead and run this and what most of
these sklearn modules do is when you set
up the variable in this case when we set
the clf entropical decision tree
classifier it automatically prints out
what's in that decision tree there's a
lot of variables you can play Within
here and it's quite beyond the scope of
this tutorial able to go through all of
these and how they work but we're
working on entropy that's one of the
options we've added that it's completely
a random state of 100 so 100 percent and
we have a max depth of three now the max
depth if you remember above when we were
doing the different graphs of animals
means it's only going to go down three
layers before it stops and then we have
minimal samples of leaves is five so
it's going to have at least five leaves
at the end so I'll have at least three
splits I'll have no more than three
layers and at least five end leaves with
the final result at the bottom now that
we've created our decision tree
classifier not only created it but
trained it let's go ahead and apply it
and see what that looks like so let's go
ahead and make a prediction and see what
that looks like we're going to paste our
predict code in here
and before we run it let's just take a
quick look at what it's doing here we
have a variable y predict that we're
going to do
and we're going to use our variable clf
entropy that we created
and then you'll see dot predict and it's
very common in the SK learn modules that
they're different tools have the predict
when you're actually running a
prediction
in this case we're going to put our X
test data in here
now if you delivered this for use an
actual commercial use and distributed it
this would be the new loans you're
putting in here to guess
whether the person is going to be pay
them back or not in this case though we
need to test out the data and just see
how good our sample is how good of our
tree does at predicting the loan
payments and finally since Anaconda
Jupiter notebook is works as a command
line for python we can simply put the
why predict e-ad to print it I could
just as easily have put the print
and put brackets around y predict en to
print it out and we'll go ahead and do
that it doesn't matter which way you do
it
and you'll see right here that it runs a
prediction this is roughly 300 in here
remember it's 30 percent of a thousand
so you should have about 300 answers in
here
and this tells you which each one of
those lines of our test went in there
and this is what our why predict came
out
so let's move on to the next step we're
going to take this data and try to
figure out just how good a model we have
so here we go since sklearn does all the
heavy lifting for you and all the math
we have a simple line of code to let us
know what the accuracy is and let's go
ahead and go through that and see what
that means and what that looks like
let's go ahead and paste this in and let
me zoom in a little bit
there we go
so you have a nice full picture
and we'll see here we're just going to
do a print accuracy is
and then we do the accuracy score
and this was something we imported
earlier if you remember at the very
beginning let me just scroll up there
real quick so you can see where that's
coming from
that's coming from here down here from
sklearn.metrix import accuracy score
and you could probably run a script make
your own script to do this very easily
how accurate is it how many out of 300
do we get right and so we put in our y
test that's the one we ran the predict
on and then we put in our y predict en
that's the answers we got and we're just
going to multiply that by a hundred
because this is just going to give us an
answer as a decimal and we want to see
it as a percentage and let's run that
see what it looks like
and if you see here we got an accuracy
of 93.6667
so when we look at the number of loans
and we look at how good our model fit we
can tell people it has about a 93.6
fitting to it so just a quick recap on
that we now have accuracy set up on here
and so we have created a model that uses
a decision tree algorithm to predict
whether a customer will repay the loan
or not the accuracy of the model is
about 94.6 percent the bank can now use
this model to decide whether it should
approve the loan request from a
particular customer or not and so this
information is really powerful we may
not be able to as individuals understand
all these numbers because they have
thousands of numbers that come in but
you can see that this is a smart
decision for the bank to use a tool like
this to help them to predict how good
their profit is going to be off of the
loan balances and how many are going to
default or not
how does a random Forest work as a whole
so to begin our random Forest classifier
let's say we already have built three
trees and we're going to start with the
first tree that looks like this just
like we did in the example this tree
looks at the diameter if it's greater
than or equal to three it's true
otherwise it's false so one side goes to
the smaller diameter one side goes to
larger diameter and if the color is
orange it's going to go to the right
true we're using oranges now instead of
lemons and if it's red it's going to go
to the left false we build a second tree
very similar but split differently
instead of the first one being split by
a diameter this one when they created it
if you look at that first Bowl it has a
lot of red objects so it says is the
color red because that's going to bring
our entropy down the fastest and so of
course if it's true it goes to the left
if it's false it goes to the right and
then it looks at the shape false or true
and so on and so on and tree three e is
the diameter equal to one and it came up
with this because there's a lot of
cherries in this bowl so that would be
the biggest split on there is is the
diameter equal to one it's going to drop
the entropy the quickest and as you can
see it splits it into true if it goes
false and they've added another category
does it grow in the summer and if it's
false it goes off to the left if it's
true it goes off to the right let's go
ahead and bring these three trees you
can see them all in one image so this
would be three completely different
trees categorizing a fruit and let's
take a fruit now let's try this and this
fruit if you look at it we've blackened
it out you can't see the color on it so
it's missing data remember one of the
things we talked about earlier is that a
random Forest works really good if
you're missing data if you're missing
pieces so this fruit has an image but
maybe as a person had a black and white
camera when they took the picture and
we're going to take a look at this and
it's going to have they put the color in
there so ignore the color down there but
the diameter equals three we find out it
grows in the the sum or equals yes and
the shape is a circle and if you go to
the right you can look at what one of
the decision trees did this is the third
one is the diameter greater than equal
to three is a color orange well it
doesn't really know on this one but if
you look at the value it say true and go
to the right tree two classifies it as
cherries is a color equal red is the
shape a circle true it is a circle so
this would look at it and say oh that's
a cherry and then we go to the other
classifier and it says is the diameter
equal one well that's false does it grow
in the summer true so it goes down and
looks at as oranges so how does this
random Forest work the first one says
it's an orange the second one said it
was a cherry and the third one says it's
an orange
and you can guess that if you have two
oranges and one says it's a cherry when
you add that all together the majority
of the vote says orange so the answer is
it's classified as an orange even though
we didn't know the color and we're
missing data on it I don't know about
you but I'm getting tired of fruit so
let's switch and I did promise you we'd
start looking at a case example and get
into some python coding today we're
going to use the case the iris flower
analysis
oh this is the exciting part as we roll
up our sleeves and actually look at some
python coating before we start the
python coding we need to go ahead and
create a problem statement wonder what
species of Iris do these flowers belong
to let's try to predict the species of
the flowers using machine learning in
Python let's see how it can be done so
here we begin to go ahead and Implement
our python code and you'll find that the
first half of our implementation is all
about organizing and exploring the data
coming in let's go ahead and take this
first step which is loading the
different modules into Python and let's
go ahead and put that in our favorite
editor whatever your favorite editor is
in this case I'm going to be using the
Anaconda Jupiter notebook which is one
of my favorites certainly there's
notepad plus plus and eclipse and dozens
of others or just even using the python
terminal window any of those will work
just fine to go ahead and explore this
python coding so here we go let's go
ahead and flip over to our Jupiter
number book and I've already opened up a
new page for Python 3 code and I'm just
going to paste this right in there and
let's take a look and see what we're
bringing into our python the first thing
we're going to do is from the
sklearn.datasets import load Iris now
this isn't the actual data this is just
the module that allows us to bring in
the data the load Iris and the iris is
so popular it's been around since 1936
when Ronald Fisher published a paper on
it and they're measuring the different
parts of the flower and based on those
measurements predicting what kind of
flower it is and then if we're going to
do a random Forest classifier we need to
go ahead and import our random forest
classifier from the sklearn module so
sklearn dot Ensemble import random force
classifier and then we want to bring in
two more modules and these are probably
the most commonly used modules in Python
and data science with any of the other
modules that we bring in and one is
going to be pandas we're going to import
pandas as PD p is a common term used for
pandas and pandas is basically creates a
data format for us where when you create
a pandas data frame it looks like an
Excel spreadsheet and you'll see that in
a minute when we start digging deeper
into the code panda is just wonderful
because it plays nice with all the other
modules in there and then we have numpy
which is our numbers Python and the
numbers python allows us to do different
mathematical sets on here we'll see
right off the bat we're going to take
our NP and we're going to go ahead and
Seed the randomness with it with zero so
np.random.seed is seating that as zero
this code doesn't actually show anything
we're going to go ahead and run it
because I need to make sure I have all
those loaded and then let's take a look
at the next module on here the next six
slides including this one are all about
exploring the data remember I told you
half of this is about looking at the
data and getting it all set so let's go
ahead and take this code right here the
script and let's get that over into our
jupyter notebook and and here we go
we've gone ahead and run the Imports now
I'm going to paste the code down here
and let's take a look and see what's
going on the first thing we're doing is
we're actually loading the iris data and
if you remember up here we loaded the
module that tells it how to get the IRS
data now we're actually assigning that
data to the variable Iris and then we're
going to go ahead and use the DF to
Define data frame
and that's going to equal PD and if you
remember that's pandas as PD so that's
our pandas
and Panda data frame and then we're
looking at Iris data and columns equals
Irish feature names
and we're going to do the DF head and
let's run this so you can understand
what's going on here
the first thing you want to notice is
that our DF has created what looks like
an Excel spreadsheet and in this Excel
spreadsheet we have set the columns so
up on the top you can see the four
different columns and then we have the
data iris.data down below it's a little
confusing without knowing where this
data is coming from so let's look at the
bigger picture and I'm going to go print
I'm just going to change this for a
moment and we're going to print all the
virus and see what that looks like
so when I print all a virus I get this
long list of information and you can
scroll through here and see all the
different titles on there
what's important to notice is that first
off there's a brackets at the beginning
so this is a python dictionary
and in a python dictionary you'll have a
key or a label and this label pulls up
whatever information comes after it so
feature names which we actually used
over here under columns is equal to an
array of simple length simple width
petal length petal width these are the
different names they have for the four
different columns and if you scroll down
far enough you'll also see data down
here oh goodness it came up right
towards the top and data is equal to the
different data we're looking at
now there's a lot of other things in
here like Target we're going to be
pulling that up in a minute and there's
also the names the target names which is
further down and we'll show you that
also in a minute let's go ahead and set
that back
to the Head
and this is one of the neat features of
pandas and Panda data frames
is when you do df.head or the panda
dataframe dot head it will print the
first five lines of the data set in
there along with the headers if you have
them in this case we have the column
header set to Iris features and in here
you'll see that we have 0 1 2 3 4 in
Python most arrays always start at zero
so when you look at the first five it's
going to be zero one two three four not
one two three four five so now we've got
our IRS data imported into a data frame
let's take a look at the next piece of
code in here and so in this section here
of the code we're going to take a look
at the Target and let's go ahead and get
this into our notebook this piece of
code so we can discuss it a little bit
more in detail so here we are in our
jupyter notebook I'm going to put the
code in here and before I run it I want
to look at a couple things going on so
we have DF species and this is
interesting because right here you'll
see where I have DF species in Brackets
which is the key code for creating
another column and here we have
iris.target now these are both in the
pandas setup on here so in pandas we can
do either one I could have just as
easily done Iris and then in Brackets
Target depending on what I'm working on
both are acceptable let's go ahead and
run this code and see how this changes
and what we've done is we've added the
target from the iris data set as another
column on the end
now what species is this is what we're
trying to predict so we have our data
which tells us the answer for all these
different pieces and then we've added a
column with the answer that way when we
do our final setup we'll have the
ability to program our our neural
network to look for these this different
data and know what a setosa is or a Vera
color which we'll see in just a minute
or virginica those are the three that
are in there and now we're going to add
one more column I know we're organizing
all this data over and over again it's
kind of fun there's a lot of ways to
organize it what's nice about putting
everything onto one data frame is I can
then do a printout and it shows me
exactly what I'm looking at and I'll
show you where you where that's
different where you can alter that and
do it slightly differently but let's go
ahead and put this into our script up to
that now and here we go we're going to
put that down here
and we're going to run that
and let's talk a little bit about what
we're doing now we're exploring data
and one of the challenges is knowing how
good your model is did your model work
and to do this we need to split the data
and we split it into two different parts
they usually call it the training and
the testing and so in here we're going
to go ahead and put that in our database
so you can see it clearly and we've set
it DF remember you can put brackets this
is creating another column is train so
we're going to use part of it for
training and this equals NP remember
that stands for numpy DOT random.uniform
so we're generating a random number
between 0 and 1 and we're going to do it
for each of the rows that's where the
length DF comes from so each row gets a
generated number and if it's less than
0.75 it's true and if it's greater than
0.75 it's false this means we're going
to take 75 percent of the data roughly
because there's a Randomness involved
and we're going to use that to train it
and then the other 25 percent we're
going to hold off to the side and use
that to test it later on on so let's
flip back on over and see what the next
step is so now that we've labeled our
database for which is training and which
is testing let's go ahead and sort that
into two different variables train and
test and let's take this code and let's
bring it into our project and here we go
let's paste it on down here and before I
run this let's just take a quick look at
what's going on here is we have up above
we created remember there's our def dot
head which prints the first five rows
and we've added a column is train at the
end and so we're going to take that
we're going to create two variables
we're going to create two new data
frames one's called train one's called
test 75 percent in train 25 in test
and then to sort that out
we're going to do that by doing DF our
main original data frame with the iris
data in it and if DF is trained equals
true
that's going to go in the train and if
DF is train equals false it goes in the
test and so when I run this
we're going to print out the number in
each one let's see what that looks like
and you'll see that it puts 118 in the
training module and it puts 32 in the
testing module which lets us know that
there was 150 lines of data in here so
if you went and looked at the original
data you could see that there's 150
lines and that's roughly 75 percent in
one and 25 percent for us to test our
model on afterward so let's jump back to
our code and see where this goes in the
next two steps
we want to do one more thing with our
data and let's make it readable to
humans I don't know about you but I hate
looking at zeros and ones so let's start
with the features and let's go ahead and
take those and make those readable to
humans and let's put that in our code
let's see here we go paste it in and
you'll see here we've done a couple very
basic things we know that the columns in
our data frame again this is a panda
thing the DF columns
and we know the first four of them 0 1 2
3 that'd be the first four are going to
be the features or the titles of those
columns and so when I run this
you'll see down here that it creates an
index sepa length sepa width petal
length and petal width and this should
be familiar because if you look up here
here's our column titles going across
and here's the first four
one thing I want you to notice here is
that when you're in a command line
whether it's Jupiter notebook or you're
running command line in the terminal
window if you just put the name of it
it'll print it out this is the same as
doing print
features
and the shorthand is you just put
features in here if you're actually
writing a code
and saving the script and running it by
remote you really need to put the print
in there but for this when I run it
you'll see it gives me the same thing
but for this we want to go ahead and
we'll just leave it as features because
it doesn't really matter and this is one
of the fun thing about Jupiter notebooks
is I'm just building the code as we go
and then we need to go ahead and create
the labels for the other part so let's
take a look and see what that for our
final step in prepping our data before
we actually start running the training
and the testing is we're going to go
ahead and convert the species on here
into something the computer understands
so let's put this code into our script
and see where that takes us
all right here we go we've set y equal
to PD dot factorize train species of
zero so let's break this down just a
little bit we have our pandas right here
PD factorize what's factorize doing I'm
going to come back to that in just a
second let's look at what train species
is and why we're looking at the group
zero on there
and let's go up here and here is our
species
remember this on we created this whole
column here for species
and then it has setosis cytosis cytosis
cytosa and if you scroll down enough
you'd also see virginica and Vera color
we need to convert that into something
the computer understands zeros and ones
so the trained species of zero because
this is in the format of a of an array
of arrays so you have to have the zero
on the end and then species is just that
column factorize goes in there and looks
at the fact that there's only three of
them so when I run this you'll see that
y generates an array that's equal to in
this case it's the training set and it's
zeros ones and twos representing the
three different kinds of flowers we have
so now we have something the computer
understands and we have a nice table
that we can read and understand and now
finally we get to actually start doing
the predicting so here we go we have two
lines of code oh my goodness that was a
lot of work to get to two lines of code
but there is a lot in these two lines of
code so let's take a look and see what's
going on here and put this into our full
script that we're running and let's
paste this in here and let's take a look
and see what this is we have we're
creating a variable clf and we're going
to set this equal to the random forest
classifier and we're passing two
variables in here and there's a lot of
variables you can play with as far as
these two are concerned they're very
standard in jobs all that does is to
prioritize it not something to really
worry about usually when you're doing
this on your own computer you do in jobs
equals two if you're working in a larger
or big data and you need to prioritize
it differently this is what that number
does is it changes your priorities and
how it's going to across the system and
things like that and then the random
state is just how it starts zero is fine
for here
but let's go ahead and run this
we also have clf.fit train features
comma Y and before we run it let's talk
about this a little bit more clf dot fit
so we're fitting we're training it we
are actually creating our random Forest
classifier right here this is a code
that does everything and we're going to
take our training set remember we kept
our test off to the side and we're going
to take our training set with the
features and then we're going to go
ahead and put that in and here's our
Target the Y so the Y is 0 1 and 2 that
we just created and the features is the
actual data going in that we put into
the training set let's go ahead and run
that
and this is kind of an interesting thing
because it printed out the random Force
classifier
and everything around it
and so when you're running this in your
terminal window or in a script like this
this automatically treats this like just
like when we were up here and I typed in
y and it printed out y instead of print
y
this does the same thing it treats this
as a variable and prints it out but if
you're actually running your code that
wouldn't be the case and what is printed
out is it shows us all the different
variables we can change and if we go
down here you can actually see in jobs
equals two
you can see the random State equals zero
those are the two that we sent in there
you would really have to dig deep to
find out all these the different
meanings of all these different settings
on here some of them are
self-explanatory if you kind of think
about it a little bit like Max features
is auto so all the features that we're
putting in there is just going to
automatically take all four of them
whatever we send it it'll take some of
them might have so many features because
you're processing words there might be
like 1.4 million features in there
because you're doing legal documents and
that's how many different words are in
there at that point you probably want to
limit the maximum features that you're
going to process in leaf nodes that's
the end notes remember we had the fruit
and we're talking about the leaf nodes
like I said there's a lot in this we're
looking at a lot of stuff here so you
might have in this case there's probably
only think three leaf nodes maybe four
you might have thousands of leaf nodes
at which point you do need to put a cap
on that and say okay you can only go so
far and then we're going to use all of
our resources on processing this and
that really is what most of these are
about is limiting the process and making
sure we don't overwhelm a system and
there's some other settings in here
again we're not going to go over all of
them warm start equals false alarm start
is if you're programming at one piece at
a time externally since we're not we're
not going to have like we're not going
to continually to train this particular
Learning Tree and again like I said
there's a lot of things in here that
you'll want to look up more detail from
the SK learn and if you're digging in
deep and running a major project on here
for today though all we need to do is
fit or train our features and our Target
y so now we have our training model
what's next if we're going to create a
model
we now need to test it remember we set
aside the test feature test group 25
percent of the data so let's go ahead
and take this code and let's put it into
our script and see what that looks like
okay here we go
and we're going to run this
and it's going to come out with a bunch
of zeros ones and twos which represents
the three type of flowers the Sentosa
the virginica and the Versa color and
what we're putting into our predict is
the test features and I always kind of
like to know what it is I am looking at
so real quick we're going to do test
features and remember features is an
array
of simple length sepal width pedal
length pedal width so when we put it in
this way it actually loads all these
different columns that we loaded into
features so if we did just features let
me just do features in here seeing so
what features looks like this is just
playing with the with pandas data frames
you'll see that it's an index so when
you put an index in like this
into test features into test it then
takes those columns and creates a panda
data frames from those columns and in
this case
we're going to go ahead and put those
into our predict
so we're going to put each one of these
lines of data
the 5.0 3.4 1.5.2 and we're going to put
those in and we're going to predict what
our new Forest classifier is going to
come up with and this is what it
predicts it predicts uh zero zero zero
one two one one two two two and and
again this is the flower type sotosa
virginica and Versa color so now that
we've taken our test features let's
explore that let's see exactly what that
data means to us so the first thing we
can do with our predicts is we can
actually generate a different prediction
model when I say different we're going
to view it differently it's not that the
data itself is different so let's take
this next piece of code and put it into
our script
so we're pasting it in here and you'll
see that we're doing uh predict and
we've added underscore proba for
probability so there's our clf DOT
predict probability so we're running it
just like we ran it up here but this
time with this we're going to get a
slightly different result and we're only
going to look at the first 10.
so you'll see down here instead of
looking at all of them uh which was what
27 you'll see right down here that this
generates a much larger field on the
probability and let's take a look and
see what that looks like and what that
means
so when we do the predict underscore
praba for probability it generates three
numbers so we had three leaf nodes at
the end and if you remember from all the
theory we did this is the predictors the
first one is predicting a one for setosa
it predicts a zero for virginica and it
predicts a zero for Versa color and so
on and so on and so on and let's uh you
know what I'm going to change this just
a little bit let's look at 10
to 20 just because we can
and we start to get a little different
of data and you'll see right down here
it gets to this one this line right here
and this line has 0 0.5 0.5 and so if
we're going to vote and we have two
equal votes it's going to go with the
first one so it says uh satosha gets
zero votes virginica gets 0.5 votes
Versa color gets 0.5 votes but let's
just go with the virginica since these
two are equal and so on and so on down
the list you can see how they vary on
here so now we've looked at both how to
do a basic predict of the features and
we've looked at the predict probability
let's see what's next on here so now we
want to go ahead and start mapping names
for the plants we want to attach names
so that it makes a little more sense for
us and this we're going to do in these
next two steps we're going to start by
setting up our predictions and mapping
them to the name so let's see what that
looks like
and let's go ahead and paste that code
in here and run it and this goes along
with the next piece of code so we'll
skip through this quickly and then come
back to a little bit so here's Iris dot
Target names
and uh if you remember correctly this
was the the names that we've been
talking about this whole time the setosa
virginica versus color and then we're
going to go ahead and do the prediction
again we've run we could have just hit a
variable equal to this instead of
re-running it each time but we'll go
ahead and run it again clf dot predict
test features remember that Returns the
zeros the ones in the twos and then
we're going to set that equal to
predictions so this time we're actually
putting it in a variable and when I run
this
it distributes it it comes out as an
array and the array is setosis cytosis
cytosa we're only looking at the first
five we could actually do let's do the
first 25 just so we can see a little bit
more on there and you'll see that it
starts mapping it to all the different
flower types the Versa color and the
virginica in there and let's see how
this goes with the next one so let's
take a look at the top part of our
species in here and we'll take this code
and put it in our script
and let's put that down here and paste
it there we go and we'll go ahead and
run it and let's talk about both these
sections of code here
and how they go together the first one
is our predictions and I went ahead and
did predictions through 25 let's just do
five
and so we have cytosis cytosis cytosis
cytosis that's what we're predicting
from our test model
and then we come down here we look at
test species I remember I could have
just done
test.species.head and you'll see it says
cytosis cytosis cytosis cytosa and they
match so the first one is what our
forest is doing
and the second one is what the actual
data is now is we need to combine these
so that we can understand what that
means we need to know how good our
forest is how good it is at predicting
the features so that's where we come up
to the next step which is lots of fun
we're going to use a single line of code
to combine our predictions and our
actuals so we have a nice chart to look
at and let's go ahead and put that in
our script in our jupyter notebook here
let's see let's go ahead and paste that
in and then I'm going to because I'm on
the Jupiter notebook I can do a control
minus you can see the whole line there
there we go resize it and let's take a
look and see what's going on here we're
going to create in pandas remember PD
stands for pandas and we're doing a
cross tab this function takes two sets
of data and creates a chart out of them
so when I run it you'll get a nice chart
down here and we have the predicted
species
so across the top you'll see the Sentosa
versus color virginica and the actual
species cystosa versicolor virginica and
so the way to read this chart and let's
go ahead and take a look on how to read
this chart here when you read this chart
you have setosa where they meet you have
versicolor where they meet and you have
virginica where they meet and they're
meeting where the actual and the
predicted agree so this is the number of
accurate predictions so in this case it
equals 30. if you had 13 plus 5 plus 12
you get 30. and then we notice here
where it says virginica but it was
supposed to be versacolor this is
inaccurate so now we have two two
inaccurate predictions and 30 accurate
predictions so I will say that the model
accuracy is 93 that's just 30 divided by
32 and if we multiply by a hundred we
can say that it is 93 accurate so we
have a 93 accuracy with our model I did
want to add one more quick thing in here
on our scripting before we wrap it up so
let's flip back on over to my script in
here we're going to take this line of
code from up above I don't know if you
remember it but predicts equals the iris
dot Target underscore names so we're
going to map it to the names
and we're going to run the prediction
and we read it on test features but you
know we're not just testing it we want
to actually deploy it so at this point I
would go ahead and change this and this
is an array of arrays this is really
important when you're running these to
know that
so you need the double brackets and I
could actually create data maybe let's
let's just do two flowers so maybe I'm
processing more data coming in and we'll
put two flowers in here
and then I actually want to see what the
answer is so let's go ahead and type in
preds and print that out and when I run
this
you'll see that I've now predicted two
flowers so maybe I measured in my front
yard as versacolor and versacolor
not surprising since I put the same data
in for each one
this would be the actual end product
going out to be used on data that you
don't know the answer for
so that's going to include our scripting
part of this
today we're going to cover the K nearest
neighbors that refer to as k n n and KNN
is really a fundamental place to start
in the machine learning it's the basis
of a lot of other things and just the
logic behind it is easy to understand
and Incorporated in other forms of
machine learning so today what's in it
for you why do we need knnn what is k n
how do we choose the factor k
when do we use k n n
how does k n algorithm work and then
we'll dive in to my favorite part the
use case predict whether a person will
have diabetes or not that is a very
common and popular used data set as far
as testing out models and learning how
to use the different models in machine
learning by now we all know machine
learning models make predictions by
learning from the past data available so
we have our input values our machine
learning model Builds on those inputs of
what we already know and then we use
that to create a predicted output is
that a dog a little kid looking over
there and watching the black cat cross
their path no dear you can differentiate
between a cat and a dog based on their
characteristics cats
cats have sharp claws uses to climb
smaller length of ears meows and purrs
doesn't love to play around dogs they
have dull claws bigger length of ears
barks loves to run around you usually
don't see a cat running around people
although I do have a cat that does that
where dogs do and we can look at these
we can say we can evaluate their
sharpness of the claws how sharp are
their claws and we can evaluate the
length of the ears and we can usually
sort out cats from dogs based on even
those two characteristics now tell me if
it is a cat or a dog an odd question
usually little kids no cats and dogs by
now unless you live a place where
there's not many cats or dogs so if we
look at the sharpness of the claws the
length of the ears and we can see that
the cat has a smaller ears and sharper
claws than the other animals its
features are more like cats it must be a
cat sharp claws length of ears and goes
in the cat group because KNN is based on
feature similarity we can do
classification using kn and classifier
so we have our input value the picture
of the black cat it goes into our
trained model and it predicts that this
is a cat coming out so what is KNN what
is the k n algorithm
K nearest neighbors is what that stands
for it's one of the simplest supervised
machine learning algorithms mostly used
for classification so we want to know is
this a dog or is not a dog is it a cat
or not a cat it classifies a data point
based on how his neighbors are
classified KNN stores all available
cases and classifies new cases based on
a similarity measure and here we've gone
from cats and dogs right into wine
another favorite of mine k n stores all
available cases and classifies new cases
based on a similarity measure and here
you see we have a measurement of sulfur
dioxide versus the chloride level and
then the different wines they've tested
and where they fall on that graph based
on how much sulfur dioxide and how much
chloride K and K N is a perimeter that
refers to the number of nearest
neighbors to include in the majority of
the voting process and so if we add a
new glass of wine there red or white we
want to know what the neighbors are in
this case we're going to put k equals
five we'll talk about K in just a minute
a data point is classified by the
majority of votes from its five nearest
neighbors here the unknown point would
be classified as red since four out of
five neighbors are red so how do we
choose K how do we know k equals five I
mean this was the value we put in there
so we're going to talk about it how do
we choose the factor K and N algorithm
is based on feature similarity choosing
the right value of K is a process called
parameter tuning and is important for
better accuracy so at k equals three we
can classify we have a question mark in
the middle as either a as a square or
not is it a square or is it in this case
a triangle and so if we set k equals to
3 we're going to look at the three
nearest neighbors we're going to say
this as a square and if we put k equals
to 7 we classify as a triangle depending
on what the other data is around it you
can see as the K changes depending on
where that point is that drastically
changes your answer and we jump here we
go how do we choose the factor of K
you'll find this in all machine learning
choosing these facts factors that's the
face you get it's like oh my gosh you
say choose the right K did I set it
right my values in whatever machine
learning tool you're looking at so that
you don't have a huge bias in One
Direction or the other and in terms of
knnn the number of K if you choose it
too low the bias is based on it's just
too noisy it's it's right next to a
couple things and it's going to pick
those things and you might get a skewed
answer and if your K is too big then
it's going to take forever to process so
you're going to run into processing
issues and resource issues so what we do
the most common use and there's other
options for choosing K is to use the
square root of n so N is a total number
of values you have you take the square
root of it in most cases you also if
it's an even number so if you're using
uh like in this case squares and
triangles if it's even you want to make
your K value odd that helps it select
better so in other words you're not
going to have a balance between two
different factors that are equal so
usually take the square root of n and if
it's even you add one to it or subtract
one from it and that's where you get the
K value from that is the most common use
and it's pretty solid it works very well
when do we use knnn we can use K N when
data is labeled so you need a label on
it we know we have a group of pictures
with dogs dogs cats cats data is Noise
free and so you can see here when we
have a class and we have like
underweight 140 23 Hello Kitty normal
that's pretty confusing we have a high
variety of data coming in so it's very
noisy and that would cause an issue data
set is small so we're usually working
with smaller data sets where you might
get into a gig of data if it's really
clean it doesn't have a lot of noise
because K N is a lazy learner I.E it
doesn't learn a discriminative function
from the training set so it's very lazy
so if you have very complicated data and
you have a large amount of it you're not
going to use the k n but it's really
great to get a place to start even with
large data you can sort out a small
sample and get an idea of what that
looks like using the knnn and also just
using for smaller data sets k n works
really good how does the K N algorithm
work consider a data set having two
variables height in centimeters and
weight in kilograms and each point is
classified as normal or underweight so
we can see right here we have two
variables you know true false are either
normal or they're not they're
underweight on the basis of the given
data we have to classify the below set
as normal or underweight using k n n so
if we have new data coming in this says
57 kilograms and 177 centimeters is that
going to be normal or underweight to
find the nearest neighbors we'll
calculate the euclidean distance
according to the euclidean distance
formula the distance between two points
in the plane with the coordinates x y
and a b is given by distance D equals
the square root of x minus a squared
plus y minus B squared and you can
remember that from the two edges of a
triangle we're Computing the third Edge
since we know the X side and the Y side
let's calculate it to understand clearly
so we have our unknown point and we
placed it there in red and we have our
other points where the data is scattered
around the distance D1 is the square
root of 170 minus 167 squared plus 57
minus 51 squared which is about 6.7 and
distance 2 is about 13 and distance 3 is
about 13.4 similarly we will calculate
the euclidean distance of unknown data
point from all the points in the data
set and because we're dealing with small
amount of data that's not that hard to
do and it's actually pretty quick for a
computer and it's not a really
complicated Mass you can just see how
close is the data based on the euclidean
distance hence we have calculated the
euclidean distance of unknown data point
from all the points as shown where X1
and y1 equal 57 and 170 whose class we
have to classify so now we're looking at
that we're saying well here's the
euclidean distance who's going to be
their closest neighbors now let's
calculate the nearest neighbor at k
equals three and we can see the three
closest neighbors put some at normal and
that's pretty self-evident when you look
at this graph it's pretty easy to say
okay what you know we're just voting
normal normal three votes for normal
this is going to be a normal weight so
majority of neighbors are pointing
towards normal hence as per k n
algorithm the class of 57 170 should be
normal so a recap of k n n positive
integer K is specified along with a new
sample we select the K entries in our
database which are closest to the new
sample we find the most common
classification of these entries this is
the classification we give to the new
sample so as you can see it's pretty
straightforward we're just looking for
the closest things that match what we
got so let's take a look and see what
that looks like in a use case in Python
so let's dive into the predict diabetes
use case so use case predict diabetes
the objective predict whether a person
will be diagnosed with diabetes or not
we have a data set of 768 people who
were or were not diagnosed with diabetes
and let's go ahead and open that file
and just take a look at that data and
this is in a simple spreadsheet format
the data itself is comma separated very
common set of data and it's also a very
common way to get the data and you can
see here we have columns a through I
that's what one two three four five six
seven eight eight columns with a
particular attribute and then the ninth
column which is the outcome is whether
they have diabetes as a data scientist
the first thing you should be looking at
is insulin well you know if someone has
insulin they have diabetes that's why
they're taking it well that could cause
issue in some of the machine learning
packages but for a very basic setup this
works fine for doing the KNN and the
next thing you notice is it didn't take
very much to open it up I can scroll
down to the bottom of the data there's
768. it's pretty much a small data set
you know at 769 I can easily fit this
into my ram on my computer computer I
can look at it I can manipulate it and
it's not going to really tax just a
regular desktop computer you don't even
need an Enterprise version to run a lot
of this so let's start with importing
all the tools we need and before that of
course we need to discuss what IDE I'm
using certainly can use any particular
editor for python but I like to use for
doing very basic visual stuff the
Anaconda which is great for doing demos
with the jupyter notebook and just a
quick view of the Anaconda Navigator
which is the new release out there which
is really nice you can see under home I
can choose my application we're going to
be using python36 I have a couple
different versions on this particular
machine if I go under environments I can
create a unique environment for each one
which is nice and there's even a little
button there where you can install
different packages so if I click on that
button and open the terminal I can then
use a simple pip install to install
different packages I'm working with
let's go ahead and go back under home
and we're going to launch our notebook
and I've already you know kind of like
the old cooking shows I've already
prepared a lot of my stuff so we don't
have to wait for it to launch because it
takes a few minutes for it to open up a
browser window in this case I'm going
it's going to open up Chrome because
that's my default that I use and since
the script is pre-done you'll see I have
a number of windows open up at the top
the one we're working in and since we're
working on the KNN predict whether a
person will have diabetes or not let's
go and put that title in there and I'm
also going to go up here and click on
Cell actually we want to go ahead and
first insert a cell below and then I'm
going to go back up to the top cell and
I'm going to change the cell type to
markdown that means this is not going to
run this python it's a markdown language
so if I run this first one it comes up
in nice big letters which is kind of
nice remind us what we're working on and
by now you should be familiar with doing
all of our Imports we're going to import
the pandas as PD import numpy is NP
pandas is the pandas data frame and
numpy is a number array very powerful
tools to use in here so we have our
Imports so even brought in our pandas
are numpy our two general python tools
and then you can see over here we have
our train test split by now you should
be familiar with splitting the data we
want to split part of it for training
our thing and then training our
particular model and then we want to go
ahead and test the remaining data to see
how good it is pre-processing a standard
scalar preprocessor so we don't have a
bias of really large numbers remember in
the data we had like number of
pregnancies isn't going to get very
large where the amount of insulin they
take and get up to 256 so 256 versus 6.
that will skew results so we want to go
ahead and change that so they're all
uniform between
-1 and 1. and then the actual tool this
is the K neighbors classifier we're
going to use
and finally the last three are three
tools to test all about testing our
model how good is it we just put down
test on there and we have our confusion
Matrix our F1 score and our accuracy so
we have our two general python modules
we're importing and then we have our six
modules specific from the SK learn setup
and then we do need to go ahead and run
this so these are actually imported
there we go and then move on to the next
step and so in this set we're going to
go ahead and load the database we're
going to use pandas remember pandas is
PD and we'll take a look at the data in
Python we looked at it in a simple
spreadsheet but usually I like to also
pull it up so we can see what we're
doing so here's our data set equals
pd.read CSV that's a pandas command and
the diabetes folder I just put in the
same folder where my IPython script is
if you put in a different folder you
need the full length on there we can
also do a quick Links of the data set
that is a simple python on command Len
for length we might even let's go ahead
and print that we'll go print and if you
do it on its own line links.dataset in
the jupyter notebook it'll automatically
print it but when you're in most of your
different setups you want to do the
print in front of there and then we want
to take a look at the actual data set
and since we're in pandas we can simply
do data set head and again let's go
ahead and add the print in there if you
put a bunch of these in a row you know
the data set one head data set two head
it only prints out the last one so I
usually always like to keep the print
statement in there but because most
projects only use one data frame pandas
data frame doing it this way it doesn't
really matter the other way works just
fine and you can see when we hit the Run
button we have the 768 lines which we
knew and we have our pregnancies it's
automatically given a label on the left
remember the head only shows the first
five lines so we have zero through four
and just a quick look at the data you
can see it matches what we looked at
before we have pregnancy glucose blood
pressure sure all the way to age and
then the outcome on the end and we're
going to do a couple things in this next
step we're going to create a list of
columns where we can't have zero there's
no such thing as zero skin thickness or
zero blood pressure zero glucose any of
those you'd be dead so not a really good
Factor if they don't if they have a zero
in there because they didn't have the
data and we'll take a look at that
because we're going to start replacing
that information with a couple of
different things and let's see what that
looks like so first we create a nice
list as you can see we have the values
talked about glucose blood pressure skin
thickness and this is a nice way when
you're working with columns is to list
the columns you need to do some kind of
transformation on a very common thing to
do and then for this particular setup we
certainly could use the there's some
Panda tools that will do a lot of this
where we can replace the N A but we're
going to go ahead and do it as a data
set column equals datasetc column dot
replace this is this is still pandas you
can do a direct there's also it's that
you look for your nan a lot of different
options in here but the Nan num pnan is
what that stands for is none it doesn't
exist so the first thing we're doing
here is we're replacing the zero with a
numpy none there's no data there that's
what that says that's what this is
saying right here so put the zero in and
we're going to play zeros with no data
so if it's a zero that means a person's
well hopefully not dead hope they just
didn't get the data the next thing we
want to do is we're going to create the
mean which is the integer from the data
set from the column dot mean where we
skip in A's we can do that and that is a
pandas command there the skip n a so
we're going to figure out the mean of
that data set and then we're going to
take that data set column and we're
going to replace all the npnan with the
means why did we do that and we could
have actually just taken this step and
gone right down here and just replace 0
and Skip anything where except you can
actually there's a way to skip zeros and
then just replace all the zeros but in
this case we want to go ahead and do it
this way so you can see that we're
switching this to a non-existent value
then we're going to create the mean well
this is the average person so if we
don't know what it is if they did not
get the data and the data is missing one
of the tricks is you replace it with the
average what is the most common data for
that this way you can still use the rest
of those values to do your computation
and it kind of just brings that
particular value of those missing values
out of the equation let's go ahead and
take this and we'll go ahead and run it
doesn't actually do anything so we're
still preparing our data if you want to
see what that looks like we don't have
anything in the first few lines just not
going to show up but we certainly could
look at a row let's do that let's go
into our data set with the printed data
set and let's pick in this case let's
just do glucose and if I run this this
is going to print all the different
glucose levels going down and we
thankfully don't see anything in here
that looks like missing data at least on
the ones shows you can see I skipped a
bunch in the middle because that's what
it does we have too many lines in
Jupiter notebook it'll skip a few and go
on to the next in a data set let me go
and remove this and we'll just zero out
that and of course before we do any
processing before proceeding any further
we need to split the data set into our
train and testing data that way we have
something to train it with and something
to test it on and you're going to notice
we did a little something here with the
pandas database code there we go my
drawing tool we've added in this right
here of the data set and what this says
is that the first one in pandas this is
from the PD pandas it's going to say
within the data set we want to look at
the eye location and it is all rows
that's what that says so we're going to
keep all the rows but we're only looking
at zero columns 0 To 8. Remember column
nine here it is right up here we printed
it in here is outcome well that's not
part of the training data that's part of
the answer yes column nine but it's
listed as eight number eight so zero to
eight is nine columns so eight is the
value and when you see it in here 0 this
is actually zero to seven it doesn't
include the last one and then we go down
here to Y which is our answer and we
want just the last one just column eight
and you can do it this way with this
particular notation and then if you
remember we imported the train test
split that's part of the SK learn right
there and we simply put in our X and our
y we're going to do random State equals
zero you don't have to necessarily seed
it that's a seed number I think the
default is one one when you seated I
have to look that up and then the test
size test size is 0.2 that simply means
we're going to take 20 percent of the
data and put it aside so that we can
test it later that's all that is and
again we're going to run it not very
exciting so far we haven't had any
printout other than to look at the data
but that is a lot of this is prepping
this data once you prep it the actual
lines of code are quick and easy and
we're almost there with the actual
running of our k n we need to go ahead
and do a scale the data if you remember
correctly we're fitting the data in a
standard scalar which means instead of
the data being from you know 5 to 303 in
one column and the next column is one to
six we're going to set that all so that
all the data is between
-1 and 1. that's what that standard
scalar does keeps it standardized and we
only want to fit the scalar with the
training set but we want to make sure
the testing set is the X test going in
is also transformed so it's processing
it the same so here we go with our
standard scalar we're going to call it
SC underscore X for the scalar and we're
going to import the standard scalar into
this variable and then our X train
equals SC underscore x dot fit transform
so we're creating the scalar on the X
train variable and then our X test we're
also going to transform it so we've
trained and transformed the X train and
then the X test isn't part of that
training it isn't part of the of
training the Transformer it just gets
transformed that's all it does and again
we're going to run this and if you look
at this we've now gone through these
steps all three of them we've taken care
of replacing our zeros for key columns
it shouldn't be zero and we replace that
with the means of those columns that way
that they fit right in with our data
models we've come down here we split the
data so now we have our test data and
our training data and then we've taken
and we've scaled the data so all of our
data going in now no we don't TR we
don't train the Y part though y train
and Y test that never has to be trained
it's only the data going in that's what
we want to train in there then Define
the model using K Nabors classifier and
fit the train data in the model so we do
all that data prep and you can see down
here we're only going to have a couple
lines of code where we're actually
building our model and training it
that's one of the cool things about
Python and how far we've come it's such
an exciting time to be in machine
learning because there's so many
automated tools let's see before we do
this let's do a quick Links of and let's
do y we want let's just do length of Y
and we get 768 and if we import math we
do math dot square root let's do y train
there we go it's actually supposed to be
X train before we do this let's go ahead
and do import math and do math square
root length of Y test and when I run
that we get 12.409 I want to see show
you where this number comes from we're
about to to use 12 is an even number so
if you know if you're ever voting on
things remember the neighbors all vote
don't want to have an even number of
neighbors voting so we want to do
something odd and let's just take one
away we'll make it 11. let me delete
this out of here that's one of the
reasons I love Jupiter notebook because
you can flip around and do all kinds of
things on the fly so we'll go ahead and
put in our classifier we're creating our
classifier now and it's going to be the
K neighbors classifier and neighbors
equal 11. remember we did 12 minus 1 for
11. we have an odd number of neighbors P
equals 2 because we're looking for is it
are they diabetic or not and we're using
the euclidean metric there are other
means of measuring the distance you
could do like square square means value
there's all kinds of measure this but
the euclidean is the most common one and
it works quite well it's important to
evaluate the model let's use the
confusion Matrix to do that and we're
going to use the confusion Matrix
wonderful tool and then we'll jump into
the F1 score
and finally accuracy score which is
probably the most commonly used quoted
number when you go into a meeting or
something like that so let's go ahead
and paste that in there and we'll set
the cm equal to confusion Matrix why
test why predict so those are the two
values we're going to put in there and
let me go ahead and run that and print
it out and the way you interpret this is
you have the Y predicted which would be
your title up here you could do let's
just do p r e d
predicted across the top and actual
going down actual
it's always hard to write in here actual
that means that this column here down
the middle that's the important column
and it means that our prediction said 94
and prediction in the actual agreed on
94 and 32. this number here the 13 and
the 15 those are what was wrong so you
could have like three different if
you're looking at this across three
different variables instead of just two
you'd end up with the third row down
here in the column going down the middle
so in the first case we have the the and
I believe the zero has a 94 people who
don't have diabetes the prediction said
that 13 of those people did have
diabetes and were at high risk and the
32 that had diabetes it had correct but
our prediction said another 15 out of
that 15 it classified as incorrect so
you can see where that classification
comes in and how that works on the
confusion Matrix then we're going to go
ahead and print the F1 score let me just
run that and you see we get a 0.69 in
our F1 score the F1 takes into account
both sides of the balance of false
positives where if we go ahead and just
do the accuracy account and that's what
most people think of is it looks at just
how many we got right out of how many we
got wrong so a lot of people when you're
a data scientist and you're talking to
other data scientists they're going to
ask you what the F1 score the F score is
if you're talking to the general public
or the decision makers in the business
they're going to ask what the accuracy
is and the accuracy is always better
than the F1 score but the F1 score is
more telling it lets us know that
there's more false positives than we
would like on here but 82 percent not
too bad for a quick flash look at
people's different statistics and
running an sklearn and running the knnn
the K nearest neighbor on it so we have
created a model using KNN which can
predict whether a person will have
diabetes or not or at the very whether
they should go get a checkup and have
their glucose checked regularly or not
the print accurac score we got the 0.818
was pretty close to what we got and we
can pretty much round that off and just
say we have an accuracy of 80 percent
tells us it is a pretty fair fit in the
model
why support Vector machine so in this
example last week my son and I visited a
fruit shop dad is that an apple or a
strawberry so the question comes up what
fruit do they just pick up from the
Fruit Stand after a couple of seconds
you could figure out that it was a
strawberry so let's take this model a
step further and let's uh why not build
a model which can predict an unknown
data and in this we're going to be
looking at some sweet strawberries or
crispy apples we want it to be able to
label those two and decide what the
fruit is and we do that by having data
already put in so we already have a
bunch of strawberries we know our
strawberries and they're already labeled
as such we already have a bunch of
apples we know our apples and are
labeled as such then once we train our
model that model then can be given the
new data and the new data is this image
in this case you can see a question mark
on it and it comes through and goes it's
a strawberry in this case we're using
the support Vector Machine model svm is
a supervised learning method that looks
at data and sorts it into one of two
categories and in this case we're
sorting the strawberry into the
strawberry site at this point you should
be asking the question how does the
prediction work before we dig into an
example with numbers let's apply this to
our fruit scenario we have our support
Vector machine we've taken it and we've
taken labeled sample of data
strawberries and apples and we draw on a
line down the middle between the two
groups this split now allows us to take
new data in this case an apple and a
strawberry and place them in the
appropriate group based on which side of
the line they fall in and that way we
can predict the unknown as colorful and
tasty as the food example is let's take
a look at another example with some
numbers involved and we can take a
closer look at how the math Works in
this example we're going to be
classifying men and women and we're
going to start with a set of people with
a different height and a different
weight and to make this work we'll have
to have a sample data set a female where
you have their height weight 174 65 174
88 and so on and we'll need a sample
data set of the mail they have a height
170 990 180 to 80 and so on let's go
ahead and put this on a graph so we have
a nice visual so you can see here we
have two groups based on the height
versus the weight and on the left side
we're going to have the women on the
right side we're going to have them in
now if we're going to create a
classifier let's add a new data point
and figure out if it's male or female so
before we can do that we need to split
our data first we can split our data by
choosing any of these lines in this case
we've drawn two lines through the data
in the middle that separates them in
from the women but to predict the gender
of a new data point we should split the
data in the best possible way and we say
the best possible way because this line
has a maximum space that separates the
two classes here you can see there's a
clear split between the two different
classes and in this one there's not so
much a clear split this doesn't have the
maximum space it separates the two that
is why this line best splits the data we
don't want to just do this by eyeballing
it and before we go further further we
need to add some technical terms to this
we can also say that the distance
between the points and the line should
be as far as possible in technical terms
we can say the distance between the
support vector and the hyperplane should
be as far as possible and this is where
the support vectors are the extreme
points in the data set and if you look
at this data set they have circled two
points which seem to be read on the
outskirts of the women and one on the
outskirts of the men and hyperplane has
a maximum distance to the support
vectors of any class now you'll see the
line down the middle and we call this
the hyperplane because when you're
dealing with multiple Dimensions it's
really not just a line but a plane of
intersections and you can see here where
the support vectors have been drawn in
dashed lines the math behind this is
very simple we take D plus the shortest
distance to the closest positive point
which would be on the Min side and D
minus is the shortest distance to the
closest negative point which is on the
women's side the sum of D plus and D
minus is called the distance margin or
the distance between the two support
vectors that are shown in the dashed
lines and then by finding the largest
distance margin we can get the optimal
hyperplane once we've created an optimal
hyperplane we can easily see which side
the new data fits in and based on the
hyperplane we can say the new data point
belongs to the male gender hopefully
that's clear and how that works on a
visual level as a data scientist you
should also be asking what happens if
the hyperplane is not optimal if we
select a hyperplane having low margin
then there is a high chance of
misclassification this particular svm
model the one we discussed so far is
also called referred to as the ls VM so
far so clear but a question should be
coming up we have our sample data set
but instead of looking like this what if
it looked like this where we have two
sets of data but one of them occurs in
the middle of another set you can see
here where we have the blue and the
yellow and then blue again on the other
side of our data line in this data set
we can't use a hyperplane so when you
see data like this it's necessary to
move away from a 1D view of the data to
a two-dimensional view of the data and
for the transformation we use what's
called a kernel function the kernel
function will take the 1D input and
transfer it to a two-dimensional output
as you can see in this picture here the
1D when transferred to a two-dimensional
makes it very easy to draw a line
between the two data sets what if we
make it even more complicated how do we
perform an svm for this type of data set
here you can see we have a
two-dimensional data set where the data
is in the middle surrounded by the green
date on the outside in this case we're
going to segregate the two classes we
have our sample data set and if you draw
a line through it's obviously not an
optimal hyperplane in there so to do
that we need to transfer the 2D to a 3D
array and when you translate it into a
three-dimensional array using the kernel
you can see where you can place a
hyperplane right through it and easily
split the data before we start looking
at a programming example and dive into
the script let's look at the advantage
of the support Vector machine we'll
start with high dimensional input space
or sometimes referred to as the curse of
dimensionality we looked at earlier one
dimension two Dimension three dimension
when you get to a thousand dimensions a
lot of problems start occurring with
most algorithms that have to be adjusted
for the svm automatically does that in
high dimensional space one of the high
dimensional space one high dimensional
space that we work on is sparse document
vectors this is where we tokenize the
words and documents so we can run our
machine learning algorithms over them
I've seen ones get as high as 2.4
million different tokens that's a lot of
vectors to look at and finally we have
regularization parameter the realization
parameter or Lambda is a parameter that
helps figure out whether we're going to
have a bias or overfitting of the data
whether it's going to be overfitted to a
very specific instance or it's going to
be biased to a high or low value with
the svm it naturally avoids the
overfitting and bias problems that we
see in many other algorithms these three
advantages of the support Vector machine
make it a very powerful tool to add to
your repertoire of machine learning
tools now we did promise you a used case
study we're actually going to dive into
some Python Programming and so we're
going to go into a problem statement and
start off with the zoo so in the zoo
example we have family members going to
the zoo we have the young child going
dead is that a group of crocodiles or
alligators well that's hard to
differentiate and zoos are a great place
to start looking at science and
understanding how things work especially
as a young child and so we can see the
parents sitting here thinking well what
is the difference between a crocodile
and an alligator well one crocodiles are
larger in size alligators are smaller in
size snout width the crocodiles have a
narrow snout and alligators have a wider
snout and of course in the modern day
and age the father's hitting here is
thinking how can I turn this into a
lesson for my son and he goes let us
support Vector machine segregate the two
groups I don't know if my dad ever told
me that that but that would be funny now
in this example we're not going to use
actual measurements in data we're just
using that for imagery and that's very
common in a lot of machine learning
algorithms and setting them up but let's
roll up our sleeves and we'll talk about
that more in just a moment as we break
into our python script so here we arrive
in our actual coding and I'm going to
move this into a python editor in just a
moment but let's talk a little bit about
what we're going to cover first we're
going to cover in the code the setup how
to actually create our svm and you're
going to find that there's only two
lines of code that actually create it
and the rest of it is done so quick and
fast that it's all here in the first
page and we'll show you what that looks
like as far as our data because we're
going to create some data I talked about
creating data just a minute ago and so
we'll get into the creating data here
and you'll see this nice correction of
our two blobs and we'll go through that
in just a second and then the second
part is we're going to take this and
we're going to bump it up a notch we're
going to show you what it looks like
behind the scenes but let's start with
actually creating our setup I like to
use the anacond Honda Jupiter notebook
because it's very easy to use but you
can use any of your favorite python
editors or setups and go in there but
let's go ahead and switch over there and
see what that looks like so here we are
in the Anaconda python notebook or
anaconda Jupiter notebook with python
we're using python3 I believe this is
3.5 but it should be work in any of your
3x versions and you'd have to look at
the SK learn and make sure if you're
using a 2X version an earlier version
let's go ahead and put our code in there
and one of the things I like about the
Jupiter notebook is then go up to view
and I'm going to go ahead and toggle the
line numbers on to make it a little bit
easier to talk about and we can even
increase the size because this is edited
in in this case I'm using Google Chrome
Explorer and that's how it opens up for
the editor although anyone any like I
said any editor will work now the first
step is going to be our Imports and
we're going to import four different
parts the first two I want you to look
at are line one and line two are numpy
as NP and matplot library.pi plot and as
PLT now these are very standardized
Imports when you're doing work the first
one is the numbers python we need that
because part of the platform we're using
uses that for the numpy array and I'll
talk about that in a minute so you can
understand why we want to use a numpy
array versus the standard python array
and normally it's pretty standard setup
to use NP for numpy the map plot library
is how we're going to view our data so
this has do you need the NP for the
sklearn module but the matplot library
is purely for our use for visualization
and so you really don't need that for
the svm but we're going to put it there
so you have a nice visual aid and we can
show you what it looks like that's
really important at the end when you
finish everything so you have a nice
display for everybody to look at and
then finally we're gonna I'm gonna jump
one ahead to line number four that's the
sklearn.datasets dot samples generator
import make blobs and I told you that we
were going to make up data and this is a
tool that's in the SK learning to make
up data I personally don't want to go to
the zoo to get in trouble for jumping
over the fence and probably get eaten by
the crocodiles or alligators as I work
on measuring their snouts and width and
length instead we're just going to make
up some data and that's what that make
blobs is It's a Wonderful tool if you're
ready to test your your setup and you're
not sure about what data you're going to
put in there you can create this blob
and it makes it real easy to use and
finally we have our actual svm the
sklearn import svm on line three so that
covers all our Imports we're going to
create remember I used to make blobs to
create data and we're going to create a
capital x and a lowercase y equals make
blobs in samples equals 40. so we're
going to make 40 lines of data it's
going to have two centers with a random
State equals 20s which each each group
is going to have 20 different pieces of
data in it and the way that lux is that
we'll have under X an X Y plane so I
have two numbers under X and Y will be 0
or 1. that's the two different centers
so we have yes or no in this case
alligator or crocodile that that's what
that represents and then I told you that
the actual SK learner the svm is in two
lines of code and we see it right here
with clf equals
svm.svc kernel equals linear and I set
SQL to 1 although in this example since
we are not regularizing the data because
we want to be very clear and easy to see
I went ahead you can set it to a
thousand a lot of times when you're not
doing that but for this thing linear
because it's a very simple linear
example we only have the two dimensions
and it'll be a nice linear hyper plane
it'll be a nice linear line instead of a
full plane so we're not dealing with a
huge amount of data and then all we have
to do is do clf dot fit X comma Y and
that's it clf has been created and then
we're going to go ahead and display it
and I'm going to talk about this display
here in just a second but let me go
ahead and run this code and this is what
we've done is we've created two blobs
you'll see the blue on the side and then
kind of an orangish on the other side
that's our two sets of data they
represent one represents crocodiles and
one represents alligators and then we
have our measurements in this case we
have like the width and length of the
snout and I did say I was going to come
up here and talk just a little bit about
our plot and you'll see PLT that's what
we imported we're going to do a scatter
plot that means we're just putting dots
on there and then look at this notation
I have the capital x and then in
brackets I have a colon comma zero
that's from numpy if you did that in a
regular array you'll get an error and a
python array you have to have that in a
numpy array it turns out that our make
blobs returns a numpy array and this
notation is great because what it means
is the first part is the colon means
we're going to do all the rows that's
all the data in our blob we created
under capital x and then the second
Point has a comma zero we're only going
to take the first value and then if you
notice we do the same thing but we're
going to take the second value remember
we always start with zero and then one
so we have column 0 and column one and
you can look at this as our X Y plots
the first one is the X plot and the
second one is the Y plot so the first
one is on the bottom zero two four six
eight and ten and then the second one X
of the one is the four five six seven
eight nine ten going up the left hand
side s equals 30 is just the size of the
dot so we can see them instead little
tiny dots and then C map equals plt.cm
dot paired and you'll also see the C
equals y That's the color we're using
two colors zero one and that's why we
get the nice blue and the two different
colors for the alligator and the
crocodile now you can see here that we
did this the actual fit was done in two
lines of code a lot of times there'll be
a third line where we regularize the
data we set it between like minus one
and one and we reshape it but for this
it's not necessary and it's also kind of
nice because you can actually see what's
going on and then if we wanted to we
wanted to actually run a prediction
let's take a look and see what that
looks like and to predict some new data
and we'll show this again as we get
towards the end of digging in deep you
can simply simply assign your new data
in this case I am giving it a width and
length 3 4 and a with the length 5 6 and
note that I put the data as a set of
brackets and then I have the brackets
inside and the reason I do that is
because when we're looking at data it's
designed to process a large amount of
data coming in we don't want to just
process one line at a time and so in
this case I'm processing two lines and
then I'm just going to print and you'll
see clf.predict new data so the clf and
the dot predict part is going to give us
an answer and let's see what that looks
like and you'll see 0 1 so predicted the
first one the 3 4 is going to be on the
one side and the 5 6 is going to be on
the other side so one came out as an
alligator and one came out as a
crocodile now that's pretty short
explanation for this setup but really we
want to dug in and see what's going on
behind the scenes and let's see what
that looks like so the next step is to
dig in deep and find out what's going on
behind the scenes and also put that in a
nice pretty graph graph we're going to
spend more work on this and we did
actually generating the original model
and you'll see here that we go through a
few steps and I'll move this over to our
editor in just a second we come in we
create our original data it's exactly
identical to the first part and I'll
explain why we redid that and show you
how not to redo that and then we're
going to go in there and add in those
lines we're going to see what those
lines look like and how to set those up
and finally we're going to plot all that
on here and show it and you'll get a
nice graph with the what we saw earlier
when we were going through the theory
behind this where it shows the support
vectors and the hyper plane and those
are done where you can see the support
vectors as the dashed lines and the
solid line which is the hyperplane let's
get that into our Jupiter notebook
before I scroll down to a new line I
want you to notice line 13 it has Plot
show and we're going to talk about that
here in just a second but let's scroll
down to a new line down here and I'm
going to paste that code in and you'll
see that the plot show has moved down
below low let's scroll up a little bit
and if you look at the top here of our
new section one two three and four is
the same code we had before and let's go
back up here and take a look at that
we're going to fit the values on our svm
and then we're going to plot scatter it
and then we're going to do a plot show
so you should be asking why are we
redoing the same code well when you do
the plot show that blanks out what's in
the plot so once I've done this Plot
show I have to reload that data now we
could do this simply by removing it up
here re-running it and then coming down
here and then we wouldn't have to rerun
these first four lines of code now in
this it doesn't matter too much and
you'll see the plot show is down here
and then removed right there on line
five I'll go ahead and just delete that
out of there because we don't want to
blank out our screen we want to move on
to the next setup so we can go ahead and
just skip the first four lines because
we did that before and let's take a look
at the ax equals
plt.gca now right now we're actually
spending a lot of time just graphing
that's all we're doing here okay so this
is how we display a nice graph with our
results and our data ax is very standard
used variable when you're talking about
PLT and it's just setting it to that
axis the last axes in the PLT they can
get very confusing if you're working
with many different layers of data on
the same graph and this makes a very
easy to reference to ax so this
reference is looking at the PLT that we
created and we already mapped out our
two blobs on and then we want to know
the limits so we want to know how big
the graph is we can find out the X limit
and the Y limit simply with the get X
limit and get y limit commands which is
part of our met plot library and then
we're going to create a grid and you'll
see down here we have we've set the
variable XX equal to mp.line Space X
limit 0 x limit 1 comma 30 and we've
done the same thing for the y space and
then we're going to go in here and we
create a mesh grid and this is a numpy
command so we're back to our numbers
python let's go through what these numpy
commands mean with the line space and
the mesh grid we've taken XX small x x
equals NP line space and we have our X
limit 0 and our X limit 1 and we're
going to create 30 points on it and
we're going to do the same thing for the
y-axis now this has nothing to do with
our evaluation it's all we're doing is
we're creating a grid of data and so
we're creating a set of points between 0
and the X limit we're creating 30 points
and the same thing with the Y and then
the mesh grid Loops those all together
so it forms a nice grid so if we were
going to do this say between the limit 0
and 10 and do 10 points we would have a
0 0 1 1 0 1 0 2 0 3 0 4 to 10 and so on
you can just imagine a point at Each
corner one of those boxes and the mesh
grid combines them all so we take the YY
and the XX we created and creates the
full grid and we've set that grid into
the YY coordinates and the XX
coordinates now remember we're working
with numbi and python we like to
separate those we like to have instead
of it being X comma 1 you know X comma Y
and then X2 comma Y2 and this in the
next set of data it would be a column of
x's and a column of y's and that's what
we have here is we have a column of y's
and we put it as a capital YY and a
column of X is capital x x with all
those different points being listed and
finally we get down to the numpy v stack
just as we created those in the mesh
grid we're now going to put them all
into one array X Y array now that we've
created the stack of data points we're
going to do something interesting here
we're going to create a value Z and the
Z equals the clf that's our that's our
support Vector machine we created and
we've already trained and we have a DOT
decision function and we're going to put
the X Y in there so here we have all
this data we're going to put that X Y in
there that data and we're going to
reshape it and you'll see see that we
have the XX dot shape in here this
literally takes the XX resets it up
connected to the Y and the Z value lets
us know whether it is the left hand side
it's going to generate three different
values the Z value does and it'll tell
us whether that data is a support Vector
to the left the hyperplane in the middle
or the support Vector to the right so it
generates three different values for
each of those points and those points
have been reshaped so if they're right
on a line on those three different lines
so we've set all of our data up we've
labeled it to three different areas and
we've reshaped it and we've just taken
30 points in each direction if you do
the math you have 30 times 30 so that's
900 points of data and we separate it
between the three lines and reshaped it
to fit those three lines we can then go
back to our map plot Library where we've
created the ax and we're going to create
a contour and you'll see here we have
Contour Capital XX capital y y these
have been reshaped to fit those lines Z
is the labels so now we have have the
three different points with the labels
in there and we can set the colors
equals K and I told you we had three
different labels but we have three
levels of data the alpha is just makes
it kind of see-through so it's only 0.5
of the value in there so when we graph
it the data will show up from behind it
wherever the lines go and finally the
line Styles this is where we set the two
support vectors to be Dash dashed lines
and then a single one is just a straight
line that's what all that setup does and
then finally we take our ax dot scatter
we're going to go ahead and plot the
support vectors but we've programmed it
in there so that they look nice like the
dash dashed line in the dashed line on
that grid and you can see here when we
do the clf Dot support vectors we are
looking at column 0 and column one and
then again we have the S equals 100 so
we're going to make them larger and the
line width equals one face colors equals
none let's take a look and see what that
looks like when we show it and you can
see we get down to our end result it
creates a really nice graph we have our
two support vectors and dashed lines and
they have the near data so you can see
those two points or in this case the
Four Points where those lines nicely
cleave the data and then you have your
hyperplane down the middle which is as
far from the two different points as
possible creating the maximum distance
so you can see that we have our nice
output for the size of the body and the
width of the snout and we've easily
separated the two groups of crocodile
and alligator congratulations you've
done it we've made it of course these
are pretend data for our crocodiles and
alligators but this Hands-On example
will help you to encounter any support
Vector machine projects in the future
and you can see how easy they are to set
up and look at in depth so what is
k-means clustering k-means clustering is
an unsupervised learning algorithm in
this case you don't have labeled data
unlike in supervised learning so you
have a set of data and you want to group
them and as as the name suggests you
want to put them into clusters which
means objects that are similar in nature
similar in characteristics need to be
put together so that's what K means
clustering is all about the term k is
basically is a number so we need to tell
the system how many clusters you need to
perform so if K is equal to 2 there will
be two clusters if K is equal to 3 3
clusters and so on and so forth that's
what the k stands for and of course
there is a way of finding out what is
the best or Optimum value of K for a
given data we will look at that so that
is K means cluster so let's take an
example K means clustering is used in
many many scenarios but let's take an
example of Cricket the game of cricket
let's say you received data of a lot of
players from maybe all over the country
or all over the world and this data has
information about the runs scored by the
people ordered by the player and the
wickets taken by the player and based on
this information we need to Cluster this
data into two clusters batsman and
Bowlers so this is an interesting
example let's see how we can perform
this so we have the data which consists
of primarily two characteristics which
is the runs and the wickets so the
bowlers basically take wickets and the
batsman score runs there will be of
course a few Bowlers who can score some
runs and similarly there will be some
batsmen who will Who would have taken a
few wickets but with this information we
want to Cluster those players into
batsmen and Ballers so how does this
work let's say this is how the data is
so there are information there is
information on the y-axis about the runs
code and on the x-axis about the wickets
taken by the players so if we do a quick
plot this is how it would look and then
we do the clustering we need to have the
Clusters like shown in the third diagram
out here so we need to have a cluster
which consists of people who have scored
High runs which is basically the batsman
and then we need a cluster with people
who have taken a lot of wickets which is
typically the bowlers there may be a
certain amount of overlap but we will
not talk about it right now so with
cayman's clustering we will have here
that means K is equal to 2 and we will
have two clusters which is batsman and
Bowlers so how does this work the way it
works is the first step in k-means
clustering is the allocation of two
centroids randomly so two points are
assigned as so-called centroids so in
this case we want two clusters which
means K is equal to two so two points
have been randomly assigned as centroids
keep in mind these points can be
anywhere there are random points they
are not initially they are not really
the centroids centroid means it's a
central point of a given data set but in
this case when it starts off it's not
really the central idea okay so these
points though in our presentation here
we have shown them one point closer to
these data points and another closer to
these data points they can be assigned
randomly anywhere okay so that's the
first step the next step is to determine
the distance of each of the data points
from each of the randomly assigned
centroids so for example we take this
point and find the distance from this
centroid and the distance from this
center right this point is taken and the
distance is found from this centroid and
the center and so on and so forth so for
every point the distance is measured
from both the centroids and then
whichever distance is less that point is
assigned to that centroid so for example
in this case visually it is very obvious
that all these data points are assigned
to this centroid and all these data
points are assigned to this centroid and
that's what is represented here in blue
color and in this yellow color the next
step is to actually determine the
central point or the actual centroid for
these two clusters so we have this one
initial class star this one initial
cluster but as you can see these points
are not really the centroid centroid
means it should be the central position
of this data set Central position of
this data set so that is what needs to
be determined as the next step so the
central point on the actual centroid is
determined and the original randomly
allocated centroid is repositioned to
the actual centroid of this new clusters
and this process is actually repeated
now what might happen is some of these
points may get reallocated in our
example that is not happening probably
but it may so happen that the distance
is found between each of these data
points once again with these centroids
and if there is if it is required some
points may be reallocated we will see
that in a later example but for now we
will keep it simple so this process is
continued till the centroid
repositioning stops and that is our
final cluster so this is our so after
iteration we come to this position this
situation where the centroid doesn't
need any more repositioning and that
means our algorithm has converged
convergence has occurred and we have the
cluster two clusters we have the
Clusters with a centroid so this process
is repeated the process of calculating
the distance and repositioning the
centroid is repeated till the
repositioning stops which means that the
algorithm has converged and we have the
final cluster with the data points and
the centroids so this is what you're
going to learn from this session we will
talk about the types of clustering what
is k-means clustering application of
k-means clustering k-means clustering is
done using distance measure so we will
talk about the common distance measures
and then we will talk about how means
clustering works and go into the details
of k-means clustering algorithm and then
we will end with the demo and a use case
for k-means clustering so let's begin
first of all what are the types of
clustering there are primarily two
categories of clustering hierarchical
clustering and then partitional
clustering and each of these categories
are further subdivided into
agglomerative and divisive clustering
and k-means and fuzzy c means clustering
let's take a quick look at what each of
these types of clustering are
in hierarchical clustering the Clusters
have a tree-like structure and
hierarchical clustering is further
divided into agglomerative and divisive
agglomerative clustering is the
bottom-up approach we begin with each
element as a separate cluster and merge
them into successively larger clusters
so for example we have a b c d e f which
start by combining B and C form one
cluster DNA form one more then we
combine d e and f one more bigger
cluster and then add BC to that and then
finally a to it compared to that
divisive clustering or divisive
clustering is a top-down approach we
begin with the whole set and proceed to
divide it into successively smaller
clusters so we have ABCDEF we first take
that as a single cluster and then break
it down
to A B C D E and F then we have
partitional clustering split into two
subtypes k means clustering and fuzzy c
means in k-min's clustering the objects
are divided into the number of clusters
mentioned by the number K that's where
the K comes from so if we say k is equal
to 2 the objects are divided into two
clusters C1 and C2 and the way it is
done is the features or characteristics
are compared and all objects having
similar characteristics are clubbed
together so that's how K means
clustering is done we will see it in
more detail as we move forward and fuzzy
c means is very similar to k-means in
the sense that it clubs objects that
have similar characteristics together
but while in k-means clustering two
objects cannot belong to or any object a
single object cannot belong to two
different clusters in c means objects
can belong to more than one cluster so
that is the primary difference between k
means and fuzzy c means so what are some
of the applications of k-means
clustering k-means clustering is used in
a variety of examples or variety of
business cases in real life starting
from academic performance diagnostic
systems search engines and wireless
sensor networks and many more so let us
take a little deeper look at each of
these examples academic performance So
based on the scores of the students
students are categorized into a b c and
so on clustering forms a backbone of
search engines when a search is
performed the search results need to be
grouped together the search engines very
often use clustering to do this and
similarly in case of wireless sensor
networks the clustering algorithm plays
the role of finding the cluster heads
which collects all the data in its
respective cluster so clustering
especially k-means clustering uses
distance measure so let's take a look at
what is distance pressure so while these
are the different types of clustering in
this video we will focus on k-means
clustering so distance measure tells how
similar some objects are so the
similarity is measured using what is
known as distance measure and what are
the various types of distance measures
there is euclidean distance there is
Manhattan distance then we have squared
euclidean distance measure and cosine
distance measure these are some of the
distance measures supported by k-means
clustering let's take a look at each of
these what is euclidean distance measure
this is nothing but the distance between
two points so we have learned in high
school how to find the distance between
two points this is a little
sophisticated formula for that but we
know a simpler one is square root of Y2
minus y1 whole square plus x 2 minus x 1
whole Square so this is an extension of
this formula so that is the euclidean
distance between two points what is the
squared euclidean distance measure it's
nothing but the square of the euclidean
distance as the name suggests so instead
of taking the square root we leave the
square as it is and then we have
Manhattan distance measure in case of
Manhattan distance it is the sum of the
distances across the x-axis and the
y-axis and note that we are taking the
absolute value so that the negative
values don't come into play so that is
the Manhattan distance measure then we
have cosine distance measure in this
case we take the angle between the two
vectors formed by joining the points
from the origin so that is the cosine
distance measure okay so that was a
quick overview about the various
distance measures that are supported by
k-means now let's go and check how
exactly K means clustering works okay so
this is how k-mains clustering works
this is like a flowchart of the whole
process there is a starting point and
then we specify the number of clusters
that we want now there are a couple of
ways of doing this we can do by trial
and error so we specify a certain number
maybe K is equal to 3 or 4 or 5 to start
with and then as we progress we keep
changing until we get the best clusters
or there is a technique called elbow
technique whereby we can determine the
value of K what should be the best value
of K how many clusters should be formed
so once we have the value of K we
specify that and then the system will
assign that many centroid so it picks
randomly that to start with randomly
that many points that are considered to
be the centroids of these clusters and
then it measures the distance of each of
the data points from these centroids and
assigns those points to the
corresponding centroid from which the
distance is minimum so each data point
will be assigned to the centroid Which
is closest to it and thereby we have K
number of initial clusters however this
is not the final clusters The Next Step
it does is for the new groups for the
Clusters that have been formed it
calculates the mean position thereby
calculates the new centroid position the
position of the centroid moves compared
to the randomly allocated one so it's an
iterative process once again the
distance of each point is measured from
this new centroid point and if required
the data points are reallocated to the
new centroids and the mean position or
the new centroid is calculated once
again if the centroid moves then the
iteration continues which means the
convergence has not happened the
clustering has not converged so as long
as there is movement of the centroid
this iteration keeps happening but once
the centroid stops moving which means
that the cluster has converged or the
clustering process has converged that
will be the end result so now we have
the final position of the centroid and
the data points are allocated
accordingly to the closest centroid I
know it's a little difficult to
understand from this simple flowchart so
let's do a little bit of visualization
and see if we can explain it better
let's take an example if we have a data
set for a grocery shop so let's say we
have a data set for a grocery shop and
now we want to find out how many
clusters this has to be spread across so
how do we find the optimum number of
clusters there is a technique called the
elbow method so when these clusters are
formed there is a parameter called
within sum of squares at the lower this
value is the better the cluster is that
means all these points are very close to
each other so we use this within sum of
squares as a measure to find find the
optimum number of clusters that can be
formed for a given data set so we create
clusters or we let the system create
clusters of a variety of numbers maybe
of 10 10 clusters and for each value of
K the within SS is measured and the
value of K which has the least amount of
within SS or WSS that's taken as the
optimum value of K so this is the
diagrammatic representation so we have
on the y-axis the within sum of squares
or WSS and on the x-axis we have the
number of clusters so as you can imagine
if you have K is equal to 1 which means
all the data points are in a single
cluster the witnesses value will be very
high because they are probably scattered
all over the moment you split it into
two there will be a drastic fall in the
within SS value that's what is
represented here but then as the value
of K increases the decrease the rate of
decrease will not be so high it will
continue to decrease but probably the
rate of decrease will not be high so
that gives us an idea so from here we
get an idea for example the optimum
value of K should be either 2 or 3 or at
the most 4 but beyond that increasing
the number of clusters is not
dramatically changing the value in WSS
because that pretty much gets stabilized
okay now that we have got the value of K
and let's assume that these are our
delivery points the next step is
basically to assign two centroids
randomly so let's say C1 and C2 are the
centroids assigned randomly now the
distance of each location from the
centroid is measured and each point is
assigned to the centroid Which is
closest to it so for example these
points are very obvious that these are
closest to C1 whereas this point is far
away from C2 so these points will be
assigned which are close to C1 will be
assigned to C1 and these points or
locations which are close to C2 will be
assigned to C2 and then so this is the
how the initial grouping is done this is
part of C1 and this is part of C2 then
the next step is to calculate the actual
centroid of this data because remember
C1 and C2 are not the centroids they've
been randomly assigned points and only
thing that has been done was the data
points which are closest to them have
been assigned but now in this step the
actual centroid will be calculated which
may be for each of these data sets
somewhere in the middle so that's like
the main point that will be calculated
and the centroid will actually be
positioned or repositioned there same
with C2
so the new centroid for this group is C2
in this new position and C1 is in this
new position once again the distance of
each of the data points is calculated
from these centroids now remember it's
not necessary that the distance Still
Remains the or each of these data points
still remain in the same group by
recalculating the distance it may be
possible that some points get
reallocated like so you see this so this
point earlier was closer to C2 because
C2 was here but after recalculating
repositioning it is observed that this
is closer to C1 than C2 so this is the
new grouping so some points will be
reassigned and again the centroid will
be calculated and if the centroid
doesn't change so that is the repetitive
process iterative process and if the
centroid doesn't change once the
centroid stops changing that means the
algorithm has converged and this is our
final cluster with this as the centroid
C1 and C2 as the centroids these days
data points as a part of each cluster so
I hope this helps in understanding the
whole process iterative process of K
means clustering so let's take a look at
the K means clustering algorithm let's
say we have X1 X2 X3 n number of points
as our inputs and we want to split this
into K clusters or we want to create K
clusters so the first step is to
randomly pick K points and call them
centroids they are not real centroids
because centroid is supposed to be a
center point but they are just called
centroids and we calculate the distance
of each and every input point from each
of the centroids so the distance of X1
from C1 from C2 C3 each of the distances
we calculate and then find out which
distance is the lowest and assign X1 to
that particular random centroid repeat
that process for X2 calculate its
distance from each of the centroid c 1 C
to C3 up to c k a and find which is the
lowest distance and assign X2 to that
particular centroid same with X3 and so
on so that is the first round of
assignment that is done now we have K
groups because there are we have
assigned the value of K so there are K
centroids and so there are K groups all
these inputs have been split into K
groups however remember we picked the
centroids randomly so they are not real
centroids so now what we have to do we
have to calculate the actual centroids
for each of these groups which is like
the mean position which means that the
position of the randomly selected
centroids will now change and they will
be the main positions of this newly
formed K groups and once that is done we
once again repeat this process of
calculating the distance right so this
is what we are doing as the part of Step
4 we repeat step two and three so we
again calculate the distance of X1 from
the centroid C1 C2 C3 and then C which
is the lowest value and assign X1 to
that calculate the distance of X2 from
C1 C to C3 or whatever up to c k and
find whichever is the lowest distance
and assign X2 to that centroid and so on
in this process there may be some
reassignment X1 Pro was probably
assigned to Cluster C2 and after doing
this calculation maybe now X1 is
assigned to C1 so that kind of
reallocation may happen so we repeat the
steps 2 and 3 till the position of the
centroids don't change or stop changing
and that's when we have convergence so
let's take a detailed look at it at each
of these steps so we randomly pick K
cluster centers we call them centroids
because they are not initially they are
not really the centroids so we let us
name them C1 C2 up to CK and then step
two we assign each data point to the
closest Center so what we do we
calculate the distance of each x value
from each C value so the distance
between X1 C1 distance between X1 C2 X1
C3 and then we find which is the lowest
value right that's the minimum value we
find and assign X1 to that particular
centroid then we go next to X2 find the
distance of X2 from C1 X2 from C2 X2
from C3 and so on up to c k and then
assign it to the point or to the
centroid which has the lowest value and
so on so that is Step number two in Step
number three We Now find the actual
centroid for each group so what has
happened as a part of Step number two we
now have all the points all the data
points grouped into K groups because we
we wanted to create K clusters right so
we have K groups each one may be having
a certain number of input values they
need not be equally distributed by the
way based on the distance we will have K
groups but remember the initial values
of the C1 C2 were not really the
centroids of these groups right we
assign them randomly so now in step 3 we
actually calculate the centroid of each
group which means the original point
which we thought was the centroid will
shift to the new position which is the
actual centroid for each of these groups
okay and we again calculate the distance
so we go back to step 2 which is what we
calculate again the distance of each of
these points from the newly positioned
centroids and if required we reassign
these points to the new centroids so as
I said earlier there may be a
reallocation so we now have a new set or
a new group we still have K groups but
the number of items and the actual
assignment may be different from what
was in Step 2 here okay so that might
change then we perform step 3 once again
to find the new centroid of this new
group so we have again a new set of
clusters new centroids and new
assignments we repeat this step two
again once again we find and then it is
possible that after iterating through
three or four or five times the centroid
will stop moving in the sense that when
you calculate the new value of the
centroid that will be same as the
original value or there will be very
marginal change so that is when we say
convergence has occurred and that is our
final cluster that's the formation of
the final cluster all right so let's see
a couple of demos of k-means clustering
we will actually see some live demos and
python notebook using python notebook
but before that let's find out what's
the problem that we are trying to solve
the problem statement is let's say
Walmart wants to open a chain of stores
across the State of Florida
and it wants to find the optimal store
locations now the issue here is if they
open too many stores close to each other
obviously the they will not make profit
but if they if the stores are too far
apart then they will not have enough
sales so how do they optimize this now
for an organization like Walmart which
is an e-commerce giant they already have
the addresses of their customers in
their database so they can actually use
this information or this data and use
k-means cluster to find the optimal
location now before we go into the
python notebook and show you the Live
code I wanted to take you through very
quickly a summary of the code in the
slides and then we will go into the
python notebook so in this block we are
basically importing all the required
libraries like numpy matplotlab and so
on and we are loading the data that is
available in the form of let's say the
addresses for simplicity's sake we will
just take them as some data points then
the next thing we do is quickly do a
scatter plot to see how they are related
to each other with respect to each other
so in the scatter plot we see that there
are a few distinct groups already being
formed so you can actually get an idea
about how the cluster would look and how
many clusters what is the optimal number
of clusters and then starts the actual
k-means clustering process so we will
assign each of these points to the
centroids and then check whether they
are the optimal distance which is the
shortest distance and assign each of the
points data points to the centroids and
then go through this iterative process
till the whole process converges and
finally we get an output like this so we
have four distinct clusters and which is
you we can say that this is how the
population is probably distributed
across Florida State and the centroids
are like the location where the store
should be the optimum location where the
store should be so that's the way we
determine the best locations for the
store and that's how we can help Walmart
find the best locations for their stores
in Florida so now let's take this into
python notebook let's see how this looks
when we are learning running the code
live all right so this is the code for
k-means clustering in jupyter Notebook
we have a few examples here which we
will demonstrate how k-means clustering
is used and even there is a small
implementation of k-means clustering as
well okay so let's get started okay so
this block is basically a importing the
various libraries that are required like
matplotlib and numpy and so on and so
forth which would be used as a part of
the code then we are going and creating
blobs which are similar to clusters now
this is a very neat feature which is
available in scikit-learn make blobs is
a nice feature which creates clusters of
data sets so that's a wonderful
functionality that is readily available
for us to create some test data kind of
thing okay so that's exactly what we are
doing here we are using make blobs and
we can specify how many clusters we want
so centers we are mentioning here so it
will go ahead and so we just mentioned
four so it will go ahead and create some
test data for us
and this is how it looks as you can see
visually also we can figure out that
there are four distinct classes or
clusters in this data set and that is
what make blobs actually provides now
from here onwards we will basically run
the standard k-means functionality that
is readily available so we really don't
have to implement k-means itself the
cayman's functionality or the or the
function is readily available you just
need to feed the data and we'll create
the Clusters so this is the code for
that we import k-means and then we
create an instance of k-means and we
specify the value of K this n underscore
clusters is the value of K remember K
means in K means K is basically the
number of clusters that you want to
create and it is a integer value so this
is where we are specifying that so we
have K is equal to 4. and so that
instance is created we take that
instance and as with any other machine
learning functionality fit is what we
use the function or the method rather
fit is what we use to train the model
here there is no real training kind of
thing but that's the call okay so we are
calling fit and what we are doing here
we are just passing the data so X has
these values the data that has been
created right so that is what we are
passing here and this will go ahead and
create the Clusters and then we are
using
after doing uh fit We Run The predict
which basically assigns for each of
these observations which cluster it
belongs to all right so it will name the
Clusters maybe this is cluster one this
is two three and so on or I will
actually start from zero cluster 0 1 2
and 3 maybe and then for each of the
observations it will assign based on
which cluster it belongs to it will
assign a value so that is stored in y
underscore K means when we call predict
that is what it does and we can take a
quick look at these y underscore K means
or the cluster numbers that have been
assigned for each observation so this is
the cluster number assigned for
observation one maybe this is for
observation two observation three and so
on so we have how many about I think 300
samples right so all the 300 samples
there are 300 values here each of them
the cluster number is given and the
class faster number goes from 0 to 3 so
there are four clusters so the numbers
go from 0 1 2 3 so that's what is seen
here okay now so this was a quick
example of generating some dummy data
and then clustering that okay and this
can be applied if you have proper data
you can just load it up into X for
example here and then run the game so
this is the central part of the k-means
clustering program exam so you basically
create an instance and you mentioned how
many clusters you want by specifying
this parameter and underscore clusters
and that is also the value of K and then
pass the data to get the values now the
next section of this code is the
implementation of a k means now this is
kind of a rough implementation of the
k-means algorithm so we will just walk
you through I will walk you through the
code uh at each step what it is doing
and then we will see a couple of more
examples of how k-means clustering can
be used in maybe some real life examples
real life use cases all right so in this
case here what we are doing is basically
implementing K means clustering and
there is a function or a library
calculates for a given two pairs of
points it will calculate the the
distance between them and see which one
is the closest and so on so this is like
this is pretty much like what k-means
does right so it calculates the distance
of each point or each data set from
predefined centroid and then based on
whichever is the lowest this particular
data point is assigned to the so that is
basically available as a standard
function and we will be using that here
so as explained in the slides the first
step that is done in case of k-means
clustering is to randomly assign some
centroids so as a first step we randomly
allocate a couple of centroids which we
call here we are calling as centers
and then we put this in a loop and we
take it through an iterative process
for each of the data points we first
find out using this function pairwise
distance argument for each of the points
we find out which one which Center or
which are randomly selected centroid is
the closest and accordingly we assign
the data or the data point to that
particular centroid or cost staff and
once that is done for all the data
points we calculate the new centroid by
finding out the mean position with the
center position right so we calculate
the new centroid and then we check if
the new centroid is the coordinates or
the position is the same as the previous
centroid the positions we will compare
and if it is the same that means the
process has converged so remember we do
this process till the centroids or the
centroid doesn't move anymore right so
the centroid gets relocated each time
this reallocation is done so the moment
it doesn't change anymore the position
of the centroid doesn't change anymore
we know that convergence has occurred so
till then so you see here this is like
an infinite Loop while true is an
infinite Loop it only breaks when the
centers are the same the new center and
the old Center positions are the same
and once that is done we return the
senders and the labels now of course as
explained this is not a very
sophisticated and advanced
implementation very basic implementation
because one of the flaws in this is that
sometimes what happens is the centroid
the position will keep moving but in if
the change will be very minor so in that
case also with that is actually
convergence right so for example the
change is 0.0001 we can consider that as
convergence otherwise what will happen
is this will either take forever or it
will be never ending so that's a small
flaw here so that is something
additional checks may have to be added
here but again as mentioned this is not
the most sophisticated implementation
this is like a kind of a rough
implementation of the k-means clustering
so if we execute this code this is what
we get as the output so this is the
definition of this particular function
and then we call that find underscore
clusters and we pass our data X and the
number of clusters which is 4 and if we
run that and plot it this is the output
that we get so this is of course each
cluster is represented by a different
color so we have a cluster in green
color yellow color and so on and so
forth and these big points here these
are the centroids this is the final
position of the centroids and as you can
see visually also this appears like a
kind of a center of all these points
here right similarly this is like the
center of all these points here and so
on so this is the example or this is an
example of a implementation of K means
clustering and and next we will move on
to see a couple of examples of how
k-means clustering is used in maybe some
real life scenarios or use cases in the
next example or demo we are going to see
how we can use k-means clustering to
perform color compression we will take a
couple of images so there will be two
examples and we will try to use k-means
clustering to compress the colors this
is a common situation in image
processing when you have an image with
millions of colors but then you cannot
render it on some devices which may not
have enough memory so that is the
scenario where where something like this
can be used so before again we go into
the python notebook let's take a look at
quickly the the code as usual we import
the libraries and then we import the
image and then we will flatten it so the
reshaping is basically we have the image
information stored in the form of pixels
and if the images like for example 427
by 640 and it has three colors so that's
the overall dimension of the of the
initial image we just reshape it and
then feed this to our algorithm and this
will then create clusters of only 16
clusters so this this colors there are
millions of colors and now we need to
bring it down to 16 colors so we use K
is equal to 16 and this is uh when we
visualize this is how it looks there are
these are all about 16 million possible
colors the input color space has 16
million possible colors and we just
sub compress it to 16 colors so this is
how it would look when we compress it to
16 colors and this is how the original
image looks and and after compression to
16 colors this is uh the new image looks
as you can see there is not a lot of
information that has been lost though
the image quality is definitely reduced
a little bit
so this is an example which we are going
to now see in Python notebook let's go
into the python node and once again as
always we will import some libraries and
load this image called
flower.jpg okay so let me load that and
this is how it looks this is the
original image which has I think 16
million colors and this is the shape of
this image which is basically what is
the shape is nothing but the overall
size right so this is 427 pixel by 640
pixel and then there are three layers
which is this three basically is for RGB
which is red green blue so color image
will have that right so that is the
shape of this now what we need to do is
data let's take a look at how data is
looking so let me just create a new cell
and show you what is in data basically
we have captured this information
so data is what let me just show you
here
all right so let's take a look at
China what are the values in China and
if we see here this is how the data is
stored this is nothing but the pixel
values okay so this is like a matrix and
each one has about for this 427 by 640
pixel all right so this is how it looks
now the issue here is these values are
large the numbers are large so we need
to normalize them to between 0 and 1
right so that's why we will basically
create one more variable which is data
which will contain the values between 0
and 1 and the way to do that is divide
by 255 so we divide China by 255 and we
get the new values in data so let's just
run this piece of code and this is the
shape so we now have also yeah what we
have done is we changed using reshape we
converted into the three dimensional
into a two-dimensional data set and let
us also take a look at how
let me just insert
probably a cell here and take a look at
how data is looking all right so this is
how data is looking and now you see this
is the values are between 0 and 1 right
so if you earlier noticed in case of
china the values were large numbers now
everything is between 0 and 1. this is
one of the things we
need all right so after that the next
thing that we need to do is to visualize
this and we can take random set of maybe
10 000 points and plot it and check and
see how this looks so let us just plot
this
and so this is how the original the
color the pixel distribution is these
are two plots one is red against Green
and another is red against Blue and this
is the original distribution of the
color
then what we will do is we will use
k-means clustering to create just 16
clusters for the various colors and then
apply that to the image now what will
happen is since the data is large
because there are millions of colors
using regular k-means may be a little
time consuming so there is another
version of k-means which is called mini
batch came in so we will use that which
is which processes in the overall
concept Remains the Same but this
basically processes it in smaller
batches that's the only thing okay so
the results will pretty much be the same
so let's go ahead and execute this piece
of code and also visualize this so that
we can see that there are this this is
how the 16 colors would look so this is
red against Green and this is red
against Blue there is quite a bit of
similarity between this original color
schema and the new one right so it
doesn't look very very completely
different or anything like
now we apply this the newly created
colors to the image and we can take a
look how this is looking now we can
compare both the images so this is our
original image and this is our new image
so as you can see there is not a lot of
information that has been lost it pretty
much looks like the original image yes
we can see that for example here there
is a little bit uh it appears a little
dullish compared to this one right
because we kind of took off some of the
finer details of the color but overall
the high level information has been
maintained at the same time the main
advantage is that now this can be this
is an image which can be rendered on a
device which may not be that very
sophisticated now let's take one more
example with a different image in the
second example we will take an image of
the Summer Palace in China and we repeat
the same process this is a high
definition color image with millions of
colors and also three-dimensional
now we will reduce that to 16 colors
using k-means clustering and we do the
same process like before we reshape it
and then we cluster the colors to 16 and
then we render the image once again and
we will see that the color the quality
of the image slightly deteriorates as
you can see here this has much finer
details in this which are probably
missing here but then that's the
compromise because there are some
devices which may not be able to handle
this kind of a high density images so
let's run this chord in Python notebook
all right so let's apply the same
technique for another picture which is
even more intricate and has probably
much complicated color schema so this is
the image now once again we can take a
look at the shape which is 427 by 640 by
3
and this is the new data would look
somewhat like this compared to the
flower image so we have some new values
here and we will also bring this as you
can see the numbers are much big so we
will much bigger so we will now have to
scale them down to values between 0 and
1 and that is done by dividing by 255 so
let's go ahead and do that
and reshape it okay so we get a
two-dimensional Matrix and we will then
as a next step we will go ahead and
visualize this how it looks the 16
colors and this is basically how it
would look 16 million colors and now we
can
create the Clusters out of this the 16
k-means clusters we will create so this
is how the distribution of the pixels
would look with 16 colors and then we go
ahead and apply this
visualize how it is looking for with the
new just the 16 color so once again as
you can see this looks much richer in
color but at the same time and this
probably doesn't have as we can see it
doesn't look as rich as this one but
nevertheless the information is not lost
the shape and all that stuff and this
can be also rendered on a slightly a
device which is probably not that
sophisticated okay so that's pretty much
it so we have seen two examples of how
color compression can be done using
k-min's clustering and we have also seen
in the previous examples of how to
implement k-means the code to roughly
how to implement k-means clustering and
we use some sample data using blob to
just execute the k-means clustering all
right so with that let's move on we talk
about the principal component analysis
we're going to cover dimensionality
reduction principal component analysis
what is it important PCA terminologies
and you'll see it abbreviated normally
as PCA principle component analysis PCA
properties PCA example and then we'll
pull up some python code in our jupyter
notebook and have some Hands-On demo on
the PCA and how it's used dimensionality
reduction dimensionality reduction
refers to the technique that reduces the
number of input variables in a data set
and so you can see on the table on the
right shows the orders made at an
automobile parts retailer the retailer
sells different automobile parts from
different companies and you can see we
have company bpacs isomax and they have
the item the tire the axle an order ID a
price number and a quantity in order to
predict the future cells we find out
that using correlation analysis that we
just need three attributes therefore we
have reduced the number of attributes
from five to three
and clearly we don't really care about
the part number I don't think the part
number would have an effect on how many
tires are bought
and even the store who's buying them
probably does not have an effect on that
in this case that's what they've
actually done is remove those and we
just have the item the tire the price
and the quantity one of the things you
should be taking away from this is in
the scheme of things
we are in the descriptive phase we're
describing the data and we're
pre-processing the data what can we do
to clean it up why dimensionality
reduction
well number one less dimensions for a
given data set means less computation or
training time that can be really
important if you're trying a number of
different models and you're rewriting
them over and over again and even if you
have seven gigabytes of data that can
start taking days to go through all
those different models
so this is huge this is probably the
hugest part as far as reducing our data
set
redundancy is removed after removing
similar entries from the data set
again pre-processing some of our models
like a neural network if you put in two
of the same data it might give them a
higher weight than they would if it was
just one so when you get rid of that
redundancy
it also increases the processing time if
you have multiple data coming in
space required to store the data is
reduced
so if we're committing this into a big
data pool we might not send the company
that bought it why would we want to
store two whole extra columns when we
added into that pool of data
makes the data easy for plotting in 2D
and 3D plots this is my favorite part
very important you're in your
shareholder meeting you want to be able
to give them a really good clear and
simplified version you want to reduce it
down to something people can take in
it helps to find out the most
significant features and skip the rest
which also comes in in postcribing leads
to better human interpretation that kind
of goes with number four what makes data
easy for plotting you have a better
interpretation we're looking at it
principal component analysis so what is
it
uh principal component analysis is a
technique for reducing the
dimensionality of data sets increasing
interpretability but at the same time
minimizing information loss so we take
some very complex data set with lots of
variables we run it through the PCA we
reduce the variables we know what the
reduced variable setup
this is very confusing to look at
because if you look at the end result we
have the different colors all lined up
so what we're going to take a look at is
let's say we have a picture here let's
say you are asked to take a picture of
some Toddlers and you are deciding which
angle would be the best to take the
picture from so if we come up here we
look at this we say okay this is you
know one angle we get the back of a lot
of heads not many faces so we'll do it
from here we might get the one person up
front smiling a lot of the people in the
class are missing so we have a huge
amount off to the right a blank space
Maybe from up here again we have the
back of someone's head
and it turns out that the best angle to
click the picture from might be this
bottom left angle you look at it and you
say hey that makes sense it's a good
configuration of all the people in the
picture now when we're talking about
data it's not you really can't do it by
what you think is going to be the best
we need to have some kind of
mathematical formula so it's consistent
and so it makes sense in the back end
one of the projects I worked on many
years ago
has something similar to the iris if
you've ever done the iris data set it's
probably one of the most common ones out
there where they have the flower
and they're measuring the stamina in the
petals and they have width and they have
length of the petal
instead of putting through the width and
the length of the pedal we could just as
easily do the width to length ratio we
can divide the width by the length and
you get a single number where you had
two
that's the kind of idea that's going on
into this in pre-processing and looking
at what we can do to bring the data down
the very simplified example on my Iris
petal example
when we look at the similarity in PCA we
find the best picture or projection of
the data points
and so we look down at from one angle
we've drawn a line down there we can see
these data points based on in this case
just two variables now keep in mind
we're usually talking about 36 40
variables almost all of your business
models usually have about 26 to 27
different variables they're looking at
same thing with like a bank loan model
we're talking 26 to 36 different
variables they're looking at that are
going in
so we want to do is we want to find the
best view in this case we're just
looking at the X Y
we look down at it and we have our
second idea PC2 and again we're looking
at the x i this X Y this time from a
different direction here for our E's we
can consider that we get two principal
components namely pc1 and PC2
comparing both the principal components
we find the data points are sufficiently
spaced in pc1
so if we look at what we got here we
have pc1 you can see along the line how
the data points are spaced versus the
spacing in PC2 and that's what they're
coming up with what is going to give us
the best look for these data points when
we combine them and we're looking at
them from just a single angle
whereas in PC2 they are less spaced
which makes the observation and further
calculations much more difficult
therefore we accept the pc1 and not the
PC2 as the data points are more spaced
now obviously the back end calculations
are a little bit more complicated when
we get into the math of how they decide
what is more valuable
this gives you an idea though that when
we're talking about this we're talking
about the perspective which would help
in understanding how PCA analysis works
we want to go ahead and do is dive into
the important terminologies under a PCA
and important terminology is our views
the perspective through which data
points are observed
and so you'll hear that if someone's
talking about a PCA presentation and
they're not taking the time to reduce it
to something that the average person
shareholders can understand you might
hear them refer to it as the different
views what view are we taking
Dimension number of columns in a data
set are called the dimensions of that
data set and we talked about you'll hear
features Dimensions
um I was talking about features there's
usually when you're running a business
you're talking 25 26 27 different
features minimal and then you have the
principal component new variables that
are constructed as linear combinations
or mixtures of the initial variables
principle component is very important
it's a combination if you remember my
flower example it would be the width
over the length of the petal as opposed
to putting both width and length in you
just put in the ratio instead which is a
single number versus two separate
numbers projections
the perpendicular distance between the
principal component and the data points
and that goes to that line we had
earlier it's that right angle line of
where those Point how those points fall
onto the line important properties
important properties number of principal
components is always less than or equal
to the number of attributes
that just makes Common Sense uh you're
not going to do
10 principal properties with only three
features now you're trying to reduce
them so that's just kind of goofy but it
is important to remember that people
will throw weird code out there and just
randomly do stuff with instead of really
thinking it through principle components
are orthogonal
and this is what we're talking about
that right angle from the line when we
when we do pc1 we're looking at how
those points fall on to that line same
thing with PC2 we want to make sure that
pc1 does not equal PC2 we don't want to
have the same two principal points when
we do two points
the priority of principal components
decreases as their numbers increase
this is important to understand if
you're going to create
one principle
component everything is summarized into
that one component as we go to two
components the priority how much it
holds value decreases as we go down so
if you have five different points each
one of those points is going to have
less value than just the one point which
has everything summarized in it how PCA
works
I said there was more in the back end we
talk about the math this is what we're
talking about is how does it actually
work
so now we have understanding that you're
looking at a perspective
now we want to see how that math side
Works PCA performs the following
operations in order to evaluate the
principal components for a given data
set
first we start with the standardization
then we have a covariance matrix
computation
and we use that to generate our I Gene
vectors and I Gene values
which is the feature vector and if you
remember the I Gene Vector is like a
translation for moving the data from x
equals one to x equals two or whatever
we're altering it and the hygiene value
is the final value that we generate
when we talk about standardization the
main aim of this step is to standardize
the range of the attributes so that each
one of them lie within similar
boundaries
this process involves removal of the
mean from the variable values and
scaling the data with respect to the
standard deviation
and you can see here we have Z equals
the variable values minus the mean over
the standard deviation
The covariance Matrix computation
covariance Matrix is used to express the
correlation between any two or more
attributes in multi-dimensional data set
The covariance Matrix has the entries as
the variance and the covariance of the
attribute values the variance is denoted
by VAR and the covariance is denoted by
Cove on the right we can see the
covariance Matrix for two attributes and
their values
when we do a Hands-On look at the code
we'll do a display of this so you can
see what we're talking about and what
that looks like
for now you can just notice that this is
a matrix that we're generating with the
variance and then the covariance of x to
y
on the right side we can see the
covariance table for more than two
attributes in a multi-dimensional data
set
this is what I was talking about we
usually are looking at not just one
feature two features we're usually
looking at 25 30 features going on
and so if we do a setup like this we
should see all those different features
as the different variables
covariance Matrix tells us how the two
or more variables are related positive
covariance indicate that the value of
one variable is directly proportional to
the other variable
negative covariance indicate that the
value of one variable is inversely
proportional to the other variable that
is always important to note whenever
we're doing any of these matrixes that
we're going to be looking at that
positive and negative whether it's
inverted or not
and then we have the I Gene values and
the I Gene vectors
iodine values and hygiene vectors are
the mathematical value that are
extracted from the covariance table
they are responsible for the generation
of a new set of variables from the old
set of variables which further lead to
the construction of the principal
components
iodine vectors do not change directions
after linear transformation I Gene
values are the scalars or the magnitude
of the I Gene vectors
and again this is just chain
transforming that data so we're going to
change uh the vector B to the B Prime as
denoted on the chart and so we have like
multiple variables how do we calculate
that new variable and then we have
feature vectors feature vectors is
simply a matrix that has hygiene vectors
of the components that we decide to keep
as the columns
here we decide whether we must keep or
discard the less significant principal
components that we have generated in the
above steps this becomes really
important as we start looking at the
back end of this and we'll do this in
the demo but one of the more important
steps to understand
and so we have the PC example consider
Matrix X within rows or observations and
K columns or variables now for this
Matrix we would construct a variable
space with as many dimensions as the
variable
before our Simplicity let's consider
this three dimensions for now
each observation row of the Matrix X is
placed in the K dimensional variable
space such that the rows in the data
table form a swarm of points in this
space
now we find the mean of all the
observations and then place it along the
data points on the plot
the first principle component is a line
that best accounts for the shape of the
point swarm it represents the maximal
variance Direction in the data
each observation may be projected onto
this line in order to get a coordinate
value along the pc1 this value is known
as a score
usually only one principal component is
insufficient to model the systematic
variation for a data set thus a second
principal axis is created
the second principle component is
oriented such that it reflects the
second largest source of variation in
the data while being orthogonal to pc1
PC2 also passes through the average
point
let's go ahead and pull this up and just
see what that means inside our python
scripting
I'm going to use the Anaconda Navigator
and I will be in Python 3.6 for this
example I believe there's even like a
3.9 out
I tend to stay in 3.6 because a lot of
the models I use especially with the
neural networks are stable in 3 6.
and then we open up our Jupiter I'm in
Chrome and we go ahead and create a new
python 3.
and for ease of use uh our team in the
back was nice enough to put this
together for me
and we'll go ahead and start with the
libraries the first thing I like to do
whenever I'm looking at any new setup
well you know what Let's do let's do the
libraries first we're going to do our
basic libraries which is matplot Library
the PLT from the matplot library pandas
our data frame PD numpy our numbers
array NP Seaborn for graphing SNS that
goes with the plot that actually sits on
matplot Library so the Seaborn sits on
there
and then we have our Amber sign because
we're in jupyter Notebook matplot
library in line the newer version
actually doesn't require that but I put
it in there either anyway just because
I'm so used to it
and then we want to go ahead and take a
look at the data
and in this case we're going to pull in
certainly you can have lots of fun with
different data but we're going to use
the cancer data set and one of the
reasons the cancer data set is it has
like 36 35 different features
so it's kind of fun to use that as our
base for this
and we'll go ahead and run this and look
at our keys
and the first thing we notice in our
keys for the cancer data set is we have
our data we have our Target our frame
Target names description feature names
and file name
so what we're looking for in all this
is well let's take a look at the
description let's go in here and pull up
the description on here
I'm not going to spend a huge amount of
time on the description
because this is we don't want to get
into a medical domain we want to focus
on our PCA setup what's important is you
start looking at what the different
attributes are what they mean if you
were in the medical field you'd want to
note all these different things whether
what they're measuring where it's coming
from
you can actually see the actual
different
measurements they're taking
no missing attributes
let me page all the way to the bottom
and you're going to have your data in
this case our Target
and if you dig deep enough to the Target
let's actually do this let's go ahead
and print Target names
real quick here I always like to to just
take a look and see what's on the other
end of this
uh
Target
names
run that
and so the target name is is it
malignant or is it B9 so in other words
is this uh dangerous growth or is it
something we don't have to worry about
that's the bottom line with the cancer
in this case
go ahead and load our data and you know
what let me go up a just a notch here
for easy of reading
sorry to get that just right I guess
I'll have to do uh so let's go ahead and
look at our data uh our our we're going
to use our pandas
and we're going to go ahead and do our
data frame it's going to equal cancer
data columns equals cancer feature
equals feature names so remember up here
we already loaded the the names up of
our of the features in there what is
going to come out of this let me just
see if we can get to that
it's at the top of Target names
um that's just this list of names here
in the setup
and we can go ahead and run this code
and it'll print the head and you can see
here we have the mean radius the mean
texture mean perimeter
I don't know about you this is a
wonderful data set if you're playing
with it because like many of the data
that most of the data that comes in half
the time we don't even know we're
looking at uh we're just handed a bunch
of stuff as a data scientist going what
the heck is this and so this is a good
place to start because this has a number
of different features in there we have
no idea what these feature means or
where they come from we want to just
look at the data and figure that out
and now we actually are getting into the
PCA side of it as we've noticed before
it's difficult to visualize High
dimensional data we can use PCA to find
the first two principal components and
visualize the data this new
two-dimensional space with a single
scatter plot before we do this we need
to go ahead and scale our data
now
I haven't run this to see if you really
have to scale the data on this
um but as just a general
run time I almost do that as the first
step of any modeling even if it's
pre-modeling as we're doing here in
neural networks that is so important
with PCA visualization it's already
going to scale it when we do the means
and deviation inside the PCA but just in
case it's always good to scale it
and then we're going to take our PCA
with the site kit learn uses very
similar process to other pre-processing
functions that come with scikit-learn we
instantiate a PCA object find the
principal components using the fit
method then apply the rotation and
dimensionality reduction by calling
transform we can also specify how many
components we want to keep when creating
the PCA object
and so the code for this
oops getting a little bit ahead let me
go ahead and run this code
so the code for this
is
from sklearn decomposition import PCA
PCA equals PCA and components equals two
and that's really important to note that
because we're only going to want to look
at two components
I would never go over four components
especially if you're going to demo this
with somebody else if you're showing
this to the shareholders
the whole idea is to reduce it to
something people can see
and then the PCA fit we're going to is
going to take the scaled data that we
generated up here and then you can see
we've created our PCA model with in
components equals two
now whenever I use a new tool I like to
go in there and actually see what I'm
using so let's go to the scikit uh web
page for the PCA
and you can see in here here's our call
statement it describes what all the
different uh setups you have on there
probably the biggest one to look at
would be well the biggest one is your
components how many components do you
want which you have to put in there
pretty much and then you also might look
at the SVD solver it's on auto right now
but you can override that and do
different things with it it does a
pretty good job as it is
and if we go down all the way down to
um
here we go to our methods
if you notice we have fit we have fit
transform
nowhere in here is predict because this
is not used for prediction it's used to
look at the data again we're in the
describe setup we're fitting the data
we're taking a look at it we've already
looked at our minimum maximum we've
already looked at what's in each quarter
we've done a full description of the
data this is part of describing the data
um that's the biggest thing I take away
when I come zooming in here and of
course they have examples of it down
here if you forget
and the biggest one of course is the
number of components and then uh I mean
the rest you can play with the actual
solver whether you're doing a full
randomize there's different things it
does pretty good on the auto
and now we can transform this data to
its first two principal components
and so we have our xpca
we're going to set that equal to PCA
transform scaled data so there we go
there's our first transformation
and let's just go ahead and print the
scaled data shape and the xpca data
shape
and the reason we want to do this is
just to show us what's going on here
we've taken 30 features I think I said
36 or something like that but it's 30.
and we've compressed it down to two
features and we decided we wanted two
features and that's where this comes
from we still have 569 data sets
I mean data rows not data sets we still
have 569 rows of data but instead of
computing 30 features we're now only
doing our model on two features
so let's go ahead and plot these and
take a look and see what's going on
and uh
we're just going to use our PLT figure
we'll set the figure size on here here's
our scatter plot xpca X underscore PCA
of uh of one these are two different
perceptions we're using and then you'll
see right here C for color cancer equals
Target and so remember we have 0 we have
one
and if I remember correctly 0 was
malignant one was B9 so everything in
the zero column is going to be one color
and the other color is going to be one
and then we're going to use the plasma
map just kind of telling you what color
it is add some labels first principal
component second principle component and
we'll go ahead and run this
and you can see here instead of having a
chart one of those heat maps with 30
different columns in it we can look at
this and say hey this one actually did a
pretty good job
of separating the data
in a couple things when I'm looking at
this that I notice is first
we have a very clear area or it's
clumped together
where it's going to be benign and we
have a huge area it's still clumped
together more spread out where it's
going to be malignant or I think I have
that backwards
and then in the middle because we're
dealing with something in this
particular case cancer we would try to
separate I would be exploring how to
separate this middle group out
in other words there's an area where
everything overlaps and we're not going
to have a clear result on it just
because those are the people you want to
go in there and have extra tests or
treat it differently versus going in and
saying just cutting into the can into
the cancers as the body absorbs it and
it dissipates versus uh actively going
in there removing it testing it going
through chemo and all the different
things that's a big difference you know
as far as what's going to happen here in
that middle line where the overlap is
going to be huge that's domain specific
going back to the data we can see here
clearly by using these two components we
can easily separate these two classes so
the next step is what does that mean
interpreting the components
unfortunately with this great power of
dimensionality reduction comes a cost of
not being able to easily understand what
these components represent
I don't know a principal component one
looks represents or second principle the
components correspond to combinations of
original features the components
themselves are stored as an attribute of
the filtered PCA object and so we talk
look at that we can go ahead and do look
at the PCA components this is in our
model we built we've trained it we can
run that and you can see here's the
actual components it's the two
components have each have their own
array
and within the array you can see the
what the scores are using and these
actually give weight to what features
are doing what
so in this numpy Matrix array each row
represents a principal component and
each column relates back to the original
features
what's really neat about this is we can
now go in reverse
and drop this onto a heat map
and start seeing what this means and so
let me go ahead and just put this down
oops I already got it down here
uh we'll go ahead and put this in here
we're going to use our DF comp data
frame and we do our PCA components
and I want you to notice how easy this
is uh we're going to set our columns
equal to cancer feature names
that just makes it really easy
and we're dumping it into a data frame
what's needed about a data frame is when
we get to Seaborn it will pull that data
frame apart in and set it up for us when
we want and so we're just going to do
the CIA the Seabourn heat map of our
data frame composition and we'll use a
plasma coloring
and it creates a nice little color graph
here you can see we have the mean radius
and all the different features along the
bottom on the right we have a scale so
we can see we have the dark colors all
the way to the really light colors which
are what's really shining there this is
like the primary stuff we want to look
at
so this heat map in the color bar
basically represent the correlation
between the various features and the
principal component itself
so you know very powerful map to look at
and then you can go in here and we might
notice that the mean radius look how how
on the bottom of the map it is
um on some of this uh so you have some
interesting correlations here that
change the variations on that and what
means what this is more when you get to
uh postscribe you can also use this to
try to guess as what these things mean
what you want to change to get a better
result
feature selection in machine learning
today we're going to look at what's in
it for you the need for feature
selection what is feature selection
feature selection methods feature
selection statistics
so the need for feature selection to
train a model we collect huge quantities
of data to help the machine learn better
consider a table which contains
information on old cars the model
decides which cars must be crushed for
spare parts and when we talk about huge
quantities of data there they save
everything from people's favorite cat
pictures to and you can imagine there's
so much data out there even in a company
they'll save all these little pieces of
information about people and companies
and corporations you need some way to
sort through because if you try to run
your models on all of it you'll end up
with these very clunky models and they
might have issues which we'll talk about
later but in this case we're talking
about cars and crushing but not all this
data will be useful to us some classes
are part of the data may not contribute
much to our model and can be dropped and
you can see right here we have who was
the owner of the car
in our data a car will not be crushed
based on its previous owner so you know
that's kind of a clear cut you can see
that why would I care who owned the car
before once it's in the junkyard and
we're crushing the cars we're not going
to really care about that
so here we have dropped the owner column
as it does not contribute to the model
having too much unnecessary data can
cause the model to be slow the model may
also learn from this irrelevant data and
be inaccurate so feature selection is a
process of reducing the input variable
to your model by using only relevant
data and getting rid of the noise in the
data consider the database given below a
library wants to donate some old books
to make place in their shelves for new
books we want to train a model to
automate this task
in this case the color of the book does
not matter and keeping it can cause a
model to learn to donate books based on
color we can remove this as a feature
using feature selection we can optimize
our model in several ways
and so the number one is to prevent
learning from noise and overfitting
that's actually the huge one because we
we don't want it to give us a wrong
prediction and that means also improved
accuracy so instead of giving us a wrong
prediction we also want it to be as
close to the right answer as we can get
and we want to reduce the training time
it's an exponential growth in some of
these models so the each feature you add
in increases that much
more training time we talk about feature
selection methods we put together a nice
little flow chart that shows the various
methods used for feature selection
and you have your basic feature
selection and then there is supervised
and unsupervised under supervised
there's intrinsic wrapper method filter
method so we talk about unsupervised
feature selection refers to the method
which does not need the output label
class for feature selection and that was
you can see here under super
unsupervised we don't have I mean that's
really a growing market right now
unsupervised learning and so feature
selection is the same thing supervised
feature selection refers to the method
which uses the output label class for
the feature selection and if you
remember we looked at three different we
have intrinsic wrapper and filter method
so we're going to start with the filter
method on this now remember we know what
the output is so we're going to be
looking at that output to see how well
it's doing versus the features in this
method features are dropped based on
their relation to the output or how they
are correlating to the output
and you can see here we have a set of
features selecting best feature learning
algorithm and then performance and so we
want to find out which feature
correlates to the performance on the
output consider the example of book
classifier here we drop the color column
based on simple deduction and that kind
of sums it up in the in a nutshell is we
want to filter out things that clearly
do not go with what we're looking for if
we look at the wrapper method in the
wrapper method we split our data into
subsets and train a model using this
based on the output of the model we add
and subtract features and train the
model again and you can see here in the
wrapper method we have a set of features
we generate a subset we run it through
the algorithm and we see how each one of
those subset of features performs
consider the book data set by using the
wrapper method we would use a subset of
different features to train the machine
and adjust the subset according to the
output and so you can see here let's say
we take a name and number of times red
and we run just those and we look at the
output and if we looked at them with all
four inputs and look at the output we'd
see quite a different variation in there
and we might say you know what condition
of the book and color really doesn't
affect what we're looking for and you
can see here we've run it on a condition
of the book and color
depending on the output of the model we
will choose our final set of features
these features will give us the best
result for our model
and it might come up that the name
number of times red is probably pretty
important the intrinsic method this
method combines the qualities of both
filter and wrapper method to create the
best subset the model will train and
check the accuracy of different subsets
and select the best among them we kind
of looked at a little overview of some
of the stuff some of the common feature
selection algorithms based on which
method they belong to are given below
and you'll see it's primarily under
supervised there's not like I said a lot
of unsupervised methods and the ones
that are usually used these methods and
finds a way to create a supervised
connection between the data
and we talk about supervised methods we
have our filter method which we talked
about and it uses like the Pearson's
coefficient Chi Squared Anova
coefficient those are all under the
filter method and in the wrapper method
recursive feature elimination so
remember we're choosing a subset and we
want to go through there and look at
each one so you're just doing a lot of
Loops or recursive calculations to see
which one works best and which ones
don't have an impact on the output
and there's a lot of genetic algorithms
to go with this too on the wrapper
method and how they evaluate it and with
the intrinsic method there's the two
main ones we're looking at is the lasso
regularization the lasso algorithms are
basically your standard regression model
so it's finding out how these different
methods fit together and which ones have
the best add together to have the least
amount of error
the other one used in the intrinsic
method is a decision tree it says hey if
this one is this one produces this
result this one produces this result yes
no which way do we go based on the input
and the output variables we can choose
our feature selection model so you have
your numeric input coming in you have
your numeric output if you use the
Pearson's correlation coefficient or
spearman's rank coefficient you can then
select what features you're going to
feed into that specific model and you
maybe have a numerical input and a
categorical input so we're going to be
looking more at Anova correlation
coefficient or Kindles rank coefficient
and if you have a core categorical input
and a numerical output we might be
looking at an over correlation
coefficient in Kindle's rank coefficient
So based on the input and the output
variables we can choose our feature
selection model and you can see we have
categorical to categorical we might be
looking at the chi-squared test
contingency tables and mutual
information let's go and take a look and
see in the python code what we're
talking about here
and I'm going to go ahead and use for my
IDE the Jupiter notebook in the and they
always launch it out of anaconda on here
and we'll go ahead and go up here and
create a new Python 3 module
call it
feature
select
and since we're in Python we're going to
be working mainly with your numpy your
pandas your matplot library so we have
our number array our data frame setup
which goes with the number array the
numpy the pandas data frame and then we
want to go ahead and graph everything so
we're going to import these three
modules
and then we put down other some data
we're going to read this in it's Kobe
Bryant I guess he's a basketball player
our guys in the back we have a number of
them guys both we have a lot of men and
women so it's probably a misnomer our
team in the back they have a some of
them have a liking for basketball and
they know who Kobe Bryant is and they
want to learn a little bit more about
Kobe Bryant what's going in for
whatever's going on with his game in
basketball so we're going to take a look
at him
and once we import the data we can see
what columns are available original
features count so we can see how many
features there are
um the length of it and we'll actually
have a list of them and then print just
the data head the top five rows
and so when we do this
we can see from the CSV file we have 25
original features
our original features are your action
type combined shot type game event ID
and so forth there's a lot of features
in here that they're recorded on all of
his shots this is what we talk about
like a massive amount of data I mean
imagine people are sitting there and
they record all this stuff and they
import this stuff for different reasons
but depending on what we want to look at
do we really want all those features
maybe the question we're going to ask is
what's the chance of him making any one
specific shot
um in right from the beginning we can
look at the some of these things and say
team name team name probably I don't
know maybe it does matter because the
other team might be really good at
defense uh game date maybe we don't
really want to look at the game date
team ID definitely not of importance in
any of this so we look at this we have
25 features and some of these features
just really don't matter to us we also
have location X location y latitude and
longitude I'm guessing that's the same
data we've actually imported the the
very similar data maybe they're slightly
zoned differently but as far as our
program we don't want to repeat data
some of the models when you repeat data
into them and this is true for most
models create a huge bias they weigh
that data over other data
so just at a glance these are the things
we're looking at we want to find out
well how do we get this these features
down and get rid of this bias and all
these extraneous features that we don't
really want to spend time running our
models on and programming on
and as I pointed out there's a location
x a location y latitude and longitude
let's take a look at that and see what
we're looking at here we'll go ahead and
create a plot of these
and we'll just plot we'll do a scatter
plot of location X and location Y and
then we'll do a scatter plot of data
lawn data lat which is probably
longitude and latitude and the scatter
plot I'm just going to actually put a
little Title Here location and Scatter
on there and we'll just go ahead and
plot these and when you look at this
uh coming in
these two graphs are pretty identical
except they're flipped and so when we
look at the location from which they're
shooting from they're probably the same
and at this point we can say okay we can
get rid of one of these sets of datas we
don't need both X and Y and latitude and
longitude because it's the same data
coming in and as we look at this
particular data the latitude longitude
we might also ask does it really make a
difference which side of the Court
you're on whether you're on the left
side or the right side
and so we might go ahead and explore
instead of looking at this as
X Y we might look at it as a distance
and an angle and we can easily compute
that and you can see we can create our
data distance equals location X plus the
location y squared standard euclidean
geometry or triangular geometry
hypotenuse squared equals the each side
squared
and then once we've done that
we can also compute the angle so the
data angle is based on the arc tangent
and so forth on here so this is all this
is we're just going to compute the angle
here and then set that up pi over 2 to
get our angle
and we'll go ahead and run that
and you'll see some errors Run come up
and that's because when we took slices
over here we took a slice of a slice
there's ways to fix that but it's really
not important for this example so if you
do see that you want to start looking up
here for
instead of data location X of uh not
location x0 this would be like I believe
the term is ilo.ilocation if this was
yeah this is in pandas so there's
different things in there but for this
it doesn't really matter these are just
warnings that's all they are and then
let's combine our remaining minutes and
seconds column into one there's another
one so if you remember up here we're
trying to get rid of these columns
do we really need let me see if I can
find it on here there we go there's our
minutes remaining
and then they had
what was it it was uh minutes remaining
and seconds column so there's also a
seconds column on here
and find that one
this is where it really gets kind of
crazy because here's our seconds
remaining so you can see that and here's
our minutes remaining this gets crazy
when you're looking at hundreds of these
features and you can see that if if I'm
going to say
write a model that's going to predict a
lot of this and I want it to run in this
case it's a basketball and how good his
shots are as the data comes in and let's
say I want to have it run on your phone
if I'm running it across hundreds of
features it's going to just hang up on
your phone where if I can get it down to
just a handful
we'll actually be able to come in here
and run it on a smaller device and not
use up as much memory
or processing power so we'll go ahead
and take data remaining time here
and data minutes remaining times 60 plus
data seconds remaining so we're just
going to combine those and we'll go
ahead and reprint our data so we can see
what we got
um
coming across we have our action type
combined and this is we do this a lot we
want to take a look at oops I got so so
zoomed in let me see if I can zoom out
just a little bit
there we go boom
all right so we come up here you can see
that we now have our distance our angle
remaining time which is now just a
number that computes both the minutes
and seconds together
and we still have we've been adding
columns I thought you said we're
supposed to subtract columns right
um we're going to delete the obsolete
columns when we get to them so we're
just filtering out and then this is a
filter method we're just filtering
through the things that we really
don't need and next let's go ahead and
explore team ID and team name let me
just go ahead and run that and if you
look at this we have Los Angeles Lakers
and then they have the team ID here and
they're unique uh it's not that's not
really anything that's going to come up
because that's this particular athletes
works for that team so it's the same on
every line so there's another thing we
can filter out on there team ID and
tname is just useless the whole column
contains only one value each and it's
pretty much useless
let's go ahead and take a look at
matchup and opponent that's an
interesting one and we see here that we
have the Lal versus por and the opponent
is POR and IND
again here's a lot of duplicate
information
so this basically contains the same
information on here again we're
filtering all this stuff out and this is
because we're only looking at one
athlete this might change if you're
looking at multiple athletes that kind
of thing now these are easy to see but
we might have something that looks more
like this we might have something where
we're looking at the distance which we
computed and the shot distance are they
the same thing
and what we can do is we can plot that
and plot them against each other on here
and we see it just draws a nice straight
line and so again we're looking at the
same information so again we're
repeating stuff and we really don't want
to be running our model on repeat
information on here so again it contains
the same information so now let's look
at the shot Zone area shot Zone basic
shot Zone range
so now we're looking at the zones and
what does that mean and we'll go ahead
and do this also in a scatter plot in
this case we're going to just create
three of these side by side so we're
going to create our plot figure side 20
by 10 and then we're going to Define our
scatter plot by category feature and
we're going to do each one setup button
here give it a slightly different color
and so our shot Zone area is going to be
plot subplot 131 scatter one three one
is how that's read by the way
meaning that it's number one
we have three across
and this is the first one down so one
one one our scatter plot by category is
going to be the shot Zone area we're
going to plot that
and then we're going to do the shot Zone
basic and then the shot Zone range and
you just push them through our
definition so each of those areas go
through and you'll see one three one one
three two one three three again it's a 1
by 3 setup and then it's just a place on
each one and so we look at this we can
see that these shots they map out the
same so it's very again redundant
information that should be intuitive
when we're looking at this in this color
graphs
it kind of helps you start looking at
something you that's very intuitive like
this is and you start to realize that
some of this stuff you'll be looking for
in data you might not understand and
you'll see these circular patterns where
they match or they mostly match and you
start to realize when you're looking at
these that they're repetitive data then
you want to explore them more closely
depending on what domain you're working
in so we we look at these and we look at
them and they look just like the regions
of the court but we already have stored
this information in angle and distance
columns so we've seen this image before
let me go back up here and here's our
the similar image
and repeating that image is down here
and so let's go ahead and drop some of
this in this stuff uh so now let's drop
all the useless columns and we can drop
the shot ID team ID team name shot Zone
area shot Zone range shot Zone basic uh
the match up the longitude and latitudes
are putting that into distance seconds
remaining minutes remaining because we
combine that into one column shot
distance because we have just distance
on there location X location y the game
event ID game ID all this stuff is just
being dropped on here and we'll just go
ahead and loop through our drops
and this is a nice way of doing this
because as you're playing with this this
kind of data
putting your list into one setup helps
because then you're just running it
through an iteration
and you can come back and change it you
might be playing with different models
and do this with models you might be
looking at all kinds of different things
that you can drop and add in as you test
these out and again we're working in the
filter method so this is a lot of human
interaction with the data and it takes a
lot of critical thinking to look at this
stuff and say what matches and what
doesn't and so when we look at the
remaining features let me go ahead and
just run this
uh the original to the new count we had
25 features now we have 11 features you
can see that right there let me just
circle that there's our 25 and there's
uh old new now we're down to 11. so
we've cut it down to less than half
and you can just see
the actual different information on here
and the remaining time at this point we
filtered it through and then we'd move
into the next process which would be to
run our model on this
and maybe we would drop some of the
features and see if it runs better or
worse and what happens
that's kind of would be the next step on
there versus this is the filter setup
and that would be one of the other
setups depending on which algorithm you
use so that wraps up our demo on filter
the feature selection and going through
and seeing how these different features
are being repeated in this particular in
a basketball setup
we're going to cover reinforcement
learning today and what's in it for you
we'll start with why reinforcement
learning we'll look at what is
reinforcement learning
we'll see what the different kinds of
learning strategies are that are being
used today in computer models under
supervised versus unsupervised versus
reinforcement
will cover important terms specific to
reinforcement learning we'll talk about
markov's decision process and we'll take
a look at a reinforcement learning
example well we'll teach a tic-tac-toe
how to play why reinforcement learning
training a machine learning model
requires a lot of data which might not
always be available to us further the
data provided might not be reliable
learning from a small subset of actions
will not help expand the vast realm of
solutions that may work for a particular
problem
you can see here we have the robot
learning to walk
very complicated setup when you're
learning how to walk and you'll start
asking questions like if I'm taking one
step forward and left what happens if I
pick up a 50 pound object how does that
change how a robot would walk
these things are very difficult to
program because there's no actual
information on it until it's actually
tried out learning from a small subset
of actions will not help expand the vast
realm of solutions that may work for a
particular problem
and we'll see here it learned how to
walk this is going to slow the growth
that technology is capable of machines
need to learn to perform actions by
themselves and not just learn off humans
and you see the objective climb a
mountain a real interesting point here
is that as human beings we can go into a
very unknown environment and we can
adjust for it and kind of explore and
play with it
most of the models the non-reinforcement
models in computer machine learning
aren't able to do that very well there's
a couple of them that can be used or
integrated to see how it goes is what
we're talking about with reinforcement
learning so what is reinforcement
learning
reinforcement learning is a sub-branch
of machine learning that trains a model
to return an Optimum solution for a
problem by taking a sequence of
decisions by itself
consider a robot learning to go from one
place to another
the robot is given a scenario must
arrive at a solution by itself the robot
can take different paths to reach the
destination
it will know the best path by the time
taken on each path it might even come up
with a unique solution all by itself
and that's really important as we're
looking for Unique Solutions we want the
best solution but you can't find it
unless you try it so we're looking at
our different
systems or different model we have
supervised versus unsupervised versus
reinforcement learning and with the
supervised learning that is probably the
most controlled environment we have a
lot of different supervised learning
models whether it's linear regression
neural networks there's all kinds of
things in between decision trees the
data provided is labeled data with
output values specified and this is
important because we talk about
supervised learning you already know the
answer for all this information you
already know the picture has a
motorcycle in it so your supervised
learning you already know that the
outcome for tomorrow for you know going
back a week you're looking at stock you
can already have like the graph of what
the next day looks like so you have an
answer for it
and you have labeled data which is used
you have an external supervision and
solves Problems by mapping labeled input
to known output
so very controlled
unsupervised learning and the interim
learning is really interesting because
it's now taking part in many other
models they start within you can
actually insert an unsupervised learning
model
in almost either supervised or
reinforcement learning as part of the
system which is really cool
uh data provided is unlabeled data the
outputs are not specified machine makes
its own predictions used to solve
association with clustering problems
unlabeled data is used no supervision
solves Problems by understanding
patterns and discovering output
so you can look at this and you can
think some of these things go with each
other they belong together so it's
looking for what connects in different
ways and there's a lot of different
algorithms that look at this when you
start getting into those there's some
really cool images that come up of what
unsupervised learning is how it can pick
out say the area of a donut one model
will see the area of the donut and the
other one will divide it into three
sections based on this location versus
what's next to it so there's a lot of
stuff that goes in with unsupervised
learning
and then we're looking at reinforcement
learning probably the biggest industry
in today's market in machine learning or
growing Market is very it's very infant
stage as far as how it works and what
it's going to be capable of the machine
learns from its environment using
rewards and errors used to solve reward
based problems no predefined data is
used no supervision follows Trail and
error problem solving approach so again
we have a random at first you start with
a random I try this it works and this is
my reward doesn't work very well maybe
or maybe it doesn't even get you where
you're trying to get it to do and you
get your reward back and then it looks
at that and says well let's try
something else and it starts to play
with these different things finding the
best route
so let's take a look at important terms
in today's reinforcement model
and this has become pretty standardized
over the last few years so these are
really good to know we have the agent
agent is the model that is being trained
via reinforcement learning so this is
your actual entity that has however
you're doing it whether using a neural
network or a
cue table or whatever combination
thereof this is the actual agent that
you're using this is the model
and you have your environment the
training situation that the model must
optimize to is called its environment
and you can see here I guess we have a
robot just trying to get a chest full of
gyms or whatever and that's the output
and then you have your action this is
all possible steps that can be taken by
the model and it picks one action and
you can see here that's picked three
different routes to get to the chest of
diamonds and gems
we have a state the current position
condition returned by the model
and you could look at this if you're
playing like a video game this is the
screen you're looking at so when you go
back here the environment is a whole
game board so if you're playing one of
those Mobius games
you might have the whole game board
going on but then you have your current
position where are you on that game
board what's around that what's around
you if you were talking about a robot
the environment might be moving around
the yard where it is in the yard and
what it can see what input it has in
that location that would be the current
position condition returned by the model
and then the reward to help the model
move in the right direction it is
rewarded points are given to it to
appraise some kind of action so yeah you
did good or if I didn't do as good
trying to maximize the reward and have
the best reward possible
and then policy policy determines how an
agent will behave at any time it acts as
a mapping between action and present
State this is part of the model what is
your action that you're you're going to
take what's the policy you're using to
have an output from your agent one of
the reasons they separate policy as its
own entity
is that you usually have a prediction
of a different options and then the
policy well how am I going to pick the
best based on those predictions I'm
going to guess at different options
and we'll actually weigh those options
in and find the best option we think
will work
so it's a little tricky but the policy
thing is actually pretty cool how it
works let's go ahead and take a look at
a reinforcement learning example and
just in looking at this we're going to
take a look consider what a dog that we
want to train so the dog would be like
the agent so you have your your puppy or
whatever and then your environment is
going to be the whole house or whatever
it is or you're training them and then
you have an action we want to teach the
dog to fetch
so action equals fasting
uh and then we have a little biscuits so
we can get the dog to perform various
actions by offering incentives such as a
dog biscuit as a reward
the dog will follow a policy to maximize
this reward and hence will follow every
command and might even learn new actions
like begging by itself so yeah you know
so we started off with fetching it goes
oh I get a biscuit for that it tries
something else you get a handshake or
begging or something like that and goes
oh this is also reward based and so it
kind of explores things to find out what
will bring it is biscuit
and that's very much like how reinforced
model goes is it looks for different
rewards how do I find can I try
different things and find a reward that
works
the dog also will want to run around and
play an explore its environment this
quality of model is called exploration
so there's a little Randomness going on
in Exploration
and explores new parts of the house
climbing on the sofa doesn't get a
reward in fact it usually gets kicked
off the sofa
so let's talk a little bit about
markov's decision process
markov's decision process
is a reinforcement learning policy used
to map a current state to an action
where the agent continuously interacts
with the environment to produce new
Solutions and receive rewards and you'll
see here's all of our different
vocabulary we just went over we have a
reward our state our agent our
environment interaction and so even
though the environment kind of contains
everything
that you really when you're actually
writing the program your environment's
going to put out a reward and state that
goes into the agent
the agent then looks at this state or it
looks at the reward usually um first and
it says okay I got rewarded for whatever
I just did or it didn't get rewarded and
then looks at the state and then it
comes back and if you remember from
policy the policy comes in and then we
have a reward the policy is that part
that's connected at the bottom
and so it looks at that policy and it
says hey what's a good action that will
probably be similar to what I did or
sometimes are completely random but
what's a good action that's going to
bring me a different reward
so taking the time to just understand
these different pieces as they go
is pretty important in most of the
models today and so a lot of them
actually have templates based on this
you can pull in and start using
pretty straightforward as far as once
you start seeing how it works you can
see your environment sends it says hey
this is what the agent did this if
you're a character in a game this
happened and it shoots out a reward in a
state the agent looks at the reward
looks at the new state and then takes a
little guess and says I'm going to try
this action and then that action goes
back into the environment it affects the
environment the environment then changes
depending on what the action was and
then it has a new state and a new reward
that goes back to the agent so in the
diagram Sean we need to find the
shortest path between node A and D each
path has a reward associated with it and
the path with a maximum reward is what
we want to choose
the node a b c d denote the nodes that
travel from node A to B is an action
reward is the cost of each path and
policy is each path taken
and you can see here a can go to B
or a can go to C right off the bat or it
can go right to D and if explored all
three of these you would find that a
going to D was a zero reward a going to
C and D would generate a different
reward
or you could go acbd there's a lot of
options here and so when we start
looking at this diagram you start to
realize
that even though today's reinforced
learning models do really good at
finding an answer they end up trying
almost all the different directions you
see
and so they take up a lot of work or a
lot of processing time for reinforcement
learning they're right now in their
infant stage and they're really good at
solving simple problems and we'll take a
look at one of those in just a minute in
a tic-tac-toe game but you can see here
once it's gone through these and it's
explored it's going to find the ACD is
the best reward it gets a full 30 points
for it
so let's go ahead and take a look at a
reinforcement learning
demo
in this demo we're going to use
reinforcement learning to make a tic tac
toe game you'll be playing this game
Against the Machine learning model
and we'll go ahead we're doing it in
Python so let's go ahead and go through
my always not always I actually have a
lot of python tools let's go through
Anaconda which will open up a Jupiter
notebook seems like a lot of steps but
it's worth it to keep all my stuff
separate and it's also has a nice
display when you're in the jupyter
notebook for doing python
so here's our Anaconda Navigator I open
up the notebook which is going to take
me to a web page and I've gone in here
and created a new python folder in this
case I've already done it and enabled it
to change the name to Tic-tac-toe and
then for this example we're going to go
ahead and
import a couple things we're going to
import numpy as NP we'll go ahead and
import pickle numpy of course is our
number array and then pickle is just a
nice way sometimes for
storing different information different
states that we're going to go through on
here
and so we're going to create a class
called State we're going to start with
that
and there's a lot of lines of code to
this class that we're going to put in
here don't let that scare you too much
there's not as much here it looks like
there's going to be a lie here but there
really is just a lot of setup going on
in the in our class State and so we have
up here we're going to initialize it
um
we have our board
it's a Tic Tac Toe board so we're only
dealing with nine spots on the board we
have player one player two
is end we'll create a board hash uh
we'll look at that in just a minute
we're just going to store some
information in there symbol of player
equals one
so there's a few things going on as far
as the initialization and then something
simple we're just going to get the hash
of the board we're going to get the
information from the board on there
which is columns and rows we want to
know when a winner occurs so if you get
three in a row that's what this whole
section here is for let me go ahead and
scroll up a little bit and you can get a
copy of this code if you send a note
over to Simply learn we'll send you over
this particular file and you can play
with it yourself and see how it's put
together I don't want to spend a huge
amount of time on this because this is
just some real General python coding
but you can see here we're just going
through all the rows and you add them
together and if it equals three three in
a row the same thing with columns
diagonal so you gotta check the diagonal
that's what all this stuff does here so
just goes through the different areas
actually let me go ahead and put
and then it comes down here and we do
our sum and it says true
minus three just says did somebody win
or is it a tie so you got to add up all
the numbers on there anyway just in case
they're all filled up
and next we also need to know available
positions these are ones that don't no
one's ever used before this way when you
try something or the computer tries
something it's not going to give it an
illegal move that's what the available
positions is doing then we want to
update our state
and so you have your position going in
we're just sending in the position that
you just chose and you'll see there's a
little user interface we put in there we
can pick the row and column in there
and again
I mean this is a lot of code so really
it's kind of a thing you'd want to go
through and play with a little bit and
just read through it get a copy of it a
great way to understand how this works
and here is a given reward so we're
going to give a reward result equals
self winner this is one of the hearts of
what's going on here is we have a result
self.winner
so if there's a winner then we have a
result if the result equals one here's
our feedback
uh if it doesn't equal one then it gets
a zero so it only gets a reward in this
particular case if it wins
and that's important to note because
different
systems of reinforced learning
do rewarding a lot differently depending
on what you're trying to do this is a
very simple example with a three by
three board imagine if you're playing a
video game certainly you only have so
many actions but your environment is
huge you have a lot going on in the
environment and suddenly a reward system
like this is going to be just is going
to have to change a little bit it's
going to have to have different rewards
and different setup and there's all
kinds of advanced ways to do that as far
as weighing you add weights to it and so
they can add the weights up depending on
where the reward comes in so it might be
that you actually get a reward in this
case you get the reward at the end of
the game
I'm spending just a little bit of time
on this because this is an important
thing to note but there's different ways
to add up those rewards it might have
like if you take a certain path the
first reward is going to be weighed a
little bit less than the last reward
because the last reward is actually
winning the game or scoring or whatever
it is so this reward system gets really
complicated on some of the more advanced
setups
um in this case though
you can see right here that they give a
0.1 and a 0.5 reward
um just for getting picking the right
value and something that's actually
valid instead of picking an invalid
value so rewards again that's a key it's
huge how do you feed the rewards back in
then we have a board reset that's pretty
straightforward it just goes back and
resets the board to the beginning
because it's going to try out all these
different things whilst learning it's
going to do it by trial and error so you
have to keep resetting it and then of
course there's the play we want to go
ahead and play rounds equals 100 depends
on what you want to do on here you can
set this different you obviously set
that to higher level but this is just
going to go through and you'll see in
here that we have player one and player
two
this isn't this is a computer playing
itself one of the more powerful ways to
learn to play a game or even learn
something that isn't a game is to have
two of these models
that are basically trying to beat each
other and so they always they keep
finding exploring new things this one
works for this one so this one tries new
things it beats this we've seen this in
chess I think with some big one where
they had the two players in chess with
reinforcement learning it was one of the
ways they trained one of the top
computer chess playing algorithms
uh so this is just what this is it's
going to choose an action it's going to
try something and the more it try stuff
um the more we're going to record the
hash we actually have a board hash where
the self get the hash set up on here
where it stores all the information
and then once you get to a win one of
them wins it gets the reward uh then we
go back and reset and try again and then
kind of the fun part we actually get
down here is uh we're going to play with
a human so we'll get a chance to come in
here and see what that looks like when
you put your own information in and then
it just comes in here and does the same
thing it did above it gives it a reward
for its things or sees if it wins or
ties
it looks at available positions all that
final fun stuff and then finally we want
to show the board so it's going to print
the board out each time
really as an integration is not that
exciting what's exciting in here is one
looking at this reward system whoops
Play One More up the reward system is
really the heart of this how do you
reward the different uh setup and the
other one is when it's playing it's got
to take an action
and so what it chooses for an action is
also the heart of reinforcement learning
how do we choose that action and those
are really key to right now where
reinforcement learning is
in
today's technology is figuring this out
how do we reward it and how do we guess
the next best action so we have our
environment and you can see the
environment is we're going to be or the
state which is kind of like what's going
on we're going to return the state
depending on what happens
and we want to go ahead and create our
agent in this case our player so each
one is let me go and grab that and so we
look at a class player
um
this is where a lot of the magic is
really going on is what how is this
player figuring out how to maneuver
around the board and then the board of
course returns a state that it can look
at and a reward uh so we want to take a
look at this we have a name self-state
this is class player and when you say
class player we're not talking about a
human player we're talking about just a
uh the computer players and this is kind
of interesting so remember I told you
depending on what you're doing there's
going to be a Decay gamma
um
explore rate
these are what I'm talking about is how
do we train it
um
as you try different moves it gets to
the end the first move is important but
it's not as important as the last one
and so you could say that the last one
has the heaviest weight and then as you
as you get there the first one let's see
the first move gives you a five reward
the second gives you a two reward and
the third one gives you a 10 reward
because that's the final ending you got
it the 10 is going to count more than
the first step and here's our we're
going to you know get the board
information coming in and then choose an
action this was the second part that I
was talking about that was so important
so once you have your training going on
we have to do a little Randomness and
you can see right here is our NP random
uniform so it's picking out a random
number
take a random action this is going to
just pick which row and which column it
is and so choosing the action this one
you can see we're just doing random
States Choice length of positions action
position
and then it skips in there and takes a
look at the board for p in positions and
you get it's actually storing the
different boards each time you go
through so it has a record of what it
did so it can properly weigh the values
and this simply just depends a hash
State what's the last date pinned it to
the uh to our states on here here's our
feedback
rewards the reward comes in and it's
going to take a look at this and say is
it none what is the reward and here is
that formula remember I was telling you
about up here
that was important because it has
Decay gamma times the reward
this is where
as it goes through each step and this is
really important this is this is kind of
the heart of this of what I was talking
about earlier you have step one
and this might have a reward of two you
have step two I should probably should
have done ABC this has a step three
step four and so on until you get to
step in
and this might have a reward of 10. so
wrote reward at 10
we're going to add that but we're not
adding let's say this one right here
let's see this reward here before 10 was
um let's say it's also 10. that just
makes the the
math easy so we had 10 and 10. we had 10
this is 10 and 10 n whatever it is but
it's time it's 0.9 so instead of putting
a full 10 here
we only do 9. that's a 0.9 times
10.
and so this formula as far as the Decay
times the reward minus the cell State
value
it basically adds in it says here's one
or here's two I'm sorry I should have
done this a b c would have been easier
so the first move goes in here and it
puts 2 in here
uh then we have ourself
setup on here you can see how this gets
pretty complicated in the math but this
is really the key is how do we train
our states and we want the the final
State the when to get the most points if
you win you get most points and the
first step gets the least amount of
points
so you're really training this almost in
Reverse you're retraining you're
training it from the last place where
you have like it says okay this is now I
need to sum up my rewards and I want to
sum them up going in reverse and I want
to find the answer in Reverse kind of an
interesting uh play on the mind when
you're trying to figure this stuff out
and of course we want to go ahead and
reset the board down here and save the
policy load policy
these are the different things that are
going in between the agent and the state
to figure out what's going on let's go
ahead and load that up and then finally
we want to go ahead and create a human
player
and the human player is going to be a
little different
in that you choose an action row and
column here's your action if action is
if action in positions meaning positions
that are available
you return the action
if not it just keeps asking you until
you get the action that actually works
and then we're going to go ahead and
append to the hash state which we don't
need to worry about because it Returns
the action up here
and feed forward
again this is because it's a human
at the end of the game bat propagate and
update State values this part isn't
being done because it's not programming
uh
the model the models is getting its own
Rewards
so we've gone ahead and loaded this in
here so here's all our pieces and the
first thing we want to do
is set up a P1 player one P2 player two
and then we're going to send our players
to our state so now it has P1 P2 and
it's going to play and it's going to
play 50 000 rounds now we can probably
do a lot less than this and it's not
going to get the full results in fact
you know what uh let's go ahead and just
do five just to play with it because I
want to show you something here
oops somewhere in there I forgot to load
something
to run this run
it's forgot a reference there for the
board rows and columns three by three
there's actually in the state it
references that we just tack it on on
the end it was supposed to be at the
beginning
so now I've only set this up with um
see where we go in here I've only set
this up to train
five times
and the reason I did that is we're going
to uh
come in and actually play it and then
I'm going to change that and we can see
how it differs on there
there we go and did you make it through
a run and we're going to go ahead and
save the policy
so now we have our player one and our
player two policy the way we set it up
it has two separate policies loaded up
in there
and then we're going to come in here and
we're going to do player one is going to
be the computer experience rate zero
load policy one human player human and
we're going to go ahead and play this
now remember I only went through it um
just one round of training in fact
minimal training and so it puts an X
there and I'm going to go ahead and do
row 0 column one
you can see this is very uh basic on
here and so I put in my zero and then
I'm going to go zero block it zero zero
and you can see right here let me win uh
just like that I was able to win zero
two and woo human wins
so I only trained it five times we're
going to run this again and this time
instead of five let's do five thousand
or fifty thousand I think that's what
the guys in the back had
and this takes a while to train it
this is where reinforcement learning
really falls apart look how simple this
game is we're talking about a three by
three set of columns
and so for me to train it on this I
could do a q table which would take
which would go much quicker you could
build a quick Q table with almost all
the different options on there and uh
you would probably get a the same result
much quicker we're just using this as an
example so when we look at reinforcement
learning you need to be very careful
what you apply it to it sounds like a
good deal until you do like a large
neural network where you're doing you
set the neural network to a learning
increment of one so every time it goes
through it learns
and then you do your actions you pick
from the learning setup and you actually
try actions on the learning setup until
you get the what you think is going to
be the best action
so you actually feed what you think is
right back through the neural network
there's a whole layer there which is
really fun to play with
and then it has an output well think of
all those processes I mean that is just
a huge amount of work it's going to do
uh let's go ahead and Skip ahead here
and give it a moment it's going to take
a a minute or two to go ahead and run
now to train it we went ahead and let it
run and it took a while this this took
um I got a pretty powerful processor and
it took about five minutes plus to run
it
and we'll go ahead and uh
run our player setup on here oops it
brought in the last whoops I brought in
the last round so give me just a moment
to re do the policy save there we go I
forgot to save the policy back in there
and then go ahead and run our player
again
so we've saved the policy and then we
want to go ahead and load the policy for
P1 as a computer
and we can see the computer's gone in
the bottom right corner I'm going to go
ahead and go 1 1 which is the center
and it's gone right up the top and if
you have ever played tic-tac-toe you
know the computer has me but we'll go
ahead and play it out row zero
column two
there it is and then it's gone here and
so I'm going to go ahead and go row 0 1
2 no 0 1 there we go and column zero
that's where I wanted
oh and it says okay you your action
there we go boom uh so you can see here
we've got a didn't catch the win on this
it said Ty
um kind of funny they didn't catch the
win on there
but if we play this a bunch of times
you'll find it's going to win more and
more the more we train it the more the
reinforcement happens
this lengthy training process is really
the stopper on reinforcement learning as
this changes reinforcement learning will
be one of the more powerful packages
evolving over the next decade or two in
fact I would even go as far as to say it
is the most important machine learning
tool in artificial intelligence tool out
there as it learns not only a simple
tic-tac-toe board but we start learning
environments and the environment would
be like in language if you're
translating a language or something from
one language to the other so much of it
is lost if you don't know the context
it's in what's the environments it's in
and so being able to attach environment
and context and all those things
together is going to require
reinforcement learning to do
so again if you want to get a copy of
the Tic Tac Toe board it's kind of fun
to play with run it you can test it out
you can do you know test it for
different uh values you can switch from
P1 computer
where we loaded the policy one to load
the policy two and just see how it
varies there's all kinds of things you
can do on there hello and welcome to
this tutorial on deep learning my name
is Mohan and in the next book one right
now cars I will take you through what is
deep learning and into tensorflow
environment to show you an example of
deep learning now there are several
applications of deep learning really
very interesting and Innovative
applications and one of them is
identifying the geographic location
based on a picture and how does this
work the way it works is pretty much the
train an artificial neural network with
millions of images which are tagged
their geolocation is tagged and then
when we feed a new picture it will be
able to identify the geolocation of this
new image for example you have all these
images especially with maybe some
significant monuments or
significant locations and you train with
millions of such images and then when
you feed another image it need not be
exactly one of those that you have
claimed it can be completely different
that is the whole idea of cleaning it
will be able to recognize for example
that this is a picture from Paris
because it is able to recognize the
Eiffel Tower so the way it works
internally if we have to look a little
bit under the hood as these images are
nothing but this is digital information
in the form of pixels so each image
could be a certain size it can be 256 by
256 pixel kind of a resolution and then
each pixel is either having a certain
grade of color and all that is fed into
the neural network and it then gets
trained in and it's able to based on
these pixels pixel information it is
able to get trained and able to
recognize the features and extract the
features and thereby it is able to
identify these images and the location
of these images and then when you feed a
new image it kind of based on the
training it will be able to figure out
where this image is from so that's the
way a little bit under the hood how it
works so what are we going to do in this
tutorial we will see what is deep
learning and what do we need for deep
learning and one of the main components
of deep learning is neural networks we
will see what is neural network what is
a perceptron and how to implement logic
gates like and or not and so on using
perceptrons the different types of
neural networks and then applications of
deep learning and we will also see how
neural networks works so how do we do
the training of neural networks
and at the end we will end up with a
small demo code which will take you
through in tensorflow now in order to
implement deep learning code there are
multiple libraries or developing
environments that are available and
tensorflow is one of them so the focus
at the end of this would be on how to
use tensorflow to write a piece of code
using python as a programming language
and we will take up an example which is
a very common one which is like the
hello world of deep learning the
handwriting number recognition which is
a mnist commonly known as MNS database
so we will take a look at MNS database
and how we can train a neural network to
recognize handwritten numbers so that's
what you will see in this particular
video so let's get started what is deep
learning deep learning is like a subset
of what is known as a high level concept
called artificial intelligence you must
be already familiar must have heard
about this term artificial intelligence
so artificial intelligence is like the
high level concept if you will and in
order to implement artificial
intelligence applications we use what is
known as machine learning and within
machine learning a subset of machine
learning is deep learning machine
learning is a little bit more generic
concept and deep learning is one type of
machine learning if you will and we will
see a little later in maybe the
following slides a little bit more in
detail how deep learning is different
from traditional machine learning but to
start with we can mention here that deep
learning uses one of the differentiators
between deep learning and traditional
machine learning is that deep learning
uses neural networks and we will talk
about what are neural networks and how
we can Implement neural networks and so
on and so forth as a part of this
tutorial it's a little deeper into deep
learning deep learning primarily
involves working with complicated
unstructured data compared to
traditional machine learning where we
normally use structured data in deep
learning the data would be primarily
images or Voice or maybe text file so
and it is large amount of data as well
and deep learning can handle complex
operations it involves complex
operations and the other difference
between traditional machine learning and
deep learning is that the feature
extraction happens pretty much
automatically in traditional machine
learning feature engineering is done
manually the data scientists we data
scientists have to do feature
engineering feature extraction but in
deep learning that happens automatically
and of course deep learning for large
amounts of data complicated unstructured
data deep learning gives very good
performance now as I mentioned one of
the secret
sources of deep learning is neural
networks let's see what neural networks
is neural networks is based on our
biological neurons the whole concept of
deep learning and artificial
intelligence is based on human brain and
human brain consists of billions of tiny
stuff called neurons and this is how a
biological neuron looks and this is how
an artificial neuron look so neural
networks is like a simulation of our
human brain human brain has billions of
biological neurons and we are trying to
simulate the human brain using
artificial neurons this is how a
biological neuron looks it has dendrites
and the corresponding component with an
artificial neural network is or an
artificial neuron are the inputs they
receive the input through dendrites and
then there is the cell nucleus which is
basically the processing unit in a way
so in artificial neuron also there is a
piece which is an equivalent of this
cell nucleus and based on the weights
and biases we will see what exactly
weights and biases are as we move the
input gets processed and that results in
an output in a biological neuron the
output is sent through a synapse and in
an artificial neuron there is an
equivalent of that in the form of an
output and biological neurons are also
interconnected so there are billions of
neurons which are interconnected in the
same way artificial neurons are also
interconnected so this output of this
neuron will be fed as an input to
another neuron and so on now in neural
network one of the very basic units is a
perceptron so what is the perceptron a
perceptron can be considered as one of
the fundamental units of neural networks
it can consist at least one neuron but
sometimes it can be more than one neuron
but you can create a perceptron with a
single neuron and it can be used to
perform certain functions it can be used
as a basic binary classifier it can be
trained to do some basic binary
classification and this is how a basic
perceptron looks like and this is
nothing but a neuron you have inputs X1
X2 X 2 X N and there is a summation
function and then there is what is known
as an activation function and based on
this input what is known as the weighted
sum the activation function either gets
gives an output like a zero or a one so
we say the neuron is either activated or
not so that's the way it works so you
get the inputs these inputs are each of
the inputs are multiplied by a weight
and there is a bias that gets added and
that whole thing is fed to an activation
function and then that results in an
output and if the output is correct it
is accepted it is wrong if there is an
error then that error is fed back and
the neuron then adjusts the weights and
biases to give a new output and so on
and so forth so that's what is known as
the training process of a neuron or a
neural network there's a concept called
perceptron learning so perceptron
learning is again one of the very basic
learning processes the way it works is
somewhat like this so you have all these
inputs like X1 to xn and each of these
inputs is multiplied by a weight and
then that sum this is the formula or the
equation so that's some w i x i Sigma of
that which is the sum of all these
product of X and W is added up and then
a bias is added to that the bias is not
dependent on the input but or the input
values but the bias is common for one
neuron however the bias value keeps
changing during the training process
once the training is completed the
values of these weights W1 W2 and so on
and the value of the bias gets fixed so
that is basically the whole training
process and that is what is known as the
perceptron training so the weights and
biases keep changing till you get the
accurate output and the summation is of
course passed through the activation
function as you see here this wixi
summation plus b is passed through
activation function and then the neuron
gets either fired or not and based on
that there will be an output that output
is compared with the actual or expected
value which is also known as labeled
information so this is the process of
supervised learning so the output is
already known and that is come compared
and thereby we know if there is an error
or not and if there is an error the
error is fed back and the weights and
biases are updated accordingly till the
error is reduced to the minimum so this
iterative process is known as perceptron
learning or perceptron learning Rule and
this error needs to be minimized so till
the error is minimized this iteratively
the weights and biases keep changing and
that is what is the cleaning process so
the whole idea is to update the weights
and the bias of the perceptron till the
error is minimized the error need not be
zero error may not ever reach 0 but the
idea is to keep changing these weights
and bias so that the error is minimum
the minimum possible that it can have so
this whole process is an iterative
process and this is the iteration
continuous still either the error is 0
which is unlikely situation or it is the
minimum possible Within These given
conditions now in 1943 two scientists
Warren McCullough and Walter Pitts came
up with an experiment where they were
able to implement the logical functions
like and or and nor using neurons and
that was a significant breakthrough in a
sense so they were able to come up with
the most common logical Gates they were
able to implement some of the most
common logical Gates which could take
two inputs Like A and B and then give a
corresponding result so for example in
case of an and gate A and B and then the
output is a b in case of an or gate it
is a plus b and so on and so forth and
they were able to do this using a single
layer perceptron now now most of these
Gates it was possible to use single
layer perceptron except for XR and we
will see why that is in a little bit so
this is how an and gate works the inputs
A and B the output should be fired or
the neuron should be fired only when
both the inputs are one so if you have
zero zero the output should be 0 for 0 1
it is again 0 1 0 again 0 and 1 1 the
output should be one so how do we
implement this with a neuron so it was
found that by changing the values of
Weights it is possible to achieve this
logic so for example if we have equal
weights like 0.7 0.7 and then if we take
the sum of the weighted products so for
example 0.7 into 0 and then 0.7 into 0
will give you 0 and so on and so forth
and in the last case when both the
inputs are 1 you get a value which is
greater than one which is the threshold
so only in this case the neuron gets
activated and the output is there is an
output in all the other cases there is
no output because the threshold value is
one so this is implementation of an and
gate using a single perceptron or a
single neuron similarly an or gate in
order to implement an or gate in case of
an or gate the output will be 1 if
either of these inputs is one so for
example 0 1 will result in 1 or rather
in all the cases it is 1 except for 0 0.
so how do we implement this using a
perceptron once again if you have a
perceptron with weights for example 1.2
now if you see here if in the first case
when both are 0 the output is 0 in the
second case when it is 0 and 1 1.2 into
0 is 0 and then 1.2 into 1 is 1 and in
the second case similarly the output is
1.2 in this last case when both the
inputs are 1 the output is 2.4 so during
the training process these weights will
keep changing and then at one point
where the weights are equal to W1 is
equal to 1.2 and W2 is equal to 1.2 the
system learns that it gives the correct
output so that is implementation of or
gate using a single neuron or a single
layer perceptra now xor gate this was
one of the challenging ones they tried
to implement an xor gate with a single
level perceptron but it was not possible
and therefore in order to implement an
xor so this was like a a roadblock in
the progress of neural network however
subsequently they realize that this can
be implemented and xor gate can be
implemented using a multi-level
perceptron or MLP so in this case there
are two layers instead of a single layer
and this is how you can Implement an xor
gate so you will see that X1 and X2 are
the inputs and there is a hidden layer
and that's why it is denoted as H3 and
H4 4 and then you take the output of
that and feed it to the output at o5 and
provide a threshold here so we will see
here that this is the numerical
calculation so the weights are in this
case for X1 it is 20 and minus 20 and
once again 20 n minus 20. so these
inputs are fed into H3 and H4 so you'll
see here for H3 the input is 0 1 1 1 and
for H4 it is 1 0 1 1 and if you now look
at the output final output where the
threshold is taken as 1 if we use a
sigmoid with the threshold one you will
see that in these two cases it is 0 and
in the last two cases it is one so this
is a implementation of xor in case of x
r only when one of the inputs is 1 you
will get an output so that is what we
are seeing here if we have either both
the inputs are y one or both the inputs
are 0 then the output should be zero so
that is what is an exclusive or gate so
it is exclusive because only one of the
inputs should be one and then only you
will get an output of 1 which is
Satisfied by this condition so this is a
special implementation xor gate is a
special implementation of perceptron now
that we got a good idea about perceptron
let's take a look at what is the neural
network so we have seen what is a
perceptron we have seen what is a neuron
so we will see what exactly is a neural
network so neural network is nothing but
a network of these neurons and they are
different types of neural networks there
are about five of them these are
artificial neural network convolutional
neural network then recursive neural
network or recurrent neural network deep
neural network and deep belief Network
so and each of these types of neural
networks have a special you know they
can solve a special kind of problems for
example can only additional neural
networks are very good at performing
image processing and image recognition
and so on whereas R and N are very good
for speech recognition and also text
analysis and so on so each type has some
special characteristics and they can
they are good at performing certain
special kind of tasks what are some of
the applications of deep learning deep
learning is today used extensively in
gaming you must have heard about alphago
which is a game created by a startup
called deepmind which got acquired by
Google and alphago is an AI which
defeated the human world champion lease
it all in this game of Go so gaming is
an area where deep learning is being
extensively used and a lot of research
happens in the area of gaming as well in
addition to that nowadays there are
neural networks or special type by
generative adversarial networks which
can be used for synthesizing either
images or music or text and so on and
they can be used to compose music so the
neural network can be trained to compose
a certain kind of music and autonomous
cars you must be familiar with Google
Google's self-driving car and today a
lot of Automotive companies are
investing in this space and deep
learning is a core component of this
autonomous cars the cars are trained to
recognize for example the road the lane
markings on the road signals any objects
that are in front any obstruction and so
on and so forth so all this involves
deep learning so that's another major
application and robots we have seen
several robots including Sofia you may
be familiar with Sophia who was given a
citizenship by Saudi the Arabia and
there are several such robots which are
very human-like and the underlying
technology in many of these robots is
deep learning medical Diagnostics and
Healthcare is another major area of a
deep learning is being used and within
Healthcare Diagnostics again there are
multiple areas where deep learning and
image recognition image processing can
be used for example for cancer detection
as you may be aware if cancer is
detected early on it can be cured and
one of the challenges is in the
availability of Specialists who can
diagnose cancer using these diagnostic
images and various scans and and so on
and so forth so the idea is to train
neural network to perform some of these
activities so that the load on the
cancer specialist doctors or oncologists
comes down and there is a lot of
research happening here and and there
are already quite a few applications
that are claimed to be performing better
than human beings in this space can be
lung cancer it can be breast cancer and
so on and so forth so Healthcare is a
major area where deep learning is being
applied let's take a look at the inner
working of a neural network so how does
an artificial neural network let's say
identify can we train a neural network
to identify the shapes like squares and
circles and triangles when these images
are fed so this is how it works any
image is nothing but it is a digital
information of the pixels so in this
particular case let's say this is an
image of 28 by 28 pixel and this is an
image of a square there is a certain way
in which the pixels are lit up and so
these pixels have a certain value maybe
from 0 to 256 and 0 indicates that it is
is black orbs it is dark and 256
indicates it is completely it is white
or litter so that is like an indication
or a measure of the how the pixels are
lit up and so this is an image is let's
say consisting of information of 784
pixels so all the information what is
inside this image can be kind of
compressed into this 784 pixels the way
each of these pixels is lit up provides
information about what exactly is the
image so we can train neural networks to
use that information and identify the
images so let's take a look how this
works so each neuron the value if it is
close to 1 that means it is white
whereas if it is close to 0 that means
it is black now this is a an animation
of how the source building works so
these pixels one of the ways of doing it
is we can flatten this image and take
this complete 784 pixels and feed that
as input to our neural network the
neural network can consist of probably
several layers there can be a few hidden
layers and then there is an input layer
and an output layer now the input layer
take the 784 pixels as input the values
of each of these pixels and then you get
an output which can be of three types or
three classes one can be a square a
circle or a triangle now during the
training process there will be initially
obviously you feed this image and it
will probably say it's a circle or it
will say it's a triangle so as a part of
the training process we then send that
error back and the weights and the
biases of these neurons are adjusted
till it correctly identifies that this
is a square that is the whole training
mechanism that happens out here
now let's take a look at a circle same
way so you feed these 784 pixels there
is a certain pattern in which the pixels
are lit up and the neural network is
trained to identify that pattern
and during the training process once
again it would probably initially
identify it incorrectly saying this is a
square or a triangle and then that error
is fed back and the weights and biases
are adjusted finally till it finally
gets the image correct
so that is the training process so now
we will take a look at
same way a triangle
so now if you feed another image which
is consisting of triangle so this is the
training process now we have trained our
neural network to classify these images
into a triangle or a circle and a square
so now this neural network can identify
these three types of objects now if you
feed another image and it will be able
to identify whether it's a square or a
triangle or a circle now what is
important to be observed is that when
you feed a new image it is not necessary
that the image or the the triangle is
exactly in this position now the neural
network actually
identifies the patterns so even if the
triangle is let's say positioned here
not exactly in the middle but maybe at
the corner or in the side it would still
identify that it is a triangle and that
is the whole idea behind pattern
recognition so how does this training
process sources of work this is a quick
view of how the training process works
so we have seen that a neuron consists
of inputs it receives inputs and then
there is a weighted sum which is nothing
but this x i w i summation of that plus
the bias and this is then fed to the
activation function and that in turn
gives us a output now during the
training process initially obviously
when you feed these images when you send
maybe a square it will identify it as a
triangle and when you maybe feed a
triangle it will identify as a square
and so on so that error information is
fed back and initially these weights can
be random maybe all of them have zero
values and then it will slowly keep
changing so the as a part of the
training process the values of these
weights W1 W2 up to WN keep changing in
such a way that towards the end of the
training process it should be able to
identify these images correctly so till
then the weights are adjusted and that
is known as the training process so and
these weights are numeric values could
point five point two five point three
five and so on it could be positive or
it could be negative and the value that
is coming here is the pixel value as we
have seen it can be anything between 0
to 1 you can scale it between 0 to 1 or
0 to 256 whichever way 0 being black and
256 being white and then all the other
colors in between so that is the input
so these are numerical values this
multiplication or the product w i x i is
a numerical value and the bias is also
numerical value we need to keep in mind
that the bias is fixed for a neuron it
doesn't change with the inputs whereas
the waves are one per input so that is
one important point to be noted so but
the bias also keeps changing initially
it will again have a random value but as
a part of the training process the
weights the values of the weights W1
w2wn and the value of B will change and
ultimately once the training process is
complete these values are fixed for for
this particular neuron W1 W2 up to WN
and plus the value of the B is also
fixed for this particular neuron and in
this way there will be multiple neurons
and each there may be multiple levels of
neurons here and that's the way the
training process works so this is
another example of multi-layer so there
are two hidden layers in between and
then you have the input layer values
coming from the input layer then it goes
through multiple layers hidden layers
and then there is an output layer and as
you can see there are weights and biases
for each of these neurons in each layer
and all of them gets keeps changing
during the training process and at the
end of the training process all these
weights have a certain value and that is
a trained model and those values will be
fixed once the training is completed all
right then there is something known as
activation function neural networks
consists of one of the components in
neural networks is activation function
and every neuron has an activation
function and there are different types
of activation functions that are used it
could be a relu it could be sigmoid and
so on and so forth and the activation
function is what decides whether a
neuron should be fired or not so whether
the output should be 0 or 1 is decided
by the activation function and the
activation function in turn takes the
input which is the weighted sum remember
we talked about w i x i plus b that
weighted sum is fed as a input to the
activation function and then the output
can be either a zero or a one and there
are different types of activation
functions which are covered in an
earlier video you might want to watch
all right so as a part of the training
process we feed the inputs the label
data or the training data and then it
gives an output which is the the
predicted output by the network which we
indicate as y hat and then there is a
labeled data because we for supervised
learning we already know what should be
the output so that is the actual output
and in the initial process before the
training is complete obviously there
will be error so that is measured by
what is known as a cost function so the
difference between the predicted output
and the actual output is the error and
the cost function can be defined in
different ways there are different types
of cost functions so in this case it is
like the average of the squares of the
error so and then all the errors are
added which can sometimes be called as
sum of squares sum of square errors or
SSC and that is then fed as a feedback
in what is known as backward propagation
or back propagation and that helps in
the network adjusting the weights and
biases and so the weights and biases get
updated till this value the error value
or the cost function is minimum now
there is a optimization technique which
is used here called gradient descent
optimization and this algorithm Works in
a way that the error which is the cost
function needs to be minimized so
there's a lot of mathematics that goes
behind us for example they find the
local minimum the global Minima using
the differentiation and so on and so
forth but the idea is this so as a
training process as the as the part of
training the whole idea is to bring down
the error which is like let's say this
is the function the cost function at
certain levels it is very high the cost
value of the cost function the output of
the cost function is very high so the
weights have to be adjusted in such a
way and also the bias of course that the
cost function is minimized so there is
this optimization technique called
gradient descent that is used and this
is known as the learning rate now
gradient descent you need to specify
what should be the learning rate and the
learning rate should be optimal because
if you have a very high learning rate
then the optimization will not converge
because at some point it will cross over
to the side on the other hand if you
have very low learning rate then it
might take forever to convert so you
need to come up with the optimum value
of the learning rate and once that is
done using the gradient descent
optimization the error function is
reduced and that's like the end of the
training process all right so this is
another view of gradient descent so this
is how it looks this is your cost
function the output of the cost function
and that has to be minimized using
gradient descent algorithm and these are
like the parameters and weight could be
one of them so initially we start with
certain random values so cost will be
high and then the weights keep changing
and in such a way that the cost function
needs to come down and at some point it
may reach the minimum value and then it
may increase so that is where the
gradient descent algorithm decides that
okay it has reached the minimum value
and it will kind of try to stay here
this is known as the global Minima now
sometimes these curves may not be just
for explanation purpose this has been
drawn in a nice way but sometimes these
curves can be pretty erratic there can
be some local Minima here and then there
is a peak and then and so on so the
whole idea of gradient descent
optimization is to identify the global
Minima and to find the weights and the
bias at that particular point so that's
what is gradient descent and then this
is another example so you can have these
multiple local Minima so as you can see
at this point when it is coming down it
may appear like this is a minimum value
but then it is not this is actually the
global minimum value and the gradient is
an algorithm will make an effort to
reach this level and not get stuck at
this point so the algorithm is already
there and it knows how to identify this
Global minimum and that's what it does
during the training process now in order
to implement deep learning there are
multiple platforms and languages that
are available but the most common
platform nowadays is tensorflow and so
that's the reason we have this tutorial
we've created this tutorial for
tensorflow so we will take you through a
quick demo of how to write a tensorflow
code using Python and tensorflow is an
open source platform created by Google
so let's just take a look at the details
of tensorflow and so this is a a library
a python Library so you can use python
or any other languages it's also
supported in other languages like Java
and R and so on but python is the most
common language that is used so it is a
library for developing deep learning
applications especially using neural
networks and it consists of primarily
two parts if you will so one is the
tensors and then the other is the graphs
or the flow that's the way the name
that's the reason for this kind of a
name called tensorflow so what are
tensor sensors are like
multi-dimensional arrays if you will
that's one way of looking at it so
usually you have a one-dimensional array
so first of all you can have what is
known as a scalar which means a number
and then you have a one-dimensional
array something like this which means
this is like a set of numbers so that is
a one-dimension array then you can have
a two dimensional array which is like a
matrix and beyond that sometimes it gets
difficult so this is a three-dimensional
array but tensorflow can handle many
more Dimensions so it can have
multi-dimensional arrays that is the
strength of tensorflow and which makes
computation deep learning computation
much faster and that's the reason why
tensorflow is used for developing deep
learning applications so tensorflow is a
deep learning tool and this is the way
it works so the data basically flows in
the form of tensors and the way the
programming works as well is that you
first create a graph of how to execute
it and then you actually execute that
particular graph in the form of what is
known as a session we will see this in
the tensorflow code as we move forward
so all the data is managed or
manipulated in tensors and then the
processing happens using this graphs
there are certain terms called like for
example ranks of a 10 answer the rank of
a tensor is like a dimensional
dimensionality in a way so for example
if it is scalar so there is just a
number just a one number the rank is
supposed to be 0 and then it can be a
one dimensional Vector in which case the
rank is supposed to be 1 and then you
can have a two dimensional Vector
typically like a matrix then in that
case we say the rank is 2 and then if it
is a three-dimensional array then it
rank is three and so on so it can have
more than three as well so it is
possible that you can store
multi-dimensional arrays in the form of
tensors so what are some of the
properties of tensorflow I think today
it is one of the most popular platform
tensorflow is the most popular deep
learning platform or Library it is open
source it's developed by Google
developed and maintained by Google but
it is open source one of the most
important things about tensorflow is
that it can run on CPUs as well as gpus
GPU is a graphical Processing Unit just
like CPU Central Processing Unit now in
earlier days GPU was used for primarily
for graphics and that's how the name has
come and one of the reasons is that it
cannot perform generic activities very
efficiently like CPU but it can perform
iterative actions or computations
extremely fast and much faster than a
CPU so they are really good for
computational activities and in deep
learning there is a lot of iterative
computation that happens so in the form
of matrix multiplication and so on so
gpus are very well suited for this kind
of computation and tensorflow supports
both GPU as well as CPU and there's a
certain way of writing code in
tensorflow we will see as we go into the
code and of course tensorflow can be
used for traditional machine learning as
well but then that would be an Overkill
but just for understanding it may be a
good idea to start writing code for a
normal machine learning use case so that
you get a hang of how tensorflow code
works and then you can move into neural
networks so that is just a suggestion
but if you're already familiar with how
tensorflow works then probably yeah you
can go straight into the neural network
spot so in this tutorial we will take
the use case of recognizing handwritten
digits this is like a hello world of
deep learning and this is a nice little
MNS database is a nice little database
that has images of handwritten digits
nicely formatted because very often in
deep learning and neural networks we end
up spending a lot of time in preparing
the data for training and with MNS
database we can avoid that you already
have the data in the right format which
can be directly used for training and
mnist also offers a bunch of built-in
utility functions that we can straight
away use and call those functions
without worrying about writing our own
functions and that's one of the reasons
why MNS database is very popular for
training purposes initially when people
want to learn about deep learning and
tensorflow this is the database that is
used and it has a collection of 70 000
handwritten digits and a large part of
them are for training then you have
tests just like in any machine learning
process and then you have validation and
all of them are labeled so you have the
images and their label and these images
they look somewhat like this so they are
handwritten images collected from a lot
of individuals people have these are
samples written by human beings they
have handwritten these numbers these
numbers going from zero to nine so
people have written these numbers and
then the images of those have been taken
and formatted in such a way that it is
very easy to handle so that is MNS
database and the way we are going to
implement this in our tensorflow is we
will feed this data especially the
training data along with the label
information and the data is basically
these images are stored in the form of
the pixel information as we have seen in
one of the previous slides all the
images are nothing but these are pixels
so an image is nothing but an
arrangement of pixels and the value of
the pixel either it is lit up or it is
not or in somewhere in between that's
how the images are stored and that is
how they are fed into the neural network
and for training once the network is
trained when you provide a new image it
will be able to identify within a
certain error of course and for this we
will use one of the simpler neural
network configurations called softmax
and for Simplicity what we will do is we
will flatten these pixels so instead of
taking them in a two-dimensional
arrangement we just flatten them out so
for example it starts from here it's a
28 by 28 so there are 7 for 84 pixels so
pixel number one starts here it goes all
the way up to 28 then 29 starts here and
goes up to 56 and so on and the pixel
number 784 is here so we take all these
pixels flatten them out and feed them
like one single line into our neural
network and this is a what is known as a
soft Max layer what it does is once it
is trained it will be able to identify
what digit this is so there are in this
output layer there are 10 neurons each
signifying a digit and at any given
point of time when you feed an image
only one of these 10 neurons gets
activated
so for example if this is strained
properly and if you feed a number nine
like this then this particular neuron
gets activated so you get an output from
this neuron let me just use a pen or a
laser to show you here okay so you're
feeding a number nine let's say this has
been trained and now if you're feeding a
number nine this will get activated now
let's say you feed one to the trained
Network then this neuron will get
activated if you feed two this neuron
will get activated and so on I hope you
get the idea so this is one type of a
neural network or an activation function
known as soft Max layer so that's what
we will be using here this is one of the
simpler ones for quick and easy
understanding so this is how the code
would look we will go into our lab
environment in the cloud and we will
show you there directly but very quickly
this is how the code looks and let me
run you through briefly here and then we
will go into the Jupiter notebook where
the actual code is and we will run that
as well so as a first step first of all
we are using python here and that's why
the syntax of the language is Python and
the first step is to import the
tensorflow library so and we do this by
using this line of code saying import
tensorflow as TF TF is just for
convenience so you can name give any
name and once you do this TF is
tensorflow is available as an object in
the name of TF and then you can run its
methods and accesses its attributes and
so on and so forth and mnis database is
actually an integral part of tensorflow
and that's again another reason why we
as a first step we always use this
example mnist database example so you
just simply import mnist database as
well using this line of code and and you
slightly modify this so that the labels
are in this format what is known as one
hot true which means that the label
information is stored like an array and
let me just use pen to show what exactly
it is so when you do this one hot true
what happens is each label is stored in
the form of an array of 10 digits and
let's say the number is 8 okay so in
this case all the remaining values there
will be a bunch of zeros so this is like
array at position zero this is at
position one position two and so on and
so forth let's say this is position
seven then this is position 8 that will
be 1 because our input is 8 and again
position 9 will be zero okay so one hot
encoding this one hot encoding true will
kind of load the data and in such a way
that the labels are in such a way that
only one of the digits has a value of
one and that indicates So based on which
digit is one we know what is the label
so in this case the eighth position is
one therefore we know this sample data
the value is eight similarly if you have
a 2 here let's say then the labeled
information will be somewhat like this
so you have your labels so you have this
as 0 the 0th position the first position
is also zero the second position is one
because this indicates number two and
then you have third as zero and so on
okay so that is the significance of this
one hot true all right and then we can
check how the data is looking by
displaying the the data and as I
mentioned earlier this is pretty much in
the form of digital form like numbers so
all these are like pixel values so you
will not release see an image in this
format but there is a way to visualize
that image I will show you in a bit and
this tells you how many images are there
in each set so the training there are 55
000 images in training and in the test
set there are 10 000 and then validation
there are 5000. so all together there
are 70 000 images all right so let's
move on and we can view the actual image
by using the matplot flip library and
this is how you can view this is the
code for viewing the images and you can
view them in color or you can view them
in grayscale so the cmap is what tells
in what way we want to view it and what
are the maximum values and the minimum
values of the pixel value so these are
the Max and minimum value so of the
pixel values so maximum is one because
this is a scaled value so 1 means it is
white and 0 means it is black and in
between is it can be anywhere in between
black and white and the way to train the
model that is a certain way in which you
write your tensorflow code and the first
step is to create some placeholders and
then you create a model in this case we
will use the softmax model one of the
simplest ones and placeholders are
primarily to get the data from outside
into the neural network so this is a
very common mechanism that is used and
then of course you will have variables
which are your remember these are your
weights and biases so for in our case
there are 10 neurons and each neuron
actually has
784 because each neuron takes all the
inputs if we go back to our slide here
actually every neuron takes all the 784
inputs right this is the first neuron it
has it receives all the 784 this is the
second neuron this also receives all the
700 so each of these inputs needs to be
multiplied with the weight and that's
what we are talking about here so these
are this is a a matrix of
784 values for each of the neurons and
so it is like a 10 by 784 Matrix because
there are 10 neurons and similarly there
are biases now remember I mentioned
biases only one per neuron so it is not
one per input unlike the weights so
therefore there are only 10 biases
because there are only 10 neurons in
this case so that is what we are
creating a variable for biases so this
is something little new in tensorflow
you will see unlike our regular
programming languages where everything
is a variable here the variables can be
of three different types you have
placeholders which are primarily used
for feeding data you have variables
which can change during the course of
computation and then a third type which
is not shown here are constants so these
are like fixed numbers all right so in a
regular programming language you may
have everything has variables or at the
most variables and constants but in
tensorflow you have three different
types placeholders variables and
constants and then you create what is
known as a graph so tensorflow
programming consists of graphs and
tensors as I mentioned earlier so this
can be considered ultimately as a tensor
and then the graph tells how to execute
the whole implementation so that the
execution is stored in the form of for
graph and in this case what we are doing
is we are doing a multiplication TF you
remember this TF was created as a
tensorflow object here one more level
one more so TF is available here now
tensorflow has what is known as a matrix
multiplication or maximal function so
that is what is being used here in this
case so we are using the matrix
multiplication of tensorflow so that you
multiply your input values x with W
right this is what we were doing x w
plus b you're just adding B and this is
in very similar to one of the earlier
slides where we saw Sigma x i w i so
that's what we are doing here matrix
multiplication is multiplying all the
input values with the corresponding
weights and then adding the bias so that
is the graph we created and then we need
to Define what is our loss function and
what is our Optimizer so in this case is
we again use the tensorflows apis so TF
dot NN softmax cross entropy with logits
is the API that we will use and reduce
mean is what is like the mechanism
whereby which says that you reduce the
error and Optimizer for doing detection
of the error what Optimizer are we using
so we are using gradient descent
Optimizer we discussed about this in
couple of slides earlier and for that
you need to specify the learning rate
you remember we saw that there was a
slide somewhat like this and then you
define what should be the learning rate
how fast you need to come down that is
the learning rate and this again needs
to be tested and tried and to find out
the optimum level of this learning rate
it shouldn't be very high in which case
it will not converge or shouldn't be
very low because it will in that case it
will take very long so you define the
optimizer and then you call the method
minimize for that Optimizer and that
will kick start the training process and
so far we've been creating the graph and
in order to actually execute that graph
we create what is known as a session and
then we run that session and once the
training is completed we specify how
many times how many iterations we wanted
to run so for example in this case we
are saying Thousand Steps so that is a
exit strategy in a way so you specify
the exit condition so training will run
for thousand iterations and once that is
done we can then evaluate the model
using some of the techniques shown here
so let us get into the code quickly and
see how it works so this is our Cloud
environment now you can install
tensorflow on your local machine as well
I'm showing this demo on our existing
Cloud but you can also install
tensorflow on your local machine and
there is a separate video on how to set
up your tensorflow environment you can
watch that if you want to install your
local environment or you can go for
other any cloud service like for example
Google Cloud Amazon or Cloud Labs any of
these you can use and run and try the
code okay so it has got started
with a login
all right so this is our deep learning
tutorial code
and this is our tensorflow environment
and so let's get started
the first we have seen a little bit of a
Code walkthrough uh in the slides as
well now you will see the actual code in
action so the first thing we need to do
is import tensorflow and then we will
import the data and we need to adjust
the data in such a way that the
one hot is encoding is set to True one
hot encoding right as I explained
earlier so in this case the label values
will be shown appropriately and if we
just check what is the type of the data
so you can see that this is a data sets
python data sets and if we
check the number of images the way it
looks so this is how it looks it is an
array of type float32
similarly
the number if you want to see what is
the number of
cleaning images there are 55 000 and
there are test images 10 000 and then
validation images
5000. now let's take a quick look at the
data itself visualization so we will use
um matplotlib for this
and if we take a look at the shape now
shape gives us like the dimension of the
tensors or or the arrays if you will so
in this case the training data set if we
see the size of the training data set
using the method shape it says there are
55 000 and 50 000 by 784 so remember the
784 is nothing but the 28 by 28 28 into
28 so that is equal to 784 so that's
what it is showing now we can take just
one image and just see what is the the
first image and see what is the shape so
again size obviously it is only 784.
similarly you can look at the image
itself the data of the first image
itself so this is how it it shows a
large part of it will probably be zeros
because as you can imagine
in the image only certain areas are
written rest is blank so that's why you
will mostly see Zero say that it is
black or white but then there are these
values are so the values are actually
they are scaled so their values are
between 0 and 1. okay so this is what
you're seeing so certain locations there
are some values and then other locations
there are zeros
so that is how the data is stored and
loaded
if we want to actually see what is the
value of the handwritten image if you
want to view it this is how you view it
so you create like do this reshape and
matplotlab has this
feature to show you these images so we
will actually use the function called I
am show and then if you pass this
parameters appropriately you will be
able to see the different images now I
can change the values in this position
so which image we are looking at right
so we can say
if I want to see what is there in maybe
5000
right so
5000 has three similarly you can just
say five what is in five
five as eight
what is in fifty
again eight so basically by the way if
you're wondering uh how I'm executing
this code shift enter in case you're not
familiar with Jupiter notebooks shift
enter is how you execute each cell
individual cell and if you want to
execute the entire program you can go
here and say run all
so that is how
this score gets executed and here again
we can check what is the maximum value
and what is the minimum value of this
pixel values as I mentioned this is it
is scaled so therefore it is between the
values lie between 1 and 0. now this is
where we create our model
the first thing is to create the
required placeholders and variables and
that's what we are doing here as we have
seen in the slides so we create one
placeholder and we create two variables
which is for the weights and biases
these two variables are actually
matrices so each variable has 784 by 10
chill values okay so one for this 10 is
for each neuron there are 10 neurons and
784 is for the pixel values inputs that
are given which is 28 into 28 and the
biases as I mentioned one for each
neuron so there will be 10 biases they
are stored in a variable by the name b
and this is the graph which is basically
the multiplication of these matrix
multiplication of X into W and then the
bias is added for each of the neurons
and the whole idea is to minimize the
error so let me just execute I think
this code is executed then we Define
what is our the Y value is basically the
label value so this is another
placeholder we had X as one placeholder
and white underscore true as a second
placeholder and this will have values in
the form of uh 10 digit 10 digit arrays
and since we set one hot encoded the
position which has a one value indicates
what is the label for that particular
number all right then we have cross
entropy which is nothing but the loss
loss function and we have the optimizer
we have chosen gradient descent as our
Optimizer that the training process
itself so the training process is
nothing but to minimize the cross
entropy which is again nothing but the
loss function so we Define all of this
in the form of a graph so the up to here
remember what we have done is we have
not exactly executed any tensorflow code
till now we are just preparing the graph
the execution plan that's how the
tensorflow code works so the whole
structure and format of this code will
be completely different from how we
normally do programming so even with
people with programming experience may
find this a little difficult to
understand it and it needs quite a bit
of practice so you may want to view this
video also maybe a couple of times to
understand this flow because the way
tensorflow programming is done is
slightly different from the normal
programming some of you who let's say
have done maybe spark programming to
some extent will be able to easily
understand this but even in spark the
the programming the code it's self is
pretty straightforward behind the scenes
the execution happens slightly
differently but in tensorflow even the
code has to be written in a completely
different way so the code doesn't get
executed in the same way as you have
written so that that's something you
need to understand and a little bit of
practices needed for this so so far what
we have done up to here is creating the
variables and feeding the variables and
or rather not feeding but setting up the
variables and the graph that's all
defining maybe the what kind of a
network you want to use for example we
want to use a soft Max and so on so you
have created the variables how to load
the data loaded the data viewed the data
and prepared everything but you have not
yet executed anything in tensorflow now
the next step is the execution in
tensorflow so the first step for doing
any execution in 10 the flow is to
initialize the variables so anytime you
have any variables defined in your code
you have to run this piece of code
always so you need to basically create
what is known as a node for initializing
so this is a node you still are not yet
executing anything here you just created
a node for the initialization
so let us go ahead and create that and
here onwards is where you will actually
execute your code in tensorflow and in
order to execute the code what you will
need is a session tensorflow session so
TF dot session will give you a session
and there are a couple of different ways
in which you can do this but one of the
most common methods of doing this is
with what is known as a with Loop so you
have a width TF dot session as says and
with the colon here and this is like a
block starting of the block and these
indentations tell how far this block
goes and this session is valid till this
block gets executed so that is the
purpose of creating this with block this
is known as a width block so with TF dot
session as says you say SAS dot run in
it now says Dot will execute a node that
is specified here so for example here we
are saying says dot run SAS is basically
an instance of the session right so here
we are saying TF dot session so an
instance of the session gets created and
we are calling that SAS and then we run
a node within that one of the nodes in
the graph so one of the nodes here is
init so we say run that particular node
and that is when the initialization of
the variables happens now what this does
is if you have any variables in your
code in our case we have W is a variable
and B is a variable so any variables
that we created you have to run this
code you have to run the initialization
of these variables otherwise you will
get an error
that is the that's what this is doing
then
we within this with block we specify a
for Loop and we are saying we want the
system to iterate for thousand steps and
perform the training
that's what this for Loop does run
training for 1000 iterations
and what it is doing basically is it is
fetching the data or these images
remember there are about 50 000 images
but it cannot get all the images in one
shot because it will take up a lot of
memory and performance issues will be
there so this is a very common way of
Performing deep learning training you
always do in batches so we have maybe 50
000 images but you always do it in
batches of 100 or maybe 500 depending on
the size of your system and so on and so
forth so in this case we are saying okay
get me 100 images at a time and get me
only the training images remember we use
only the training data for training
purpose and then we use test data for
test purpose you must be familiar with
machine learning so you must be aware of
this but in case you are not in machine
learning also not this is not specific
to deep learning but in machine learning
in general you have what is known as as
training data set and test data set your
available data typically you will be
splitting into two parts and using the
training data set for training purpose
and then to see how well the model has
been trained you use the test data set
to check or test the validity or the
accuracy of the model so that's what we
are doing here and You observe here that
we are actually calling an mnist
function here so we are saying mnist
train dot next batch
right so this is the advantage of using
MNS database because they have provided
some very nice helper functions which
are readily available otherwise this
activity itself we would have had to
write a piece of code to fetch this data
in batches that itself is a lengthy
exercise so we can avoid all that if we
are using MNS database and that's why we
use this for the initial learning phase
okay so when we say fetch what it will
do is it will fetch the images into X
and the labels into Y and then you use
this batch of 100 images and you run the
cleaning so SAS dot run basically what
we are doing here is we are running the
training mechanism which is nothing but
it passes this through the neural
network passes the images through the
neural network finds out what is the
output and if the output obviously
initially it will be wrong so all that
feedback is given back to the new
Network and thereby all the Ws and B's
get updated till it reaches thousand
iterations in this case the exit
criteria is thousand but you can also
specify probably accuracy rate or
something like that for the as an exit
criteria so here it is it just says that
okay this particular image was wrongly
predicted so you need to update your
weights and biases that's the feedback
given to each neuron and that is run for
1000 iterations and typically by the end
of this thousand iterations the model
would have learned to recognize these
handwritten images obviously it will not
be 100 accurate okay so once that is
done
after so this happens for 1000
iterations once that is done you then
test the accuracy of these models by
using the test data set right so this is
what we are trying to do here the code
may appear a little complicated because
if you're seeing this for the first time
you need to understand uh the various
methods of tensorflow and so on but it
is basically comparing the output with
what has been what is actually there
that's all it is doing so you have your
test data and you are trying to find out
what is the actual value and what is the
predicted value and seeing whether they
are equal or not TF dot equal right and
how many of them are correct and so on
and so forth and based on that the
accuracy is calculated as well so this
is the accuracy and that is what we are
trying to see how accurate the model is
in predicting these numbers or these
digits okay so let us run this this
entire thing is in one cell so we will
have to just run it in one shot it may
take a little while let us see and uh
not bad so it has finished the thousand
iterations and what we see here as an
output is the accuracy so we see that
the accuracy of this model is around 91
percent okay now which is pretty good
for such a short exercise within such a
short time we got 90 percent accuracy
however in real life this is probably
not sufficient so there are other ways
in to increase the accuracy we will see
probably in some of the later tutorials
how to improve this accuracy how to
change maybe the hyper parameters like
number of neurons or number of layers
and so on and so forth and so that this
accuracy can be increased Beyond 90
percent all right so what is the
tensorflow object detection API this is
an open source framework which is
actually provided by the tensorflow team
and there are trained models available
and the sample code is also available
which we can use in order to easily
detect objects in images and videos this
is pretty robust and can detect objects
fairly quickly and this is very easy for
people to use as well people with very
less or no knowledge of machine learning
or deep learning can also with a little
bit of Python Programming knowledge can
actually use this API this library to
build object detection applications this
is a list of libraries that are required
and they have been shown in the code as
well the exact purpose of each of this
Library why it is required is out of the
scope of this tutorial but we will see
in the code as we walk through some of
these libraries how and why they are
used the Coco data set Coco stands for
common objects in context so this data
set consists of 300 000 images of 90
most commonly found objects like chairs
and tables and so on and so forth so
this model has been trained or in fact a
set of models have been trained on this
data set and this is pretty good to
detect the most common objects in the
images and videos so with that let's get
into the code all right so the first
part is to import all these libraries
and this we have shown you in the slides
as well again a large part of this will
be for doing some helper functions and
maybe for visualizing the images and so
on and so forth so that's the reason
they are required as I said the exact
details of each and every Library
probably is out of scope but these are
needed so as a first step maybe you just
go ahead and include these libraries and
run the code and maybe a later point we
can discuss what each of these libraries
does now this will work with tensorflow
version higher than 1.4 so if you are
having tensorflow version below 1.4 you
may have to upgrade to a higher version
so let me go ahead and execute the cell
and we also need this line of code to
make sure that once we run this object
detection the labeled images are
displayed within this notebook and many
of you by now must be familiar with this
and we will import a few utility
libraries and you will see that we will
be using some of these for visualization
purpose so once the objects are detected
we need to display the information what
that object is and then what percentage
of confidence the model has so all these
details that's the utility functions
that stored here and then the next part
is to prepare the model as I mentioned
we will be using an existing trained
model the tensorflow team has actually
provided these models the one that we
will be using is SSD with the mobile net
but you can actually use any one of the
ones that are listed in this URL let me
just quickly take you through this URL
these are a bunch of models trained
models that are readily available for
anybody to use it is open source and let
me scroll down the only thing is that if
there are some of them with where the
accuracy is much higher but they take
longer and there are some where the
accuracy is not so high and they are
much faster so they are faster but
accuracy men on T that very high so you
can play around with some of these and
we in this particular exercise or in
this particular tutorial we are using
this SSD model which is SSD mobile at
question one so that's the model that
we'll be using so in this cell we are
primarily creating a bunch of variables
with various for example the name of the
model the path and so on and so forth so
that we will be using these names in The
Next Step which is to download this
model and install it locally these are
also referred to as Frozen models so
once they are trained and then you kind
of extract or you freeze the model so
that's the reason they are called Frozen
models so that others can just use this
without any further training so this is
where we download and extract our model
locally so this will take a little while
let me see if I can wait or maybe pause
the video and come back once it is done
might take a little while let's see if
it completes I have a pretty high speed
network but even then it takes some time
so that's good but this part is over now
let us see this part and yes both are
done so once that is done we need to
load some label mapping basically what
this will do is your model as you may be
aware by now if you do some
classification the model will actually
not give any output the text it will
give some numbers so if there are five
classes it will say okay this belongs to
model class one or two or three or five
and so on the numbers now each of these
will obviously the numbers will not make
any sense to the outside world so we
need to do some small mapping so in this
case let's say one maybe a chair two
maybe a table three maybe a balloon and
so on so that kind of number to text
mapping we need to do and that is what
is being done in this particular cell
and then we have a helper code which
will load the image and convert it into
a numpy array so that the number array
is what gets processed and used by the
model to do the detection part of it so
that is what this method is all about so
we will be later on we will be calling
that function and next is preparation
for detection so here we are basically
telling where the images are stored and
how many images or what is the naming
convention or format of the images now
if you want you can modify this code for
example recurrently I have test
underscore images as my folder so let me
go and show this to you so this is under
my object detection folder I have
another subfolder where I am storing my
images which is text underscore images
now you can rename this folder and give
some other name and then in your code
you can probably give that particular
name for the sample similarly the format
of these files what is the name and
format of this files here it is in a
very simple format which is the names of
the files are like beach one beach two
B3 and so on so I have taken Beach as
the theme so I have images which are
related to beaches so this is beach one
beach two and then these three are few
others but we will use these three for
our demo and so that's what I'm saying
here the name of the images will be
Beach something dot jpeg which is jpeg
format and in this curly braces
basically we will will be filled with
either one two or three based on in this
particular for Loop okay so that is what
this is doing all right so the next step
is to run inference on these images in a
loop and what we are basically doing
here is getting these images one by one
and then running through the model to
find out what are the objects that can
be detected and then against each of the
object a box will be drawn and it will
be labeled with the name and the
percentage of accuracy or confidence
that the model detects these objects
okay so that is the function here and so
let me just run that piece of code and
here is basically where we are calling
this function so we are loading this
images and then we are calling this
function for each image and then we are
displaying this using the matplotlab
library so let's run this it will take
one image at a time and then detect the
images now the beauty is that the same
format of the code can be used for doing
object detection in a video so we have
another video for doing object detection
in a video so most of the code out there
will be reused from here and the only
thing is that that instead of reading
the images from the local storage we
read the frames from the video and there
is a neat little video reader that is
available and it will be shown in the
other video and frame by frame we read a
video and then pass on to this function
and it will act as if each of these
frames is an image and then it will do
the same object detection on the entire
video so that's in a separate video just
look out for that and actually the
information about that is provided in
the cards the I symbol so that's the the
video object detection in video that's
the separate tutorial all right so now
that we have all the pieces together
this the last cell is where the whole
action takes place so let's run this and
see how it looks so it will take
probably a little while and there are
about three images let's see what it
detects there we go so good so the first
one it has detected a person and that
two with 97 accuracy which is uh I think
pretty good okay and then the next image
it detects
umbrella and chair there are a few other
options but it's not able to detect but
it has detected umbrella with 63 percent
accuracy or confidence rather and the
chair with 58 again not bad
then let's see the next image so here
these are actually balloons hot air
balloons but the model thinks it is a
kite which is uh probably not that bad
it sees there's something in the sky and
therefore probably it thinks it is a
kite and it detects that with 65 percent
confidence okay so that was pretty much
all I wanted to show you in this
particular tutorial about object
detection in images
hey everyone in this video called Keras
tutorial we will teach you everything
that you need to know about Keras in
machine learning and data analysis to
begin we will explain the concept of
Keras to you we will answer the question
what is Keras and how it is used in deep
learning then we will list out some
common differences between Keras
tensorflow and Pi torch so that you can
understand why Keras is the superior
framework after this we will dive deep
into Keras and see how you can use Keras
to implement deep learning models with
Hands-On tutorials and finally we will
take a look at Kera sequential models a
type of model and Keras which makes
implementing neural networks extremely
easy
so let's get started the first question
that arises in our mind is what is deep
learning deep learning is generally
considered to be a subset of machine
learning which is nothing but a subset
of artificial intelligence what is
artificial intelligence artificial
intelligence is the intelligence which
is demonstrated by machines unlike the
natural intelligence which is displayed
by humans and animals which also
involves Consciousness and emotionality
meanwhile machine learning is an
application of artificial intelligence
that provides systems the ability to
automatically learn and improve from
experience without being programmed to
specifically now deep learning as I said
is a class of machine learning
algorithms that uses multiple layers to
progressively extract higher level
features from raw input for example if
you have a system which we are using for
image processing lower layers in the
system may be used to detect the
outlines of images while higher layers
May identify the concepts which are
relevant to the image in case of the
picture of a human it might recognize
features such as digits or letters or
face now in deep learning we use neural
networks to use multiple mathematical
operations to break down a complex
problem into smaller parts which are
solved individually each mathematical
expression is called a neuron
deep learning models are trained using
large sets of labeled data and neural
network architectures that learn
features directly from the data without
the need for manual feature extraction
one of the most popular types of deep
neural networks is known as a
convolution neural network a convolution
neural network convolves learned
features with input data and uses 2D
convolutional layers making this
architecture Well Suited processing 2D
data such as images now in deep learning
neural networks will play a very
important part what exactly are neural
networks neural networks are modeled
from the way the human brain works in
our brain we have neurons which carry
electrical impulses from our brain to
different parts of our body and help in
transmitting instructions from the brain
all over our body in neural networks a
neuron acts as nothing more but a
mathematical operation multiple neurons
are required to break down a complex
problem into various paths to make it
easier to solve now a neural network
contains layers of interconnected nodes
each node in a neural network is a
perceptron and is similar to a multiple
linear regression the perceptron feeds
the signals which are produced by
multiple linear regressions into an
activation function that may be
non-linear hidden layers will fine-tune
the input weightings until neural
Network's margin of error is minimal and
we get the optimal output
now what is Keras before we move on to
Keras let's take a look at tensorflow
tensorflow is a software Library which
was created by Google to implement
large-scale machine learning models and
to help solve complex numerical problems
tensorflow is an end-to-end open source
machine learning platform which you can
think of as an infrastructure layer for
differentiable programming it combines
four key abilities first efficiently
executing lower level tensor operations
on CPU and GPU or TPU Computing gradient
of arbitrary differential Expressions
scaling computation to many devices and
exporting programs to external runtime
such as servers browsers or mobiles
but what does Keras have to do with all
of this Keras is nothing more than a
high level deep learning API which is
written in Python for neural networks it
has multiple backend neural network
competitions and it makes implementing
neural networks easy as we know Keras is
a powerful and easy to use free open
source python library for developing and
evaluating deep learning models
Keras is a higher level API of
tensorflow 2. an approachable highly
productive interface for solving machine
learning problems with a focus on Modern
deep learning it provides essential
abstractions and building blocks for
developing and shipping machine learning
Solutions with high iteration velocity
Keras empowers engineers and researchers
to take full advantage of the
scalability and cross-platform
capabilities of tensorflow 2. you can
run Keras on TPU or on large classes of
GPU and you can export your Keras model
to run in the browser or on a mobile
device Keras provides a python interface
for artificial neural networks Keras
acts as an interface for tensorflow
liability and it helps in implementation
of commonly used neural network building
block layers such as objectives
activation functions and optimizers and
a host of tools to make working with
images and Text data easier to simplify
the coating necessary for writing deep
neural network code basically it wraps
efficient numerical computation
libraries such as theano and tensorflow
and allows you to Define and train
neural network models in just a few
lines of code in fact tensorflow and
Keras are so tied together that even in
tensorflow the official get started with
tensorflow tutorial viewers high level
Keras apis embedded in tensorflow
instead of just using tensorflow by
itself
some of the important features of Keras
include Keras being a high level
interface which uses theano or
tensorflow for its backend
Keras can run smoothly on both CPU and
GPU it supports almost all the models
required for you to create a complete
neural network
and it is extremely modular in nature
which means it is incredibly expressive
flexible and app for Innovative research
so why do we need Keras
first of all Keras is a API which is
designed for human beings not for
machines which means that it follows
best practices for reducing cognitive
load it offers consistent and simple
apis it minimizes the number of users
action required for common use cases and
it provides clear and actionable
feedback upon user error it is extremely
powerful and easy to use and it also
wraps numerical computation libraries
such as piano and tensorflow and allows
you to train and define neural networks
in a few lines of code
this makes Keras easy to learn and use
as a Keras user you can be more
productive allowing you to try more
ideas than your competition faster which
in turn will help you win machine
learning competition this ease of use
does not come at the cost of reduced
flexibility because Keras integrates
deeply with low level tensorflow
functionality it enables you to develop
highly hackable workflows where any
piece of functionality can be customized
or reused easily building a model in
Keras let's take a look at the basic
pipeline which you will have to follow
when you are using Keras to define a
neural network first of all you will
have to define a neural network you will
have to select the activation functions
and the various mathematical operations
that are gonna be taking place in your
neural network along with the different
weightages next you will use Keras to
compile your network which is basically
you will use Keras to build it
after this you will fit to your network
you will fit all the draining data that
you have to your network and use your
testing data to evaluate how the network
works finally you will use this network
to make predictions
this entire five-step pipeline can be
executed in Keras within two to three
lines of code neural networks which
would have taken people days or even
weeks to make can be executed in Keras
in a couple of hours
now uses of Keras the first use of Keras
is to productize deep models on
smartphones deep models require a lot of
computation power to run but with the
help of Keras we can literally make deep
models a product which can be sold and
executed on smartphones
the next use of Keras is in distributed
training of deep learning models
distributed training means that we can
split a deep learning model into
different parts and train it on systems
all across the globe this makes training
of our deep learning model extremely
fast along with saving time we are also
saving on the computational power of our
system as it is not only our system
which has to run such a heavy program by
Distributing it across various systems
all the resources required to train a
deep learning model go down
significantly before we can see the
differences between these platforms we
first need to know what exactly each of
these platforms are let's start with
tensorflow what is tensorflow tensorflow
is a low level software Library which is
created by Google to help Implement
machine learning models and to solve
complex numerical problems tensorflow is
nothing but a free and open source
software library for machine learning it
can be used across the range of tasks
but has a particular focus on training
and inference of deep neural networks
tensorflow is a symbolic math Library
Based on data flow and differential
programming it is used for both research
and production at Google what do you
mean by data flow it basically means
that we perform calculations by
converting every element into graphical
form the variables of the graph are
called tensors and mathematical
operations are called operators here in
the computational graph shown you can
see that x y and 2 are the variables
they will also be called tensors and
division multiplication and addition are
the operators
this graph basically shows us the
calculation that is going to occur in a
machine learning model
where X and Y are going to be divided
and Y and 2 are going to be multiplied
the results of these two calculations
are then going to be added to give us
the final output I told you that x y and
2 are also called tensors what exactly
are tensors tensors are
multi-dimensional arrays with a uniform
type all tensors are immutable like
python numbers and strings which means
that you cannot update the contents of a
tensor you can only create a new tensor
next let's look at the API Keras
what exactly is Keras Keras is a higher
level deep learning API written in
Python for easy implementation and
computation of neural networks Keras is
an open source software library that
provides a tensorflow interface for
artificial neural networks Keras acts as
an interface for the tensorflow library
which means that it runs on top of
tensorflow up until version 2.3 Keras
supported multiple back-ends including
tensorflow Microsoft cognitive toolkit
or cntk R Tiano and played ml as of
version 2.4 only tensorflow is supported
as of version 2.4 only tensorflow is
supported designed to help enable fast
experimentation with deep neural
networks in it focuses on being
user-friendly modular and extensible
Keras is a high level API of tensorflow
an approachable highly productive
interface for solving machine learning
problems with the focus on Modern deep
learning it provides essential
abstractions and building blocks for
developing and shipping machine learning
Solutions with high iteration velocity
Keras does not perform its own low level
operations such as tensor products and
convolution it relies on back-end
engines for that even though Keras
supports multiple back-end engines its
primary back end in genus tensorflow and
its primary supporter is Google which
means that Keras acts as nothing more
but a wrapper class around tensorflow
thiano cntk played ml or mxnet which are
low level apis next we will look at
pytorch what exactly is pi touch pytorch
is a low level API which is developed by
Facebook for national language
processing and computer vision it is a
more powerful version of numpy
it is an open source machine learning
library based on the torch Library used
for applications such as computer vision
and natural language processing
primarily developed by Facebook's AI
research lab it is free and open source
software released under the modified BSD
license although the python interface is
a more polished and the primary focus of
development pytorch also has a C plus
plus interface python is a widely liked
language because it is easy to
understand and write pytorch emphasizes
flexibility and allows deep learning
models to be expressed in basic python
pytouch is mainly used for national
language processing
and for computer vision now let's move
on to the differences between tensorflow
Keras and pytouch
the first difference that we'll be
looking at is called level of API there
are two main types of apis a low level
API and a high level API apis stands for
application programming interface
a low-level application programming
interface is generally more detailed and
allows you to have more detailed control
to manipulate functions within them on
how to use and Implement them while a
high level API is more generic and
simple and provides more functionality
with one command statements than a lower
level API high level interfaces are
comparatively easier to learn and to
implement the models using them they
allow you to write code in a shorter
amount of time and to be less involved
with the details in this case tensorflow
is a high and low level API
pure tensorflow is a low level API while
tensorflow wrapped in Keras is a high
level API
Keras in itself is a high level API
which uses multiple low-level apis as a
backend and simplifies the operation of
these low-level apis
pytorch is a low level API
the next criteria that we'll be looking
at is speed tensorflow is very fast and
is used for high performances Keras is
slower as it works on top of tensorflow
not only does it have to wait for
tensorflow to finish implementation it
then starts its own implementation
meanwhile Pi torch works at the same
speed as tensorflow as both of them are
both low level apis now Keras is a
wrapper class for tensorflow and has
added abstraction functionalities on top
of tensorflow which make it slower than
tensorflow and Pie torch in computation
speed both tensorflow and Pi torch are
almost equal and in development speed
Keras is faster as it has built-in
functionalities which can significantly
reduce your development time the next
difference is on the architecture
tensorflow is not very easy to use and
even though it provides Keras as a
framework that makes it work easier
tensorflow still has a very complex
architecture which is hard to use
meanwhile Keras is a simpler
architecture and is easier to use it
provides a high level of abstraction
which makes implementation of programs
in Keras significantly easier pie touch
on the other hand also has a complex
architecture and the readability is less
when compared to Keras tensorflow uses
computational graphs which makes it very
complex and hard to interpret but it has
amazing computational ability across
platforms pytorch is a little hard for
beginners but is really good for
computer vision and deep learning
purposes data sets and debugging
tensorflow works with large data sets
due to its high execution speed and
debugging is really hard in tensorflow
due to its complex nature meanwhile
Keras only works with very small data
sets as its speed of execution is low
programs do not require frequent
debugging in Keras as they are
relatively simpler and pytorch can
manage high level tasks in higher
Dimension data sets and is easier to
debug than both Keras and tensorflow
next we'll be looking at ease of
development as we said before tensorflow
works with many hard Concepts such as
computational graphs and tensors which
means that writing code in tensorflow is
very hard it is generally used by people
when they are doing research work and
really need very specific
functionalities
Keras on the other hand provides a high
level of abstraction which makes it very
easy to use it is best for people who
are just starting out with python and
machine learning pie torch is easier
than tensorflow but is still
comparatively hard than Keras it is not
very easy to learn for beginners but is
significantly more powerful than just
plain carers ease of deployment
tensorflow is very easy to deploy as it
uses tensorflow serving tensorflow
serving is a flexible high performance
serving system for machine learning
models designed for production
environments tensorflow serving makes it
easy to deploy new algorithms and
experiments while keeping the same
server architecture and apis tensorflow
serving provides out of the box
integration with tensorflow models but
can be easily extended to serve other
types of models and data in Keras model
deployment can be done with either
tensorflow serving or flask which makes
it relatively easy but not as easy as
you as it would be with tensorflow and
pytorch pie touch uses iTouch mobile
which makes deployment easy but again
for tensorflow deployment is way easier
as tensorflow serving can update your
machine learning backend on the fly
without the user even realizing there's
a growing need to execute ml models on
edge devices to reduce latency preserve
privacy and enable new interactive use
cases in the past Engineers used to
train models separately they would then
go through a multi-step error-prone and
often complex process to train the
models for execution on a mobile device
the mobile runtime was often
significantly different from the
operations available during training
leading to inconsistent developer and
eventually user experience
all of these frictions have been removed
by pi torch Mobile by allowing a
seamless process to go from training to
deployment by staying entirely within
the pytorch ecosystem it provides an
end-to-end workflow that simplifies the
research to production environment for
mobile devices in addition it paves the
way for privacy preserving features via
Federated learning techniques
at the end of the day the question that
really matters is which framework should
you use Keras tensorflow or Pi torch
now tensorflow has implemented various
levels of abstraction to make
implementation of deep learning and
neural networks easy this has also made
debugging easier Keras is simple and
easy but not as fast as tensorflow it is
more user friendly than any other deep
learning API however and is easier to
learn for beginners by torch on the
other hand is the preferred deep
learning API for teachers but it is not
as widely used in production as
tensorflow is it is faster but it has
lower GPU utilization
at the end of the day the framework that
we would suggest that you use is
tensorflow
why while Pi torch may have been the
preferred deep learning library for
researchers tensorflow is much more
widely used in day-to-day production pie
torches ease of use combined with the
default ego execution mode for easier
debugging predestines it to be used for
fast hacky Solutions and smaller scale
models but tensorflow's extensions for
deployment on both servers and mobile
devices combined with the lack of python
overhead makes it the preferred option
for companies that work with deep
learning models in addition the
tensorflow board visualization features
offers a nice way of showing the inner
workings of your model to say your
customers
meanwhile between tensorflow and Keras
the main difference isn't in performance
tensorflow is a bit faster due to less
overhead but also the level of control
you would like Keras is much easier to
start with than plain tensorflow but if
you want to do something with Keras that
doesn't come out of the box it will be
harder to implement that tensorflow on
the other hand allows you to create any
arbitrary computational graph providing
much more flexibility so if you're doing
more research type of work tensorflow is
the sure route to go due to the
flexibility that it provides if getting
your learning started is half the battle
what if you could do that for free visit
skillup by simply learn click on the
link in the description to know more
so we're going to dive right into what
is Karas we'll also go all the way
through this into a couple of tutorials
because that's where you really learn a
lot is when you roll up your sleeves so
we talk about what is cross cross is a
high level deep learning API written in
Python for easy implementation of neural
networks it uses deep learning
Frameworks such as tensorflow Pi torch
Etc as backend to make computation
faster
and this is really nice because as a
programmer there is so much stuff out
there and it's evolving so fast it can
get confusing and having some kind of
high level order in there we can
actually view it and easily program
these different neural networks is
really powerful it's really powerful to
to have something out really quick and
also be able to start testing your
models and seeing where you're going so
cross works by using complex deep
learning Frameworks such as tensorflow
pytorch ml played Etc as a back-end for
fast computation while providing a
user-friendly and easy to learn
front-end and you can see here we have
the cross API specifications and under
that you'd have like TF Karas for
tensorflow thano cross and so on and
then you have your tensorflow workflow
that this is all sitting on top of
and this is like I said it organizes
everything the heavy lifting is still
done by tensorflow or whatever you know
underlying package you put in there and
this is really nice because you don't
have to
um dig as deeply into the heavy and
stuff while still having a very robust
package you can get up and running
rather quickly and it doesn't distract
from the processing time because all the
heavy lifting is done by packages like
tensorflow this is the organization on
top of it so the working principle of
Karas
uh the working principle of cross is
cross uses computational graphs to
express and evaluate mathematical
expressions
you can see here we put them in blue
they have the expression expressing
complex problems as a combination of
simple mathematical operators where we
have like the percentage or in this case
in Python that's usually your left your
remainder or multiplication you might
have the operator of x to the power of
0.3 and it uses useful for calculating
derivatives by using back propagation so
if we're doing with neural networks we
send the error back up to figure out how
to change it this makes it really easy
to do that without really having not
banging your head and having to hand
write everything it's easier to
implement distributed computation and
for solving complex problems specify
input and outputs and make sure all
nodes are connected
and so this is really nice as you come
in through is that as your layers are
going in there you can get some very
complicated uh different setups nowadays
which we'll look at in just a second
and this just makes it really easy to
start spinning this stuff up and trying
out the different models
so we look at Cross models uh cross
model we have a sequential model
sequential model is a linear stack of
layers where the previous layer leads
into the next layer
and this if you've done anything else
even like the SK learn with their neural
networks and propagation in any of these
setups this should look familiar you
should have your input layer it goes
into your layer one layer two and then
to the output layer
and it's useful for simple classifier
decoder models
and you can see down here we have the
model equals across sequential and this
is the actual code if you can see how
easy it is we have a layer that's dense
your layer one as an activation they're
using the relu in this particular
example and then you have your name
layer one layer dense relu name Layer
Two and so forth and they just feed
right into each other so it's really
easy just to stack them as you can see
here and it automatically takes care of
everything else for you and then there's
a functional model and this is really
where things are at this is new make
sure you update your cross or you'll
find yourself running this doing the
functional model you'll run into an
error code because this is a fairly new
release
and he uses multi-input and multi-output
model the complex model which Forks into
two or more branches
and you can see here we have our image
inputs equals your cross input shape
equals 32 by 32 by 3.
you have your dense layers dense 64
activation relu and this should look
similar to what you already saw before
but if you look at the graph on the
right it's going to be a lot easier to
see what's going on you have two
different inputs and one way you could
think of this is maybe one of those is a
small image and one of those is a full
sized image and that feedback goes into
you might feed both of them into one
node because it's looking for one thing
and then only into one node for the
other one and so you can start to get
kind of an idea that there's a lot of
use for this kind of split and this kind
of setup we have multiple information
coming in but the information is very
different even though it overlaps and
you don't want it to send it through the
same neural network and they're finding
that this trains faster and is also has
a better result depending on how you
split the date up and how you Fork the
models coming down
and so in here we do have the two
complex models coming in we have our
image inputs which is a 32 by 32 by
three or three channels or four if
you're having an alpha channel uh you
have your dense your layers dense is 64
activation using the ray Lou very common
x equals dense inputs X layers dense x64
activation equals relu x outputs equals
layers dense 10 X model equals cross
model inputs equals inputs outputs
equals outputs name equals minced model
uh so we add a little name on there and
again this is this kind of split here
this is setting us up to have the input
go into different areas
so if you're already looking across you
probably already have this answer what
are neural networks but it's always good
to get on the same page and for those
people who don't fully understand neural
networks to dive into them a little bit
or do a quick overview
neural networks are deep learning
algorithms modeled after the human brain
they use multiple neurons which are
mathematical operations to break down
and solve complex mathical problems
and so just like the neuron one neuron
fires in and it fires out to all these
other neurons or nodes as we call them
and eventually they all come down to
your output layer
and you can see here we have the really
standard graph input layer a hidden
layer and an output layer one of the
parts of any data processing is your
data pre-processing
so we always have to touch base on that
with a neural network like many of these
models they're kind of uh when you first
start using them they're like a black
box you put your data in you train it
and you test it and see how good it was
and you have to pre-process that data
because bad data in is bad outputs so in
data pre-processing we will create our
own data examples set with Karas the
data consists of a clinical trial
conducted on 2100 patients ranging from
ages 13 to 100 with a the patients under
65 and the other half over 65 years of
age
we want to find the possibility of a
patient experiencing side effects due to
their age and you can think of this in
today's world with covid uh what's going
to happen on there and we're going to go
ahead and do an example of that in our
lay of Hands-On like I said most of this
you really need to have Hands-On to
understand so let's go ahead and bring
up our anaconda and open that up and
open up a Jupiter notebook for doing the
python code in
if you're not familiar with those you
can use pretty much any of your setups I
just like those for doing demos and
showing people especially shareholders
it really helps because it's a nice
visual so let me go and flip over to our
anaconda and the Anaconda has a lot of
cool two tools they just added data lore
and IBM Watson Studio Cloud into the
Anaconda framework but we'll be in the
Jupiter lab or Jupiter notebook I'm
going to do a Jupiter notebook for this
because I use the lab for like large
projects with multiple pieces it has
multiple tabs where the notebook will
work fine for what we're doing
and this opens up in our browser window
because that's how Jupiter dope so
Jupiter notebook is set to run
it will go under new create a new Python
3 and it creates an Untitled python
we'll go ahead and give this a title and
we'll just call this uh cross
tutorial
and let's change that to Capital there
we go I'm going to just rename that and
the first thing we want to go ahead and
do is uh
get some pre-processing tools involved
and so we need to go ahead and import
some stuff for that like our numpy do
some random number generation
I mentioned SK learner your site kit if
you're installing sklearn the sklearn
stuff it's a site kit you want to look
up
that should be a tool of anybody who is
doing data science if you're not if
you're not familiar with the sklearn
toolkit it's huge but there's so many
things in there that we always go back
to and we want to go ahead and create
some train labels and train samples for
training our data
and then just a note of what we're we're
actually doing in here let me go ahead
and change this this is kind of a fun
thing you can do we can change the code
to markdown
and then markdown code is nice for doing
examples once you've already built this
our example data we're going to do
experimental
there we go experimental drug was tested
on 2100 individuals between 13 to 100
years of age half the participants are
under 65 and 95 percent of participants
are under 65. experience no side effects
well 95 percent of participants over 65
experience side effects
so that's kind of where we're starting
at and this is just a real quick example
because we're going to do another one
with a little bit more complicated
information
uh and so we want to go ahead and
generate
our setup so we want to do for I and
range and we want to go ahead and create
if you look here we have random integers
train the labels of pen so we're just
creating some random data
and let me go ahead and just run that
and so once we've created our random
data and if you if I mean you can
certainly ask for a copy of the code
from Simply learning they'll send you a
copy of this or you can zoom in on the
video and see how we went ahead and did
our train samples append
um
and we're just using this I do this kind
of stuff all the time I was running a
thing on uh that had to do with errors
following a bell-shaped curve on a
standard distribution error and so what
do I do I generate the data on a
standard distribution error to see what
it looks like and how my code processes
it since that was the Baseline I was
looking for in this we're just doing uh
generating random data for our setup
button here
and we could actually go in
print some of the data up let's just do
this print
we'll do train
samples and we'll just do the first five
pieces of data in there to see what that
looks like and you can see the first
five pieces of data in our train samples
is 49 85 41 68 19 just random numbers
generated in there that's all that is
and we generated significantly more than
that
let's see 50 up here 1000 yeah so
there's 1 000 here 1000 numbers we
generated and we could also if we wanted
to find that out we could do a quick
print the links of it
so or you could do a shape kind of thing
and if you're using numpy
although the link for this is just fine
and there we go it's actually 2100 like
we said in the demo setup in there
I'm going to go ahead and take our
labels oh that was our train labels we
also did samples didn't we
so we could also print
do the same thing
labels
and let's change this to
labels
and labels
and run that just to double check and
sure enough we have 2100 and they're
labeled one zero one zero one zero I
guess that's if they have symptoms or
not one symptoms zero none and so we
want to go ahead and take our train
labels and we'll convert it into a numpy
array and the same thing with our
samples
and let's go ahead and run that
and we also Shuffle this is just a neat
feature you can do in numpy right here
put my drawing thing on which I didn't
have on earlier I can take the data and
I can Shuffle it so we have our so it's
it just randomizes it that's all that's
doing we've already randomized it so
it's kind of an Overkill it's not really
necessary
but if you're doing a larger package
where the data is coming in and a lot of
times it's organized somehow and you
want to randomize it just to make sure
that that you know the input doesn't
follow a certain pattern
that might create a bias in your model
and we go ahead and create a scalar the
scalar range minimum Max scalar feature
range zero to one
and then we go ahead and scale the
scaled train samples so we're going to
go ahead and fit and transform the data
so it's nice and scaled and that is the
age so you can see up here we have 49 85
41. we're just moving that so it's going
to be between 0 and 1. and so this is
true with any of your neural networks
you really want to convert the data to
zero and one otherwise you create a bias
so if you have like a hundred crates of
bias versus
the math behind it gets really
complicated if you actually start
multiplying stuff because a lot of
multiplication Edition going on in there
that higher end value will eventually
multiply down and it will have a huge
bias as to how the model fits it and
then it will not fit as well and then
one of the fun things we can do in
jupyter Notebook is that if you have a
variable you're not doing anything with
it it's the last one on the line it will
automatically print
um and we're just going to look at the
first five samples on here and so it's
going to print the first five samples
and you can see here we go
0.9195.791 so everything's between 0 and
1.
and that just shows us that we scaled it
properly and it looks good it really
helps a lot to do these kind of
print-ups halfway through you never know
what's going to go on there
I don't know how many times I've gotten
down and found out that the data sent to
me that I thought was scaled was not
and then I have to go back and track it
down and figure it out on there
so let's go ahead and create our
artificial neural network
and for doing that this is where we
start diving into tensorflow and cross
tensorflow
if you don't know the history of
tensorflow
it helps to jump into we'll just use
Wikipedia
careful don't quote Wikipedia on these
things because you get in trouble
but it's a good place to start back in
2011 Google brain built disbelief as a
proprietary machine learning setup
tensorflow became the open source for it
so tensorflow is a Google product and
then it became
open sourced and now it's just become
probably one of the de factos when it
comes for neural networks as far as
where we're at so when you see the
tensorflow setup
it's got like a huge following there are
some other setups like the side kit
under the sklearn has their own little
neural network but the tensorflow is the
most robust one out there right now and
cross sitting on top of it makes it a
very powerful tool so we can leverage
both the cross easiness in which we can
build a sequential setup on top of
tensorflow
and so in here we're going to go ahead
and do our input of tensorflow
and then we have the rest of this is all
Karas here from number two down uh we're
going to import from tensorflow the
cross connection and then you have your
tensorflow cross models import
sequential it's a specific kind of model
we'll look at that in just a second if
you remember
from the files that means it goes from
one layer to the next layer to the next
layer there's no funky splits or
anything like that
uh and then we have from tensorflow
across layers we're going to import our
activation and our dense layer
and we have our Optimizer atom
um this is a big thing to be aware of
how you optimize uh your data when you
first do it atoms as good as any atom is
usually there's a number of Optimizer
out there there's about there's a couple
main ones but atom is usually assigned
to bigger data
it works fine usually the lower data
does it just fine but atom is probably
the mostly used but there are some more
out there and depending on what you're
doing with your layers your different
layers might have different activations
on them and then finally down here
you'll see our setup where we want to go
ahead and use the metrics
and we're going to use the tensorflow
cross metrics for categorical cross
entropy so we can see how everything
performs when we're done that's all that
is a lot of times you'll see us go back
and forth between tensorflow and then
scikit has a lot of really good metrics
also for measuring these things again
it's the end of the you know at the end
of the story how good is your model do
and we'll go ahead and load all that and
then comes the fun part I actually like
to spend hours messing with these things
and uh four lines of code you're like oh
you're gonna spend hours on four lines
of code
um no we don't spend hours on four lines
of code that's not what we're talking
about when I say spend hours on four
lines of code uh what we have here let
me explain that in just a second we have
a model and it's a sequential model if
you remember correctly we mentioned the
sequential up here where it goes from
one layer to the next
and our first layer is going to be your
input
it's going to be what they call dense
which is uh usually it's just dense and
then you have your input and your
activation
how many units are coming in we have 16
what's the shape What's the activation
and this is where it gets interesting
because we have in here uh relu
on two of these and softmax activation
on one of these
there are so many different options for
what these mean and how they function
how does the relu how does the softmax
function
they do a lot of different things
um we're not going to go into the
activations in here that is what really
you spend hours doing is looking at
these different activations
um and just some of it is just almost
like you're playing with it
like an artist you start getting a fill
for like a inverse tangent activation or
the 10h activation
takes up a huge processing amount so you
don't see it a lot yet it comes up with
a better solution especially when you're
doing uh when you're analyzing Word
documents and you're tokenizing the
words and so you'll see this shift from
one to the other because you're both
trying to build a better model and if
you're working on a huge data set it'll
crash the system it'll just take too
long to process
and then you see things like soft Max
softmax
generates an interesting
setup
where a lot of these when you talk about
Rayleigh oops let me do this uh regular
there we go relu has a setup where if
it's less than zero it's zero and then
it goes up and then you might have what
they call Lazy setup where it has a
slight negative to it so that the errors
can translate better same thing with
softmax it has a slight laziness to it
so that errors translate better all
these little details
make a huge difference on your model so
one of the really cool things about data
science that I like is you build your
what they call you build to fail and
it's an interesting design setup oops I
forgot the end of my code here
to build a fail is you want the model as
a whole to work so you can test your
model out
so what you can do uh
you can get to the end and you can do
your let's see where was it overshot
down here you can test your test out the
quality of your setup on there
and see where did I do my tensorflow oh
here we go I did it was right above me
there we go we started doing your cross
entropy and stuff like that is you need
a full functional set of code so that
when you run it
you can then test your model out and say
hey it's either this model works better
than this model and this is why and then
you can start swapping in these models
and so when I say spend a huge amount of
time on pre-processing data it's
probably 80 of your programming time
um well between those two it's like 80
20. you'll spend a lot of time on the
models once you get the model down once
you get the whole code and the flow down
set
depending on your data your models get
more and more robust as you start
experimenting with different inputs
different data streams and all kinds of
things and we can do a simple model
summary here
's our sequential here's our layer our
output a parameter
this is one of the nice things about
cross is you just you can see right here
here's our sequential one model boom
boom boom boom everything's set and
clear and easy to read so once we have
our model built the next thing we're
going to want to do is we're going to go
ahead and train that model
and so the next step is of course model
training
and when we come in here this a lot of
times it's just paired with the model
because it's so straightforward it's
nice to print out the model setup so you
can have a tracking
but here's our model
the keyword in Cross is compile
Optimizer atom learning rate another
term right there that we're just
skipping right over that really becomes
the meat of
the setup is your learning rate uh so
whoops I forgot that I had an arrow but
I'll just underline it
a lot of times the learning rate set to
0.0 uh set to 0.01 depending on what
you're doing this learning rate can
overfit and underfit so you'd want to
look up I know we have a number of
tutorials out on overfitting and
underfitting that are really worth
reading once you get to that point in
understanding and we have our loss
sparse categorical cross entropy so this
is going to tell Karas how far to go
until it stops
and then we're looking for metrics of
accuracy so we'll go ahead and run that
and now that we've compiled our model
we want to go ahead and run it fit it so
here's our model fit we have our scale
to train samples
our train labels our validation split in
this case we're going to use 10 of the
data for validation
batch size another number you kind of
play with not a huge difference as far
as how it works
but it does affect how long it takes to
run and it can also affect the bias a
little bit most of the time though a
batch size is between 10 to 100
depending on just how much data you're
processing in there we want to go ahead
and Shuffle it we're going to go through
30 epics
and we'll put a verbose of two and let
me just go ahead and run this and you
can see right here here's our epic
here's our training
here's our loss now if you remember
correctly up here we set the loss let's
see where was it
compiled our data
there we go loss so it's looking at the
sparse categorical cross entropy this
tells us that as it goes how how much
how much does the error go down is the
best way to look at that and you can see
here the lower the number the better it
just keeps going down and vice versa
accuracy we want let's see where's my
accuracy
value accuracy at the end and you can
see 0.619 0.69 0.74 it's going up we
want the accuracy would be ideal if I
made it all the way to one but we also
the loss is more important because it's
a balance you can have 100 accuracy and
your model doesn't work because it's
overfitted
again you will look up overfitting and
underfitting models
and we went ahead and went through 30
epics it's always fun to kind of watch
your code going
to be a honest I usually uh the first
time I run it I'm like oh that's cool I
get to see what it does and after the
second time of running it I'm like I
like to just not see that and you can
request those of course in your code
repress the warnings in the printing
and so the next step is going to be
building a test set and predicting it
now so here we go we want to go ahead
and build our test set and we have just
like we did our training set
a lot of times you just split your your
initial setup but we'll go ahead and do
a separate set on here
and this is just what we did above
there's no difference as far as
the randomness that we're using to build
this set on here the only difference is
that
we are already did our scalar up here
well it doesn't matter because the data
is going to be across the same thing but
this should just be just transformed
down here instead of fit transform
because you don't want to refit your
data on your testing data
oh
there we go now we're just transforming
it because you never want to transform
the test data easy mistake to make
especially on an example like this where
we're not doing uh you know we're
randomizing the data anyway so it
doesn't matter too much because we're
not expecting something weird
and then we went ahead and do our
predictions the whole reason we built
the model is we take our model we
predict and we're going to do here's our
X scale data batch size 10 verbose and
now we have our predictions in here and
we could go ahead and do a
oh we'll print
predictions
and then I guess I could just put down
predictions in five so we can look at
the first five of the predictions
and what we have here is we have our age
and the prediction on this age versus
what is what we think it's going to be
but what we think is going to they're
going to have symptoms or not and the
first thing we notice is it's hard to
read because we really want a yes no
answer so we'll go ahead and just round
off the predictions
using the ARG Max the numpy ARG Max for
prediction so it just goes to a zero one
and if you remember this is a Jupiter
notebook so I don't have to put the
print I can just put in rounded
predictions and we'll just do the first
five and you can see here zero one zero
zero zero so that's what the predictions
are that we have coming out of this is
no symptoms symptoms no symptoms
symptoms no symptoms and just as we were
talking about at the beginning we want
to go ahead and take a look at this
there we go confusion matrixes for
accuracy check
most important part
when you get down to the end of the
story how accurate is your model before
you go and play with the model and see
if you can get a better accuracy out of
it and for this we'll go ahead and use
the site kit the SK learn metrics site
kit being where that comes from
import confusion Matrix some iteration
tools and of course a nice matplot
Library
that makes a big difference so it's
always nice to um
I have a nice graph to look at pictures
worth a thousand words
um and then we'll go ahead and call it
CM for confusion Matrix y true equals
test labels y predict rounded
predictions
and we'll go ahead and load in our cm
and I'm not going to spend too much time
on the plotting going over the different
plotting code
you can spend like whole we have whole
tutorials on how to do your different
plotting on there but we do have here is
we can do a plot confusion Matrix
there's our CM our classes normalize
false title confusion Matrix cmap is
going to be in blues
and you can see here we have uh to the
nearest cmap titles all the different
pieces whether you put tick marks or not
the marks the classes the color bar
so a lot of different information on
here as far as how we're doing the
printing of the of the confusion Matrix
you can also just dump the confusion
Matrix into a Seaborn and real quick get
an output it's worth knowing how to do
all this when you're doing a
presentation to the shareholders you
don't want to do this on the Fly you
want to take the time to make it look
really nice like our guys in the back
did and let's go ahead and do this I
forgot to put together our CM plot
labels
we'll go ahead and run that
and then we'll go ahead and call the
little the definition
for our mapping
and you can see here plot confusion
Matrix that's our the little script we
just wrote and we're going to dump our
data into it so our confusion Matrix our
classes title confusion Matrix and let's
just go ahead and run that
and you can see here we have our basic
setup no side effects 195 had side
effects 200 no side effects that had
side effects so we predicted the 10 of
them
who actually had side effects and that's
pretty good I mean I don't know about
you but you know that's five percent
error on this and this is because
there's 200 here that's where I get five
percent is divide these both by by two
and you get five out of a hundred uh you
can do the same kind of math up here not
as quick on the flags is 15 and 195 not
an easily rounded number but you can see
here where they have 15 people
who predicted to have no uh with the no
side effects but had side effects
kind of set up on there and these
confusion Matrix are so important at the
end of the day this is really where
where you show whatever you're working
on comes up and you can actually show
them hey this is how good we are or not
how messed up it is
so this was a uh I spent a lot of time
on some of the parts but you can see
here is really simple uh we did the
random generation of data but when we
actually built the model coming up here
here's our model summary
and we just have the layers on here that
we built with our model on this and then
we went ahead and trained it and ran the
prediction
now we get a lot more complicated let me
flip back on over here because we're
going to do another demo
so that was our basic introduction to it
we talked about the uh there we go okay
so implementing a neural network with
Karas after creating our samples and
labels we need to create our cross
neural network model we will be working
with a sequential model which has three
layers and this is what we did we had
our input layer our hidden layers and
our output layers and you can see the
input layer coming in was the age Factor
we had our hidden layer and then we had
the output are you going to have
symptoms or not so we're going to go
ahead and go with something a little bit
more complicated
training our model is a two-step process
we first compile our model and then we
train it in our training data set so we
have compiling compiling converts the
code into a form of understandable by
machine
we use the atom in the last example a
gradient descent algorithm to optimize a
model and then we trained our model
which means it let it learn on training
data
and I actually had a little backwards
there but this is what we just did is we
if you remember from our code we just
had let me go back here
's our model that we created
summarized uh we come down here and we
compile it
so it tells it hey we're ready to build
this model and use it and then we train
it and this is the part where we go
ahead and fit our model and and put that
information in here and it goes through
the training on there and of course we
scaled the data which was really
important to do and then you saw we did
the creating a confusion Matrix with
Karas as we are performing
classifications on our data we need a
confusion Matrix to check the results a
confusion Matrix breaks down the various
misclassifications as well as correct
classifications to get the accuracy
and so you can see here this is what we
did with the true positive false
positive true negative false negative
and that is what we went over let me
just scroll down here
on the end we printed it out and you see
we have a nice printout of our confusion
Matrix with the true positive false
positive false negative true negative
and so the blue ones we want those to be
the biggest numbers because those are
the better side and then uh we have our
false predictions on here as far as this
one so I had no side effects but we
predicted
let's see no side effects predicting
side effects and vice versa if getting
your learning started it's half the
battle what if you could do that for
free visit skillup by simply learn click
on the link in the description to know
more
now uh saving and loading models with
Karas we're going to dive into a more
complicated demo
uh and you're gonna say oh that was a
lot of complication before well if you
broke it down we randomized some data we
created the cross setup we compiled it
we trained it we predicted and we ran
our Matrix
uh so we're going to dive into something
a lot a little bit more fun is we're
going to do a face mask detection with
Karas so we're going to build a cross
model to check if a person is wearing a
mask or not in real time and this might
be important if you're at the front of a
store this is something today which is
um
might be very useful as far as some of
our you know making sure people are safe
and so we're going to look at mask and
no mask and let's start with a little
bit on the data
and so in my data I have with a mask and
you can see they just have a number of
images showing the people in masks and
again if you want some of this
information contact simply learn and
they can send you some of the
information as far as people with and
without masks so you can try it on your
own and this is just such a wonderful
example of this setup button here
so before I dive into the mass detection
talk about being in the current with
covid and seeing if people are wearing
masks this particular example I had to
go ahead and update to a python 3.8
version it might run in a three seven
I'm not sure I haven't I kind of skipped
three seven and installed three eight
so I'll be running in a three Python 3 8
and then you also want to make sure your
tensorflow is up to date because the the
call functional
uh layers with that's where they split
if you remember correctly from back uh
oh let's take a look at this remember
from here the functional model in a
functional layer allows us to feed in
the different layers into different you
know different nodes into different
layers and split them a very powerful
tool very popular right now in the edge
of where things are with neural networks
and creating a better model so I've
upgraded to python 3.8 and let's go
ahead and open that up and go through
our next example which includes multiple
layers
programming it to recognize whether
someone wears a mask or not and then
saving that model so we can use it in
real time so we're actually almost a
full
end-to-end development of a product here
of course this is a very simplified
version and it'd be a lot more to it
you'd also have to do like recognizing
whether it's someone's face or not all
kinds of other things go into this
so let's go ahead and jump into that
code and we'll open up a new Python 3
oops python3
it's working on it there we go
um and then we want to go ahead and
train our mask we'll just call this
train mask
and we want to go ahead and train mask
and save it
so it's it's uh it's save mask train
mask detection not to be confused with
masking data a little bit different
we're actually talking about our
physical mask on your face
and then from the cross standpoint we
got a lot of imports to do here
and I'm not going to dig too deep on the
Imports we're just going to go ahead and
notice a few of them so we have in here
oops let me go alt D there we go
have something to draw with a little bit
here we have our image processing
and the image processing right here let
me underline that
deals with how do we bring images in
because most images are like a a square
grid and then each value in there has
three values for the three different
colors cross and tensorflow do a really
good job of working with that so you
don't have to do all the heavy listening
and figuring out what's going to go on
uh and then we have the mobile net
average pooling 2D this again is
how do we deal with the images and
pulling them dropout's a cool thing
worth looking up if you haven't when as
you get more and more into cross and
tensorflow it'll Auto drop out certain
nodes that way you'll get a better
um the notes just kind of die and they
find that they actually create more of a
bias and help and they also add
processing time so they remove them and
then we have our flattened that's where
you take that huge array with the three
different colors and you find a way to
flatten it so it's just a
one-dimensional array instead of a two
by two by three
dense input we did that in the other one
so that should look a little familiar
oops there we go our input our model
again these are things we had on the
last one here's our Optimizer with our
atom
um we have some pre-processing on the
input that goes along with bringing in
the data in more pre-processing with
image to array loading the image this
stuff is so nice it looks like a lot of
works you have to import all these
different modules in here but the truth
is is it does everything for you you're
not doing a lot of pre-processing you're
letting the software do the
pre-processing
um and we're gonna be working with the
setting something to categorical
again that's just a conversion from a
number to a category 0 1 doesn't really
mean anything it's like true false
um label binarizer the same thing uh
we're changing our labels around and
then there's our train test split
classification report
our IM utilities let me just go ahead
and scroll down here a notch for these
this is something a little different
going on down here this is not part of
the tensorflow or the SK learn this is
the site kit setup and tensorflow above
the path this is part of opencv and
we'll actually have another tutorial
going out with the opencv so if you want
to know more about opencv you'll get a
glance on it in this software especially
the net the second piece when we reload
up the data and hook it up to a video
camera we're going to do that on this
round
but this is part of the opencv thing and
you'll see CV2 is usually how that's
referenced but the IM utilities has to
do with how do you rotate pictures
around and stuff like that
and resize them and then the matplot
library for plotting because it's nice
to have a graph tells us how good we're
doing and then of course our numpy
numbers array and just a straight OS
access wow so that was a lot of imports
uh like I said I'm not going to spend I
spend a little time going through them
but we didn't want to go too much into
them
and then I'm going to create some
variables that we need to go ahead and
initialize we have the learning rate
number of epics to train for and the
batch size and if you remember correctly
we talked about the learning rate to the
negative
4.0001 a lot of times it's 0.001 or
0.001
usually it's in that variation depending
on what you're doing and how many epics
and they kind of play with the epics the
epics is how many times we're going to
go through all the data
now I have it as two um the actual setup
is for 20 and 20 works great the reason
I have it for two is it takes a long
time to process one of the downsides of
Jupiter
is that Jupiter isolates it to a single
kernel so even though I'm on an eight
core processor with 16 dedicated threads
only one thread is running on this no
matter what so it doesn't matter so it
takes a lot longer to run even though
tensorflow really scales up nicely and
the batch size is how many pictures do
we load at once in process again those
are numbers you have to learn to play
with depending on your data and what's
coming in and the last thing we want to
go ahead and do is there's a directory
with a data set we're going to run
uh and this just has images of mass and
not masks
and if we go in here you'll see data set
and you have pictures with mass they're
just images of people with mass on their
face and then we have the opposite let
me go back up here without masks so it's
pretty straightforward they look kind of
Askew because they tried to format them
into very similar setup on there so
they're they're mostly squares you'll
see some that are slightly different on
here
and that's kind of important thing to do
on a lot of these data sets
get them as close as you can to each
other and we'll we actually will run in
the in this processing of images up here
and the cross layers and importing and
dealing with images it does such a
wonderful job of converting these at a
lot of it we don't have to do a whole
lot with
so you have a couple things going on
there
and so we're now going to be this is now
loading the images and let me see
and we'll go ahead and create data and
labels here's our here's the features
going in which is going to be our
pictures and our labels going out and
then for categories in our list
directory directory and if you remember
I just flashed that at you it had a face
mask or or no face mask those are the
two options
and we're just going to load into that
we're going to append the image itself
and the labels so we're just create a
huge array and you can see right now
this could be an issue if you had more
data at some point
thankfully I have a 32 gig hard drive or
Ram
even that doesn't you could do with a
lot less of that probably under 16 or
even eight gigs would easily load all
this stuff
and there's a conversion going on in
here I told you about how we are going
to convert the size of the image so it
resizes all the images that way our data
is all identical the way it comes in
and you can see here with our labels we
have without mask without mask without
mask
the other one would be with mask those
are the two that we have going in there
and then we need to change it to the one
not hot encoding
and this is going to take our
up here we had was it labels and data we
want the labels to be categorical so
we're going to take labels and change it
to categorical and our labels then equal
a categorical list
we'll run that and again if we do uh
labels and we just do the last or the
first 10. let's do the last 10 just
because
minus 10 to the end there we go just so
we can see where the other side looks
like we now have one that means they
have a mask one zero one zero so on
one being they have a mask and zero no
mask
and if we did this in reverse
I just realize that this might not make
sense if you've never done this before
let me run this
0 1.
so 0 is do they have a mask on zero do
they not have a mask on one so this is
the same as what we saw up here without
mask one equals the second value is
without mass so with mass without mask
and that's just a with any of your data
processing
we can't really zero if you have a zero
one output
it causes issues as far as training and
setting it up so we always want to use a
one hot encoder if the values are not
actual linear value or regression values
they're not actual numbers
if they represent a thing
and so now we need to go ahead and do
our trainex test X train y test y train
split test data
and we'll go ahead and make sure it's
going to be random and we'll take 20
percent of it for testing and the rest
for setting it up as far as training
their model
this is something that's become so cool
when they're training these set they
realize we can augment the data what
does augment mean well if I rotate the
data around and I zoom in I zoom out I
rotate it share it a little bit flip it
horizontally
fill mode as I do all these different
things to the data it is able to it's
kind of like increasing the number of
samples I have so if I have all these
perfect samples what happens we only
have part of the face or the face is
tilted sideways or all those little
shifts cause a problem if you're doing
just a standard set of data so we're
going to create an augment in our image
data generator which is going to rotate
zoom and do all kinds of cool things and
this is worth looking up this image data
generator and all the different features
it has
um a lot of times I'll the first time
through my models I'll leave that out
because I want to make sure there's a
thing we call build to fail which is
just cool to know you build the whole
process and then you start adding these
different things in uh so that you can
better train your model and so we go and
run this and then we're going to load
um and then we need to go ahead and you
probably would have gotten an error if
you hadn't put this piece in right here
I haven't run it myself because the guys
in the back did this we take our base
model and one of the things we want to
do is we want to do a mobile net V2 and
this will be this is a big thing right
here include the top equals false
a lot of data comes in with a label on
the top row so we want to make sure that
that is not the case and then the
construction of the head of the model
that will be placed on the top of the
base model
we want to go ahead and set that up
and you'll see a warning here I'm kind
of ignoring the warning because it has
to do with the size of the pictures and
the weights for input shape
um so they'll switch things to defaults
just saying hey we're going to Auto
shape some of this stuff for you you
should be aware of that with this kind
of imagery we're already augmenting it
by moving it around and flipping it and
doing all kinds of things to it so
that's not a bad thing in this but
another data it might be if you're
working in a different domain
and so we're going to go back here and
we're going to have we have our base
model we're going to do our head model
equals our base model output
and what we've got here is we have an
average pooling 2D pool size 77 head
model
head model flatten so we're flattening
the data so this is all processing and
flattening the images and the pooling
has to do with some of the ways it can
process some of the data we'll look at
that a little bit when we get down to
the lower level in this processing it
and then we have our dense we've already
talked a little bit about a dense just
what you think about and then the head
model has a drop out of 0.5
what we can do with a drop out
the Dropout says that we're going to
drop out a certain amount of nodes while
training so when you actually use the
model
it will use all the nodes but this drops
certain ones out and it helps stop
biases from performing so it's really a
cool feature in here they discovered
this a while back we have another dense
mode and this time we're using soft Max
activation lots of different activation
options here softmax is a real popular
one for a lot of things so is a relu
and you know there's we could do a whole
talk on activation formulas uh and why
what their different uses are and how
they work
when you first start out you'll you'll
use mostly the relu and the softmax for
a lot of them uh just because they're
they're some of the basic setups it's a
good place to start
and then we have our model equals model
inputs equals base model dot input
outputs equals head model so again we're
still building our model here we'll go
ahead and run that
and then we're going to Loop over all
the layers in the base model and freeze
them so they will not be updated during
the first training process
so for layer and base model layers
layers.trainable equals false
a lot of times when you go through your
data you want to kind of jump in part
way through I I'm not sure why in the
back they did this for this particular
example but I do this a lot when I'm
working with series and and specifically
in stock data I wanted to iterate
through the first set of 30 Data before
it does anything
um I would have to look deeper to see
why they froze it on this particular one
and then we're going to compile our
model so compiling the model atom
init layer Decay
initial learning rate over epics and we
go ahead and compile our loss is going
to be the binary cross entropy which
will have that print out Optimizer for
opt metrics is accuracy
same thing we had before not a huge jump
as far as the previous code
and then we go ahead and we've gone
through all this and now we need to go
ahead and fit our model so train the
head of the network print info training
head run
now I skipped a little time because
until you'll see the run time here is at
80 seconds per epic takes a couple
minutes for it to get through on a
single kernel
one of the things I want you to notice
on here while we're well it's finishing
the processing
is that we have up here our augment
going on so anytime the trainex and
trading y go in there's some Randomness
going on there it is jiggling it around
what's going into our setup of course
we're batch sizing it so it's going
through whatever we set for the batch
values how many we process at a time
and then we have the steps per epic uh
the trainx the batch size validation
data here's our test X and Test Y where
we're sending that in
uh and this again it's validation one of
the important things to know about
validation is our
um when both our training data and our
test data have about the same accuracy
that's when you want to stop that means
that our model isn't biased if you have
a higher accuracy on your testing you
know you've trained it and your accuracy
is higher on your actual test data then
something in there is probably has a
bias and it's overfitted
so that's what this is really about
right here with the validation data and
validation steps
so it looks like it's let me go ahead
and see if it's done processing looks
like we've gone ahead and gone through
two epics again you could run this
through about 20 with this amount of
data and it would give you a nice
refined model at the end we're going to
stop at two because I really don't want
to sit around all afternoon and I'm
running this on a single thread so now
that we've done this we're going to need
to evaluate our model and see how good
it is and to do that we need to go ahead
and make our predictions
these are predictions on our test X to
see what it thinks they're going to be
so now it's going to be evaluating the
network and then we go ahead and go down
here
and we will need to uh turn the index in
because remember it's it's either 0 or 1
it's a zero one zero one so you have two
outputs uh not wearing uh wearing a mask
not wearing a mask and so we need to go
ahead and take that argument at the end
and change those predictions to a zero
or one coming out and then to finish
that off we want to go ahead and let me
just put this right in here and do it
all in one shot we want to show a nicely
formatted classification report so we
can see what that looks like on here
and there we have it we have our
Precision uh it's 97 with the mask
there's our F1 score support without a
mask 97 percent so that's pretty high
high setup on there you know you three
people are going to sneak into the store
who are without a mask and it thinks
they have a mask and there's going to be
three people with a mask that's going to
flag the person at the front to go oh
hey look at this person you might not
have a mask that's if I guess it's a
setup in front of a store
um so there you have it and of course
one of the other cool things about this
is if someone's walking in to the store
and you take multiple pictures of them
um you know this is just an it would be
a way of flagging and then you can take
that average of those pictures and make
sure they match or don't match if you're
on the back end and and this is an
important step because we're gonna this
is just cool I love doing this stuff uh
so we're going to go ahead and take our
model and we're going to save it so
model save massdetector.model we're
going to give it a name we're going to
save the format in this case we're going
to use the H5 format
and so this model we just programmed has
just been saved so now I can load it up
into say another program what's cool
about this is let's say I want to have
somebody work on the other part of the
program well I just save the model they
upload the model now they can use it for
whatever and then if I get more
information and we start working with
that at some point
I might want to update this model and
make a better model and this is true of
so many things where I take this model
and maybe I'm running a prediction on
making money for a company and as my
model gets better
I want to keep updating it and then it's
really easy just to push that out to the
actual end user and here we have a nice
graph you can see the training loss and
accuracy as we go through the epics we
only did the you know only shows just
the one Epic coming in here but you can
see right here as the
value loss train accuracy and value
accuracy
it starts switching and they start
converging and you'll hear converging
this is the convergence they're talking
about when they say you're you're I know
when I work in the site kit with sklearn
neural networks this is what they're
talking about a convergence is our loss
and our accuracy come together and also
up here and this is why I'd run it more
than just two epics as you can see they
still haven't converged all the way
so that would be a cue for me to keep
going
but what we want to do is we want to go
ahead and create a new Python 3
program
and we just did our train mask so now
we're going to go ahead and import that
and use it and show you in a live action
get a view of both myself in the
afternoon along with my background of an
office which is in the middle still of
reconstruction for another month
and we'll call this a mask
detector
and then we're going to grab a bunch of
a few items coming in
uh we have our
mobilenet V2 import pre-processing input
so we're still going to need that we
still have our tensorflow image to array
we have our load model that's where most
of the stuff's going on
this is our CV2 or opencv again I'm not
going to dig too deep into that we're
going to flash a little opencv code at
you and we actually have a tutorial on
that coming out
our numpy array our IM utilities which
is part of the opencv or CV2 setup
and then we have of course time and just
our operating system so those are the
things we're going to go ahead and set
up on here and then we're going to
create
this takes just a moment
our module here which is going to do all
the heavy lifting so we're going to
detect and predict the mask we have
frame face net Mass net
these are going to be generated by our
opencv we have our frame coming in and
then we want to go ahead and create a
mask around the face it's going to try
to detect the face and then set that up
so we know what we're going to be
processing through our model
um and then there's a frame shape here
this is just our height versus width
that's all each W stands for they've
called it blob which is a CV2 DNN blob
form image frame so this is reformatting
this Frame that's going to be coming in
literally from my camera and we'll show
you that in a minute that little piece
of code that shoots that in here
and we're going to pass the blob through
the network and obtain the face
detections so facenet.set in Port blob
detections face net forward
print detections shape
so this is this is what's going on here
this is that model we just created we're
going to send that in there and I'll
show you in a second where that is but
it's going to be under face net
and then we go ahead and initialize our
list of faces they're corresponding
locations and the list of predictions
from our face mask Network
we're going to Loop over the detections
and this is a little bit more work than
you think as far as looking for
different faces what happens if you have
a crowd of faces
so We're looping through the detections
and the shapes going through here
and probability associated with the
detection here's our confidence of
detections
we're going to filter out weak detection
by ensuring the confidence is greater
than the minimum confidence
so we've said it remember zero to one so
0.5 would be our minimal confidence
probably is pretty good
um
and then we're going to put in compute
bounding boxes for the object if I'm
zipping through this it's because we're
going to do an opencv and I really want
to stick to just the cross part
and so I'm just kind of jumping through
all this code you can get a copy of this
code from Simply learn and take it apart
or look for the opencv coming out
and we'll create a box the box sets it
around the image
ensure the bounding boxes fall within
the dimensions of the frame
so we create a box around what's going
to we hope it's going to be the face
extract the face Roi convert it from BGR
to RGB Channel
again this is an opencv issue not really
an issue but it has to do with the order
I don't know how many times I've
forgotten to check the order colors
we're working with opencv
because there's all kinds of fun things
when red becomes blue
and blue becomes red uh and we're going
to go ahead and resize it process it
frame it face frame setup again the face
the CBT color we're going to convert it
we're going to resize it image to array
pre-process the input pin the face
locate face start x dot y and x boy that
was just a huge amount and I skipped
over a ton of it but the bottom line is
we're building a box around the face and
that box because the opencv does a
decent job of finding the face and that
box is going to go in there and see hey
does this person have a mask on it
uh and so that's what that's what all
this is doing on here and then finally
we get down to this where it says
predictions equals massnet.predict faces
batch size 32. so these different images
where we're guessing where the face is
are then going to go through and
generate an array of faces if you will
and we're going to look through and say
does this face have a mask on it and
that's what's going right here is our
prediction that's the big thing that
we're working for
and then we return the locations and the
predictions the location just tells
where on the picture it is and then the
prediction tells us what it is is it a
mask or is it not a mask
all right so we've loaded that all up
so we're going to load our serialized
face detector model from disk
and we have our the path that it was
saved in obviously you're going to put
in a different path depending on where
you have it or however you want to do it
and how you saved it on the last one
where we trained it
and then we have our weights path
and so finally our face net here it is
equals
cv2.dnn dot read net
prototext path awaits path and we're
going to load that up on here so let me
go ahead and run that
and then we also need to I'll just put
it right down here I always hate
separating these things in there and
then we're going to load the actual mass
detector model from disk this is the the
model that we saved so let's go ahead
and run that on there also so this is
pulling in all the different pieces we
need for our model and then the next
part is we're going to create open up
our video
and this is just kind of fun because
it's all part of the opencv
the video setup
let me just put this all in as one
there we go so we're going to go ahead
and open up our video we're going to
start it and we're going to run it until
we're done
and this is where we get some real like
kind of live action stuff which is fun
this is what I like to work about with
images and videos is that when you start
working with images and videos it's all
like right there in front of you it's
Visual and you can see what's going on
uh so we're going to start our video
streaming this is grabbing our video
stream Source zero start
that means it's grabbing my main camera
I have hooked up
and then you know starting video you're
going to print it out
here's our video Source equals zero
start Loop over the frames from the
video stream
oops a little redundancy there let me
close
I'll just leave it that's how they had
it in the code so so while true we're
going to grab the frame from the
threaded video stream and resize it to
have the maximum width of 400 pixels so
here's our frame we're going to read it
from our visual stream
we're going to resize it
and then we have a returning remember we
return from the our procedure the
location and the prediction so detect
and predict mask we're sending it the
frame we're sending it the face net and
The Mask net so we're sending all the
different pieces that say this is what's
going through on here
and then it returns our location and
predictions and then for our box and
predictions in the location and
predictions
and the boxes is again this is an open
CV set that says hey this is a box
coming in from the location
because you have the two different
points on there
and then we're going to unpack the box
and predictions and we're going to go
ahead and do mask without a mask equals
prediction
we're going to create our label no mask
and create color if the label equals
mask l0 225 and you know this is going
to make a lot more sense when I hit the
Run button here but we have the
probability of the label
we're going to display the label
bounding box rectangle on the output
frame
and then we're going to go ahead and
show the output from the frame CV2 IM
show frame frame and then the key equals
CV2 weight key one we're just going to
wait till the next one comes through
from our feed
and we're going to do this until we hit
the stop button pretty much
so are you ready for this to see if it
works we've distributed our our model
we've loaded it up into our distributed
code here we've got it hooked into our
camera and we're going to go ahead and
run it
and there it goes it's going to be
running and we can see the data coming
down here and we're waiting for the
pop-up
and there I am in my office with my
funky headset on
uh and you can see in the background my
unfinished wall and it says up here no
mask oh no I don't have a mask on I
wonder if I cover my mouth
what would happen
you can see my no mask
goes down a little bit I wish I brought
a mask into my office it's up at the
house but you can see here that this
says you know there's a 95 98 percent
chance that I don't have a mask on and
it's true I don't have a mask on right
now and this could be distributed this
is actually an excellent little piece of
script that you can start you know you
install somewhere on a video feed on a
on a security camera or something and
then you'd have this really neat setup
saying hey do you have a mask on when
you enter a store or a public
transportation or whatever it is where
they're required to wear a mask
let me go ahead and stop that
now if you want a copy of this code
definitely give us a holler we will be
going into opencv and another one so I
skipped a lot of the opencv code in here
as far as going into detail
really focusing on the cross saving the
model uploading the model and then
processing a streaming video through it
so you can see that the model works we
actually have this working model that
hooks into the video camera
which is just pretty cool and a lot of
fun
so I told you we're going to dive in and
really Roll Up Our Sleeve and do a lot
of coding today we did the basic demo up
above for just pulling it across and
then we went into a cross model where we
pulled in data to see whether someone
was wearing a mask or not so very useful
in today's world as far as a fully
running application sequential models in
Karas
so what is Karas cross is a high level
python deep learning API
which is used for easy implementation of
neural networks it has multiple
low-level back-ends like tensorflow
thano Pi torch Etc which are used for
fast computation
so you could think of this as cross
being almost its own little programming
language and then it sits on neural
networks in this case uh the ones listed
were tensorflow thano and pytorch which
can all integrate with the cross model
this makes a very diverse and also makes
it very easy to use and switch around
with different things cross is very user
friendly as far as neural network
software goes as a high level API
computational graphs so computational
graphs are really the heart and soul of
neural networks we talk about a
computational graph there are a visual
representation of expressing and
evaluating mathematical equations the
nodes and data flow in a graph
correspond to mathematical operations
and variables
you'll hear a lot some of the terms you
might hear are node and Edge The Edge
being the data flow in this case it
could also represent an actual value
they have oh I think in spark they have
a graph x which works just on Computing
edges there's all kinds of stuff that
has evolved from computational graphs
we're focusing just on Karas and on
neural networks so we're not going to go
into great detail on everything a
computational graph does it is the core
component of a neural network is what's
important to know on this
so cross offers a python user-friendly
front-end while maintaining a strong
computation Power by using a low level
API like tensorflow Pi torch Etc which
use computational graphs as a back end
so one this allows for abstraction of
complex problems while specifying
control flow
if you've ever looked at some of the
back end or the original versions of
tensorflow it's really a nightmare you
have all these different settings you
have to put in there and create it's a
lot a lot of back-end programming this
is like the old computers when you had
to tell it how to dispose of a variable
and how to properly re-allocate the
memory for use
all that is covered nowadays in our
higher level programming well this is
the same thing with cross is it covers a
lot of this stuff and does things for
you that you would could spend hours on
just trying to figure out
it's useful for calculating derivatives
by using back propagation we're
definitely not going to teach a class on
derivatives in this little video but
understanding a derivative is the rate
of change so if you have a particular
function you're using in your neural
network a lot of them is just simple y
equals MX plus b your euclidean geometry
where you just have a simple slope times
the intercept and they get very
complicated they have the inverse
tangent function for Activation as
opposed to just a linear euclidean model
and you can think about this as you have
your data coming in and you have to
alter it somehow well you alter it going
down to get an answer you end up with an
error and that error goes back up and
you have to have that back propagation
with the derivative you want to know how
it changed so that you can figure out
how to adjust it for the error
a lot of that's hidden so you don't even
have to worry about it with Karas and in
today's cross it'll even if you create
your own formula for computing an answer
it will automatically compute the back
prop the the derivative for you in a lot
of cases
it's easier to implement distributed
computation so cross is really nice way
to package it and get it off on
different computers and share it and it
allows parallelism which means that two
operations can run simultaneously
so as we start developing these back
ends it can do all kinds of cool things
and utilize multiple cores gpus on a
computer to get that parallel processing
up
what are neural networks
well like I said there are already we've
talked about in computational edges you
have a node and you have a connection or
your Edge so neural networks are
algorithms fashioned after the human
brain which contain multiple layers each
layer contains a node called a neuron
which performs a mathematical operation
they break down complex problems into
simple operations
so one an input layer takes in our data
and pre-processes it when we talk about
pre-processing when you're dealing with
neural networks you usually have to
pre-process your data so that it's
between
-1 and 1 or 0 and 1 into some kind of
value that's usable that occurs before
it gets to the neural network in fact 80
percent of data science is usually in
prepping that data and getting it ready
for your different models
two you have hidden layer performs a
non-linear transformation of input now
it can do a hidden a linear
transformation it can use just a basic
euclidean geometry and you could think
of a node adding all the different
connections coming in so each connection
would have a weight and then it would
add to that weight plus an Intercept in
the node itself so you can actually use
euclidean geometry but a lot of these
get really complicated they have all
these different formulas and they're
really cool to look at but when you
start looking at them look at how they
work you really don't need to know the
high math behind it to figure them out
and figure out what they're doing
which is really cool that means a lot of
people can use this without having to go
get a PhD in mathematics
number three the output layer takes the
results from hidden layer transform them
and gives a final output
so sequential models so what makes this
a sequential model sequential models are
linear stacks of layers where one layer
leads to the next it is simple and easy
to implement and you just have to make
sure that the previous layer is the
input to the next layer
so you have used for plain stack of
layers where each layer has one input
and one output tensor and this is what
tensorflow is named after is each one of
these layers is like a tensor each node
is a tensor and then the layer is also
considered a tensor of values
and it's used for simple classifier
declassifier models you can it's also
used for regression models too so it's
not just about this is something this is
a teapot this is a cat this is a dog
it's also used for generating
regret the actual values you know this
is worth ten dollars that's worth thirty
dollars the weather is going to be 90
degrees out or whatever it is so you can
use it for both classifier and
declassifier models
and one more note when we talk about
sequential models the term sequential is
used a lot and it's used in different
areas in different notations when you're
in data science so when we talk about
time series we'll talk about sequential
that is something very different uh
sequential in this case means it goes
from the input to layer 1 to layer 2 to
the output so it's very directional it's
important to note this because if you
have a sequential model can you have a
non-sequential model and the answer is
yes if you master the basics of a
sequential model you can just as easily
have another model that shares layers
you can have another model where you
have an input coming in and it splits
and then you have one set that's doing
one set of nodes maybe they're doing a
yes no kind of node where it's either
putting out a 0 or a one a classifier
and the other one might be regression
it's just processing numbers and then
you recombine them for the output that's
what they call across the cross API
so there's a lot of different
availabilities in here and all kinds of
cool things you can do as far as
encoding and decoding and all kinds of
things and you can share layers and
things like that
we're just focusing on the basic cross
model with the sequential model
so let's dive into the meat of the
matter let's do and do a demo on here
today's demo in this demo we'll be
performing flower classification using
sequential model and cross and we'll use
our model to classify between five
different types of flowers
now for this demo and you can do this
demo whatever platform you want or
whatever
user interface for developing python I'm
actually using anaconda and then I'm
using Jupiter notebooks to develop in
and if you're not familiar with this you
can go under environment once you've
created an environment you can come in
here to open a terminal window and if
you don't have the different modules in
here you can do your conda install
whatever module it is
just happened that this particular setup
didn't have a Seaborn in it which I
already installed
so here's our anaconda and then I'm
going to go back
and start up my Jupiter notebook
where I already created a new python
project Python 3 come in Python 3.8 on
this particular one
sequential model for flowers
so lots of fun there so we're going to
jump right into this the first thing is
to make sure you have all your modules
installed
so if you don't have numpy pandas
matplot library in Seabourn in the cross
and sklearn or site kits not actually
sklearn you'll need to go ahead and
install all of those
now having done this for years and
having switched environments and doing
different things
I get all my imports done and then we
just run it and if we get an error we
know we have to go back and install
something
um right off the bat though we have
numpy pandas matplot Library Seaborn
these are built on top of each other
append is a data frame and built on top
of numpy the
um data array
and then we bring in our SK learn or
scikit this is the site kit setup SCI
kit even though you use sklearn to bring
it in it's a side kit and then our cross
we have our pre-processing the images
image data generator
or model this is our basic model our
sequential model
and then we bring in from Cross layers
import dents optimizers
these optimizers a lot of them already
come in these are your different
optimizers and it's almost a lot of this
is so automatic now Adam
is the a lot of times the default
because you're dealing with a large data
and then we get our SGD which is uh
smaller data does better on smaller
pieces of data and I'm not going to go
into all of these uh different
optimizers we didn't even use these in
the actual demo you just have to be
aware that they are different optimizers
and the Digger the more you dig into
these models you'll hit a point where
you do need to play with these a little
bit but for the most part leave it at
the default when you're first starting
out
and we're doing just the sequential
you'll see here layers dense
and then if we come down a little bit
more when they put this together and
they're running the dense layers you
also see they have drop out they have
flattened they have activation they have
the convolutional layer 2D Max pooling
2D batch normalization
what are all these layers and we get to
the model we're going to talk about them
a lot of times when you're just starting
you can just import cross dot layers and
then you have your drop out your flatten
your convolutional neural network 2D
and we'll we'll cover what these do in
the actual example when we get down
there what I want you to take from here
though is you need to run your Imports
and load your different aspects of this
and of course your tensorflow TF because
this is all built on tensorflow
and then finally import random as RN
just for random generation
and then we get down here we have our
CV2
that is your open image or your opencv
they call it for processing images
that's what the cvd 2 is
uh we have our tqdm
the TQ DM is for is a progress bar just
a fancy way of adding when you're
running a process you can view the bar
going across in the Jupiter setup not
really necessary but it's kind of fun to
have
um we want to be able to shuffle some
files again these are all different
things pills and other
um
image processor it goes with the CV2 a
lot of times you'll see both of those
and so we run those we got to bring them
all in
and the next thing is to set up our
directories
and so we come into the directories
there's an important thing to note on
here other than we're looking at a lot
of flowers which is fun
uh as we get down here we have our
directory archive flowers that just
happens to be where the different files
for different flowers are put in
we're denoting an X and a z and the x is
the date of the image and the Z is the
tag for it what kind of flower is this
and the image size is really important
because we have to resize everything if
you have a neural network and if you
remember from our neural networks let me
flip back to that slide
we look at this slide we have two input
nodes here with an image you have an
input node depending on how you set it
up for each pixel and that pixel has
three different color schemes usually in
it sometimes four so if you have a
picture that's 150 by 150 you multiply
150 times 150 times three that's how
many nodes input layers coming in that
means this is a massive input a lot of
times you think oh yeah it's just a
small amount of data or something like
that no it's a full image coming in and
then you have your hidden layers A lot
of times they match what the image size
is coming in so each one of those is
also just as big and then we get down to
just a single output
so that's kind of a thing to note in
here what's going on behind the scenes
and of course each one of these layers
has a lot of processes and stuff going
on
and then we have our different
directories on here let me go and run
that so I'm just setting the directories
that's all this is archive flowers Daisy
sunflower tulip dandelion Rose just our
different directories that we're going
to be looking at
and then we want to go ahead and we need
to assign labels remember we defined x
and z
so we're just going to create a
definition here
and the first thing is a return flower
type okay
just returns it what kind of flower it
is I guess assign the label to it but
we're going to go ahead and make our
train data
and when you look at this there's a
couple things to take away from here the
first one is we're just appending right
onto our numpy array the image we're
gonna let numpy handle all that
different aspects as far as 150 by 150
by three we just dump it right into the
numpy which makes it really easy we
don't have to do anything funky on the
processing and we want to leave it like
that and I'm going to talk about that in
a minute and then of course we have to
have the string a pin the label on there
and I want you to notice right here
we're going to read the image in
and then we're going to size it and this
is important because we're just changing
this to 150 by 150. we're resizing the
image so it's uniform every image comes
in identical to the other ones this is
something that's so important is um when
you're resizing or reformatting your
data you really have to be aware of
what's going on with images it's not a
big deal because with an image you just
resize it so it looks squishy or spread
out or stretched the neural network
picks up on that and it doesn't really
change how it processes it
so let's go ahead and run that
and now we've got our definition set up
on there
and then we want to go ahead and make
our
training data so make the train data
daisy flower daisy directory print
length of X so here we go let's go and
run that and we're just loading up the
flower daisy so this is going all in
there and it's setting it's adding it in
to the our setup on there to our x and z
set up and we see we have 769.
and then of course you can see this nice
bar here this is the bar going across is
that little added code in there that
just makes it really cool for doing
demos not necessarily when you're
building your own model or something
like that but if you're going to display
this to other people adding that little
what was it called
um
tqdm I can never remember that but the
tqdm module in there is really nice and
we'll go ahead and do sunflowers and of
course you could have just created an
array of these
but this has an interesting problem
that's going to come up and I want to
show you something
it doesn't matter how good the people in
the back are or how good you are
programming
errors are going to come up and you got
to figure out how to handle them and so
when we get all the way down to
the um where is it dandelion here's our
dandelion directory we're going to build
Jupiter has some cool things it does
which makes this really easy to deal
with
but at the same time you'd want to go
back in there depending on how many
times you rerun this how many times you
pull this so when you're finding errors
I'm going in here there's a couple
things you can do and we're just going
to oh it wasn't there it is there's our
error I knew there was an error
this processed
1062 out of 1065.
now I can do a couple things one I could
go back into our definition
and I can just put in here try and so if
it has a bad conversion because this is
where the error is coming from uh just
skip it that's one way to do it
um when you're doing a lot of work in
data science and you look at something
like this where you're losing three
points of data at the end you just say
okay I lost three points who cares or
you can go in there and try to delete it
it really doesn't matter for this
particular demo
and so we're just going to leave that
error right alone and skip over because
it's already added all the other files
in there and this is a wonderful thing
about jupyter notebook is that I can
just continue on there in the x and z
which we're creating is still running
and we'll just go right into the next
flower row so all these flowers are in
there
um that's just a cool thing about
Jupiter notebook
and then we can go ahead and just take a
quick look and see
what we're dealing with and this is of
course really when you're dealing with
the other people and showing them stuff
this is just kind of fun where we can
display it on the plot Library here
and we're just going to go through and
let's see what we got here uh looks like
we're going to do like five of each of
them I think
is that how they set this up
plot Library five by two okay oh I see
how they did it okay so two each so we
have five by two set up on our axes and
we're just going to go in and look at a
couple of these flowers
it's always a good thing to look at some
of your data no matter what you're doing
we've reformatted this to 150 by 150 you
can see how it really blurs this one up
here on the Tulip that is that resize to
150 by 150. and these are what's
actually going in these are all 150 by
150 images you can check the dimensions
on the side
and you can see just a quick sampling of
the flowers we're actually going to
process on here and again like I said at
the beginning most of your work in data
science is reprocessing
this different information so we need to
go ahead and take our labels
and run a label encoder on there and
then we're just going to Ellie is a
label encoder one of the things we
imported
and then we always use the fit
to categorical y comma 5 x here's our
array
X so if you look at this here's our fit
we're going to transform Z
that's our Z array we created
and then we have Y which equals that and
then we go ahead and do to categorical
we want five different categories
and then we create our X NP array of x x
equals x over 255.
so what's going on here there's two
different Transformations one we've
turned our categories into zero one two
three four five as the output and we
have taken our X array
and remember the X array is three values
of your different colors
this is so important to understand when
we do this across the numpy array this
takes every one of those three colors so
we have 150 by 150 pixels
out of those 150 by 150 pixels they each
have three
um color arrays and those color arrays
range from 0 to 250. so when we take the
x equals x over 255
I'm sorry range between 0 to 255. this
converts all those pixels to a number
between 0 and 1. and you really want to
do that when you're working with neural
networks now if you do a linear
regression model it doesn't affect it as
much and so you don't have to do that
conversion if you're doing straight
numbers but when you're running neural
networks if you don't do this you're
going to create a huge bias and that
means they'll do really good on
predicting one or two things and they'll
just totally die on a lot of other
predictions
so now we have our
X and Y values X being the data n y
being our known output
and with any good setup we want to
divide this data into our training so we
have X train we have our X test this is
the data we're not going to program the
model with and of course your y train
corresponds to your X train and your y
test corresponds to your X test the
outputs and this is when we do the train
test split this was from the site kit
sklearn we imported train test split and
we're just going to go ahead and do the
test size at about a quarter of the data
0.25 and of course random is always good
this is such a good tool I mean
certainly you can do your own division
um
you know you could just take the first
you know 0.25 of the data or whatever do
the length of the data not real hard to
do but this is randomized so if you're
running this test a few times you can
kind of get an idea whether it's going
to work or not
sometimes what I will do
is I'll just split the data into three
parts
and then I'll test it on two with one
being the or I train it on two of those
parts with one being the test and I
rotate it so I come up with three
different answers which is a good way of
finding out just how good your model is
but for setting up let's stick with the
X train X test and the SK learn package
and then we're going to go ahead and do
a random seed
now a lot of times the cross actually
does this automatically but we're going
to go ahead and set it up on here and
you can see we did an NP random seed
from 42 and we get a nice RN number and
then we do TF random we set the seed so
you can set your Randomness at the
beginning of your tensorflow and that's
what the tf.random dot set is
so that's a lot of prep
um all this prep and then we finally get
to the exciting part
um this is where you probably spend once
you have the data prepped and you have
your pipeline going and you have
everything set up on there this is the
part that's exciting is building these
models
and so we look at this model one we're
going to designate a sequential they
have the API which is across the cross
tensorflow API versus sequential
sequential means we're going one layer
to the next so we're not going to split
the layer and bring it back together
it looks almost the same with the
exception of bringing it back together
so it's not a huge step to go from this
to an API
and the first thing we're going to look
at is our convolutional neural network
in 2D
so what's going on here there's a lot of
stuff that's going on here the default
for well let's start with the beginning
what is a convolutional 2d Network
well a convolutional 2d Network creates
a number of small windows and those
small Windows float over the picture and
each one of them is their own neural
network and it's basically becomes like
a uh a categorization and then it looks
at that and it says oh if we add these
numbers up a certain way uh we can find
out whether this is the right flower
based on this this little window
floating around which looks at different
things
and we have filters 32 so this is
actually creating 32 Windows is what
that's doing
and the kernel size is five by five so
we're looking at a five by five square
remember it's 150 by 150. so this
narrows it down to a five by five it's a
2d so it has your X Y coordinates and we
look at this five by five remember each
one of these is it's actually looking at
five by five by three
so we're actually looking at 15 by 15
different pixels
and padding is just news I just ignore
that
activation by default is relu we went
ahead and put the railu in there
there's a lot of different activations
relu is for your smaller uh when you
remember I mentioned atom when you have
a lot of data data use an atom kind of
activation or use an atom processing
we're using the rayleu here uh
it kind of gives you a yes or no but it
it doesn't give you a full yes or no it
has a a zero and then it kind of shoots
off at an angle
very common it's the most common wand
and then of course here's our input
shape 150 by 150 by 3 pixels
and then we have to pool it so whenever
you have a two convolutional 2D
layer we have to bring this back
together and pull this into a neural
network and then we're going to go ahead
and repeat this
so we're going to add another Network
here one of the cool things if you look
at this is that it as it comes in it
just kind of automatically assumes
you're going down to the next layer
and so we have another convolutional
neural network 2D here's our Max pooling
again we're going to do that again Max
pooling and we're just going to filter
on down
now one of the things they did on this
one is they changed the kernel size they
changed the number of filters and so
each one of these steps
kind of looks at the data a little bit
differently and that's kind of cool
because then you get a little added
filtering on there
this is where you start playing with the
model you might be looking at a
convolutional neural network which is
great for image classifications
um
we get down to here one of the things we
see is flattened so we add we just
flatten it remember this is 150 by 150
by three well and actually the pool size
changes so it's actually smaller than
that flatten just puts that into a 1D
array so instead of being you know a
tensor of this really complexity with
the the pixels and everything it's just
flat and then the dense
it's just another activation on there by
default it is probably relu as far as
this activation
and then oh yeah here we go in
sequential they actually added the
activation as relu so this just because
this is sequential this activation is
attached to the dense
and there's a lot of different
activations but Rayleigh is the most
common one and then we also see a soft
Max softmax is similar but it has its
own kind of variation and one of the
cool things you know what let me bring
this up because if we if you don't know
about these activations this doesn't
make sense
and I just did a quick Google search on
images of tensorflow activations
um I should probably look at which
website this is
but this is the output of the values so
as your X as it adds in all those
weighted X values going into the node
it's going to activate it a certain way
and that's a sigmoid activation and you
can see it goes between 0 and 1 and has
a nice curve there this also shows the
derivatives and if we come down the
seven popular activation functions
non-linear activations there's a lot of
different options on this let me see if
I can find the
oops let me find the specific to rayleau
so this is a leaky Ray Lou and you can
see instead of it just being 0 and then
a value between going up it has a little
leaky there otherwise your Rayleigh
loses some notes they just become
inactive
um but you can see there's a lot of
different options here here's a good one
right here with the ray Lou you can see
the Rayleigh function on the upper on
the upper left here and then the Leaky
Rayleigh over here on the right which is
very commonly used also
one of the things I use with processing
um language is the Sig is the
exponential one or the tangent H the
hyperbolic tangent because they have
that nice funky curve that comes in that
has a whole different meaning and
captures word use better
again these are very specific to domain
and you can spend a lot of time playing
with different models for a basic model
we'll stick to the relu and the soft Max
on here and we'll go ahead and run and
build this model
so now that we've had fun playing with
all these different models that we can
add in there we need to go ahead and
have a batch size on here
128 epics 10
this means that we're going to send 128
uh rows of data or flowers at a time to
be processed
and the epics 10 that's how many times
we're going to Loop through all the data
and then there's all kinds of stuff you
can do again this is now built into a
lot of cross models already by default
um
so there's different ways to reduce the
values and verbose verbose equals one
means that we're going to show what's
going on
value the monitor what we're monitoring
we'll see that as we actually train the
model this is what's what's going to
come out of there if you set the verbose
equal to zero you don't have to watch it
train the model although it is kind of
nice to actually know what's going on
sometimes
and since we're still working on
bringing the data in here's our batch
side here's our epics we need to go
ahead and create a data generator this
is our image data generator
and it has all the different settings in
here almost all of these are defaults so
if you're looking at this going oh my
gosh this is confusing most of the time
you can actually just ignore most of
this vertical flip so you can randomly
flip pictures you can randomly
horizontally flick them you can shift
the picture around this kind of helps
gives you multiple data off of them
zooming rotation there's all kinds of
different things you can do with images
most of these we're just going to leave
as false we don't really need to do all
that um setup because we already have a
huge amount of data if your short data
you can start flipping like a horizontal
picture and it will generate it's like
doubling your data almost
so the upside is you double your data
the downside is that if you already have
a bias in your data you already have
[Music]
um
5000 sunflowers and only two roses
that's a huge bias it's also going to
double that bias that is the downside of
that
and so we have our model compiled and
this you're going to see in all the
cross we're going to take this model
here we're going to take all this
information as far as how we want it to
go and we're going to compile it
this actually builds the model and so
we're going to run that and I want you
to notice uh learning rate
very important this is the default zero
zero one there's there you really don't
this is how slowly it adjusts to find
the right answer
and the more data you have you might
actually make this a smaller number with
larger with you have a very small sample
of data you might go even larger in that
and then we're going to look at the loss
categorically categorical cross entropy
most commonly used
and this is uh how how much it improves
the model is improving is what this
number means or yeah that's that's
important on there and then the accuracy
we want to know just how good our model
is on the accuracy
and then
one of the cool things to do is if
you're in a group of people who are
studying the model if you're in
shareholders you don't want to do this
is you can run the model summary
I do this by default and you can see the
different layers that you built into
this model just a quick summary on there
so we went ahead and we're going to go
ahead and create a
we'll call it history but we want to do
a model fit generator
and so what this history is doing is
this is tracking what's going on as well
it fits the model
now there's a lot of new setups in here
where they just use fit and then you put
the generator in here
we're going to leave it like this even
though the new default
is a little different on that it doesn't
really matter it does the same thing and
we'll go ahead and just run that
and you can see while it's running right
here we're going through the epics we
have one of ten now we're going through
6 to 25 here's our loss we're printing
that out so you can see how it's
improving and our accuracy the accuracy
gets better and better and this is 6 out
of 25. this is going to take a couple
minutes to process because we are
training 150 by 150 by 3 pixels across
six layers or eight layers whatever it
was
that is a huge amount of processing so
this will take a few minutes to process
this is when we talk about the hardware
and the problems that come up in data
science and why it's only now just
exploding being able to do neural
networks this is why this process takes
a long time
now you should have seen a jump on the
screen here because I did pause the
recorder to let this go ahead and run
all the way through its epics
let's go ahead and take a look and see
what these epics are and if you set the
verbose to zero instead of one it won't
show what's going on in the behind the
scenes that is training it so we look at
this epic 10 epic so we went through all
the data 10 times if I remember
correctly there's roughly a gig of data
there so that's a lot of data
the first thing you're going to notice
is the 270 seconds that's how much each
of those epics took to run and so if you
divide 60 in there you roughly get about
five minutes worth of each epic so if I
have 10 epics that's 50 minutes almost
an hour of run time
that's a big deal we talk about
processing uh in on this particular
computer I actually have what is it uh
eight cores with 16 dedicated threads so
it runs like a 16 core computer it
alternates the threads going in and it
still takes it five minutes for each one
of these epics so you start to see that
if you have a lot of data this is going
to be a problem if you have a number of
models you want to find out how good the
models are doing what model to use
and so each of those models could take
all night to run in fact I have a model
I'm running now that takes over uh takes
about a day and a half to test each
model it takes four days to do the whole
data so what I do is I actually take a
small piece of the data
test it out to find out get an idea of
how the different setups are going to do
and then I increase that size of the
data and then increase it again and I
can just take that that curve and kind
of say okay if the data is doing this
then I need to add in more dense layers
or whatever so you can do a small chunks
of data then figure out what it costs to
do a large set of data and what kind of
model you want
the loss as we see here continues to go
down this is the error this is how much
error is in there it really isn't a
user-friendly number other than the more
it Trends down the better and so if you
continue to see the loss going down
eventually it'll get to the point where
it stops going down and it goes up and
down and kind of wavers a little bit at
that point you know you've run too many
epics you're starting to get a bias in
there and it's not going to give you a
good model fit
the accuracy just turns us into
something that we can use and so the
accuracy is what percentage of guesses
in this case is categorical so this is
the percentage of guesses are correct
value loss is similar you know it's a
minus a value loss
and then you have the value accuracy and
you'll see the value accuracy is pretty
similar to the accuracy just rounds it
off basically and so a lot of times you
come down here and you go okay we're
doing 0.5.6
0.7 and that is 70 accuracy or in this
case
68.59 accuracy that's a very usable
number and it's very important to have
if you're identifying uh flowers that's
probably good enough if you can get
within a close distance and knowing what
flower you're identifying if you're
trying to figure out whether someone's
going to die from a heart attack or not
you might want to rethink it a little
bit or re-key how you're building your
model so if I'm working with a uh a
group of clients shareholders in a
company or something like that you don't
really want to show them this you don't
want to show them hey you know this is
what's going on with the accuracy these
are just numbers and so we want to go
and put the finishing touches just like
when you are building a house and you
put in the frame and the trim on the
house it's nice to have something a nice
view of what's going on and so we'll go
ahead and do a pi plot and we'll just
plot the history of the loss the history
of the value loss
over here epics train and test and so
we're just going to compute these this
is really important and what I want you
to notice right here is when we get to
about oh five epics a little more than
five six epics you see a cross over here
and it starts Crossing as far as the
value loss and what's going on here is
you have the loss in your actual model
and your actual data and you have the
value loss where it's testing it against
the the test data the data wasn't used
to program your model wasn't used to
train your model on
and so when we see this crossing over
this is where the bias is coming in this
is becoming overfitted and so when you
put these two together uh right around
five and six you start to see how it
does this this switch over here and
that's really where you need to stop
right around five yeah six
um it's always hard to guess because at
this point the model is kind of a black
box
uh see but you know that right around
here if you're saving your model after
each run you want to use the one that's
right around five epics because that's
the one that's going to have the least
amount of bias so this is really
important as far as guessing what's
going on with your model and its
accuracy and when to stop uh it also is
you know I don't show people this mess
up here
um I show somebody this kind of model
and I say this is where the training and
the testing comes in on this model
it just makes it easier to see and
people can understand what's going on
so that completes our demo and you can
see we did what we were set out to do we
took our flowers and we were able to
classify them within about yeah 68 70
accuracy whether it's going to be a
dahlia sunflower cherry blossom Rose a
lot of other things you can do with your
output as far as a uh different tables
to see where the errors are coming from
and what problems are coming up
companies across the globe are
generating data at a rapid rate and data
science is helping them draw valuable
insights and make business decisions
hello everyone welcome to this video
tutorial on data science interview
questions
in this video we will learn some of the
important questions that are often asked
in any data science interview
we'll discuss questions related to
various aspects of data science such as
programming SQL mathematics statistics
linear algebra and probability finally
we'll get an idea about the top machine
learning and deep learning interview
questions that are frequently Asked in
interviews so let's begin
hi everyone
welcome to this live session by simply
learn today we are going to talk about
some of the essential programming
interview questions for a data scientist
role now since python is widely used
programming language for data science
machine learning and deep learning we
will be solving our problems in the
Python programming language using
jupyter notebook
so let's wait for a few minutes for
people to join in
also if you haven't subscribed to our
Channel yet then please make sure to
subscribe to it now and stay updated
with all the recent trending
Technologies
and if you want to get the code file
that we will use in this session then
please put your email IDs in the chat
section of the video our team will share
the interactive IPython notebook with
you
so let's begin with our first question
our first question is
how do you get a list of all the keys in
a dictionary
now in Python for data science data
structures play an important role to
store and manipulate data dictionary is
one such data structure that is declared
using curly braces and they are written
as key value pairs you can see on the
screen I have my dictionary called D and
it is assigned using curly braces
now to get a list of all the keys in a
dictionary we use the dot Keys function
now let me take you to my jupyter
notebook so here is my jupyter notebook
and you can see my first cell I have my
dictionary defined
so the name of the dictionary is d
and
it has the key values the keys are the
country names and the values are the
capitals of each country so I have India
and the capital is New Delhi then I have
USA the capital is Washington similarly
I have Spain Madrid and we have Germany
and Berlin
now India USA France Spain all these are
my keys so to get the keys I'll use the
dot Keys function so I'll write my
dictionary name that is d dot
I'll give keys
close the brackets and hit shift enter
you can see these are the list of my
keys in the dictionary d
all right
okay
now let's move to our next question
so our next question is what do you mean
by list comprehension in Python
so list comprehensions are a sort and
easy way to define and create new lists
based on existing lists
so suppose you want to iterate through a
list and find all the even numbers in
the list you can either use a for Loop
and an if condition as shown here or you
can write a list comprehension to do it
in one line of code you can see this is
my list comprehension
so let me show you both the ways
all right so here on my jupyter notebook
let me declare my list as L equal to
I'll pass in a few numbers let's say 5
comma 12
then we have 14
3
11
7
9 and let's say the last element in the
list is six
all right let me run this list
okay now
let me create an empty list let's say
list one and let this be an empty list
with square brackets
then
I'll use the looping construct to find
the even elements in the list so I'll
write a for Loop called
for item in
my list l
I'll give colon
then I'll give my condition if
item
percentage
2 is equal to equal to 0 which means if
the remainder is 0
then
I'll
append the items
to my empty list which is list one so
I'll write list one dot append
and I'll pass in the argument by item
now
let me go ahead and
print my list
I'll write print
the
final list
using a for Loop
I'll give colon give a comma
and pass in my new list name which is
list one
now let's run this
you can see
using this for Loop and the if condition
I have got all the even elements in my
list which was L you can see my original
list and 12 10 14 and 6 where all the
even numbers present in my list now this
task you can do it using list
comprehensions as well
so let me show you how to do that
so I have my list
defined let me just copy this here I'll
paste it
and now
I'll create another variable which is
list one
I'll write equal to
now here
using square brackets I'll write item
for
item in
l
and I'll give my if condition as if
item
percentage to
is equal to equal to 0
then
I'll print my resultant list so I'll
write print
let's say
resultant
list using
list comprehension
I'll give a colon then a comma and I'll
pass in my new list which is list one
so here
whatever is there
in my square brackets
is called the list comprehension
technique
let me just print this and I'll get the
same result so these are my even numbers
all right
so let's move to the third question so
our third question is
given a list of numbers find the squares
of the numbers using the map function in
Python
now the map function calls the specified
function for each element of an iterable
and returns a result
so let me show you an example to find
out squares of a list of numbers using
the map function
okay
so let's first Define a function called
Square so I'm using my def keyword and
I'll pass in my user defined function
name as square and I'll give my
parameter as X
and
I want to return
the square of X which is X
star X
all right and then let me Define my
list of numbers so I'll
pass in a variable called num and let's
say I want to find the squares of 5
numbers 1 2 3 comma 4 comma 5.
here
I'll pass in another variable which will
store the resultant list
so I'll write num underscore
squared
equal to
here I'll use my map function so map
function takes in two parameters first
it will take in
the user defined function name which is
square here and the list of numbers
which is num
I'll run a for Loop I'll write for let
me just scroll down
I'll write for
num in
num underscore squared
print
num
so this is my map function
so
it will map all the elements in the list
to this function which is square
I'll just print it you can see
we have the list of squares from the num
list which has
the elements one two three four and five
now moving ahead
coming to our fourth question
the fourth question is on to write a
program that will filter out the numbers
divisible by 3 from a tuple using the
Lambda functions
so Lambda functions are Anonymous
functions that are not defined by the
def keyword in Python
so here you can see I have the example
so this is a tuple I have defined and
using filter along with the Lambda
function I am trying to find out the
numbers that are divisible by 3 and
finally I am printing those numbers so
let's do it on our jupyter notebook
all right so we are on the jupyter
notebook so first I'll Define a tuple
let's say
the variable name is my underscore top
which stands for my Tuple and
using brackets I'll pass in my Tuple
values let's say 5 12 10 18
we have 43 9 7
11 and let's say 6.
so you can see
these are the numbers present in my
Tuple
now
I'll create
another variable called new Tuple
and I'll use my Tuple function
and inside the Tuple function I'll use
the filter function that will filter out
only the values
that are divisible by 3
and now I'll write my Lambda function
so notice this clearly
how to use a Lambda function I'll use
the keyword Lambda
and then
I'll pass in a variable X
now this
should consider only those values
that are divisible by 3 so I'll write X
percentage 3 should be equal to equal to
0.
and
I'll give
my Tuple
as the variable here
all right
now we have declared our Lambda function
let's just go ahead and print my new
Tuple which is new underscore tup
so here you can see I have my Tuple
function
inside the Tuple function I'll use my
filter function that will filter only
those values divisible by 3 and to
filter those values I have used my
Lambda function here and passed in my
original Tuple which is my Tuple
it's actually my tup so let's delete the
last two letters
let's just go ahead and print this there
you go you can see we have 12 18 9 and 6
are the only numbers that are divisible
by 3 from this Tuple moving to the next
question
so our next question is how to create a
data frame from a dictionary
Now to create a data frame from a
dictionary you can use the pandas
library and the dot data frame function
now pandas is a popular library in
Python that is used for data
manipulation
so let me show you how you can create an
employee dictionary and then convert
that into a data frame
okay
so here on my jupyter notebook let me go
ahead and import my pandas Library so
I'll write import
pandas as
PD
and
next let's create my employee dictionary
so I'll write EMP which stands for
employee and and within curly braces
I'll give my key value pairs so my first
column would be the name of the employee
and
let's pass in a list of employee names
I'll write Emmy
let's say we have
Angela
the third employee's name let's say we
have Samuel
and
let Danny be our fourth employee
I'll give a comma
now the second attribute in our
dictionary will be the age of the
employee
so my key would be age and
let's create a list of
age values
I'm assigning random values to those
employee
each
30 and let's say 32
and let's create another
variable called City
here variable refers to the column name
of the data frame
let's say Amy is from New York
then we have
Angela from Chicago
let's say Samuel is from Boston
and we have
Danny from let's say Seattle
okay
so if I run this I have my employee
dictionary created
so if you want to see the dictionary you
can write print and pass the variable
name which is EMP here
so this is my dictionary
now to convert this dictionary into a
data frame you can simply write DF
and you can give the pandas Library
SPD and use the data frame function
so I'll write data frame
and
I'll pass in my dictionary name which is
EMP here
let's run it
okay so we have successfully created our
data frame now let me just print the
data frame I'll write DF
there you go you can see I have a nice
table here which is actually a data
frame here you can see the indices and
you can see our column names as name
each and City and these are the values
or the rows
all right
now let's see the sixth question we have
so the question is
what is the difference between loc and
iloc in Python
now as data scientists you would often
want to analyze a chunk of data and not
all the rows in it
so loc and iloc functions can help you
fetch specific rows and columns from
your data frame
loc function uses row or column labels
to select and slice data from a data
frame while iloc uses integer index to
select specific rows and columns
remember iloc stands for index location
I'll use a card data frame to show how
loc and iloc works now on your screens
you can see the data frame so it has a
column called brand which has different
brand names such as Ford this Hyundai
Tata Mahindra this maruti the skia
motors
then we have the year in which the car
was manufactured
we have the number of kilometers the car
has run we have the city and the mileage
the car gives
so let me first create this data frame
on my jupyter notebook
okay so here on my notepad I have my
code written to create the data frame
that is car I'll just copy this code and
I'll paste it
to this cell
okay let me just run it
so I go on top you can see we have
successfully created our car data frame
now let me go ahead and print this data
frame so I'll write print
in Brackets I'll give my data frame
escar
let me run it all right so here is my
data frame
now suppose you want to see the data
from second row till the fifth row for
brand and City columns so you can use
the loc function so I want my data from
the second row till the fifth row for
brand and City columns so let me show
you how to do that so first I'll write
my data frame name that is car and then
I'll give my function as Loc
and
within brackets I'll give my
row index values so I'll give from 2 to
5 which is second row to fifth row and
I'll give my column names as brand
you give a comma and my second column
which I want is City
so here
I'm using the labeled based indexing let
me just run it there you go you can see
we have the information available for
the second row third row fourth row and
the fifth row
all right
similarly let's see one more example for
index location based slicing
so suppose you want to see the data from
second third and fourth row for year
kilometers and City columns so for that
you can use the iloc function
so I want the value for
second third and fourth row and the
columns that I want are years kilometers
and the city column
so for that
I'll give my data frame name as car dot
this time I'll use iloc
and within
square brackets I'll give my
row values and the column values
now one thing you should notice here is
if you use iloc python will exclude the
last index which is the fifth row and
similarly here it will exclude the last
index from the column which is the
fourth column
if I run this
you get the desired result
now
let's see another example let's say this
time I want to print the rows that have
mileage less than 25 for brand and City
columns so you can use a print function
and I'll pass in my data frame as car
and use
Luc here
I'll give my condition as
car
and I want my condition that is mileage
should be
less than 25
and
I want
the brand name
so I'll use a square bracket and give my
brand column
and I also want the city column
okay
let me
close it
if you run this
now these are the brand names Renault
Tata and Kia which have mileage less
than 25 let me just verify this
you can see all these values are less
than 25
similarly you can use the
iloc function as well
I can write print
we'll give car dot iloc
within square brackets I'll give car
and my
condition column is mileage
now this should be less than 25
I'll use dot values
and
then I'll give my
column numbers as 0 comma 2.
let me run it
there you go
the same result we have got it using
iloc function this time
and you can see the columns were 0 comma
2 which means we wanted the
First Column which is brand and the
second column which was kilometers
all right
now moving to our seventh question in
the list of questions that we have
the question is how is list dot append
different from list dot extend
Now list dot append adds its arguments
as a single element to the end of the
list the length of the list increases by
1 here
and list dot extend iterates over its
arguments and adds each element to the
list the length of the list increases by
the number of elements in its arguments
so let me show you
the difference on my jupyter notebook
okay
so let's create a list called list one
equal to
I'll create a list let's say it has some
country names like India
then we have
USA
and I'll consider one more country let's
say
Canada
okay
now let me print the length of this list
also
I'll use the length function for that
all right now
let me append one more list so I'll use
the append function I'll write list one
dot append
and
Within
brackets I'll give my new list which has
two more country names let's say
France and
spin
okay
and here let me print
list one and let's also print the length
of the list again
I'll use the length function
and my listening which is list one
so just Mark we are using append here
let me run this
there you go
so here you can see
my original list had three elements
since the length is three now when I
appended this new list which had two
elements France and Spain
our size increased by 1 and the length
of the new list is 4.
now
let me just copy this code
and
instead of append let me write extend
so that we can see the difference
between append and extend
we'll just scroll down
run it here you can see
our original list had three elements
but after we extended our
list with two more elements
the length of the new list became five
so these are the
five elements in the list
and here you can see
there were total four elements in the
list
when we appended the new list
okay now moving to our eighth question
suppose you have a car data set which
has a mileage column
you need to create a new field called
MLG that will accept two values
if mileage is less than 25 then the
value should be low MLG and if the
mileage is greater than or equal to 25
then the mileage should be high MLG or
the value should be high MLG
now while analyzing data data scientists
often need to create extra columns in
the data frame to add more values that
will help you make your data analysis
more efficient
so this question is related to one such
instance or one such idea
so here you can see
my data set
it has the brand name year kilometers
City and the mileage column
now if mileage is less than 25 I'll add
the value low MLG to my new column else
if mileage is greater than or equal to
25 I'll add High MLG to my new column
so here you can see my final output
wherever the mileage is less than 25 it
should print Lu MLG else it should print
high MLG now let's do this on my jupyter
notebook
so we'll use Lambda functions that we
learned earlier to perform this task
I'll write a function let's say f is
equal to
you can call this as a function or a
variable
I'll
give the keyword Lambda and use x as my
parameter
now X should be
low MLG
if
X is less than 25 which is mileage here
else
if it's not less than 25 it should
return High MLG
now
what I'll do is
I'll give my
data frame escar and I'll create my new
column that is MLG
then I'll write
equal to car dot
I'll give my
column name from the original data frame
which is mileage
and I use my apply function
and pass in the variable f
now let's just print this data frame
if I run it
you can see the output here we have
created a new column which is MLG you
can see our new column name was MLG and
these are the results so for Renault
Tata and Kia for the last three rows
since the mileage is less than 25 it
returned low MLG for the remaining it
gave us High MLG
now moving to the ninth question
so this is our next question we want to
create a four Cross Four Matrix and find
the sum of all the diagonal elements in
The Matrix
data scientists often work with multiple
dimensional arrays or multi-dimensional
arrays n matrices to build models this
question is related to an operation on a
four Cross Four Matrix so first we need
to create an array which will be a numpy
array of 4 cross 4
shape and then we'll see how to
calculate the diagonal elements or the
sum of the diagonal elements
so I am on my jupyter notebook first
let's import the numpy library so I'll
write import
numpy or numpy as NP
let me run it
okay now I'll create my four Cross Four
trick so I'll write
m is equal to
NP Dot array
and I'll create my
4 cross 4 Matrix
let's say the values are one two three
four in the first row
and in the second row I'll pass 5 comma
6 comma 7 comma 8.
in the third row we'll have 9 10 and 11
also 12.
and finally we'll have 13
14 15 and 16.
in the fourth row
I'll just run it
now if I print m
you can see this is a 4 cross 4 Matrix
so this is one way of creating a four
Cross Four Matrix which is a bit tedious
and lengthy instead of that you can also
use a range function
so if I write m is equal to
NP Dot
a range function
and here I'll give my values starting
from 1 till
17 so this will actually exclude 17 and
take from 1 to 16.
and I'll give my shape
as 4 comma 4.
let me print it
there you go it has printed the exact
same Matrix that we saw above
all right now to print the diagonal
elements we'll use the diagonal function
so I'll write print
m dot
diagonal
let me just run it
so 1 6 11 and 16 are my diagonal
elements you can see this Matrix 1 6 11
and 16 are my diagonal elements
now to print the sum of all the diagonal
elements I'll use the sum function
so I'll write m dot
diagonal
along with that I'll pass my sum
function
if I run it
so the sum of 1 6 11 and 16 is 34.
okay
so coming to the last question on this
interesting session on programming
interview questions for data scientists
our question is given a vehicle data
frame find the average and maximum
mileage of the vehicles using pandas
so here
data scientists use statistical measures
to analyze their data so it is important
to have knowledge of the statistical
functions in Python here is my vehicle
data set and for each brand of vehicle
we want to find the average and the
maximum mileage so let me show you how
to do it
okay so here on the notepad I have my
code written to create the vehicle data
frame I'll just copy it
and paste it on this cell
you can see
we have the brand column the year column
kilometers City mileage here you can see
the different
brand of vehicles let me just create
this data frame first
and let me go ahead and print the
vehicle data frame so I'll write vehicle
and run it
so here is the data frame
now
I want to find the average and maximum
mileage for
each of the brands
so for this we'll use the group by
function and the aggregate function that
is available in the pandash library of
python
so let me show you how to do it
I'll give
a variable called result
and then I'll use my
data frame name as vehicle
followed by the group by function
I'll write Group by
and since I want to check for all the
brands so I'll group all the brand of
vehicles
then I'll use an aggregate function
which is egg
and
within curly braces I'll give my column
as mileage
I'll give a colon
and pass in mean which is for average
and I'll use max to find the maximum
mileage
now let's
print it I'll write print
I'll give
a message as the
average and
maximum
mileage
of
vehicles
let me just print the result
I'll run it
you can see it has resulted in an error
the reason is
vehicle the V is capital
if I run it there you go here you can
see on the left we have the brand names
we have Hyundai this maruti and Tata
because those were the only three brand
of vehicles we had and for each brand
you can see the mean mileage and the
maximum mileage
for maruti it was
26.33 and the maximum mileage is 28
similarly you can see for
Tata vehicle as well hello everyone
welcome to this live session by simply
loan
in our previous session we looked at
some of the important Python Programming
questions for data scientists today we
will learn some of the crucial SQL
interview questions for data scientists
now if you want to get the SQL script
file that we are going to use in this
demo then please put your email IDs in
the chat section our team will send the
SQL file via email
SQL is a widely used language for
querying data from databases it allows
you to store retrieve manipulate and
update data present in the form of rows
and columns data scientists often work
with structured data that is stored in
MySQL databases Microsoft SQL servers
Oracle databases or even nosql databases
such as mongodb today we will look at a
variety of SQL questions
we'll be using the mySQL database for
our demo throughout the video we'll be
using MySQL workbench and the MySQL
command line to solve the SQL queries
let me show you the database that we
will be using for this demo and the
tables we are going to use
okay so here I am on my SQL workbench
and Below you can see I have a local
instance created I'll just click on it
now it's asking me to enter the password
I'll give my password
and hit OK
now this will take me to
the SQL editor
now this is the SQL workbench so let me
first create a new
SQL query file all right
so first and foremost let me show you
the databases we have for that I'll use
the command so
databases
if I hit tab it will auto complete
I'll give a semicolon
and hit Ctrl enter to run this
there you go here you can see the list
of databases that we already have
and
from the list of databases we are going
to use the SQL underscore IQ database so
to use this database I'll write the
command use
SQL underscore IQ
I'll give a semicolon
I'll select this
and I'll click on this execute button
if I run this
you can see now I'm using SQL underscore
IQ database
now if you want to see all the tables
present in SQL underscore IQ database
we'll use the following command so I'll
write show
tables now this will list all the tables
present inside SQL underscore IQ
database if I run it
you can see here it has executed
successfully and it has given us seven
rows which means we have a total of
seven tables in this database you can
see this in author table there's a table
called books we have customers email
employee players and weather table so
all these tables we are going to use in
the course of this video
now if you want to
have a look at the data present in one
of the tables you can just use the
select statement so I'll write select
star from let's say I want to see
the data present inside the employee
table so I'll write employee
so this is my table name I give a
semicolon if I select this
hit the execute button there you go
it shows me
that it has returned six rows
and
these are the column names we have the
name of the employee the age of the
employee we have the employees salary
and the employee ID
so there are total six employees in the
table
all right
now I can do the same operations on my
SQL command line as well now let me open
the command line first
okay so this is the MySQL command line
client it is asking me to enter the
password so I'll give my password first
all right now here you can see I have
come inside my sequel now the operations
that we performed on the MySQL workbench
the same operations can be performed on
the command line as well so I'll write
use
SQL underscore
IQ
I'll hit enter
you can see the message it says database
changed now if you want to display the
tables present inside this database I
can use show tables command
I have to give a semicolon and hit enter
there you go you can see the list of
tables we have inside the database
now if you want to have a look at the
content of one of the tables I can just
write select star from let's say players
if I give a semicolon hit enter you can
see
the rows and columns in the data set all
right
and with that
let's move on to our first question on
SQL
okay
so our question is what is the
difference between where and having
clause in SQL
so where Clause is used to filter the
records based on the specified condition
while the having Clause is used to
filter the records from groups based on
specified conditions you need not worry
we'll do the demo for wherein having
Clause using the employee table
so where Clause cannot have aggregate
functions
such as count sum Min or maximum
functions
having Clause can operate on aggregate
functions
now where Clause is implemented on rows
well having Clauses implemented on
columns now let me show you an example
of how a where Clause is different from
an having clause
so I'll take you to my
MySQL workbench
all right so we are on the SQL workbench
so suppose I want to get the records of
the employees whose age is greater than
30
for this I can use a where clause so
since I want to get the employees whose
ages greater than 30 I'll use a where
Clause I'll write my SQL query as select
star
from
employee
where
I'll write age is greater than
30.
if I give a semicolon and run this
it will return all the employees whose
age is greater than 30 you can see 36 36
and 35
these are the employees whose age is
greater than 30
now
I have intentionally kept some
duplicate rows because we are going to
use this in our next questions again I
can also do the same task using the
having Clause so let me just copy this
query
I'll paste it here
now instead of where
I'll write having
so you will see
that even this query will return the
same result if I run it you can see it
gives me the same result
now
let's say I want to find all the records
where the count of age for all the
employees is greater than one in this
case I can't use a where Clause it will
throw me an error since where Clause
cannot be used with aggregate functions
so let's try using an aggregate function
in the where Clause so I'll write select
star from
employee
where
count of age
is greater than 1. so the idea is to
find all the records where the count of
age for all the employees is greater
than 1.
so if I run this SQL query you can see
it has given me an error it says invalid
use of group function
now instead we need to use a having
Clause along with the group by Clause to
solve the problem
so I'll group all the age values and
then find the count of each age value
so let me just
paste this and here I'll edit my SQL
query instead of where I'll write having
and just after the table name I'll use
the group by clause so I'll write Group
by each
now let's run it
all right you can see our SQL query has
returned only those employees
whose age occurs more than once
now let me just verify this if I
run my employee
table
you can see Angela's age that is 36 has
has been repeated twice you can see
Angela here as well and similarly we
have
Mike also repeated
twice
all right
now let's see the second question
now this is a multiple choice question
where you need to answer the correct SQL
query based on the question
so the question is
the correct SQL query to select all the
records of employees with Ari in their
names is
so we have a list of four options given
and out of that you need to find the
correct option
now
the answer to this question is
this
so if you write select start from the
table name which is employee here where
name like then if you give this
condition it will return
all the records where employees have Ari
in their names
now like is an operator in SQL that is
used to search a specified pattern
so here our pattern is we want all those
employees who have ERI in their names
the first percentage means that the name
can have any number of letters in the
beginning followed by Ari in the middle
and the last percentage means the name
can have any number of letters in the
end
so let's do this on our MySQL workbench
let me first display all the employees
in the table so I'll write select start
from
employee
I'll run this
if you see my table
I have one employee who has
Ari in its name
and that person is Karin
I now want to get this particular record
so what I can do is
I'll write a select query select star
from
employee where name
then I'll use the like operator where
name like within double quotes I'll give
percentage
ERI
again a percentage close the double
quotes and give a semicolon and if I run
this you can see
MySQL Query will return only Carin now
let's do one more example
suppose I want to find the employee
whose name starts with d
so
the way to do is something similar to
what we saw just now
so here in my condition
I'll just
replace this with d this means
the name should start with d
and after that it can contain any number
of letters
now let's run it
there you go so our table had one
employee called Danny whose name started
with d
now moving to our third question
so the question is we want to write a
query to find the players with the least
number of goals
so there are two methods to do it
one where you can use a sub query and
the other method to use the limit
operator so let me show you both the
methods
first let me go ahead and display all
the tables present
in my database that is SQL or SQL
underscore IQ
all right if I scroll down you can see
we have a table called players so I'm
going to use the players table to find
the player with the least number of
goals
so
let me show you the first method that is
by using a sub query
so I'll write select
star from my table name which is
players
where
goals
equal to
and in this I'll write my sub query
using
brackets so I'll select
minimum goals
from
the table that is players
I'll close the bracket
and then I'll give a semicolon
now let me go ahead and run this
you can see
Matt was the player from Scotland who
scored
the least number of goes that is three
now let's break it down so what we did
was
first my SQL query found out what was
the minimum goal scored by a player so
if I run this
you will see it will return me three so
three was the least number of goals
scored by one of the players
now in my outer query
what SQL did was it searched for those
goals
where the value was equal to 3 so we
wrote select star from players where
goals equal to three hence
if I run this
you will find the player whose code only
three goals and that player is matte
all right
now the other way to do is using the
limit operator so this is more simpler
than the previous method
I can write select star from
players
order by
goals
now let me first run
this SQL statement if I run this
you can see
we have ordered this in ascending order
so the person who has code
the least number of goals appears
at the top
and the person with the or the player
with the highest number of goals appears
at the bottom and from this table I only
want the first row and for that I can
use the limit operator so if I write
limit 1 it will display only the first
row
if I run it
you can see it shows me only the record
for matte moving ahead now let's see the
fourth question in our list
the question is
write the SQL query to find the player
with the second highest number of goals
now again there are two methods to do it
one is by using limit and offset and the
other method is to use a sub query so
let's see both the methods
I'll first write my SQL query using
limit and offset so
my query will be select star from
players
order by
goals
descending
let me just run this
all right so here you can see
my table has been ordered
in descending order of goals so Anthony
has scored the most number of goals then
comes Daniel followed by Sam and at the
end we have Matt who scored the least
number of goals
now out of this I want the player who
scored the second highest number of
goals which is Daniel so I want my SQL
query to return this row that has player
ID 103 name is Daniel
countries England and goals is 7. for
that MySQL has two features
so I can use
limit one
followed by
offset 1.
so what
this query does is
it will offset
one row that is it is going to offset
the first row or it is going to skip the
first row and then print only the second
row because I have set my limit to 1. so
let me just run it
you will see
we get the desired result the player
name is Daniel and the number of goals
scored is 7.
now the other method to do is using a
sub query now let me write my sub query
first I'll write my outer query which is
Select star from
and here I'll write my sub query select
star from
players
order by
the column name which is goals
in descending order
here I'll limit it to 2
then I'll give my Alias name as t
then I'll use
order by
goals
limit
one
let me just run it first
if I run this
you see here I get the same result
now let's first break it down
I'm going to run the sub query first so
here I have select star from players
order by goals
descending and limit is to
let me just run it
so the selected SQL query returns
the first two rows ordered by goals and
if you see here clearly
we have again ordered the value returned
in ascending order which means
from this table
if I order this table in ascending order
Daniel should appear first
and out of that I am limiting my result
2 1 so it will print only Daniel
if I run this again you can see I have
the desired output now let's check the
fifth question in our list
given the below email table write a SQL
query to find the distinct domain names
from the email column so if you consider
this as my email table the domain names
are gmail.com Yahoo dot in and we have
hotmail.com these are the distinct
domain names
now to solve this task
again we have two methods either you can
use the substring underscore index
function or you can use the substring
function
now the substring underscore index
function returns a substring of a given
string before a specified number of
delimiters
so let me first use the first method
okay so here on my
SQL workbench I'll write select
I'll use
the keyword that is textint then I'll
use my function which is substring
underscore index
here
substring underscore index will take
three parameters the first is the
original column name which is email
followed by the delimiter I am looking
for that is at the rate so I'll put at
the rate with single code since it is a
text or a character
and then
I'll give the number of times to search
for the delimiter here it is going to be
-1
so you can either give a positive value
or a negative value if it is a positive
number
then this function will return all
to the left of the delimiter if it is a
negative number the function returns all
to the right of the delimiter
all right
and then I'll give an alias name as
say domain name
from my table that is email
if I run this
you can see I have the list of all
unique domain names or distinct domain
names that we saw in our table
this is one of the B's the other way is
to use the substring function
now the substream function extracts a
substring from a string starting at any
position
so let me write my SQL query
I'll write select
distinct
I'll then give my function as substring
here substring will take in two
parameters
the first parameter is the column name
which is email ID here so I'll write
email
I'll give a comma
next
I want to find my delimiter so for that
I am going to use another function
called locate and here I want to locate
the index position of at the rate
so I want to
give my delimiter as at the rate
followed by the column name which is
email
and then
I want to fetch everything after the
delimiter and for that I am giving Plus
1.
again let me give a domain name as the
Alias neem
from email
if I run this
you will see will get all the unique
domain names Gmail yahoo.nn hotmail.com
so these were the two ways to solve the
same problem
now moving ahead
let's look at a sixth question
so our question is write the SQL query
to find duplicate records from a table
without using a temporary table now
one of the other variants of this
question is
to delete all the duplicate records from
a table without using a temporary table
now to solve such queries you can use
the group by anti-hiving clause
let's look at how to do it
so
here on my SQL workbench I'll write my
select query as select
star
and then
I want to count all the duplicate values
let me give it an alias name as
frequency
from
my table name which is employee here
and then I want to group by
name
having
count
star
greater than 1.
let me just run it
all right you can see it has written me
two rows
the name is Angela and Mike and
the frequencies 2 let me just verify
this to see
all the repeated records that were
present in my employee table so I'll
write select star from
employee
if I run this
and scroll down here you can see Angela
is present
here as well as at the bottom similarly
we have mic present twice in the table
and hence
rsql query
returned only Angela and Mike because
they were repeated twice you can see the
frequency here
so let's now look at our seventh
question
given the below employee table
I want to write a query to find all the
even and odd records from the table
so as you can see there are two ways to
do it one is to use a sub query and
another you can use the mod function so
let me show you both the ways
all right so let me run this SQL query
to see all the values present in the
employee table
so if you see the last column which is
the employee ID column has
all the employee IDs as unique values so
I am going to use this in my where
condition
so
to find out all the even records
I'll write my SQL query something like
this
right select star from
employee which is my table name where
employee ID
then I am going to give my
in operator
and inside the in operator I am going to
write my sub query which is Select
employee ID
from employee where
employee ID
percentage 2 is equal to 0.
this means
wherever my employee ID is divisible by
2
it will return only those records so let
me just run it there you go so here we
have easily fetched only the event
records from the table
similarly
if you want to get only the odd records
from the table you can use the mod
function also so I'll write select
star from
employee
where
here I am using the mod function
mod
EMP ID
comma
2
is not equal to 0 so I'll use the
exclamation mark is equal to 0.
so if I run this it will return only the
odd employee IDs you can see it here
so those were the two ways in which you
can fetch the even and odd records from
the table
now
let's see our eighth question
so here is the question
below is the customer table
write a query to get the first purchase
of each customer
now if you look at the table
there are three customer IDs
one zero one one zero two and one zero
three
now customer one zero one made its first
purchase on 2nd October 2018 which is
this one
and customer 102 made its first purchase
on 5th of October 2018 and since we have
just one record for customer ID 103 so
this is going to be his or her first
purchase
so I want to return
101 phone 102 tablet and 103 books
to do this
we are going to use an inner join
if you can see I have used an inner join
and I have also used a sub query
let's write this query on my
SQL workbench
okay
so this query is going to be a little
complicated if you have any questions
then please put it in the chat section
we'll be happy to help you
so I'll start with select
I am going to take an alias name as T
Dot
I'll write
cast underscore ID
comma
T Dot
purchased at
comma
the next field I am going to select is T
dot item
from
customers
as t
next I'm going to write my inner join
and
inside the inner join I'll write my sub
query
is going to be select
customer ID
comma I'm going to take the
minimum value from the purchased at
column which is
my date column
close this I'll give another Alias as
Min underscore
purchased
underscore at
from
my table name is customers
then I'm going to group it
by 1.
here 1 means
the First Column that you have put in
the select statement which is customer
IDs so I am grouping by customer ID
and close the bracket
and everything
I am going to give an alias name as T1
now I am going to join on
T Dot
customer ID
is equal to
T1 Dot
customer ID
and
my next condition would be
T Dot
purchased
at
is equal to
T1 Dot Min underscore
purchased
underscore
at
I'll give a semicolon
I would request you to go through this
SQL query in a detailed manner so that
you understand it line by line
let me just run it now
if I run it
okay it has thrown me an error which
says table SQL underscore IQ customer
does not exist the reason is my table
name is customers
all right so let me run it once again
okay so there's one more error it says
unknown column purchases ad so we had
made a spelling mistake here
it should be purchased at now let's try
and run it
all right there you go so for customer
id101
the first purchase was made on 2nd of
October 2018.
likewise for customer ID 103 the first
purchase was made on 5th of October
and similarly for customer id102
the first purchase was made on 5th of
October again
now moving to the ninth question
so
here we have a weather table we want to
write an SQL query to find all the date
IDs with higher temperature compared to
its previous dates
so if you see we have a date ID column
we have
the date and the temperature
using this table I want to find those
IDs with higher temperature compared to
its previous States so if you look for
date ID 1 the temperature is 32
for data ID 2 the temperature is 31
for date ID 3 the temperature is 30.8
now if you clearly notice for date id4
the temperature is 31.5 which is
actually higher than the previous date
ID which is 30.8
similarly for date ID 5
the temperature is 33 which is again
higher than the previous date which is
31.5
now if you look at the date values they
start from 10th of March till 15th of
March 2020.
now to solve this query we are going to
use
SQL joints again and I am going to use a
function called to underscore days
now my sequel to underscore days
function Returns the number of days
between a given date
and year 0. so let's go ahead and write
this SQL query
all right
so first
let me select
all the rows from the temperature table
which we are going to use for this
particular question
I'll run it
all right there's some error
actually the name of the table is
weather and not temperature so I'll
write weather
let me run it
okay so here you can see
these are all the rows present in the
weather table and as I explained I want
to return only the date ID for
and this date ID which is 5. now one
thing I want to
make it clear here that this data ID
should actually be six so there was a
mistake while creating the tables please
consider this date ID as 6 and not 5. so
I want to return data id4 and data id5
all right so let me go ahead and
write MySQL Query so I'll write select
I'll write
a DOT
date underscore ID
from
whether
as a
comma
I'll join
weather as
B
where
e Dot
temperature
is greater than
B Dot
temperature
and I'll give my condition and
I'll use the function to underscore days
and give my value a DOT
date underscore well
is equal to
I'll use the function again to
underscore these
B Dot
date underscore
well
plus 1
now I would like to know from our
viewers why have we used plus one in the
wear condition so please put your
comments in the chat section
let me just run it
there you go so here you can see
MySQL Query has returned the date ID 4
and 5. which means these are the date
IDs where
the temperature is higher compared to
the previous dates
all right
now coming to the last question on this
interesting session on SQL interview
questions for data scientists
so there are two tables one is the
authors table and the other is the books
table I want to write an SQL query to
find the second highest author who sold
the most number of books
now this is going to be my SQL query
again we are going to use joins on both
tables author and books along with
limit and offset operators so let me
show you how to do it
so first I am going to join my author
table and The Book Table
so I'll write select
e Dot
author underscore name comma
I'm going to find the
sum of
copies sold from the books table so I am
giving my Alias name as B
Dot
copies underscore Soul which is my table
name
I'll give an alias as
sum underscore sold
from
author as a
join
books as B
on
I'll write
B Dot
book underscore name
is equal to
a DOT
book underscore name
now I'm using book underscore name as my
criteria because
book underscore name is the common
column to both the tables
then I'll use a group by clause
I'll write Group by
e Dot
author name
then again I am going to use an order by
Clause I'll write order by
some underscore scold and I am going to
order it in descending order so I'll
write DSC
I'll set my limit as 1.
and
offset S1
if I run this
you will find
that MySQL Query has thrown an error the
reason is we have used
a column called bull underscore name so
it should be book underscore name
there was an error here
let me run it again
okay there was another small error
this would be some underscore sold
let's run it
there you go
so my SQL query has returned author 3
because
author 3 sold the second highest number
of books now let me just remove the last
few lines in the script
let me just run it now
if I run this you can see author 1 sold
the highest number of books followed by
author 3 who sold the second highest
number of books and then it was Author 2
who sold the
least number of books
so if I write limit 1 and
offset 1
what this line of code will do is it
will skip the first row and print the
next row
so let's just run it
and you can see it has written me the
author who sold the second highest
number of books if getting your learning
started is half the battle what if you
could do that for free visit skill up by
simply learn click on the link in the
description to know more
hi everyone welcome to this live session
by simply 11. in today's session we'll
be discussing some of the top
mathematics interview questions for data
science
these questions are based on various
topics in mathematics such as statistics
probability and linear algebra so let's
begin with our first question
the first question we have is what is
normal distribution
now normal distribution is a probability
distribution that is used to check how
your data is distributed
your data could be spread out to the
left or to the right but in many cases
the data tends to be around a central
value with no bias left or right as you
can see on your screens now this is
called a normal distribution where half
of the points are to the left and
another half of the points are to the
right of the center now such a
distribution is contributes to 68 of the
data falling within one standard
deviation of the mean
then we have 95 percent of the data
Falls within second standard deviation
of the mean so if you add up 13.6
percent
and the 68 percent plus 13.6 percent
this will result to 95 percent of data
falling within two standard deviations
of the mean and finally we have close to
99.7 percent of the data falling within
three standard deviations of the mean
so this is
what a normal distribution is all about
now moving on to the next question
so what are the different measures that
are used to summarize the distribution
now to get more idea about the data and
summarize the distribution
you can use three types of statistical
measures the first we have central
tendency measures then we have variation
measures and finally we have shape
measures
now under central tendency measures we
have mean median and mode now the mean
is the most frequently used measures of
central tendency in a data set let me
take you to the Wikipedia page and
explain you what mean exactly is
okay so here you can see on the
Wikipedia page it has the formula to
calculate the mean of a distribution so
this is basically the summation of all
the data points divided by the total
number of data points so here you can
see an example we want to find the
arithmetic mean of five values which are
4 30.
will pick the fourth element from our
ordered set of points that is 6 here and
hence 6 is the median for this ordered
set of points now if you have an even
number of observations as you can see
here total we have eight observations
now the median becomes x n by 2 plus x n
by 2 plus 1 divided by 2.
so your median becomes the mean of the
middle two numbers that is 4 plus 5 by 2
which is 4.5 all right
now the next statistical measure we have
on the central tendency is called the
mode
so the mode is the most frequently
occurring value in a set of data points
so here you can see an example we have a
set of data points
63966593 so your mode is going to be 6
here since 6 is occurring Thrice in the
set of data points
okay
now under variation measures we have
variance standard deviation and
interquartile range so first look at
what variance is
so variance is the average of the
squared differences from the mean now
this is the formula for variance you can
see x i represents the individual set of
data points and mu represents the
average value so mu is nothing but the
summation of all the data points divided
by the total number of data points so
this is your formula for variance
next we have standard deviation so
standard deviation is a measure of the
amount of variation or dispersion of a
set of values a low standard deviation
suggests that the values tend to be
close to the mean while a high standard
deviation suggests that the values are
very spread out now here you can see the
formula for variance now this is denoted
by Sigma and your standard deviation is
nothing but the square root of variance
so here you can see the formula for
variance you have x i minus
your mu squared which is the mean of all
the data points divided by n
then we have the interquartile range
which is also known as IQR so IQR is a
measure of variability based on dividing
a data set into quartiles so an
interquartile range measures where the
bulk of values lie so the formula for
IQR is Q3 which is the third quartile
minus the first quartile so here you can
see we have a list of data points then
we are ordering the data points first
and then
we cut the list into quarters so first
you have the q1 then you have Q2 and
finally you have Q3 so for these set of
data points
your IQR is going to be 7 minus 4 that
is 3.
then we have shape measures so under
shape measures we have skewness and
kotosis so skewness is a measure of
symmetry your distribution is symmetric
if it looks the same to the left and the
right of a Center Point
now they are majorly of two types one is
called negative skew or which is also
known as left skew then we have positive
skewed or right skewed
and finally we have kurtosis sukutosis
is a measure of the tastelessness of a
probability distribution it defines how
heavily the Tails of a distribution
differ from the Tails of a normal
distribution so if you consider these
distributions the kurtosis will
calculate
the tasteless which is actually these
areas of a distribution
now let's look at a third question
so the question is create a sparse
Matrix in Python
now in numerical analysis a sparse
Matrix or a sparse array is a matrix in
which most of the elements are zero now
there are multiple ways to create a
sparse Matrix I'll show you two ways to
create a sparse Matrix so let me take
you to my jupyter notebook where I'll
create a matrix first and then we'll see
how to create sparse Matrix out of it
so I am on my jupyter notebook here
first let me import the necessary
libraries so first I am going to import
numpy as NP and then from
PSI Pi will import
sparse
I'll run this cell okay
next let me go ahead and create a matrix
I'll use the NP dot array function
and within this function I'll create a 3
cross 3 Matrix and I'll assign the
elements
so my first row will have one two three
next we'll have 4 comma 5 comma 6
then
will have
seven eight and nine
all right let me run this cell okay
now
let me go ahead and
print my Matrix so I'll use
the print function and
let's say I'll pass in a message that is
original
Matrix
comma and then pass in my variable name
that is Matrix let's run it so here you
can see I have my original Matrix it is
a three cross three Matrix and has nine
elements in it
first let's see how using dictionary of
keys you can create a sparse Matrix so
for that
I'll write the print statement and then
I am going to use a function which is
called dok that stands for dictionary of
case underscore Matrix
and I'll pass in my variable name which
is my original Matrix so if I run this
you can see
I have got my dictionary of keys sparse
Matrix
next
I'll show you how using diagonal storage
paste you can create a spot
that I am going to use another function
that is sparse dot Dia which stands for
diagonal underscore Matrix
and within this function I'll pass in my
original Matrix variable name
if I run this you can see we have
created diagonal storage paste sparse
Matrix
all right
now moving to the next question
so given below is a matrix
now we need to find the inverse of this
Matrix
now the inverse of a square Matrix a
which is sometimes called as a
reciprocal Matrix is a matrix a inverse
such that the dot product of a and a
inverse is always I which is an identity
Matrix now in numpy we have a function
which you can see here that is NP dot
Lin ALG dot inv that helps you find the
inverse of a matrix let me show you on
the jupyter notebook
first let me print my original Matrix
all right now here
I'll use the function with Min that is
NP Dot l i n e l g dot inv which stands
for inverse and then pass me my variable
name that is Matrix if I run this you
can see I've got the inverse of my
original Matrix
now moving to our fifth question
so this question is related to
probability
you call 2 Uber X and three looks if the
time that each takes to reach you is IID
what is the probability that all the
lips arrive first next we want to find
what is the probability that all the
uberX arrive first now this is fairly a
simple question first let's look at
until the lifts arrive first
then first we need to find the
probability that the first car is a lift
which is going to be three by five since
we have three lives and total we have
five vehicles
next we want to find the probability
that the second car is a lift which is
going to be 2 by 4
third we need to find the probability
that the third curve is also a lift
which is 1 by 3. now that I first is
going to be the multiplication of the
product of
the above three probabilities that is
three by five into two by four into one
by three which is nothing but 1 by 10.
next
let's say you want to find for all the
Ubers that arrive first so you need to
find the probability that the first car
is Uber which is nothing but two by five
since we have two Ubers
next the probability that the second car
is an Uber is going to be 1 by 4 so the
total probability that all the Ubers
arrive first is going to be the product
of two by five
and one by four which is also 1 by 10.
now moving to the sixth question
the mean median and mode of a
distribution are
30 25 and 20
we need to determine if the distribution
plot will be positively skewed or
negatively skewed
so you can see from the distribution
graph the mean is to the right median to
the middle and mode to the left
so since the value of mode is less than
the mean and median the data will be
positively skewed and similarly the
skewness of the distribution is on the
left if the mean value is less than the
median and the mode occurs at the
highest frequency of the distribution
now the next question we have is
what are covariance and correlation
so covariance and correlation are
measures that help you understand the
relationship between two continuous
variables
so covariance tells the direction of the
linear relationship between two random
variables
the direction means if the variables are
directly proportional or inversely
proportional to each other
so increasing the value of one variable
might have a positive or A negative
impact on the value of the other
variable
now covariance can take any value
between minus infinity and plus infinity
So Below you can see the formula to
calculate the covariance so you have
summation x i minus X bar X bar or X Dax
here represents the mean x i represents
the individual points then this is
multiplied by y i minus y Dash or the
mean of Y divided by the total number of
observations
next we have correlation so correlation
explains the change in one variable
leads to how much proportion changes in
the second variable correlation tells
how strongly two random variables are
related to each other it takes values
between
-1 and plus one a negative correlation
means they are inversely proportional to
each other a positive correlation means
they are directly proportional to each
other if the correlation coefficient is
0 then there is no linear relationship
between the variables so here below you
can see the formula to calculate the
correlation
now moving to the eighth question
so what are the sampling methods used in
data science
now sampling is a statistical technique
that is used to select manipulate and
analyze a subset of data points and find
patterns and Trends in the large data
set bin examined
now there are two types of sampling
methods
we have probability sampling and non
probability sampling
now under probability sampling we have
something called as random sampling
so in random sampling every individual
is chosen entirely by chance and each
member of the population has an equal
chance of being selected then we have
stratified sampling so in stratified
sampling subsets of the population are
created based on a common factor and
samples are randomly collected from each
other
now talking about systematic sampling so
in systematic sampling a sample is
created by setting an interval at which
to extract data from the larger
population
an example could be selecting every
fifth row in a spreadsheet of 100 items
and creating a sample size of 20 rows
for analysis
and finally under probability sampling
we have cluster sampling so in a cluster
sampling subgroups of the population are
used as the sampling unit rather than
individuals so the population is divided
into clusters and the whole cluster is
randomly selected to be included in the
analysis
now let's look at the sampling methods
that are present under non-probability
sampling
so first we have convenience sampling so
for convenience sampling the samples are
selected based on the availability so
this method is used when the
availability of samples is rare and
costly
So based on convenience samples are
selected
second we have quota sampling so quota
sampling depends on some preset standard
the proportion of characteristics in
sample should be the same as the
population
next we have purposive sampling
it is also known as judgmental sampling
you select members of sampling on the
basis of the objective of the study
it is very useful when you want to reach
a particular or targeted sample quickly
finally we have snowball sampling so the
snowball sampling technique is used in
situations where the population is
completely unknown and rare now coming
to the ninth question in our list
so explain what p-value tells about
statistical significance
so p-value is a statistical test that is
used in hypothesis testing
it helps you to decide whether to accept
or reject the null hypothesis the null
hypothesis represented as h0
is a commonly accepted fact it is the
opposite of alternative hypothesis
researchers and scientists work to
reject or disapprove the null hypothesis
they come up with an alternative
hypothesis that they think explains a
phenomenon and they work to reject the
null hypothesis
for example null hypothesis or h0 could
be the World is Flat and an alternative
hypothesis could be the world is round
so lower the p-value the greater the
statistical significance of the observed
difference
now the p-value is just one piece of
information you can use when deciding if
your null hypothesis is true or not the
p-value lies between 0 and 1 as you can
see here
the threshold for p is set to 0.05 when
the value is below 0.05 the null
hypothesis is rejected
now coming to the final question in our
list of mathematics interview questions
for data science so the question is what
do you understand by type 1 versus type
2 error now in statistical Theory the
idealistical error is an integral part
of hypothesis testing
in statistical hypothesis testing no
test is ever 100 certain
that's because we rely on probabilities
to experiment
even though hypothesis tests are meant
to be reliable there are two type so
type 1 error occurs when the null
hypothesis is true and we reject it it
is also known as a false positive
finding or conclusion an example could
be an innocent person is convicted
and type 2 error occurs when the null
hypothesis is false and we accept it it
is also known as false negative finding
or conclusion an example could be a
guilty person is not convicted so here
you can see we have a reality where your
null hypothesis is true and here we have
the null hypothesis is false and this is
where the type 1 error lies so
this can also be represented as false
positive where the null hypothesis is
true and be rejected and this could be
false negative where the null hypothesis
is in reality it's false and we accept
it
welcome to this video tutorial series on
data science interview questions
in our previous video we had discussed
the important interview questions on
mathematics statistics probability and
linear algebra that are often asked in
any data science interview
today we will learn about some of the
machine learning and deep learning
interview questions
so let's begin with the first question
the first question is how to detect
outliers in data
so in data analytics and machine
learning
you often find data points that lie at
an abnormal distance from other points
in a random sample from a population
those are called outliers now outliers
in data can significantly impact any
prediction analysis
there are majorly three different
methods to treat outliers
first we have the univariate method
it is one of the simplest methods for
detecting outliers the univariate method
uses box plots a box plot is a graphical
display for describing the distributions
of the data
box plots use the median and the lower
and upper quartiles
this method looks for data points with
extreme values on one variable next we
have the multivariate method so the
multivariate outliers can be found in an
n-dimensional space having n features we
look for unusual combinations of all the
variables in this method finally we have
minkowski error
this method reduces the contribution of
potential outliers in the training
process the minkowski error is a loss
index that is more insensitive to
outliers than the standard mean squared
error now moving on to the second
question
what is a confusion Matrix
so a confusion Matrix is a table that is
used to describe the performance of a
classification model and a set of test
data for which the True Values are
already known
the target variable has two values
positive or negative
The Columns represent the actual values
of the target variable which you can see
here the rows represent the predicted
values of the target variable which you
can see here now there are four
important terms that are related to
confusion Matrix first we have true
positive which is this one
so in true positive the predicted value
matches the actual value
so the actual value was positive and the
model also predicted a positive value
then we have true negative
which is also represented as TN the true
negative depicts the predicted value
matches the actual value
now the actual value was negative and
the model predicted a negative value
next we have false positive now false
positive is also known as type 1 error
in false positive the predicted value
was falsely predicted the actual value
was negative but the model predicted a
positive value finally we have false
negative now false negative is also
known as type 2 error
so in false negative the predicted value
was falsely predicted the actual value
was positive but the model predicted a
negative value now moving to a third
question
which is explain the ROC curve
now the ROC curve is one of the most
important evaluation Matrix for checking
the performance of any classification
model
Roc stands for receiver operating
characteristic receiver operating
characteristic or Roc curve is a method
to compare the diagnostic test
The ROC curve is created by plotting the
true positive rate against the false
positive rate at various threshold
settings so here on the y-axis you have
the true positive rate on the x-axis we
have the false positive rate the true
positive rate indicates the proportion
of observations that were correctly
predicted to be positive out of all
positive observations similarly the
false positive rate is the proportion of
observations that are incorrectly
predicted to be positive out of all
negative observations
you can take an example suppose in
medical testing the true positive rate
is the rate in which people are
correctly identified to test positive
for the decision question let's say the
coronavirus testing Roc does not depend
on any class distribution this makes it
useful for evaluating classifiers
predicting rare events such as diseases
or disasters now moving to the fourth
question we have what are the
assumptions for linear regression
so linear regression analysis is used
for modeling the relationship between a
single dependent variable Y and one or
more feature or predictor variables
some of the important assumptions for
linear regression are
so first
they should have linearity so linear
regression needs the relationship
between the independent and the
dependent variables to be linear
it is also crucial to check for outliers
since linear regression is sensitive to
outlier effects next we have homo
sedasticity
illustrates a situation in which the
error term that is the noise or random
disturbance in the relationship between
the features and the target variable is
the same across all levels of the
dependent variables
third we have Independence so
observations should be independent of
each other
finally we have no multi-collinearity
so there should be little or no
multi-collinearity independent variables
should not be too highly correlated
now moving to our fifth question in our
list of interview questions
the question is what is regularization
in machine learning explain the L2
regularization
so regularization is a machine learning
technique that is used to reduce the
Errors By fitting the function
appropriately on the training set in
order to avoid overfitting of data so
overfitting happens when a model learns
the detail and noise in the training
data to the extent that it negatively
impacts the performance of the model on
new data so here you can see we have
a nice plot which shows how overfitting
of data can be visualized
and here
we have a good fit line over the same
data points so this is also known as the
regression line
now L2 regularization is also known as
Ridge regression so Ridge regression
modifies the Overton model by adding the
squared magnitude of coefficient as a
penalty term to the loss function so on
the right you can see a set of data
points plotted and we have our linear
regression line
and here we are calculating the cost
function for the ridge regression line
so our cost function is actually loss
plus Lambda into summation of w Square
where loss is actually the sum of
squared errors or squared residuals
Lambda stands for penalty for the errors
W is called the slope of the curve or
line
okay now consider a case
where there are two points passing
through the linear regression line
now if you calculate the cost function
we get the value as 1.69 so here we have
assumed that loss is 0 since the two
points lie directly on the line we have
taken Lambda to B1 and W is 1.3 so if
you use this function or this formula
you get the cost function as 1.69
now moving ahead
let's consider another situation where
we'll calculate the same cost function
for the ridge regression line there is
some loss for both the points as they
are not on the same line so here you can
see the sum of squared residuals is 0.05
is actually is the sum of 0.2 Square I
am assuming this as 0.2 and 0.1 for this
one
so if you square both and add it the
value is 0.05 a Lambda is again 1 and W
we have assumed to be 0.6 now if you
find the cost function the value is 0.41
let's draw the linear regression line
and the ridge regression line with all
the points we find that the reach
regression line as the best fit since
its cost function is less if getting
your learning started is half the battle
what if you could do that for free visit
scale up by simply learn click on the
link in the description to know more
now coming to the sixth question
what are the different methods to split
a tree in a decision tree algorithm
so there are three methods to split a
decision tree first we have variance
so reduction in variance is an algorithm
that is used for continuous Target
variables this algorithm uses the
standard formula of variance to choose
the best split so here you can see the
standard formula variance which is
summation of X that is all the
individual points minus X bar which is
the mean squared divided by the total
number of observations
now the split with lower variance is
selected as the criteria to split the
population
now the steps to calculate variance is
you need to calculate variance for each
node and then you need to calculate for
each split as the weighted average of
each node variance
moving ahead the second method we have
is Information Gain
so Information Gain is used for
splitting the notes when the target
variable is categorical
it works on the concept of entropy
now the degree of disorganization in a
system is known as entropy so here you
can see the formula for Information Gain
which is 1 minus entry p
finally we have gener impurity
so gini impurity is the probability of
incorrectly classifying a randomly
chosen element in the data set if it
were randomly labeled according to the
class distribution in the data set So
Below you can see the formula for Gene
impurity
so we have one minus summation of p i
whole Square where n represents the
number of classes and PO5 represents the
probability of randomly picking an
element of class I
now moving to the seventh question
so the question is how do we find the
optimum cluster value in k-means
clustering algorithm
now there are two methods to find the
optimum cluster value
so first we have the elbow method which
is one of the most well known for
finding the optimum number of clusters
so in this method you need to calculate
the within cluster sum of squared errors
for different values of K and choose the
k for which within cluster sum of
squared errors first starts to diminish
so in the below plot of squared errors
versus the number of clusters k
you can see at K is equal to 4 the
squared error starts to diminish so
hence our Optimum K value is 4.
next we have the silhouette method
so the siloid method measures how
similar a point is to its own cluster
compared to other clusters
the average siloid method computes the
average celloid of observations for
different values of K the optimum number
of clusters K is the one that maximizes
the average Deloitte over a range of
possible values for k
the silhouette score reaches its Global
maximum at the optimal K so in our case
the average siloid reaches maximum at K
is equal to 2 which you can see here so
our Optimum cluster value will be 2 here
moving ahead
the eighth question in our list is how
does the pooling layer work in a
convolutional neural network
so the pooling layer performs a down
sampling operation in order to reduce
the dimensionality of the feature map
so in the pooling operation you slide a
two-dimensional filter over each channel
of feature map and summarize the
features lying within the region covered
by the filter
it is a common practice to periodically
insert a pulling layer in between
successive convolutional layers in a
convolutional neural network
architecture
so the pooling layer operates
independently on every depth slice in
the input and resizes it spatially using
the max operation
so in the diagram shown here you can see
we have a rectified feature map we are
using a 2 cross 2 filter and performing
a Max pooling operation
so consider this as the filter if you
perform the max operation over the
values
let's say 0 5 3 and 1 so considering
this one our pool feature map maximum
value will be 5. similarly for this
chunk of data it is going to be 7 next
if you slide the filter over this square
frame you get eight and similarly here
you get six so this is also known as a
pooled feature map
moving ahead
the ninth question in our list is how
does lstm Network so long short term
memory networks are a type of recurrent
neural networks that are capable of
learning audit dependence and sequence
prediction problems so remembering
information for long periods of time is
practically their default Behavior
now lstms also have this chain-like
structure which you can see here
but the repeating module has a different
structure so instead of having a single
neural network layer there are four
interacting in a very special way now
you can see these are called as Gates
these Gates contain sigmoid activations
a sigmoid activation is similar to the
tannage activation instead of squishing
values between -1 and plus one its
squishes values between 0 and 1
and lstm has four Gates
now these are called forget remember
learn and use or output so if you see
this in the first step we use the forget
gate that decides what information
should be thrown away or kept the
information from the previous hidden
State and the information from the
current input is passed through the
sigmoid function values come out between
0 and 1.
so if the value is closer to zero it
means you need to forget that
information and if the value is closer
to 1 it means you need to keep that
information
next we have the input gate so the input
gate is used to update the cell State
first we pass the previous hidden State
and the current input into a sigmoid
function
that decides which values will be
updated by transforming the values to B
between 0 and 1 0 means not important
and 1 means important
you also pass the hidden State and
current input into the tan H function to
flatten the values between minus 1 and
plus one this helps to regulate the
network
then you multiply the tan H output with
the sigmoid output the sigmoid output
will decide which information is
important to keep from the tanh output
and finally in step 3 we have the output
gate this output gate is used to decide
what the next hidden State should be
first we pass the previous hidden State
and the current input into a sigmoid
function then we pass the newly modified
cell State into the tan H function
we then multiply the tan H output with
this sigmoid output to decide what
information the hidden sheet should
carry the output is the hidden State the
new cell State and the new hidden state
is then carried over to the next time
step finally
talking about the last question in our
list of interview questions we have
explain the concept of gradient descent
in deep learning now gradient descent is
an optimization algorithm which is
mainly used to find the minimum of a
function in machine learning gradient
descent is used to update the parameters
in a model
parameters can vary according to the
algorithm such as coefficients in linear
regression and weights in neural
networks
you can see
we have these maps and on the y axis we
have the loss
on the x axis we have the weight
and here we are trying to find the local
minimum or the global minimum
now this gradient descent method is used
to minimize the cost function and update
the parameters of the learning model
the gradient always points in the
direction of the steepest increase in
the loss function
the gradient descent algorithm takes a
step in the direction of the negative
gradient in order to reduce the loss as
quickly as possible
to determine the next point along the
loss function curve the gradient descent
algorithm adds some fraction of the
gradients magnitude to the starting
point now this process is repeated to
find the global minimum
we have reached the end of this data
science bootcamp I hope the session was
interesting and informative if you have
any questions regarding any topic
discussed in the boot camp feel free to
reach out to us a team of experts will
be happy to help you out thank you and
keep learning
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
thank you