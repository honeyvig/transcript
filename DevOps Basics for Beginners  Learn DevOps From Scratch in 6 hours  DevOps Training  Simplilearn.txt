are you ready to revolutionize the world
of software development and accelerate
your current devops devops Basics is
your gateway to unlocking the power of
continuous integration deployment and
delivery join the ranks of successful
day of professionals and embark on a
transformative Journey towards
efficiency collaboration and Innovation
devops isn't just a trend it's a game
changer by breaking down the barriers
between development and operations team
you can streamline Workforce reduce time
to Market and improve overall product
quality say goodbye to sealer processes
and hello to seamless collaboration
Automation and Agility did you know that
companies implementing Deva practices
experience 200 times more frequent
deployments and recover from failures 24
times faster devops is reshaping the
industry and the opportunities for
professionals with devops skills are
soaring but let's talk about the real
impact dios professionals command
impressive salaries with entry level
positions offering a range of 17
thousand dollars to hundred thousand
dollars per year as you advance in your
career you can unlock even more
lucrative opportunities in this rapidly
evolving field so if you are ready to
embrace this exciting career path look
no further then simply learns Caltech
postgraduate program in devops this
comprehensive program will equip you
with the knowledge and skills needed to
master the day of principles tools and
practices dive deep into
containerization orchestration and
automation Frameworks like Docker
kubernetes and Jenkins gain hands-on
experience with Cloud platforms and
learn how to Leverage The Power of
infrastructure as code don't miss out on
this chance to transform your career and
become an invaluable asset to any
organization click the link in the
description to discover more about this
day offs course here's an inspiring
success story from our satisfied
Learners who have propelled their career
with devops the future of software
development is here and it's waiting for
you to embrace it so let's get started
with it
thank you
hey there my name is Manuel canalon and
I'm Rafael Canada we're actually
brothers and we live here in Alberta
Canada we do a lot of things together
and this time we decided to take a
course in devops from Simply learn the
course proved to be very beneficial for
both our careers yeah I got a promotion
after the course and I got a hired with
a new company
I'm actually a mechanical engineering
graduate and I've been working at my
company for the last four years in 2020
I came to know that my company was
starting the hiring process for a devops
infrastructure team
at the time I was working as an
implementation specialist and my
colleagues recommend that I take a
course in devops if I wanted to be part
of the team so I started doing my
research and then he came to me saying
that he was also looking for a devops
course yeah funny enough I actually
found the course through Instagram but
you know my situation was slightly
different I had been working as a
geomatics field technician for about
four years but I wanted to change my
career into something that involved more
programming and automation so when I
came across with this course I said you
know what let's take it
and caltex name was definitely a
deciding factor in July 2021 we got our
certifications and I immediately posted
that on my LinkedIn profile you know
coincidentally in November of that same
year we got the good news
and I got hired as a dis developer
which has allowed me to do way more
programming which is what I wanted to do
around that time I got promoted to the
devops infrastructure team and saw a 21
salary hike the overall course was
awesome but the part that I enjoyed the
most was a Docker certification training
you know what I also really enjoyed the
docker certification training I think
there were brothers after all
yeah the experience was amazing and
being able to study together was
definitely very nice yeah it brought us
back to those good old days when we were
going to school together through a
number of key elements today the first
two will be reviewing models that you're
already probably using for delivering
Solutions into your company and the most
popular one is waterfall followed by
agile then we'll look at devops and how
devops differs from the two models and
how it also borrows and leverages the
best of those models we'll go through
each of the phases that are used in
typical devops delivery and then the
tools used within those phases to really
improve the efficiencies within devops
finally we'll summarize the advantages
that devops brings to you and your teams
so let's go through waterfall so
waterfall is a traditional delivery
model that's been used for many decades
for delivering Solutions not just its
Solutions and digital Solutions but even
way before that it has its history it
goes back to World War II so waterfall
is a model that is used to capture
requirements and then Cascade each key
deliverable through a series of
different stage Gates that is used for
building out the solution so let's take
you through each of those stage Gates
the first that you may have done is
requirements analysis and this is where
you sit down with the actual client and
you understand specifically what they
actually do and what they're looking for
in the software that you're going to
build and then from that requirements
analysis you'll build out a project
planning so you have an understanding of
what the level of work is needed to be
able to be successful in delivering the
solution after that you've got your plan
then you start doing the development and
that means that the programmers start
coding out their solution they build out
their applications to build out the
websites and this can take weeks or even
months to actually do all the work when
you've done your coding and development
then you send it to another group that
does testing and they'll do full
regression testing of your application
against the systems and databases that
integrate with your application you'll
test it against the actual code you'll
do manual testing you do UI testing and
then after you've delivered the solution
you go into maintenance mode which is
just kind of making sure that the
application keeps working there's any
security risks that you address those
security risks now the problem you have
though is that there are some challenges
however that you have with the waterfall
model the cascading deliveries and those
complete and separated stage Gates means
that it's very difficult for any new
requirements from the client to be
integrated into the project so if a
client comes back and it's the project
has been running for six months and
they've gone hey we need to change
something that means that we have to
almost restart the whole project it's
for very expensive and it's very time
consuming also if you spend weeks and
months away from your clients and you
deliver a solution that they are only
just going to see after you spend a lot
of time working on it they could be
pointing out things that are in the
actual final application that they don't
want or are not implemented correctly or
lead to just general unhappiness the
chance you then have is if you want to
add back in the client's feedback to
restart the whole waterfall cycle again
so the client will come back to you with
a list of changes and then you go back
and you have to start your programming
and you have to then start your testing
process again and just you're really
adding in lots of additional time into
the project so using waterfall model
companies have soon come to realize that
you know the clients just aren't able to
get their feedback and quickly
effectively it's very expensive to make
changes once the teams have started
working and the requirement in today's
digital world is that solution simply
must be delivered faster and this has
led for a specific change in agile and
we start implementing the agile model so
the agile model allows programmers to
create prototypes and get those
prototypes to the client with the
requirements faster and the client is
able to then send their requirements
back to the programmer with feedback
this allows us to create what we call a
feedback loop where we're able to get
information to the client and the client
can get back to the development team
much faster typically when we're
actually going through this process
we're looking at the engagement cycle
being about two weeks and so it's much
faster than the traditional waterfall
approach and so we can look at each
feedback loop as comprising of four key
elements we have the planning where we
actually sit down with the client and
understand what they're looking for we
then have coding and testing that is
building out the code and the solution
that is needed for the client and then
we review with the clients the changes
that have happened but we do all this in
a much tighter cycle that we call a
Sprint and that typically a Sprint will
last for about two weeks some companies
run sprints every week some run every
four weeks it's up to you as a team to
decide how long you want to actually run
a Sprint but typically it's two weeks
and so every two weeks the client is
able to provide feedback into that Loop
and so you were able to move quickly
through iterations and so if we get to
the end of Sprint 2 and the client says
hey you know what we need to make a
change you can make those changes
quickly and effectively for Sprint three
what we have here is a breakdown of the
ceremonies and the approach that you
bring to Agile so typically what will
happen is that a product leader will
build out a backlog of products and what
we call a product backlog and this will
be just a whole bunch of different
features and they may be small features
or bug fixes all the way up to large
features that may actually span over
multiple Sprints but when you go through
the Sprint planning you want to actually
break out the work that you're doing so
the team has a mixture of small medium
and large solutions that they can
actually Implement successfully into
their Sprint plan and then once you
actually start running your Sprint again
it's a two-week activity you meet every
single day the two with the actual
Sprint team to ensure that everybody is
staying on track and if there's any
blockers that those blockers are being
addressed effectively and immediately
the goal at the end of the two weeks is
to have a deliverable product that you
can put in front of the customer and the
customer can then do a review the key
advantages you have are running a Sprint
with agile is that the client
requirements are better understood
because the client is really integrated
into the scrum team they're there all
the time and the product is delivered
much faster than with a traditional
waterfall model you're delivering
features at the end of each Sprint
versus waiting weeks months or in some
cases years for a waterfall project to
be completed however there are also some
distinct disadvantages the product
itself really doesn't get tested in a
production environment it's only been
tested on the developer computers and
it's really hard when you're actually
running agile for the Sprint team to
actually build out a solution easily and
effectively on their computers to mimic
the production environment and the
developers and the operations team are
running in separate silos so you have
your development team running their
Sprint and actually working to build out
the features but then when they're done
at the end of their Sprint and they want
to do a release they kind of fling it
over the wall at the operations team and
then it's the operations team job to
actually install the software and make
sure that the environment is running in
a stable fashion that is really
difficult to do when you have the two
teams really not working together so
here we have is a breakdown of that
process with the developers submitting
their work to the operations team for
deployment and then the operations team
may submit their work to the production
servers but what if there is an error
what if there was a setup configuration
error with the developers test
environment that doesn't match the
production environment there may be a
dependency that isn't there there may be
a link to an API that doesn't exist in
production and so you have these charges
that the operations team are constantly
faced with and their challenge is that
they don't know how the code works so
this is where devops really comes in and
let's dig into how devops which is
developers and operators working
together is the key for successful
continuous delivery so devops is as an
evolution of the agile model the agile
model really is great for Gathering
requirements and for developing and
testing out your Solutions and what we
want to be able to do is kind of address
that challenge and that gap between the
Ops Team and the dev team and so so with
devops what we're doing is bringing
together the operations team and the
development team into a single team and
they are able to then work more
seamlessly together because they are
integrated to be able to build out
solutions that are being tested in a
production-like environment so that when
we actually deploy we know that the code
itself will work the operations team is
then able to focus on what they're
really good at which is analyzing the
production environment and being able to
provide feedback to the developers on
what is being successful so we're able
to make adjustments in our code that is
based on data so let's step through the
different phases of a devops team so
typically you'll see that the devops
team will actually have eight phases now
this is somewhat similar to Agile and
what I'd like to point out at the time
is that again agile and devops are very
closely related that agile and devops
are closely related delivery models that
you can use with devops it's really just
extending that model with the key phases
that we have here so let's step through
each of these key phases so the first
phase is planning and this is where we
actually sit down with a business team
and we go through and understand what
their goals are the second stage is as
you can imagine and this is where it's
all very similar to Agile is that the
code is actually start coding but they
typically they'll start using tools such
as git which is a distributed Version
Control software it makes it easier for
developers to all be working on the same
code base rather than bits of the code
that is rather than them working on bits
of the code that they are responsible
for so the goal with using tools such as
git is that each developer always has
the current and latest version of the
code you then use tools such as Maven
and Gradle as a way to consistently
build out your environment and then we
also use tools to actually automate our
testing now what's interesting is when
we use tools like selenium and junit is
that we're moving into a world where our
testing is scripted the same as our
build environment and the same as using
our get environment we can start
scripting out these environments and so
we actually have scripted production
environments that we're moving towards
Jenkins is the integration phase that we
use for our tools and another Point here
is that the tools that we're listing
here these are all open source tools
these are tools that any team can start
using we want to have tools that control
and manage the deployment of code into
the production environments and then
finally tools such as ansible and Chef
will actually operate and manage those
production environments so that when
code comes to them that that code is
compliant with the production
environment so that when the code is
then deployed to the many different
production servers that the expected
results of those servers which is you
want them to continue running is
received and then finally you monitor
the entire environment so you can just
zero in on spikes and issues that are
relevant to either the code or changing
consumer habits on the site so let's
step through some of those tools that we
have in the devops environment so here
we have is a breakdown of the devops
tools that we have and again one of the
things I want to point out is that these
tools are open source tools there are
also many other tools this is just
really a selection of some of the more
popular tools that are being used but
it's quite likely that you're already
using some of these tools today you may
already be using Jenkins you may already
be using git but some of the other tools
really help you create a fully
scriptable environment so that you can
actually start scripting out your entire
devops tool set this really helps when
it comes to speeding up your delivery
because the more you can actually script
out of the work that you're doing the
more effective you can be at running
automation against those scripts and the
more effective you can be at having a
consistent experience so let's step
through this devops process so we go
through and we have our content
continuous delivery which is a plan code
build and test environment so what
happens if you want to make a release
well the first thing you want to do is
send out your files to the build
environment and you want to be able to
test the code that you've been created
because we're scripting everything in
our code from the actual unit testing
being done to the all the way through to
the production environment because we're
testing all of that we can very quickly
identify whether or not there are any
defects within the code if there are
defects we can send that code right back
to the developer with a message saying
what the defect is and the developer can
then fix that with information that is
real on the either the code or the
production environment if however your
code passes the the scripting tests it
can then be deployed and once it's out
to deployment you can then start
monitoring that environment what this
provides you is the opportunity to speed
up your delivery so you go from the
waterfall model which is weeks months or
even years between releases to Agile
which is two weeks or four weeks
depending on your Sprint Cadence to
where you are today with devops where
you can actually be doing multiple
releases every single day so there are
some significant advantages and there
are companies out there that are really
zeroing in on those advantages if we
take any one of these companies such as
Google Google any given day will
actually process 50 to 100 new releases
on their website through their devops
teams in fact they have some great
videos on YouTube that you can find out
on how their devops teams work Netflix
is also a similar environment now what's
interesting with Netflix is that Netflix
have really fully embraced devops within
their development team and so they have
a devops team and Netflix is a
completely digital company so they have
software on phones on Smart TVs on
computers and on websites interestingly
though the devops team for Netflix is
only 70 people and when you consider
that a third of all internet traffic on
any given day is from Netflix it's
really a reflection on how effective
devops can be when you can actually
manage that entire business with just 70
people so there are some key advantages
that devops has it's the actual time to
create and deliver a software is
dramatically reduced particularly
compared to Waterfall complexity of
maintenance is also reduced because
you're automating and scripting out your
entire environment now you're improving
the communication between all your teams
so teams don't feel like they're in
separate silos but that are actually
working cohesively together and that
there is continuous integration and
continuous delivery so that your
consumer your customer is constantly
being delighted 10 most popular tools
that you'll need to have in your devops
setup what we're going to do is we're
going to break up the tools into eight
different categories now we're going to
go through the planning and code base
tools the building tools the testing
tools the all-important integration tool
the deployment operation and then
finally monitoring tools something we
should also add is that all of these
tools are open source this means that
you can get up and running with your
devops environment immediately without
having to worry about licensing costs so
let's jump into the very first tool
which is get so git is a distributed
Version Control tool that allows
development teams to be able to share
code amongst each other in a way that
allows the creation of small to
extremely large applications to be
created effortlessly if we look at the
architecture of getting the way that it
works that there are four key areas we
have the working directory the the
staging area local repository and then
remote repository so your working
directory is where you actually create
all the files that you'll be working on
in your application now the staging is
the area that you get ready to commit
those files to your local repository for
Version Control and once you actually
have then put them into your local
repository then you want to be able to
push those files using a push command
out to a remote repository such as
GitHub or gitlab in addition if you have
a distributed team you can then also
then pull down the latest files so as a
team you can always be working on all
the files all the time and then go
through the whole process Again by
working on your local files you can do
this whole process really quickly and
easily get is actually a free tool that
you can download and run through command
line off your computer finally you want
to be able to merge files that you may
be working on say for instance a
different branch which is a new feature
you want to be able to merge those files
back into the master directory the
master Branch that's running on git
itself so some of the things to a point
point out with get what makes it really
useful is that it's absolutely brilliant
at being able to track changes in your
source code this is something that
source code tools really kind of should
be able to do but get is exceptionally
good at this having large teams and
working at different phases within the
project and working on different
branches within the project is really
easy to do and the collaboration tools
will engage and make it so that your
teams can work effortlessly with each
other inherently you are creating
backups of all your code which is
fantastic and probably most important
part is that the environment itself is a
non-linear environment this means that
you don't have to work through specific
gates to be able to reach feature to
actually get a feature ready for
delivery you can actually work in a
non-linear fashion so who's using git
well as you can imagine all of the big
technology companies are using gets
including Netflix left and Intel plus
many many other companies so let's get
on to our next tool which is Maven and
the focus on Maven is to automate the
build process of code that's being
checked into a repositories such as a
git repository so we have a look at the
architecture here Maven itself actually
executes what are called Palm files and
those Palm files actually then look for
specific dependencies that will then go
against a repository to be able to build
out the project you can actually extend
the pump files with plugins and you can
actually then create a specific build
profile for your environments which is
absolutely fantastic because you can
really fine-tune the profile and again
all of this is scripted and this is
something that is common with all the
tools that you'll be using in a true
devops environment is that you can
script all of them which means that you
can automate the processes that are in
your devops environment and it really
accelerate the ability to be able to
build consistent high quality software
final step for I'm having environment is
to actually build all the software into
a in our build server so a couple of
things that are important to notice with
Maven one of the things that you'll see
that's key with all of the tools that we
have here really easy to use Maven
itself it's got a pretty simple setup
it's highly available which means that
your teams can rely on the solution
being ready for your environment and you
can actually get to it and use it very
easily and consistently you can have
multiple builds happening at the same
time which is really important
particularly if you have branched
software and you want to be able to test
the branches and the other thing as well
is that it's open source so you have
instant access to new features without
any additional configuration so it
really helps your teams to get the new
code and Implement new features within
the environment itself so you're always
working on the latest and greatest as
you can imagine devops shops such as
Zillow and Zen's and JPMorgan Chase are
already using Maven and you'll see that
many devops shops are also using Maven
for their build software testing with
selenium is very important and being
able to automate that environment is
critical so you can actually guarantee
that your code will work and selenium
itself is designed for testing and
automating web applications it's really
good in that environment and we look of
the architecture the way that it works
is that you actually have a client
library that goes against the code
you're writing whether that's C sharp
python Java JavaScript and then there
are specific drivers that go against the
web browsers to actually test out the
code that you have created so that you
can actually see whether it works in the
appropriate web browsers and all of the
popular web browsers are supported with
selenium selenium is an open source
solution it does allow you to write
scripts in multiple different languages
which is a real big bonus and you can as
well run parallel test execution so this
bounces very well with Maven and as with
other tools that we have it's really
easy to use and selenium is being used
by Google HubSpot and Salesforce as well
as many other companies so let's talk
about the really important part which is
integrating all of this and Jenkins
really is the de facto tool in this
space so the role of Jenkins is to be
able to match and allow for that
continuous integration that you need to
make devops successful so allowing the
dev the development team to integrate
effectively with the Ops the operations
team and do that through testing and
management of the code that has been
created so if we look at the
architecture here we have our remote
source code repository it's going to be
waiting for a Jenkins pull to be able to
create a commit and then that Jenkins
serve Master will then push out and
distribute the work to multiple slaves
so that you actually have the actual
code being pushed around your
environment so Jenkins is really easy to
use it's very mature it's been around
for many years there are a significant
number of plugins that allowed for the
Jenkins environment to be scaled very
effectively and it is portable across
multiple Platforms in fact most of the
leading operating systems will support
Jenkins and as you can imagine Jenkins
is being used by companies such as
LinkedIn and Microsoft eBay Dell and
Cloudera so let's talk about the other
side of devops and took about some of
the operations tool and we'll start off
with darker so the role of Docker is to
make it easy for people to be able to
simulate full environments quickly and
easily on their desktop for building and
testing but at the same time make it
really easy to be able to Port those
environments across multiple areas so
let's let me talk a little bit about how
that would work so with the docker
engine you have a Docker client locally
that allows you to emulate the whole
Docker environment without having to
have the overhead that you would
typically have if you're doing
virtualization or dedicated servers
running specific dedicated operating
systems because you are scripting the
entire environment Docker is a very
scalable and repeatable environment for
being able to take across the entire
Cloud Network so some of the features
are really important for Docker it's
incredibly portable across multiple
platforms the docker team have done
fantastic job on portability it does run
in isolation so what this means is that
you can actually have multiple Docker
environments running on a server
concurrently and they will not interrupt
each other they actually fully isolated
sandbox environments so the other part
to Dockers because it is stripped out of
needless operating system and and host
environments apis and services the
actual startup and boot time for a
Docker is really very fast which in turn
makes this very scalable and efficient
to run and you can actually reuse
volumes very easily so who's using
Docker just about everybody Spotify Uber
PayPal BBC really Docker is used by a
lot of people just a side note probably
something that would complement Docker
very well is kubernetes and kubernetes
is becoming a very popular alternative
to Docker and so you know really get to
know both of those tools so let's talk
about some of the operation and
management tools that you have for your
environment and the three big ones here
are chef ansible and puppet and we'll
start with ansible and ansible is a
configuration software package and
unlike the other two products here
unlike chef and puppet ansible doesn't
actually require a client to run on the
host server where you're pushing out a
configuration you can actually have a
defined operation configuration so
that's a web server configuration or a
database configuration or an iot Cloud
configuration you can actually have that
running effectively from a script that
is written on a client machine and then
on your local machine and then you can
push that script that you've created out
to your remote servers and have
everything work as you'd expect quickly
on your remote servers you know just
some of the key features of and support
is incredibly lightweight one of the
things that's really important with
ansible is that it's actually a very
easy easy to use there is unlike the
other two tools we'll be covering unlike
um puppet and Chef ansible actually has
a full GUI that that you can use to
actually interact and build out
Solutions so there is a visual interface
that you can actually go in so you're
not just working completely in command
line companies that are using ansible
include Twitter EA Sports Cisco and NASA
as well as Verizon so complementing
ansible but probably a little bit more
popular right now are chef and puppet
and we'll have a chef first now both
chef and puppet are somewhat similar in
configuration and both of them rely on
having a client server relationship with
their architecture the way that Chef
works is that Chef actually creates
recipes and each recipe is a server
configuration setup and what you
actually do is you create individual
recipes and then for your entire network
you create what's called a cookbook and
that cookbook actually can be used to
control the setup of your entire network
and this works really well for being
able to create consistent and highly
scalable environment governments things
are great with Chef is that it's
actually a very flexible configuration
and you can actually do an awful lot
with it it has really good security
features and which are very appealing to
particularly if you have a devsec Ops
environment where the security team are
integrated with your devops environment
and that's something that's becoming
more and more popular the chef itself is
actually pretty easy to use it is a
command line tool but it is actually
fairly easy to use and the actual
scripting of the recipes is something
that you can do even if you don't have
much scripting experience or something
there are so many recipes out and
available you can actually use many of
the templates that have already been
written and just either copy them or
modify them to your environment as you'd
expect a lot of the companies are using
Chef are the big technology companies
such as Facebook and IBM Microsoft in
particular has really invested heavily
and chef but you'll find that Chef is
probably the most popular management
tool used within the Azure environment
so a complimentary product to Chef is
pop it and if we look at puppets as you
can imagine again another open source
configuration tool the structure as you
can see is actually very similar in
concept to a chef in which you have a
local server that actually has all the
configuration environments the templates
and the files and the manifests and it
will actually then communicate with all
of your entire environments such as and
then with publicly refer to them as
nodes and within your node you have
multiple servers but it's similar in
concept to Chef in many ways and that
it's just another way of being able to
control your environment both are really
great Tools in fact really all three of
them ansible chef and puppet are great
tools what you have to do as a devops
team is really just decide which tool is
the one that's appropriate for your
environment so some things that are
great with puppet it's a multi-platform
compatible it's very scalable and it
allows you to push out changes to your
environment really really fast and who
are using puppets you've got right hat
HP Google leading devops teams are using
puppets today so let's get into the two
tools that we have that actually monitor
our operations environment Splunk and
negos so uh Splunk is as you're
expecting some monitoring tool and its
goal is to actually monitoring the
operations of all the environments that
you have to make sure that they're
working correctly and it does this by
going through and reviewing your entire
environment and writing the operational
information to disk and then searching
that environment to make sure everything
works correctly it does a lot of
analysis in real time it is a very easy
to set up and use tool for testing for
analysis and to then be able to then
troubleshoot any failures that may come
up the thing that's great about Splunk
is that you can actually put it in a
distributed environment so that's no
single point of failure which makes it
itself inherently very very scalable so
Cisco Facebook Domino's Pizza Dubai
airports are all customers that are
using Splunk today in their devops
environments so finally let's jump into
our final monitoring tool which is uh
negios and very similar to Splunk is
nego so probably the big difference is
that it's client server setup but really
again it's all about being able to
Monitor and check the status of your
entire environment and your network and
your service in real time and give you
the appropriate visualizations whether
it's on your laptop or whether it's an
alert that's sent out to your phone and
you know so you're aware in real time
how your network is performing and can
then check and adjust to actually making
changes and the bottom line in this kind
of architecture with Splunk and nagos is
that it allows you as the operations
lead to be able to be ahead of any
perceived impact so say for instance you
are putting in specific check marks for
your environment you can actually have
alerts being sent out to your team when
certain thresholds are met and so that
means that you are aware of problems
with your environment before your
customer is there's nothing worse than
somebody emailing and saying hey your
website doesn't work or like a page is
down or it's slow or something like this
with nagias and Splunk these are tools
that really address those kind of
problems so that you stay ahead of the
curve so some of the things that are
really good with Negros is negus itself
is very very comprehensive you can do an
awful lot with it and it allows you to
be able to get to the root of monitoring
and analyzing your problems within your
operations and network environment very
quickly it has a very modular
architecture which also then makes it
extremely highly available and because
the the system itself is not reliant on
just a single large application actually
the modular architecture allows it to be
scalable and very easy to manage across
the entire environments companies using
a videos include Uber and hike and
webedia again these are all technology
companies but there's no reason why you
shouldn't be using them as well a little
scenario before using it one of the
challenges that a lot of developers and
development teams would have had is
developers would be working on different
types of code whether it was database
code whether it was python or whether
it's Java or net and they would have a
central server that they would be
pushing all of their source code in but
there was little or no communication
that was actually going on in between
all the developers you know the
challenge you had with that scenario is
that when people would be checking a
code you could be like you know there'll
be conflicts and you'd have to roll back
different code versions and and this is
really kind of the challenge that git
addresses git is a tool that allows all
of a developers no matter what stack
they're working in to have access to all
of the code and it makes it much more
effective at being able to have
development teams work on small medium
or even massive application patients and
some of the biggest applications out
there are managed through a git
distributed server environment so let's
jump into what we'll be covering so you
have a clear understanding of the value
that you're going to get out of watching
this presentation so we're going to go
through devops and the tools have
available for devops we'll talk about
what version control means within a
devops environment and cover the two
different types of Version Control
centralized and distributed and then
we're going to zero in on git which is a
really fantastic distributed Version
Control System I'm going to go through
and understand the features workflow
branches and commands in git we're going
to give you a demo and then summarize
all the activities at the end of the the
presentation so what is devops well this
is one of my favorite questions I love
the idea of what devops is so devops
really is a culture of being able to
deliver Solutions faster where your
development teams and your operation
teams work effectively together the idea
is to be able to continuously build out
Solutions and have testing codes that's
built into your Solutions so that no
matter where they are in the stage of
the integration and deployment life
cycle they're always being tested and
you can always have code being ready to
be released out the idea is that instead
of having big releases that you'd have
maybe once every two weeks or once a
month that you would actually have a
continuous stream of releases because
you always have code that is being
tested you have your network that's been
tested you have your environment being
tested and you'll be able to provide
feedback directly to the appropriate
person whether they're in debts or
whether they're in arts to be able to be
successful at being able to deliver
Solutions faster the bottom line your
Dev team things like operations and your
operations team begin to feel think like
developers it's really a fantastic way
of being able to speed up delivery and
have your team just think and feel and
act in a different cultural environment
so let's have a look at some of the
tools that are available to you in
devops environments every look at devops
it's really kind of split into two areas
the dev side you have building code
planning and testing and then on your
upside you have release deployment
operating and monitoring but the tools
really all interact with each other and
what's really great is that the tools
are either open source or very very low
in cost in fact most of the tools that
you're seeing in front of you right now
are open source tools which means
there's no licensing to you and you can
actually effectively manage them and
Implement them within your team right
now so let's have a look at some of the
dev tools that you have for doing code
versioning and so some of the tools that
you may have used in the past include
subversion team Foundation serves um and
git these are all different types of
Version Control software that you have
out there one of the oldest ones is
subversion it's a centralized Version
Control System it's one of the tools I
used years ago it really is good at
doing what it's supposed to do which is
just a very simple version controlled
solution it's open source so it's free
there's no licensing there are
challenges with working with it because
it is a centralized tool rather than a
distributed tool we'll get into that in
a little bit but for a very basic
Version Control System you know it does
what you wanted to do team Foundation
Service Solutions something that many of
you in Microsoft world may be using for
a long time and the thing that's great
about TFS is it's built right into the
Microsoft environment it's the server
side the services side of building out
your Solutions with Microsoft just
recently in the last couple of weeks at
the beginning of September 2018.
Microsoft actually just rebranded TFS as
Azure devops and so you have Azure
pipelines and there are five now Azure
tools that you replace TFS but these are
all tools that are very similar in
concept the old TFS tools are very
similar to the centralized SVN tools
whereas the new tools that are part of
azure are actually much more more lines
with get so what you're seeing is
Microsoft moving from the centralized
server to a distributed server with
their new Azure devops tools which is
fantastic news for every developer
listening to this and then we have get
and get is a distributed Version Control
System it is again open source which
means that you can start using it right
now without fear of having to have any
costs or any penalties and the thing
that's really good about it is that it's
really you can use it for almost any
kind of digital project and what it's
good at is being able to create that
historical record and versioning of your
source code whether you're doing a web
application a mobile application or
you're actually building a python script
for a machine learning solution so let's
dig into what version control systems
are and how they can be of value to you
so the role of a version control system
is to allow developers to be able to
check in their files into a repository
auditory and here you can actually have
see how we can check in three files into
a repository and that repository then
becomes a snapshot or a historical
record of the files that we're working
on and you want to be able to have it so
that the repository is flexible enough
so that as you want to be able to scale
and add in new files or new versions of
a file that you can actually do that
easily within your VCS environment and
the goal is to be able to constantly
have each of these versions available so
that anybody wants to be able to check
out the file can actually have the
latest version of see a history of how
that file became what it is today let's
look at centralized Version Control
Systems and then we'll compare it
against distributed Version Control
Systems so a centralized version control
system has a central server where all
the files are stored and everybody has
to check in and check out from that
centralized server and all of the
versions are managed within that server
environment the problem is is that if
that Central server crashes which
doesn't happen but very often but it can
happen you end up losing all of your
files and I was actually on a project
where that actually happened to us and
we lost six months of history of how the
application was created fortunately for
us we actually had a backup but it
didn't have all the historical data so
we were able to at least continue
working but we just lost a lot of files
and it took a while to get back into
place so let's now look at and compare
this to a distributed version control
system with a distributed Version
Control System you still have a
centralized server that manages the
files but the difference is is that as a
developer you check out all of the files
for a project so you can actually manage
the whole project locally on your
development machine and make your
changes and then you can just check in
and out the changes that you've made to
the centralized server the opportunity
you have here is that the server itself
if it goes down you're not going to be
in a situation where you lose all of the
history because everybody that's working
on the actual application has all of the
versions of the code locally on their
machine now one of the more popular
distributed Version Control Systems that
out there is git I mean it really is
probably the most popular by a margin
compared to a system out there so you
know let's dig into you know what
actually is get so git is a version
control system for managing files and as
a developer you have a git client on
your machine and you're able to make
changes and create local repositories of
a program on your computer and then you
can sync up with a remote service such
as GitHub or gitlab or the new Azure
Labs environment where you can actually
store files remotely and then allow
remote teams to have access to those
changes in the actual application you
made so you can track your changes
easily you have it's the the tool is
inherently distributed so it makes it
very easy to manage the code for large
teams and you can bring people in very
quickly and you can have people come in
as Specialists and spin them up quickly
and have them work on a piece of the
code and then drop them out of the
project if you want to have a check on
how this works go and start contributing
to any of the the many thousands of Open
Source projects at GitHub where you can
go in with your get tools and just
contribute to those projects and then
finally one of the things that I really
like with Git is that it's not a linear
development approach it's not starting
from a and going to Z it's a non-linear
approach which allows you to have
branches that people are working on in
parallel to your master Branch where
people are actually doing the delivery
of the production code so let's step
through some of these features so get as
you can imagine the first thing it does
is track history that's probably the
most important part of having a Version
Control System it's a free open source
you can actually go and use it right now
there are no costs of actually using it
it is a non-lineared environment that
allows people to be able to build out
new features in parallel to the master
Branch so that you can be constantly
having the master code out there without
and adding in new features without
breaking the code and you can
automatically create backups by because
each developer has a version of the code
remotely it's incredibly scalable some
of the biggest projects out there now
are being managed through git and
because of this scale the collaboration
becomes a byproduct of teams working
together and branching just is so much
easier I would then get so as you can
see this whole thing leads to a very
effective distributed environment so
let's step through some of the the
workflow that we have available for git
so the way that git works is that you as
an operator as a developer but you have
the get client on your local development
machine whether it's a Linux machine a
Mac or a PC and you connect to a remote
server and you pull down the latest
working copy and then you're able to
make all of your changes uh locally and
you can modify the codes you can review
changes and you can commit those codes
to your git repository that you have
locally and then you can push those
changes back up to the remote server and
as soon as you push them into the remote
server those changes then become
available to everybody else working on
the project so they can be constantly
keeping everybody updated on all the
work that is happening so you know
typically the way that it would work is
you know you'd start off with having the
working filed working directory that you
have locally and then you would stage
those files into a staging area and then
you get those files ready to be
committed to your git repository and
these are all actions that you would do
locally and then connect to a remote
server and then later when you go go
back into work on your project the first
thing you should always do is check out
that code so you always have the latest
version of the code and everybody's kept
in sync so let's talk about branching
and get so the way that branches work is
that if you imagine you're working on a
project and projects always have a
tendency of getting bigger than you
imagine and what you want to be able to
do is keep that main product working and
keeping it working effectively but if
you want to be able to add a feature to
that product you may have two or three
different groups that are working on
multiple features
simultaneously and what you want to be
able to do is give them the freedom to
work on those individual features and
the way you do it in git is that you can
create branches off the master and while
you're um of the the main branch which
is called the master and so each person
could be working on their own separate
branch and then when they want to they
can then later merge those branches back
into the main master and all this time
time the main Master still works and
still able to produce the right code for
the customer but at the same time it
allows the developers the freedom to be
able to write in their new features and
so we can look at this in a little bit
more visually with the master branch and
we can put in small features and large
features that we can then merge them
back as we need to so this kind of
covers some of the commands that we have
in git so the First Command you'd want
to use is called get init and that's
allow you to go into a folder on your
local PC and or your local Mac and
convert that file that folder into a
local git repository and that creates
that repository then you want to be able
to make changes to that repository and
you the commands you'd be using in this
instance would be add commit or status
and you want to be able to sync your
repositories with a remote server so be
able to put the code that you have on
your computer on the remote server you
push you're going to get the code from
the remote server you'd use pull and
then add origin and then if you want to
do parallel development from the master
Branch the main branch you would use
Branch merge or rebase as ways to be
able to do parallel development and now
we're going to go through and do a demo
on git so we're going to do a demo on
get so you can feel comfortable running
the commands in the interface and you're
able to get get running correctly in
your environment so the first thing
we're going to do is we're going to set
up which version of git that you have
and then we're going to see if we can
establish some Global configurations
within your git environment and we're
going to do sandeep.s as the name email
address is sandeep.simplylearn and then
we're going to do a list of the
different configuration settings we have
so to check the version of git that we
have we do git dash dash version in our
terminal or command line window you can
also use Powershell that'll actually get
you running in the same environment as
well all of these tools are going to be
command line tools and then the next
thing we're going to do is we're going
to assign some Global use usernames and
a global email address so that we can
actually access the git account itself
locally on the device and so we're going
to Dash and we're going to put in config
dash dash Global user.name Sandeep and
then a git
config.global user.email and
sandeep.d at simplylearn.net and now
we're going to check the list of
usernames and email IDs in our
configuration environment with dashed
list and you'll see that we have
everything in there correctly here is
our username right here and our name and
our email ID so if you need any help
when you're actually doing any of your
work and get all you have to do is to
get help config or git config dash dash
help and this will allow you to actually
get access to the help screens so let's
just go and type that in git help config
return and this takes us to the help
page that's running locally on your
device and here we have a breakdown of
all the different commands and what
those commands actually do within a git
configuration
and we can go ahead and we can do git
config dash dash help and that will
actually take us to the same page so two
ways of doing the same thing so we're
going to go ahead and we're going to
create a test repository on our local
system and we're going to do that with
make directory called test and then
we're going to move our cursor within
command window terminal window to the
test folder and then we're going to
initialize that folder to become a git
repository so let's go ahead and do that
so we're going to make a new directory
and we're going to call it test and
we're going to move the cursor so it's
actually in the test folder we do CD
change directory to test and you can see
now we're actually in the test folder
and now all you have to do is initialize
this folder as a git repository and so
we're going to do this as git init and
it's now a new git instance that we can
actually now use for managing our git
environment so we're going to create a
new file and we're going to call it
info.txt and we're going to put some
content in it and we're actually going
to put it in the folder that we've just
created so you can actually see what
it's like to test out the get
environment with a file that hasn't been
checked into it yet so let's go ahead
and do that so we open up our photo
directory and here you can see we have
the test folder and we're just going to
go ahead and create a default test file
so right click Text new document
info.txt open that and we'll just you
can put really whatever you want in here
we're just going to put in this
information right now name equals Sam
registration number one two
479 and save that file close it out now
if we actually go ahead and run a get
status command you'll see that the
info.txt file is in red that's because
that file has not actually been checked
into the project it's a new file that
we've added but it hasn't been committed
yet into the git repository so let's go
ahead and see what it takes to actually
add the file to the git repository that
we just created so we're going to do
that by doing get add info.txt and then
we can commit that into the history of
that git repository so we can do so it
now has uh Version Control and we can
start doing uh forking and other kind of
sim activities when we connect later to
our GitHub git environment so we do get
add info Dot txt and remember we're
actually still in the test folder so
it's going to look for that file in that
folder and just add it in and now if we
go ahead and do get status we'll
actually be able to see that the
info.txt file is in an act active color
first of all you have to commit it so we
go get commit Dash M and then we're
going to commit and we're adding some
quick text you can write whatever you
want committing the text file it's now
actually committed into the get
repository and it's all running locally
on your PC so we're going to go make
some changes to the file and we're going
to save it and we want to be able to see
how we can use git to compare the
differences between the two files so
let's go over and we're going to make
some changes to the info.txt file here
we are here's info.txt which is going to
add in a new line we're going to add in
the line address and we're going to save
that file and now when we go over to get
we're going to be able to review the
difference so we're going to do git div
and here you actually see that we've
added in a new address file so now you
can actually see this is the difference
and the difference between the two
documents now as you can imagine when
you have code this would become more
elaborate where actually Google should
take show you code that's been pulled
out if you've run reduced any of
eliminate amazing code or if you added a
new code really useful so we're going to
go ahead and add our get username to a
remote location so we can actually start
testing out the file that we've created
into a remote repository in this case
we're going to use GitHub and you'll be
able to see how we can actually then
save that file into a git repository
that somebody else could then access and
be able to make edits to so let's just
go ahead and make those changes so git
config Dash Global user dot username and
we'll do simply learn and that will
associate it with this git repository
that we've just created locally so
simply on dash GitHub so simply learn
GitHub is the username that we use on
GitHub so we're going to go over to
GitHub here we are in GitHub and then
correct new Repository
and then as you can see with Sim so
we're in simply learn Dash GitHub and
we're going to call the new repository
test we're going to make it public so
anybody can access it and you'll see
that it automatically adds a readme file
and we have our test environment set up
correctly so we'll go ahead and copy the
URL link which is github.com simplylearn
GitHub for test.get copy that and we'll
go back over to our command line window
and so now we're going to add the
username to the GitHub configuration so
what we're doing is we're connecting the
local repository to the remote
repository so we type get remote add
origin and then we'll paste in the
address and this then allows us to
connect the local repository that we
created in the test folder with the
remote repository and now we're
connected so now that we've connected we
can actually push the file that we have
in the local repository on your PC or
Mac to the remote repository in the
GitHub server environment so let's go
ahead and do that so we're going to do
get push and push is the command to push
the documents from the origin which is
the local file to master and Masters a
remote file okay and there we are
success so what we have now is the file
that we just created in our local get
repository is actually now in the remote
repository as well so anybody can access
it so let's go have a look at that so
we're going to refresh the GitHub page
and hey there we are there's info.txt
has been uploaded and we're ready to go
that's great that's uh that's what you
should be seeing so refreshing your web
page on the remote GitHub folder and it
could be any get service which is
happening to be using GitHub because
it's free to use you'll be able to see
that the local file has been installed
and is now part of the git repository in
the remote server so what we can do now
is we're going to create three more
files and we're just going to call them
info.txt info 2 and info three and then
we're going to push them out to the
remote server and we're going to merge
everything together into a single
environment so let's go ahead and create
create those three files and we're just
going to add those into the folder so
let's go ahead create new text document
info one and then we're going to create
info two and then info three get my
typing right here okay here we go there
we are okay I'm opening info three and
I'm just gonna enter some text in here
and what we're going to do is illustrate
how we can do branching and each of
these files I'm going to save and then
close out and now I can go over and so
let's create a new branch and we're
going to call this one first underscore
branch and we go get Branch first
underscore branch and this will be a new
branch of the code environment that
we're creating and so hit return and
that allows us to create a new branch
and so what we do is we're going to move
to the new Branch so we're going to do
get checkout first underscore branch and
that allows us to move into that branch
and you'll see that info.txt is already
there which is good that's what we want
to see now what we need to do is add in
the new documents so here we have the
different steps we've taken we created a
branch we moved to the new branch and
now we're going to go ahead and add the
info 3 txt to that Branch so you can
actually see everything coming together
and we're going to go ahead get add info
3.txt so we're just adding a file like
we were previous see with our initially
our first initial git folder and that
file has now been added to the first
Branch so we're going to go ahead and
commit the file to the first branch and
then we're going to merge the documents
into the master Branch so we have
everything together so let's go ahead
and do those steps we're going to do the
commit first and then we're going to
merge everything together right so we do
git commit Dash M and we'll just put in
we're just going to say uh made changes
to First branch and that's just a
documentation so that we know what we've
done and there we are we've committed
the file so we had that file info 3 txt
committed and now we're going to go
ahead and merge the files into the main
master and so we can actually have
everything together as one consistent
environment and we do that by typing in
so this list out what we have in our
Master file so we just do LS and that
will show everything in the folder and
we have info.txt info 1 txt info 2 txt
and info 3txt are now all in the first
Branch so let's go ahead and we're going
to merge okay so actually before we
merge we have to check out the master
Branch so we do get checkout master so
it gets us into the master branch and
you'll see that we have just one file
there info.txt so we're going to list
out the files in the master branch and
so so what you'll see is info dot text
info one text and info two texts are
there but info dot info 3 text isn't
there because that was created in a
separate Bunch so let's go ahead and
merge the first Branch into the master
Branch so we do git merge and we do
first underscore branch and there we are
we've merged it in and excellent that
looks good so let's go ahead and list
out the files that we have in the master
branch and we do LS and then you'll see
that we have four files including the
info 3.tags that we handed in a separate
Branch now all in the master Branch so
if you are ready to embrace this
exciting career path look no further
than simply learns Caltech postgraduate
program in devops this comprehensive
program will equip you with the
knowledge and skills needed to master
the day of principles tools and
practices dive deep into
containerization orchestration and
automation Frameworks like Docker
kubernetes and Jenkins gain hands-on
experience with Cloud platforms and
learn how to Leverage The Power of
infrastructure as code don't miss out on
this chance to transform your career and
become an invaluable asset to any
organization click the link in the
description to discover more about this
devops course so what are we going to
cover today so we're going to introduce
the concept of Version Control that you
will use within your devops environment
then we'll talk about the different
tools that are available in a
distributed Version Control System we'll
highlight a product called git which is
typically used for Version Control today
and you'll also go through what are the
differences between git and GitHub you
may have used GitHub in the past or
other products like getlab and we'll
explain what are the differences between
get and get and services such as GitHub
and gitlab will break out the
architecture of what a get process looks
like and how do you go through and
create forks and clones how do you have
collaborators being added into your
projects how do you go through the
process of branching merging and
rebasing your project and what are the
list of commands that are available to
you in get finally I'll take you through
a demo on how you can actually run git
yourself and in this instance use the
software of git again against a public
service such as GitHub all right let's
talk a little bit about Version Control
Systems so you may have already been
using a version control system within
your environment today you may have used
tools such as Microsoft's team
Foundation services but essentially the
use of a Version Control System allows
people to be able to have files that are
all stored in a single repository so if
you're working on developing a new
program that's such as a website or an
application you would store all of your
Version Control software in a single
repository now what happens is that if
somebody wants to make changes to the
code they would check out all of the
code in the repository to make the
changes and then there would be an
addendum added to that so um there will
be the the version one changes that you
had then the person would then later on
check out that code and then be a
version 2 and added to that code and so
you keep adding on versions of that code
the bottom line is that eventually
you'll have people being able to use
your code and that your code will be
stored in a centralized location however
the charge you're running is that it's
very difficult for large groups to work
simultaneously within a project the
benefits of a VCS system a Version
Control System demonstrate that you're
able to store multiple versions of the
solution in a single repository now
let's take a step at some of the
challenges that you have with
traditional Version Control Systems and
see how they can be addressed with
distributed Version Control so in a
distributed Version Control environment
what we're looking at is being able to
have the code shared across a team of
developers so if there are two or more
people working on a software package
they need to be able to effectively
share that code amongst themselves so
that they constantly are working on the
latest piece of code so a key part of a
distributed Version Control System
that's different to just a traditional
version control system is that all
developers have the entire code on their
local systems and they try and keep it
updated all the time it is the role of
the distributed VCS server to ensure
that each client and we have a developer
here and developer here and developer
here and each of those our clients have
the latest version of the software and
then that each person can then share the
software in a peer-to-peer like approach
so that as changes are being made into
the server of changes to the code then
those changes are then being
redistributed to all of the development
team the tool to be able to do an
effective distributed VCS environment is
get now you may remember that we
actually covered get in a previous video
and we'll reference that video for you
so we start off with our remote git
repository and people are making updates
to the copy of their code into a local
environment that local environment can
be updated manually and then
periodically pushed out to the git
repository so you're always pushing out
the latest code that you've code changes
you've made into the repository and then
from the repository you're able to pull
back the latest updates and so your git
repository becomes the kind of the
center of the universe for you and then
updates are able to be pushed up and
pulled back from there what this allows
you to be able to accomplish is that
each person will always have the latest
version of the code so what is get get
is a distributed Version Control tool
used for source code management so
GitHub is the remote server for that
source code management and your
development team can connect their get
clients to that some remote Hub server
now again it is used to track the
changes of the source code and allows
large teams to work simultaneously with
each other it supports a non-linear
development because of thousands of
parallel branches and has the ability to
handle large projects efficiently so
let's talk a little bit about git versus
GitHub so get is a software tool whereas
GitHub is a service and I'll show you
how those two look in a moment you
install the software tool for git
locally on your system whereas GitHub
because it is a service it's actually
hosted on a website get is actually the
software that used to manage different
versions of source code whereas GitHub
is used to have a copy of the local
repository stored on the service on the
website itself git provides command line
tools that allow you to interact with
your files whereas GitHub has a
graphical interface that allows you to
check in and check out files so let me
just show you the two tools here so here
I am at the get website and this is the
website you would go to to download the
latest version of git and again git is a
software package that you install on
your computer that allows you to be able
to do Version Control in a peer-to-peer
environment for that peer-to-peer
environment to be successful however you
need to be able to store your files in a
server somewhere and typically a lot of
companies will use a service such as git
Hub as a way to be able to store your
files so git can communicate effectively
with GitHub there are actually many
different companies that provide similar
service to GitHub gitlab is another
popular service but you also find that
development tools such as Microsoft
Visual Studio are also incorporating
GitHub commands into their tools so the
latest version of Visual Studio team
Services also provides this same ability
but GitHub it has to be remembered is a
place where we actually store our files
and can very easily create public uh
shareable is a place where you can store
our files and create public shareable
projects you can come to GitHub and you
can do a search on projects you can see
at the moment I'm doing a lot of work on
blockchain but you can actually search
on the many hundreds of projects here in
fact I think there's something like over
a hundred thousand projects being
managed on GitHub at the moment that
number's probably actually much larger
than that and so if you are working on a
project I would certainly encourage you
to start at GitHub to see if somebody's
already maybe done a prototype that
they're sharing or they have an open
source project that they want to share
that's already available
um in GitHub certainly if you're doing
anything with um Azure you'll find that
there are thousands 45
000 Azure projects currently being
worked on interestingly enough GitHub
was recently acquired by Microsoft and
Microsoft is fully embracing open source
Technologies so that's essentially the
difference between get and GitHub one is
a piece of software and that's get and
one is a service that supports the
ability of using the software and that's
GitHub so let's dig deeper into the
actual git architecture itself so the
working directory is the folder where
you are currently working on your get
project and we'll do a demo later on
where you can actually see how we can
actually simulate each of these steps so
you start off with your working
directory where you store your files and
then you add your files to a staging
area where you are getting ready to
commit your files back to the main
branch on your git project you want to
push out all your changes to a local
repository after you've made your
changes and these will commit those
files and get them ready for
synchronization with the service and
will then push your services out to the
remote repository an example of a remote
repository would be GitHub later when
you want to update your code before you
write any more code you would pull the
latest changes from the remote
repository so that your copy of your
local software is always the latest
version of the software that the rest of
the team is working on one of the things
that you can do is as you're working on
new features within your project you can
create branches you can merge your
branches with the mainline code you can
do lots of really creative things that
ensure the that Aid the code remains at
very high quality and B that you're able
to seamlessly add in new features
without breaking the core code so let's
step through some of the concepts that
we have available and get so let's talk
about forking and cloning and get so
both of these terms are quite old terms
when it comes to development but forking
is certainly a term that goes way way
back long before we had distributed CVS
systems such as the ones that we're
using with get to Fork a piece of
software is a particularly open source
project you would take the project and
create a copy of that project and but
then you would then associate a new team
and new people around that project so it
becomes a separate project in entirety a
clone and this is important when it
comes to working with get a clone is
identical with the same teams and same
structuring as the main project itself
so when you download the code you're
downloading exact copy of that code with
all the same security and access rights
as the main code and then you can then
check that code back in and potentially
your code because it is identical could
potentially become the mainline code in
the future and that typically doesn't
happen your changes are the ones that
merge into the main branch but also but
you do have that potential where your
code could become the main code with Git
You can also add collaborators that can
work on the project which is essential
for projects where particularly where
you have large teams and this works
really well when you have product teams
where the teams themselves are
self-empowered you can do a concept
what's called branching in git and so
say for instance you are working on a
new feature that new feature and the
main version of the project have to
still work simultaneously so what you
can do is you can create a branch of
your code so you can actually work on
the new feature whereas the rest of the
team continue to work on the main branch
of the the project itself and then later
you can merge the two together pull from
remote is the concept of being able to
pull in Services software the team's
working on from a remote server and get
rebase is the concept of being able to
take a project and re-establish a new
start from the project so you may be
working a project where there have been
many branches and the team has been
working for quite some time on different
areas and maybe you kind of losing
control of what the true main branch is
you may choose to rebase your project
and what that means though is that
anybody that's working on a separate
Branch will not be able to Branch their
code back into the mainline Branch so
going through the process of a get
rebase essentially allows you to create
a new start for where you're working on
your project so let's go through forks
and clones so you want to go through the
process so you want to go ahead and Fork
the code that you're working on so let's
use this scenario that one of your team
wants to go ahead and add a new change
to the project the team member may say
yeah go ahead and you know create a
separate Fork of the actual project so
what does that look like so when you let
you go ahead and create a fork of the
repository you actually go and you can
take the version of the mainline Branch
but then you take it completely offline
into a local repository for you to be
able to work from and you can take the
mainline code and you can then work on a
local version of the code separate from
the mainland Branch it's now a separate
Fork collaborators is the ability to
have team members working on a project
together so if you know if someone is
working on a piece of code and they see
some errors in the code that you've
created none of us are perfect at
writing code I know I've certainly made
errors in my code it's great to have
other team members that have your bag
and can come in and check and see what
they can do to improve the code so to do
that you have to then add them as a
collaborator now you do that in GitHub
you can give them permission within
GitHub itself and it's really easy to do
super visual interface that allows you
to do the work quickly and easily and
depending on the type of permissions you
want to give them sometimes it can be
very limited permissions it may be just
to be able to read the files sometimes
it's being able to go in and make all
the changes you can go through all the
different permission settings on GitHub
to actually see what you can do but
you'll be able to make changes so that
people can actually have access to your
repository and then you as a team can
then start working together on the same
code let's step through branching and
get so suppose you're working on an
application but you want to add in a new
feature and this is very typical within
a devops environment so to do that you
can create a new brand and build a new
feature on that Branch so here you have
your main application on what's known as
the master branch and then you can then
create a sub branch that runs in
parallel which has your feature you can
then develop your feature and then merge
it back into the master Branch at a
later point in time now the benefit you
have here is that by default we're all
working on the master Branch so we
always have the latest code the circles
that we have here on the screen showed
various different commits that have been
made so we can keep track of the master
branch and then the branches that have
come off which have the new features and
there can be many branches in get so git
keeps you the new features you're
working on in separate branches until
you're ready to merge them back in with
the main branch so let's talk a little
bit about that merge process so you're
starting with the master branch which is
the blue line here and then here we have
a separate parallel branch which has the
new features so if we're to look at this
process the base commit of feature B is
the branch f is what's going to merge
back into the master branch and it has
to be said there can be so many
Divergent branches but eventually you
want to have everything merge back into
the master branch let's step through git
rebase so again we have a similar
situation where we have a branch that's
being worked in parallel to the master
branch and we want to do a get rebase so
we're at stage C and what we've decided
is that we want to reset the project so
that everything from here on out with
along the master branch is the standard
product however this means that any work
that's been done in parallel as a
separate Branch we'll be adding in new
features along this new rebased
environment now the benefit you have by
going through the rebase process is that
you're reducing the amount of storage
space that's required for when you have
so many branches it's a great way to
just reduce your total footprint for
your entire project so get rebase is the
process of combining a sequence of
commits to form a new base commit and
the prime reason for rebasing is to
maintain a linear project history when
you rebase and you unplug a branch and
re-plug it in on the tip of another
branch and usually you do that on the
master branch and that will then become
the new Master Branch the goal of
rebasing is to take all the commits from
a feature branch and put it together in
a single Master Branch it makes it the
project itself much easier to manage
let's talk a little bit about pull from
remote Suppose there are two developers
working together on application the
concept of having a remote repository
allows the code to the two developers
will be actually then checking in their
code into a remote repository that
becomes a centralized location for them
to be able to store their code it
enables them to stay updated on the
recent changes to the repository because
they'll be able to pull the latest
changes from that remote repository so
that they are ensuring that as
developers are always working on the
latest code so you could pull any
changes that you have made to your
thought remote repository to your local
repository the command to be able to do
that is written here and we'll go
through a demo of how to actually do
that command in a little bit good news
is if there are no changes you'll get a
notification saying that you're already
up to date and if there is a change it
will merge those changes to your local
repository and you get a list of the
changes that have been made remotely so
let's step through some of the commands
that we have in git so get init
initializes a local git repository on
your hard drive get ad adds one or more
files to your staging area get commit
Dash M commit message is a commit
changes via git command commits changes
to head up to the git command commits
changes to your local staging area get
status checks the status of your current
repository and lists the files you have
changed get lock provides a list of all
the commits made on your current Branch
get diff the user changes are used made
to the file so you can actually have
files next to each other you can
actually see the differences between the
two files get push origin Branch name so
the name of your branch command will
push the branch to the remote repository
so that others can use it and this is
what you would do at the end of your
project git config Dash Global username
or tailgate Who You Are by configuring
the author name we'll go through that in
a moment git config Global user email
will tell get the author of by the email
ID get clone creates a get repository
copy from a remote Source get remote ad
origin server connects the local
repository to the remote server and adds
the server to be able to push to it git
branch and then the branch name will
create a new Branch for you to create a
new feature that you may be working on
git checkout and then the branch name
will allow you to switch from one branch
to another Branch git merge Branch name
Will merge a branch into the active
Branch so if you're working on a new
feature you're going to merge that into
the main branch a get rebate will
reapply commits on top of another base
Tab and get rebase will reapply commit
on top of another base tip and these are
just some of the popular git commands
there are some more but you can
certainly dig into those as you're
working through using get so let's go
ahead and run a demo using get so now we
are going to do a demo using get on our
local machine and GitHub as the remote
repository for this to work I'm going to
be using a couple of tools first I'll
have the deck open as we've been using
up to this point the second is I'm going
to have my terminal window also
available and let me bring that over so
you can actually see this and the
terminal window is actually running git
bash as the software in the background
which you'll need to download and
install you can also run get batch
locally on your Windows computer as well
and in addition I'll also have the
GitHub repository that we're using for
simply learn and already set up and
ready to go all right so let's get
started so the first thing we want to do
is create a local repository so let's go
ahead and do exactly that so the local
repository is going to reside in my
development folder that I have on my
local computer and for me to be able to
do that I need to create a drive in that
folder so I'm going to go ahead and
change the directory so I'm actually
going to be in that folder before I
actually create and make the new folder
so I'm going to go ahead and change
directory
and now I'm in the development directory
I'm going to go ahead and create a new
folder
and let's go ahead and created a new
folder called hello world
I'm going to move my cursor so that I'm
actually in the hello world folder
and now that I'm in the hello world
folder I can now initialize this folder
as a get Repository
so I'm going to use the git command init
to initialize and let's go ahead and
initialize that folder so let's see
what's happened so here I have my Hello
wall folder that I've created and you'll
now see that we have a hidden folder in
there which is called dot get and we
expand that we can actually see all of
the different subfolders that git
repository will create so let's just
move that over a little bit so we can
see the rest of the work
and now if we check on our folder here
we actually see this is users Matthew
development hello world dot get and that
matches up with hidden folder here
so we're going to go ahead and create a
file called readme.txt in our folder so
here is our hello world folder and I'm
going to go ahead and using my text
editor which happens to be Sublime
I'm going to create a file and it's
going to have in the text hello world
and I'm going to call this one
readme.txt
if I go to my Hello World folder you'll
see that we have the readme.txt file
actually in the folder what's
interesting is if I select the get
status command what it'll actually show
me is that this file has not yet been
added to the commits yet for this
project so even though the file is
actually in the folder it doesn't mean
that it's actually part of the project
for us to do that we actually have to go
and select
foreign
for us to actually commit the file we
have to go into our terminal window and
we can use the get status to actually
read the files that we have there so
let's go ahead and use the git status
command and it's going to tell us that
this file has not been committed you can
use this with any folder to see which
files and subfolders haven't been
committed and what we can now do is we
can go and actually add the readme file
so let's go ahead I'm just going to
select at git add so the git command is
ADD
readme.txt so that then adds that file
into our main project and we want to
then commit those files into the main
repositories history and so it's that do
that we'll have the the get command
commit and we'll do a message in that
commit and this one will be
first commit
and it has committed that project what's
interesting is we can now go back into
readme file and I can change this so we
can go hello get
get is a very popular
Version Control solution
and we'll
we'll save that now what we can do is we
can actually go and see if we have made
differences to the readme text so to do
that we'll use the disk command forget
so we do get
diff
and it gives us two releases the first
is what the original text was which is
hello world and then what we have
afterwards is what is now the new text
in green which has replaced the original
text
so what we're going to do now is you
want to go ahead and create an account
on GitHub we already have one so what
we're going to do is we're going to
match the account from GitHub with our
local account so to do that we're going
to go ahead and say get config
and we're going to do Dash and it's
going to be a
globaluser.name and we'll put in our
username that we use for GitHub and this
instance we're using the simply learn
Dash
GitHub account name
and under the GitHub account you can go
ahead and create a new repository name
in this instance we call the repository
hello dash world
and what we want to do is connect the
local GitHub account with the remote
hello world.get account and we do that
by using this command from get which is
our remote connection and so let's go
ahead and type that in open this up so
we can see the whole thing so we can
type in git remote add origin https
GitHub
.com slash
simply learn
Dash GitHub and you have to get this
typed in correctly when you're typing in
the location hello dash world dot get
that creates the connection to your
hello world account
and now we want to do is we want to push
the files to the remote location using
the get push command commit git push
origin
master
so we're going to go ahead and connect
to our local remote GitHub so I'm just
going to bring up my terminal window
again and so let's select get remote add
origin
and we'll connect to the remote location
github.com slash
simply learn
Dash GitHub
slash
hello dash world dot get
oh we actually have already connected so
we're connected to that successfully and
now we're going to push the master Gish
so get
push origin
master and everything is connected and
successful
and if we go out to GitHub now
we can actually see that our file was
updated just a few minutes ago
so what we can actually do now is we can
go and Fork a project from GitHub and
clone it locally so we're going to use
the fork tool that's actually available
on GitHub let me show you where that is
located and here is our branching tool
it's actually changed more recently with
a new UI interface
and once complete we'll be able to then
pull a copy of that to our account using
the forks new HTTP URL address
so let's go ahead and do that
so we're going to go ahead and create a
fork of our project now to do that you
would normally go in when you go into
your project you'll see that there are
Fork options in the top right hand
corner of the screen now right now I'm
actually logged in with the default
primary count for this project so I
can't actually Fork the project as I'm
working on the main branch however if I
come in with a separate ID and here I am
I have a different ID and so I'm
actually pretending I'm somebody else I
can actually come in and select the fork
option and create a fork of this project
and this will take just a few seconds to
actually create the fork
and there we are we have gone ahead and
created the fork
so you want to say clone or download
with this and so this is the
I select it'll actually give me the web
address I can actually show you what
that looks like I'll open up my text
editor
just that's not correct
I guess that is correct
so I'm going to copy that
and
I can Fork the project locally and clone
it locally I can change the directory so
I can create a new directory that I'm
going to put my files in and then post
in that content into that file so I can
now actually have multiple versions of
the same code running on my computer
I can then go into default content and
use the patchwork command
20
so I can create a copy of that code that
we've just created in the college that's
a clone and we can create a new folder
that we're actually putting the work in
and we could for whatever reason we
wanted to we could call this photo
Patchwork and that would be maybe a new
feature and then we can then paste in
the URL of the new directory that has
the forked work in it and now at this
point we've now pulled in and created a
clone of the original content
and so this allows us to go ahead and
Fork out all of the work for our project
onto our computer so we can then develop
our work separately
so now what we can actually do is we can
actually create a branch of the fork
that we've actually pulled in onto our
computer so we can actually then create
our own code that runs in that separate
branch
and so we want to check out um the the
branch and then push the origin Branch
down to our computer
this will give us the opportunity to
then add our collaborators so we can
actually then go over to GitHub and we
can actually come in and add in our
collaborators
and we'll do that under settings and
select collaborators and here we can
actually see we have different
collaborators that have been added into
the project and you can actually then
request people to be added via their
GitHub name or by email address
or by their full name
one of the things that you want to be
able to do is ensure that you're always
keeping the code that you're working on
fully up to date by pulling in all the
changes from your collaborators
you can create a new branch and then
make changes and merge it into the
master Branch now to do that you would
create a folder and then that folder in
this instance would be called test we
would then move our cursor into the
folder called test and then initialize
that folder so let's go ahead and do
that so let's call create a new folder
and we're going to first of all change
our root folder and we're going to go to
development
and create a new folder
call it test and we're going to move
into the test folder and we will
initialize
that folder
and we're going to move some files into
that test folder
call this one test one
and then we're going to do file save as
and this one's gonna be test
two
and now we're going to commit those
files
and
add kit add and then we'll use the
dot to pull in all files
and then git commit
m
m files
edited
make sure I'm in the right folder here I
don't think I was
and now that I'm in the correct folder
let's go ahead and
and get commit
and it's gone ahead and added those
files and so we can see the two files
that were created have been added into
the master
and we can now go ahead and create a new
Branch we call this one get branch
test underscore
branch
and let's go ahead and create a third
file to go into that folder
this is
file three
into file save as we'll call this one
test three dot text
and we'll go ahead and add
that file I need to get add
test3 Dot txt
and we're going to move from the master
Branch to the test run branch
kit
check
out test underscore
branch
I switched to the test branch
and we'll be able to list out all of the
files that are in the in that Branch now
and we want to go through and merge the
files into one area so let's go ahead
and we'll do git merge test underscore
branch
as well we've already updated everything
so that's good so otherwise it would
tell us what we would be merging
and now all the files are merged
successfully into the master branch
there we go all merged together
fantastic
and so what we're going to do now is
move from Master Branch to test branch
so get
check out
test underscore branch
and we can modify the files the test3
file that we took out
and pull that file up
and we can
now modify
right
and we can then
commit
that file
back
in and we've actually been able to then
commit the file with one changes and now
we've seen as the text free change that
was made
now we can now go through a process of
checking the file back in switching back
to the master branch and ensuring that
everything is in sync correctly
we may at one point want to rebase all
the workers kind of a hard thing you
want to do but it will allow you to
allow for managing for changes in the
future so let's switch to it back to our
test branch
which I think we're actually on we're
going to create two more files
let's go to our folder here and let's go
copy those
and that's created
we'll rename those tests
four
and
five
and so we now have additional files
and we're going to add those into our
branch that we're working on so we're
going to go and select get add Dash a
and we're going to commit those files
get
commit Dash a dash m
adding
two new files
and it's added in the two new files
so we have all of our files now we can
actually list them out and we have all
the files that are in the branch
and we'll switch them to our Master
Branch we want to rebase the master
so we do get rebase
master
and that will then give us the command
that everything is now completely up to
date
and we can go
get
check out
Master to switch to the master account
this will allow us to then continue
through and rebase the test branch and
then list all the files that are all in
the same area
so let's go get rebase
test underscore
branch
and now we can list and there we have
all of our files today I'm going to
cover a few essential git commands and
walk you through a demonstration for
most of it I begin by installing a git
bash or the git client and configuring
it for its first time usage once I'm
done with configuring the git client I'm
going to spin up few repositories
locally and work on these repositories
using git commands once I'm happy with
my local changes I'm going to commit few
of them and then push these changes to a
git server of my choice for today's
demonstration I plan to use github.com
which is a cloud hosted free service and
it provides free account registrations
for anyone eventually I will cover few
topics regarding git branching and
merging because these two in my opinion
are the killer features of the git
distributed Version Control tool kit is
one of the most popular distributed
Version Control tools of recent times
and like any other distributed Version
Control tool it allows us to perform
various motion control tasks without
really needing a network connection
before I jump into my demonstration let
me spend some time explaining to you the
git client server working model what
does it take for a couple of users to
collaborate and start working with Git
the bare minimal thing that any user
would need in order to start working
with Git is something called as git bash
or the git client this comes as an
installer for all popular operating
systems like Windows all flavors of
Windows Linux Mac OS and other operating
systems so once you install git bash you
get a command line utility using which
you can fire up your git commands and
ensure that you can bring up
repositories you can work with these
repositories by adding files to it
committing changes to it and all those
git commands would work perfectly well
so you can bring up a small repository
and you can work with your repository
using this git Bash but what does it
take for you to share this repository
with another user who has also got git
bash installed on his system this is
where you would need something called as
a git server so for user one or one user
to share his repository with another
user he would need to collaborate using
something called as a git server so in
the present Market there are a bunch of
git servers which are popularly
available some of them are free some of
them come with a cost because they are
licensed a bunch of these servers that I
can think of are GitHub which is one of
the most popular kit servers that is
around in the market for a long time now
it comes with two variants one is the
cloud hosted one which is the github.com
and then the other one is an Enterprise
server which comes as a black box that
can be installed into your data centers
so typically organizations who don't
want their source code to be put up on
the cloud but go for this GitHub
Enterprise service wherein they buy this
server and these servers are hosted on
their data centers other popular variant
of the git server is bitbucket this is
from the famous atlasian products and it
integrates very well with all other
atlassian products like jira in recent
times there's one other variant of the
git server called gitlab which is
getting very very popular these days
because gitlab not just provides a git
server it also provides something called
as a runner this is a part of the
continuous integration strategy where as
soon as you have a source code check-in
that is going in you have this Runner
that kicks up and builds your particular
project so all these are bundled
together in a gitlab and gitlab also
provides you with a Community Edition
which is almost free of cost having said
that there are a bunch of these tools
available for my demonstration what I'm
going to do is I've already registered
myself on the github.com by providing my
username and my password and I'm going
to use a free account so whatever
reposit is that I'm gonna put up on this
server is all public by default so I
will register Myself by using my
username or my email ID and my password
so this set of authentication is also
referred to as https authentication so
in order for me to share my repository
with say user 2. the first thing that I
got to do is have an access to a git
server where both user 1 and user 2 are
set up so once I have that I can push my
repository onto this server and provide
right kind of access so that user 2 or
the other developer whoever wishes to
collaborate with me on this repository
can use his credentials as long as is
registered properly on the git server he
can use his credentials and pull down
this repository and work with this
repository so this is the https way of
authenticating and sharing repositories
another popular way of sharing
repositories or working with each other
is called as SSH authentication as many
of you would know SSH means is nothing
but creation of a private and a public
key a bunch of keys public and private
are created on the client machine while
I create a keys I will be asked for a
passphrase I need to type in a
passphrase and remember my passphrase
and then I take this public key and kind
of post it on to my account within my
git server so that whenever I'm going to
connect next time using my command
prompt or any of the tools it's going to
challenge me for my passphrase as long
as I remember my passwords correctly I
can authenticate myself and get into the
server so both https and SSH are popular
ways of communicating with the git
server having said this let me just get
started by installing the git bash on my
local system and then fire up some of
these git commands and build up some
repositories make some changes in my
repository commit these changes and
later use my GitHub credential to
connect to my GitHub server and push my
repositories out there let me now go
ahead and find my git bash for my
windows so that I can go ahead and
install it so I just type in git client
for windows open up a browser and type
in for this the first one that I find is
a GUI tool I don't need this GUI client
I need this one so this is a git client
or the git bash that I was referring to
it's about 40 Megs and let me just
download this pause this video for some
time so that the download happens and
come back to you once I have the
download the download seems to be
complete when I'm recording this video
the version that I have the latest
version that is there is 2.19.0 and this
is a 64-bit version for Windows so I've
got that downloaded let me go ahead and
find it and install this EXE
all right I go ahead choosing all the
default options I want it to be
installed in SQL and program files under
the git folder everything looks good for
me I'm not going to make any changes
here
get Bash from Windows yes I may want to
use git from Windows command prompt that
sounds good for me open SSH that's fine
next card next card no I don't want any
of these new features
so what I'm doing is installing the git
bash or the git client so that I can
start using my git commands
and the version that I'm using is
2.19.0 which as I record this video this
happens to be the latest version
all right so looks good I don't want to
see the release notes I'll say again
launch git Bash
so what I get is this window so this is
the git command prompt so let me check
if everything is looking good it's a Git
Version it sounds good it's a git help
all right a lot of git help commands
everything looks good so I ended up
installing the git bash I can go ahead
with all other commands
now that I have git bash installed on my
system let me open up a git bash prompt
and start using it but before I do
anything I just need to configure It For
the First Time stating what would be the
username and the email ID with which I
want the git bash to be configured with
the command to do that is get config
hyphen iPhone Global which means that
this setting that I'm going to set would
be a global setting user dot name and
this the name of the user would need to
be specified within a double code in my
case I am using a simply learn account
which is the official account and this
is the particular username of this
account the other setting is nothing but
git config hyphen iPhone Global this is
also Global setting
and this would be the email ID
the email ID need not contain a double
quote
so in my case this is the email ID that
I would be setting up it to work with
all right let me check if the
configurations were set correctly or not
all right if you see this the username
and the email ID has been picked up the
way I wanted it to be picked up looks
good so far all right so what do we do
next by default whenever you open up the
git prompt you're placed into your home
directory let me create a folder here
which would be a repository folder from
where I'm going to create all my
multiple git repositories so I will
create a folder called let's say git
underscore demo
all right
there's already a folder called git demo
so let me just delete that in the first
place and let me create this git demo
again
all right let me navigate to this
particular folder
so this is the folder let me open up
that folder using a command prompt
this is my get demo so this would be a
folder so I would use this folder as a
base folder for creating all other
repositories as a part of this demo so
let me create my first repository a
repository is nothing but a folder so
let me create my first folder called
first repo
and let me
get into this repo all right as of now
this is just an empty folder
if at all I navigate to this you could
see that this folder is empty there's
nothing much in this folder at all at
this moment okay so let me create a
repository out of this how do I create a
repository of this is pretty simple I
just run and get init command
when I say git in it it means it's going
to initialize a repository as of now
the folder contained nothing in it was
an empty repository so it initially is
an empty repository in this particular
folder for all I look at my content
you'd see that there's a hidden folder
this would be a hidden folder since I've
configured my Explorer to view all
hidden folders I see it otherwise you
wouldn't see it so a folder by name dot
get is created within this first repo
and this was created because we ran the
command kit init
if I get into this folder I see a bunch
of directories and other configurations
if you see there's something called as a
hooks directory a couple of
configuration files that are here in
this directory all these related to one
or the other type of a client hook that
can be enabled and this info this
objects
this references a bunch of stuff that is
there
so what is this folder this is a magical
git folder and this is created whenever
a repository is initialized so by
default if at all you get into any git
folder or any git repository you would
see this dot get folder a word of
caution do not get into this folder and
try to modify anything because if you
fiddle around with this and corrupt this
particular folder your whole repository
will go for a toss so let me come out of
this
and let me show you another thing now if
you notice here when I was in the folder
the repository did not have anything
after this now when I say git in it
there is something called as a master
that shows up so typically what happens
is whenever a git repository is created
for the first time it creates a default
Branch the name of the branch is called
master and that is why you would see
Master within braces this doesn't mean
anything else other than we have a
repository out in this particular folder
and by default there is a branch called
master and we are currently on that
Branch if at all I create multiple
branches in this folder whenever I
navigate to different branches the
branch name within this brackets would
change all right so far so good so we
created a folder we made it a git
repository by initializing that with the
git in it and then I'm not put anything
yet in the repository so let me try to
do that let me create a few files within
this particular repository so I use a
touch command touch is a Linux command
it kind of creates this particular
folder if all you're not comfortable
with the file creation by this way you
can always use a file explorer and
create some files in this so I create a
file called touch master.txt and open up
in the notepad
okay so this is my first file that's
what I'm going to write inside this
I save this and I come out of this all
right now let me do a git status command
so what does this tell you there's no
commit set you're in the master Branch
there is an untracked file and this
shows up in red it also gives you all
the command that is supposed to be run
so if at all you want to add this file
you would say git add and the name of
the file and then you're going to commit
it
so what happened is that when the
repository was initialized it was an
empty repository now that a git notices
that there is a new file in this
repository called master.txt if at all
you want you want us to let git know
that you want to track this file we need
to add this to the repository so command
to add that is called git add I can
specify either the name of the file or I
can give a wild character saying git add
dot now if I run the git status command
again
it says there's no commits yet added a
file and the file which was earlier in
red now shows up in green and it says
it's a new file master.txt all right it
also gives you something called git
remove command if at all you want to
unstage or undo your changes so what has
happened is the file which was earlier
untracked has been added into the git
index or it is ready for staging or it's
in a staged State now I can go ahead and
comment this file if at all that's what
I want to do so let me do that
I'll do a kit commit minus M and I give
a message that I want I would say this
is my first get commit
all right I run the get status command
again and see it says on the branch
Master nothing to commit it's all nice
and clean so typically what happened was
I created a empty directory made it into
a git folder by doing a git in it I put
a file into it I made some changes to
the file I added that file into the
repository and I committed to the
repository if a to error I run the git
log command
it says this was the commit ID or the
commit number or the commit hashtag and
this is the author if you notice this is
the email ID and the username that we
set earlier and this was done on this
date and this is the
commit message that is there all of it
looks so good let me do one more commit
into this repository so I will touch
Master One Dot text I'll put in another
file
I'll open up this particular file
write something into this
all right I save this change
let me also open up the older
file make some more change into this
all right so what I did I modified the
older file that existed I also added a
new file to my repository now if I run
the get status command
it says modified this file which is
showing in red and it says you have one
more file which is an untracked file
all right now let me add both these
files because these are the changes that
I want to
incorporate I would say git add dot I
run the git status command again
and it says now this is a modified file
this is a new file looks good to me let
me go ahead and Commit This so git
commit with the message that says this
is my second commit
all right
if I do a git log now it shows both the
commits the first comment is the one
that that shows up at the bottom and the
topmost one is the latest commit that is
there so far so good if at all you
notice
I'm doing all these git commands without
really connecting it to any git server I
don't need any network connection for
doing any of these things I can keep on
doing more and more git commands and run
git commands the way I want to
without really having any connections to
the kit server now that I have my
changes the way I wanted and I've
checked in all the code changes into my
local repository I may want to share my
repository with somebody else and work
in a collaborative way so the way to do
that is as I mentioned earlier to host
this repository or to push this
repository onto a git server I already
have a GitHub account wherein I've
registered myself using my username and
password so I'll log into my GitHub
account using my username and
credentials and let me go ahead and
create a new Repository
my repository is already there on the
local system so what I essentially need
is just a placeholder for my repository
so that I can link them together and
push the content from my local
repository to the server
if you notice the name of my repository
is his first repo
so let me copy this I would create a
folder structure or a repository
structure on the server with the same
name so that when I pushed my repository
from my local box it will go ahead and
get created in the server so the
repository name should be the same as
what I created on my local box
description I would say this is my first
get Hub Repository
on the server
this is just an optional description
that will show up and if you notice
I can't have any private repositories so
whatever repositories that I'm going to
create here would be a public repository
so I choose public and I don't want to
initialize this with a readme file or
anything of that because I already have
a repository with some content with some
code checked in all that I need is a
placeholder for my repository on the
server so with this option with just a
name and with an optional description
and the repository being public I go
ahead and click on the create repository
button
so if you notice it gives me a bunch of
commands that I would need to run in
order to create a repository and push it
onto the server and also important point
it shows me two URLs one is the HTTP s
URL and the other one is the SSH URL as
of now we do not have any SSH keys that
is set up so let me use this https URL
to connect my local repository with the
repository on this server
so I've done all these things I already
have the repositories set up all that I
need is a placeholder for me to connect
my local repository with the server URL
so this would be the command that I need
to copy
with essentially links or ads and origin
with the URL of the git server I just
copy this command
and run it here
get remote add the name of the remote is
called origin and it points to an https
URL which is nothing but the placeholder
that I've created on the server
all right so let me check if it's added
correctly
okay the URL looks good I'm all set I've
got two comments in my repository I'm
done with all the changes so let me try
to push my repository onto the server
the command for doing it for the first
time is git push hyphen U this is for
linking the Upstream
origin
master
so git push hyphen U Upstream linking
origin master link the master
a branch of my local repository with the
master branch on the server repository
and where is it going to push it is
going to push by default to An Origin
and the origin points to this URL all
right let me try doing this
so it essentially opens up an URL asking
me the credentials with which I need to
log into my GitHub server so let me just
copy and paste
the username and the password with which
I have registered
I would say login
okay success so I am able to create a
repository on the server and the content
seems to be pushed and it created a new
branch called Master it is linked the
master Branch from the local repository
with the master branch on the server so
let me go back to my server and refresh
this page
and see if all my commits have come in
master master one dot text if at all I
see the commits they seem to be two
commits this is my first comment this is
my second comment and if at all you
notice each of this commit comes with a
hash key or a hash ID which contains the
details of my commits in this commit the
files were added this was a line that
was added if you look at the second
commit
I added these lines
I made this you know one line chain that
was there I also added this new file
which was not existing these are the two
commits that existed
and if you see
the timestamp of the Commit This is the
commit which was committed for 15
minutes ago so what does this mean is
that the commit is the timestamp when we
actually commented the code in the
client the pushing is just after we have
committed the change we have pushed it
so there's no timestamp of the push that
happened
this completes the first part of the
tutorial wherein I create a repository
on the client and then I pushed it to
the server after creating a repository
Skeleton on the server now let me
demonstrate to you the SSH configuration
that is required for creating a pair of
SSH keys and ensuring that I connect to
the GitHub server using the SSH
mechanism so for doing that I need to
First create a pair of private and
public key the command to do that is SSH
Keygen
hyphen T I will use the RSA mechanism
for creating my keys Capital C and I
need to specify the email ID using which
I'm going to connect to all right I need
to create the keys for
all right so generating a private and
public key pair it says let me know what
folder should I create the keys in so by
default the keys are created in the user
home directory under a DOT SSH folder so
this is pretty good for me so let me go
ahead and say that that's fine enter a
passphrase so let me enter some
passphrase
keep in mind that I would need to
remember my passphrase because when I am
going to authenticate myself using SSH
mechanism it will challenge me for this
particular passphrase I need to enter
the right passphrase so that I get
authenticated
so the key is generated and by default
it is created in this particular folder
so let me go over to that particular
folder
and this is the folder where the keys is
generated you would see a public key and
a private key that is created let me
open up the public key with a notepad
and then copy the contents of the whole
file
and now I would need to stick it in
on the server so that I can get myself
authenticated using SSH keys so I have
logged in to GitHub using my credentials
and go to my settings
and you will see an SSH and gpg keys let
me click on that as of now there are no
SSH key or gpg keys so I will click on
new SSH key and let me paste my public
key that I've copied and I would say add
SSH key
it prompts me for my password just to
make sure that
I've got the right kind of
authentication to be adding a new key so
this one looks good that means the keys
were added successfully let me check if
my SSH keys are working well
so I do SSH hyphen capital T I would say
get
at github.com
this is just to double check if my SSH
keys
are authenticated correctly are you sure
you want the connection yes you want to
continue yes
all right now
a check for my passphrase let me enter
my passphrase I hope I remember it
correctly
all right
triggered I've been successfully
authenticated so that means now my SSH
keys are good and just by using my
passphrase I can connect to my GitHub
server using SSH all right so for the
Second Use case what I would do is
create a repository on the server with
some content in it and then clone that
repository cloning is nothing but making
a copy of that repository linking the
origin so that I create an exact replica
of the repository which is there on the
server on my local drive so let me first
create a repository on the server I'll
create a new Repository
let me give a name for my repository and
let me kind of initialize that with the
readme file I would call this my second
repo
give some description
this is my second repo
that is created straight
on the git server on the GitHub server
to be precise
it's a public repository this time I'm
going to choose this option the reason
being that I don't want to create just a
skeleton as of now I want to create a
repository with some readme file in it a
random readme file with no content in it
but then I want a repository to be
created on the server all right I say
create a repository
So Random readme file with some content
in it gets created and I can see there
are two URLs to this repository https
and SSH so this time let me copy the SSH
URL so that I will use this URL to clone
this repository on the server
rather from the server to my local box
all right so let me navigate to my root
folder the folder that I created for
making all my repositories git
repositories if you see my first deposit
is already here so let me clone my
repository the second repo that I
created on the server the command to do
that is git clone the URL of the
repository that I want to clone
oops looks like I didn't copy the URL
correctly let me copy that again
copy this URL
all right so if you notice there is no
https to it I'm just cloning the URL
that is SSH URL of the Repository
it asks for my passphrase again
okay
so I just had first repository earlier
now I have something called a second
repo this is an exact replica of
whatever was the repository content that
was created on the server so if I get
into
the repository there's just a simple
readme file that is there let me try to
add a new file out here I would say
Notepad
second dot text
some text file I'm going to put in here
yes definitely you will not find that
file there
this is my second
file that
I will push onto the server
I'm going to put in some content out
there
I'm Gonna Save this
and if I do get status
it says this this is a new untracked
file you want to add this file onto the
repository do you want to track this
file yes I definitely want to track this
file let me also commit my change to the
file
with a message
this is my first
commit for the second repo
I'm giving some particular message
and this time around since I cloned my
repository from a server the origin is
already set for me I don't really need
to add the origin and all that stuff
because what happened in the first case
was that I was creating a repository
from my local box and then pushing
another server which is why I had to set
up the Upstream branches and stuff like
that in this case I have cloned an
existing repository from the server so
when I clone I get the replica along
with all references to my repository so
I don't need to do anything more I'll
then push the content I'll just say get
push to be in the safer side I would say
origin and master
ask me for the passphrase
okay it posts the content onto the
server let me see if the contents have
come in here okay wonderful I had only
one comment now I see my second comment
so what was added as a part of the
second comment it says a new file was
added all this line that never existed
got added this is the second file the
content exactly what I pushed onto the
server
now this completes the tutorial when I
create a repository on the server cloned
it made some changes to the repository
and pushed it to the server using SSH
a quick recap of all the git commands
that we ran till now get init is used to
initialize the repository so if at all
you are creating a local repository you
can get into any of the folder which is
not a git repository and run this
command whenever you run this command a
magical folder by name dot kits gets
created in the folder and from that time
onwards git will start tracking any
changes that happen to that particular
folder it also creates a new repository
and a branch by default the branch that
is created is called the master Branch
git add dot is a wildcard character for
adding any number of new files into your
git repository if you have a bunch of
files one file two file 10 files a
folder containing multiple files in it
all of them could be added into the
repository using this command so once
you add any number of files into the
repository you can keep on adding more
and more files into the repository once
you're done with that you want to commit
the changes that has happened in the
repository you use the git commit
command git commit hyphen M with a
meaningful message this will commit all
the work items that you have done in
terms of the files that got changed the
files that got added all this with one
particular message get status will give
you the status of all the files and
folders any file that got added any file
that got created any file that was
deleted all the status of the files will
be obtained using the git status command
git log will show you all the comment
history with the latest history or the
latest commit showing up on top git add
remote origin this command is used
whenever you want to link any local
repository to a server and this is when
you want to really push a repository
from the local to the server for the
first time git push hyphen U origin and
master this is the command that you
would use whenever you want to push
contents from the local repository to
the server server git clone and the URL
of the server this is the command that
you would run whenever you have an
existing repository on the server and
you want to make a fresh clone or a
fresh copy of that particular repository
onto your local box
more than often many developers or git
users find themselves in situations
where they would want to go back in
history and modify or change some of the
files that they recently committed or
checked into the repository now from a
perspective of get this is a very very
simple activity however my perspective
Version Control tool you will have to be
really cautious about the chain that
you're gonna do in case the changes are
pretty local none of the developers or
none of the other users who are
collaborating with you would be affected
by this change you are good to go ahead
and make this change however if your
repository has been pushed to the server
and you're trying to modify something
from history this would have a very bad
or adverse effect on all the other users
who will be using it you need to be
really cautious while running these
commands in case your repository
contents are local if you've not pushed
your changes to your repository and if
at all you made some changes to the
repository maybe to fix a defect or to
include a new feature and you missed few
things you can go ahead and modify this
with some of these commands let's say I
have this simple repository called undo
undo repo and if at all I look at the
logs of these I have the commit
histories which are like this
I have about five commits that is there
in my repository and in C1 I've added a
file in C2 I added the file in C3 I've
also added some file in C4 I made some
changes to M1 and M3 and in C5 I have
added another file called M4 now this is
my commit history now let's say I made
these two commits but possibly I want to
Club them together along with some other
changes because whatever changes I did
did not really fix things the way I
expected you to fix so there is a git
command called git reset this would
allow me to go back in history to go
back to any of the snapshot get rid of
all this commit history on the commit
messages but retain the changes that
were made as a part of these comments so
if at all I need to go back in history
and go back to this particular comment
and however I just want to get rid of
these commits that are there in terms of
the commit messages but I want the
changes that were there as a part of
these commits to still exist I would do
command called get reset hyphen iPhone
soft
now this would go back or unwind my
changes back to this particular snapshot
getting rid of all the commit messages
that were there as a part of these two
comments but a soft would ensure that
the file changes that were done as a
part of these two comments would still
exist a full remain so the command for
that is git reset hyphen iPhone soft
which is the snapshot that I want to get
back to if this is the comment that I
want to get back to I just copy this
and paste this
all right it doesn't give me any message
but if at all I do a git log pretty one
line
what it says is 7 C2 now the head which
is nothing but the current pointer which
is earlier pointing to C5 it's gone back
to C3 now what happened to the changes
that were there as a part of C4 and C5
if at all you do a git status if you see
the changes that were part of C4 was
some changes to M1 and M2 and C5 a new
file got M4 got added if I don't get
status
you'll see those changes still exist
however from the git history or the git
commit history C3 is the topmost comment
that exists as of now c4 and C5 having
current rate of but the changes that
were there still exist
all right now I can make some more
changes that I want if as a part of my
commit and possibly go ahead and commit
this whole thing as a new commit
altogether maybe I'll open up a notepad
and possibly create a new file called
m6.text
and say one new file that was missed out
I make this change I go to a get status
all these files are still there I would
send it add dot I also get
m
C4
rewriting history
and I do a git
log
pretty
there you go so C1 C2 C3 still existed
and whatever was a part of C4 and C5 the
older ones I've gotten rid of this along
with those old changes I also added a
new file I made some changes and I was
able to comment it now if you look at my
git history it's all nice and clean
however do this activity only when you
don't push your git repository to the
server one other most powerful and
useful command while undoing or
resetting the history is something
called as revert this is a safe way of
undoing some things remember what I
talked about when exactly you can do a
reset it's only when your changes are
local and is not being pushed to the
repository take this scenario wherein
let's say I have a git server
and I have the changes which are C1
C2
C3
and possibly C4
all right so this is a bunch of comments
that has happened this was the oldest
comment and this is the newest comment
and if at all developer 1 is the one
who's responsible for pushing these
contents
in the first place so here's got C1 to
C4 on his history
I'm going to jump all these C2 and C3
and carry to C4 and he's pushed all
these contents out here
and there are a bunch of people from the
server who has pulled this repository
and all of them are at the same level
now imagine a use case where developer 1
then figures out that by mistake he put
in the C3 which is a wrong thing to do
and possibly you know he could have done
a better job of fixing this in in a
different way so he wants to get rid of
the C3 or C4 or any one of these
comments
if he's going to do that and push to the
server first of all the git server would
not allow him to go back in history but
assuming that he forcibly pushed
something
what would happen to all these people
who are looking to work ahead from C4
all of them would be affected in an
adverse way so in short never go back in
history if at all you need to undo
something go ahead and put in a new
comment to undo something
let's take this example maybe if at all
a developer one wants to get rid of C3
better way to do that is by adding a new
commit which is ahead say let's say C5
and what does this commit to this commit
possibly gets rid of C3 instead of going
back in history it actually is taking
your git history ahead but as a part of
this comment you are undoing some part
of the work that was done as a part of
C3
in short you're removing a commit but to
remove a commit you're adding a new
commit and this is why you're doing it
because there are a bunch of other
people who are you're using this
repository collaboratively and what plan
to get ahead in terms of History they
cannot afford to go back in history and
modify something in history all right
the command to do this is called the get
revert command
so let me take up this repository which
has got a bunch of commits in it and if
at all I do a git log
4019
I have these bunch of comments
and C1 I've added a file in C to our
another file in C3 I've added one more
file and in C4 I made some changes and
this is the latest commit and this is
the oldest commit so if at all I see the
contents of the git folder it contains
M1 M2 M3 and M4 and let's say looking at
the previous use case that I mentioned I
may want to get rid of this particular
comment
I want to get rid of the C3 which is
nothing but I've added a file called M3
dot text but I added it by mistake I
need a better way of fixing it possibly
I look at it later on but for now I
don't want this particular commit and
the changes that were made as a part of
that commit to exist so the better way
to do that is using the git revert
command so git revert and the comment ID
whatever I want to revert so if I want
to revert this
I copy this particular commit ID and
bear in mind that considering a reset
revert works on one commit at a time so
I can do one commit revert at a time I
can Club a bunch of them together and do
one commit but otherwise the revert
always works on one commit at a time so
get revert
face this
essentially what I'm trying to do I'm
trying to undo this particular commit
while I'm undoing this commit what does
git do it safely adds a new commit and a
part of this new commit It undoes
Whatever changes that were done as a
part of this command so if I say git
revert this
all right I forgot to mention a message
which is why this window is showing up
but that message is good for me I say
write and quit
all right and if I do a git log
all right if you see this C1 still is
there in history and our history is
going ahead from C5 I see another commit
but what was there as a part of this
comment actually is getting rid of this
commit C3 so if at all I see my files
here there's only M1 M2 and M4
M3 was added as a part of C3 now I've
gotten rid of M3 by getting rid of the
revert command for C3 so this is a
better way of undoing things
now that we have acquainted ourselves
with few of those basic git commands let
me get into a very very interesting
topic regarding git which is about
branching the topic is interesting
because git deals with branches in a
completely different way compared to any
other version control tool in most of
the Version Control tools whenever you
create a new Branch a complete copy or a
subset of the whole repository is
created which in my opinion is very very
cost inefficient because it's a very
very expensive operation both in times
of time and the disk space however in
Git You just create a very very
lightweight movable pointer whenever you
create a branch there is no folder or no
copy of the repository that is created
in short the branches are very very
inexpensive and great so go ahead and
create any number of branches that you
need however once you create your
branches do all the work relating to
that branch in that specific branch and
once you've completed your task go ahead
and merge your branches back into your
base branch and then delete the branch
let me go ahead and create a repository
with some branches in this and give you
a demonstration of that
so let me go back to my base directory
which is git demo and then I'm going to
create a new folder here
I'll create a folder called Arc details
all right this is my folder here
all right I navigate into this folder
and let me create a repository here I
initial as a repository and Dot get
folder is created without any files in
it so let me create a few files in it
say notepad name dot text
organization name
I create a name.txt within this maybe
I'll create one more file in this
Notepad
employees dot text
on
has over 3K employees
I just put in some content in this save
this content
all right
so if you're trying to look at status I
just see two files in it so let me go
ahead and add these two files and let me
commit my changes it's okay
commit hyphen am or iPhone m
first comment
so if I do a gig log
I see only one commit and there are two
files here you can see those two files
that is here
all right so at this stage let me go
ahead and create a new branch
and the command for creating new branch
is get Branch name of the branch so let
me name my branches Bangalore Branch blr
branch
and this created a branch but I don't
see any changes in the folder structure
or in the file that exists here I've
just created this Branch so if at all I
do a git branch
I see these two branches and whatever
star means I am currently in this master
Branch so let me go ahead and get into
the blr branch at this moment the number
of files that exist in this repository
is same there's absolutely no change in
any of the branches both master and
Baylor Branch point to the same snapshot
so let me get into
the other branch and the command to do
that is git checkout
blr Branch I've actually switched into
Bangalore Branch if you see the content
of the files there's exactly two files
the way it was earlier because I've
created a branch from my base branch
which is the master branch and when I
created a branch there were two files
existing here if you see the content of
these files they would all be the same
all right so let me create a file that
is specific to Bangalore branch
so I will just say Notepad
maybe I want to put down the address of
this organization specific to Bangalore
in this address dot text so I see
notepad address dot text
I'll say simply learn
angular office
say located in
some arteries
I would say Bangalore
that is the content of this file
and I'm creating all this new file in
this blr Branch so I would sync it add
Dot
and I would say git commit hyphen m
angalore
branch
commit
all right if you see this now I have an
address.txt which is there only in the
Bangalore branch that I'm presently on
now let me switch back to my master
Branch the command to do that is git
checkout Master Branch if you remember
when I forged out or when I created a
new Branch from the master Branch I
didn't have this address dot text in it
so let me go back to git git checkout
Master branch
and keep noticing this particular
what happened to the address dot text
it kind of disappeared how did that
happen it's all because of this magical
git folder so what happened is now git
knows that when you're in the master
Branch the file that was there in the
Bangalore branch which was called the
address dot text didn't exist
so git is playing around with the file
system snapshot so if I come back let's
say I come back to git checkout
Bangalore branch
if you see this the address.txt file
appears
so this particular file is only present
in the branch and git is playing around
with the file system snapshot by moving
the pointers across and ensuring that
whenever I am switching to Bangalore
Branch I see this address dot text when
I go back to my master Branch I don't
say this Bangalore branch
contents which is the address dot text
all right now that I created this Branch
maybe I may need to push this onto the
server so let me go ahead and create a
particular repository structure on the
server similar to whatever we did in our
previous exercise so the name of the
folder needs to be exactly this one so
let me copy this
and let me create a repository very
quickly
go here and say new Repository
all right this
is the name of the Repository
get Branch demo this is what I'm gonna
put in as a description I would say
public I don't want any readme files in
it because I already have a repository I
will just say create a repository
all right let me use
the SSH URL yeah this is this is a URL
this is good for me so let me copy this
URL
let me add it here
I'm linking my local repository with the
server and I would let me come back to
my master branch
let me first push my master Branch so I
would say git
push hyphen U origin master
it asks for my passphrase
all right
push my master Branch it looks good it
says new Bunch Master is created so if I
go back to my server and it will refresh
if you see it says only one branch
and it says what is the name of the
branch master
if you see the contents there is
employees.txt and name dot text now
let's try to push the other brands that
is there the command to do that is a
little different compared to
the way we created it on the client
because on the client we created the
branch with a git Branch command now
this Branch doesn't exist on the server
and I need to push this new Branch onto
the server so the command would be
get push hyphen new origin and the name
of the brand that I want to specifically
push
all right now it created a branch out
here
let me do a refresh of this
all right it says two branches
if we get into this Bangalore Branch I
will see the address dot text if at all
I go back into Master Branch I would not
see this under stock text
wonderful
now let me do one thing let me try to
get the changes from the branch that I
created back into the master branch that
is what I would mean by merging I would
want the changes that is there as a part
of the bangle Branch to come back into
my master Branch so if I go to click
checkout
Bangalore Branch I see this address dot
like this is there only as a part of
Bangalore Branch now assuming that I am
done with the changes from the Bangalore
Branch I want to get those changes back
into my master so let me come back
to my master branch
and the way to merge in the changes is
get merge so what is that you want to
merge I want to merge the Bangalore
Branch changes and where will it merge
it will merge it into the current brands
that I'm on presently I'm in the master
Branch so this command would merge the
changes from the Bangalore Branch into
your master Branch so with this if you
keep looking here the content that was
there that was added as a part of
Bangalore branch which is nothing but
the address dot text this file should
appear as soon as we do a merge
all right so it happened quickly and you
would see this address dot text that is
here so if you see let's check the
contents in terms of the git log
foreign
branch that we had earlier which had
only one commit right now we are at this
Bangalore Branch commit which was there
as a part of the Bangalore Branch commit
so with this change let me push this
change onto the server as well git push
origin master
okay those changes were pushed to the
server and now
if at all I do a refresh out here
okay I'm in two branches the princess's
Master even if I'm in master plans I
should see this address dot text that is
there even if I'm in the Bangalore
Branch the content is all same because
we immersed these two branches now that
I've merged this Branch I don't need
this Branch anymore so I would go ahead
and delete this locally as well as push
it under the server so that it gets
deleted from the server as well so
delete Branch locally the command is get
Branch hyphen D and the name of the
branch
only thing you need to ensure is that
you should not be currently on that
branch which you're trying to delete so
I would say get brands and I'm presently
in the master Branch so I can go ahead
and delete the other Branch so get
Branch hyphen D Bangalore branch
all right this got rid of our Pandora
Branch locally
if it's all I don't get Branch iPhone V
it shows there's only one branch as of
now however this Branch would still
exist on the server because Server
doesn't know anything about the changes
that we have done we need to
specifically push our changes onto the
server
okay there are two branches still here
so let me go ahead and push these
changes so that the branch gets deleted
from the server as well the command to
do that is get
push
origin
hyphen iPhone delete and the name of the
branch
all right
so this went ahead and deleted the
branch from the server let's check that
okay if you see now there's only one
branch
these are the basic steps that you would
need to know before you work with any of
the branches within git and as I said
before the branches are very very
inexpensive in git go ahead and create
any number of branches that you want but
just ensure that once you create your
branch finish off your task and once you
finish off your task go ahead and delete
the branch and before you delete the
brands please ensure that you merge in
the branches so that you get in or take
in the work that was done as a part of
that particular branch
to summarize the git branching commands
are as follows to create a new Branch
you issue a command git Branch the name
of the branch and this would
automatically spin off or create a new
Branch from the existing Branch from
where you issue this command if you are
on the master branch and if you execute
this command get Branch new underscore
Branch it creates a new underscore
Branch from the master Branch once you
create a new branch in order to get into
this Branch you would type in git check
out the name of the branch
this would automatically switch the file
system snapshot between different
branches now the branches that are
created are by default created on the
local repository in case you want to
push any specific Branch onto the server
you would have to issue a command get
push hyphen U origin and the name of the
branch so this will ensure that the
contents of the new Branch are pushed
onto this server at any time if you want
to list out all existing branches on
your local repositories the command
would be git Branch hyphen AV assuming
that you did all your work in a new
branch and you're happy with the changes
you would want to bring back those
changes into the master Branch the
command to merge the contents of any new
Branch into a master Branch would be git
merge and the name of the branch this
would automatically merge the branch
contents that you specified into the
branch on which you currently reside if
at all you're on the master branch and
if you issue a command get more launch
new underscore Branch this would merge
the contents of the new Branch onto the
master Branch so once you merge the
contents of any branch you would
possibly want to go ahead and delete
this Branch so the command to delete
those branch locally is git Branch
hyphen capital D and the name of the
branch this again would delete the
branch locally in order to make this
change on the server which is in order
to remove this Branch from the server
the command to do that is git push
origin hyphen iPhone delete and the name
of the branch this would also delete the
brands that you created on the server so
if you are ready to embrace this
exciting career path look no further
then simply learns Caltech postgraduate
program in devops this comprehensive
program will equip you with the
knowledge and skills needed to master
the day of principles tools and
practices dive deep into
containerization orchestration and
automation Frameworks like Docker
kubernetes and Jenkins gain hands-on
experience with Cloud platforms and
learn how to Leverage The Power of
restructure as code don't miss out on
this chance to transform your career and
become an invaluable asset to any
organization click the link in the
description to discover more about this
devops course
if we talk in the literal sense Maven
means accumulator of knowledge Maven is
a very powerful project management tool
or we can call it a build tool that
helps building documenting and managing
a project
but before we move forward and dive deep
into the basics of Maven let's
understand what is meant by the term
build tool
a build tool takes care of everything
for building a project
it generates a source code generates
documentation from a source code it even
compiles the source code and packages
the compiled codes into jar of zip files
along with that the build tool also
installs the packaged code in local
repository server repository or Central
Repository
coming back to Maven it is written in
Java or c-sharp and it is based on
Project object model or pom
again let's have a pause and understand
what is meant by this term project
object model
a project object model or pom is a
building block in maven
it is an XML file that contains
information about the project and
configuration details used by Maven to
build a project this file resides in the
base directory of the project as
pom.xml file
the pum contains information about the
project and various configuration
details
it also includes the goals and plugins
used by Maven in a project
Maven looks for the pom in the current
directory while executing a task or a
goal it reads the pom gets the needed
configuration information and then runs
the goal
coming back Mayville is used to building
and managing any Java based project
it simplifies the day-to-day work of
Java developers and helps them in their
projects
now when we know the basics of Maven
let's have a look at some reasons to
know why is Maven so popular and why are
we even talking about it so let's have a
look at the need for maven
Maven as by now we know is properly used
for Java based projects it helps in
downloading libraries or jar files used
in the project
to understand the part of why do we use
Maven or the need of Maven let's have a
look at some problems that may even
solved
the first problem is getting the jar
files in a project getting the right jar
files is a difficult task where there
could be conflicts in the versions of
the two separate packages however it
makes sure all the jar files are present
in its repositories and avoid any such
conflicting scenarios
the next problem it sorted was
downloading dependencies
we needed to visit the official website
of different software which could be a
tedious task
now instead of visiting individual
websites we could visit
mvnrepository.com which is a central
repository of the jar files
then Maven plays a vital role in the
creation of the right project structure
in servlets struts Etc
otherwise it won't be executed
then Maven also helps to build and
deploy the project so that it may work
properly
so the next point is what exactly Maven
does
it makes the building of the project
easy the task of downloading the
dependencies in jar files that were to
be done manually can now be done
automatically all the information that
is required to build the project is
readily available now
finally Maven helps manage all the
processes such as building documenting
releasing and other methods that play an
integral part in managing a project
now when we know everything about Maven
let's look at some companies that use
maven
there are over 2000 companies that use
Maven today
the companies that use Maven are mostly
located in the United States and in the
computer science Industry
Maven is also used in Industries other
than computer science like information
technology and services financial
service banking hospital and care and
much more
some of the biggest corporations that
use Maven are as follows
first we have via varijo then comes
Accenture followed by JPMorgan Chase and
Company then comes craft base and
finally we have red hat so this is what
we're going to be covering in this
session we're going to cover what life
is like before using Jenkins and the
issues that Jenkins specifically
addresses then we'll get into what
Jenkins is about and how it applies to
continuous integration and the other
continuous integration tools that you
need in your devops team then
specifically we'll Deep dive into
features of Jenkins and the Jenkins
architecture and we'll give you a case
study of a company that's using Jenkins
today to actually transform how their it
organization is operating so let's talk
a little bit about life before Jenkins
let's see this scenario I think it's
something that maybe all of you can
relate to as developers we all write
code and we all submit that code into a
code repository and we all keep working
away writing our unit tests and Hope
hopefully we're running our unit tests
but the problem is that the actual
commission actually gets sent to the
code repository aren't consistent you as
developer may be based in India you may
have another developer that's based in
the Philippines and you may have another
team lead that's based in the UK and
another development team that's based in
North America so you're all working at
different times and you have different
amounts of code going into the code
repository there's issues with the
integration and you're kind of running
into a situation that we'd like to call
development how where things just aren't
working out and there's just lots of
delays being added into the project and
the bugs just keep mounting up the
bottom line is the project is delayed
and in the past what we would have to do
is we'd have to wait until the entire
software code was built and tested and
before we could even begin checking for
errors and this just really kind of
increased the amount of problems that
you'd have in your project
the actual process of delivering
software was slow there was no way that
you could actually iterate on your
software and you just ended up with just
a big headache with teams pointing
fingers at each other and blaming each
other so let's jump into Jenkins and see
what Jenkins is and how it can address
these problems so Jenkins is a product
that comes out of the concept of
continuous integration that you may have
heard of as power developers where you'd
have two developers sitting next to each
other coding against the same piece of
information what they were able to do is
to continuously develop their code and
test their code and move on to new
sections of code Jenkins is a product
that allows you to expand on that
capacity to your entire team so you're
able to submit your codes consistently
into a source code environment so there
are two ways in which you can do
continuous delivery one is through 90
builds and one is through continuous so
the approach that you can look at
continuous delivery is modifying the
Legacy approach to building out
Solutions so what we used to do is we
would wait for nightly builds and the
way that our 90 builds would work and
operate is that as co-developers we
would all run and have a cut-off time at
the end of the day and that was
consistent around the world that we
would put our codes into a single
repository and at night all of that code
would be run and operated and tested to
see if there were any changes and a new
build would be created that would be
referred to as the nightly build with
continuous integration we're able to go
one step further we're able to not only
commit our changes into our source code
but we can actually do this continuously
there's no need to race and have a team
get all of their code in at arbitrary
time you can actually do a continuous
release because what you're doing is
you're putting your tests and your
verification Services into the build
environment so you're always running
Cycles to test against your code this is
the power that Jenkins provides in
continuous integration so let's dig
deeper into continuous integration so
the concept of continuous integration is
that as a developer you're able to pull
from a repository the code that you're
working on and then you'll be able to
then at any time submit the code that
you're working on into a continuous
integration server and the goal of that
continuous integration server is that it
actually goes ahead and validates and
passes any tests that a tester may have
created now if on the continuous
integration server a test is a pass then
that code gets sent back to the
developer and the developer can then
make their changes it allows the
developer to actually do a couple of
things it allows the developer not to
break the build and we all don't want to
break the builds that are being created
but it also allows the developer not to
actually have to run all the tests
locally on their computer write tests
particularly if you have a live large
number of tests can take up a lot of
time so if you can push that service up
to another environment like a continuous
integration server it really improves
the productivity of your developer
what's also good is that if there are
any code errors that have come up that
may be Beyond just a standard CI test so
maybe there's a Code the way that you
write your code isn't consistent those
errors can then be passed on easily from
the tester back to the developer too the
goal from doing all this testing is that
you're able to release and deploy and
your customer is able to get new code
faster and when they get that code it's
simply just works so let's talk a little
bit about some of the tools that you may
have in your continuous integration
environment so the cool thing with
working with continuous integration
tools is that they are all open source
at least the ones that we have listed
here are open source there are some that
are private but typically you'll get
started with open source tools and it
gives you the opportunity to understand
how you can accelerate your environment
quickly so bamboo is a continuous
integration tool that specifically runs
multiple builds in parallel for faster
compilation so if you have multiple
versions of your software that runs on
multiple platforms this is a tool that
really allows you to get that up and
running super fast so that your teams
can actually test how those different
builds would work for different
environments and this has integration
with and Maven and other similar tools
so one of the tools you're going to need
is a tool that allows you to automate
the software build test and release
process and buildbot is that open source
product for you again it's an open
source tool so there's no license
associated with this so you can actually
go in and you can actually get the
environment up and running and you can
then test for and build your environment
and create your releases very quickly so
buildbot's also written in Python and it
does support parallel execution jobs
across multiple platforms if you're
working specifically on Java projects
that need to be built and test then
Apache Gump is the tool for you it makes
all of those projects really easy it
makes all the Java projects easier for
you to be able to test with API level
and functionality level testing so one
of the popular places to actually store
code and create a versioning of your
code is GitHub and it's a service that's
available on the web just recently
acquired by Microsoft if you are storing
your projects in GitHub then you'll be
able to use Travis continuous
integration or Travis CI and it's a tool
design specifically for hosted GitHub
projects and so finally we're covering
Jenkins and Jenkins is a central tool
for automation for all of your projects
now when you're working with Jenkins
sometimes you'll find there's
documentation that refers to a product
called Hudson Hudson is actually the
original version of the product that
finally became Jenkins and it was
acquired by Oracle when that acquisition
happened the team behind
um Hudson was a little concerned about
the direction that Oracle May
potentially go with Hudson and so they
created a hard Fork of Hudson that they
renamed Jenkins and Jenkins has now
become that open source product it is
one of the most popular and continuously
contributed projects that's available as
open source so you're always getting new
features being added to it it's a tool
that really becomes the center for your
CI environment so let's jump into some
of those really great features that are
available available in Jenkins so
Jenkins itself is really comprised of
five key areas around easy installation
easy configuration plugins extensibility
and distribution so as I mentioned for
the easy installation Jenkins is a
self-contained Java program and that
allows it to run on most popular
operating systems including Windows Mac
OS and Unix you can even run it on Linux
it really isn't too bad to set up it
used to be much harder than it is today
the setup process has really improved
the web interface makes it really easy
for you to check for any errors in
addition you have great built-in help
one of the things that makes tools like
Jenkins really powerful for developers
and continuous integration teams in your
devops teams as a whole when you have
plugins that you can then add in to
extend the base functionality of the
product Jenkins has hundreds of plugins
and you can go and visit the update
Center and see which other plugins that
would be good for your devops
environment certainly checking out
there's just lots of stuff out there in
addition to the plugin architecture
Jenkins is also extremely extensible the
opportunity for you to be able to
configure Jenkins to fit in your
environment it's almost endless now it's
really important to remember that you
are extending Jenkins not creating a
custom version of Jenkins and that's a
great differentiation because the core
Foundation remains as the court Jenkins
product the extensibility can then be
continued with newer releases of Jenkins
so you're always having the latest
version of Jenkins and your extensions
mature with those core Foundation the
distribution and the nature of Jenkins
makes it really easy for you to be able
to have it available across your entire
network it really will become the center
of your CI environment and it's
certainly one of the easier tools and
more effective tools for devops so let's
jump into the standard Jenkins pipeline
so when you're doing development you
start off and you're coding away on your
computer the first thing you have to do
when you're working in the Jenkins
pipeline is to actually commit your code
now as a developer this is something
that you're already doing or at least
you should be doing you're committing
your code to a git server or to an SVN
server or a similar type of service so
in this instance you'll be using Jenkins
as the place for you to commit your code
Jenkins will then create a build of your
code and part of that build process is
actually going through and running
through tests and again as a developer
you're already comfortable with running
unit tests and writing those tests to
validate your code but there may be
additional tests that Jenkins is running
so for instance as a team you may have a
standard set of tests for how you
actually write out your code so that
each team member can understand the code
that's been written and those tests can
also be included in the testing process
within the Jenkins environment assuming
everything past the the test you can
then get everything placed in a stage
and release ready environment within
Jenkins and finally getting ready to
deploy or deliver your code to a
production environment Jenkins is going
to be the tool that helps you with your
server environment to be able to deploy
your code to the production environment
and the result is that you're able to
move from a developer to production code
really quickly this whole process can be
automated rather than having to wait for
people to actually test your codes or
going through a nightly build you're
looking at being able to commit your
code and go through this testing process
and release process continuously as an
example companies Etsy will release up
to 50 different versions of their
website every single day
so let's talk about the architecture
within Jenkins it allows you to be so
effective at applying a continuous
delivery devops environment so the
server architecture really is broken up
into two sections so on the left hand
side of section you have the code the
developers are doing and submitting that
code to a source code repository and
then from then Jenkins is your
continuous integration server and it
will then pull any code that's been sent
to the source code repository and we'll
run tests against it it'll use a build
service such as MAV into actually then
build the code and then every single
stage that we have that Jenkins manages
there are constant tests so for instance
if a build fails that it feedback is
sent right back to the developers so
that they can then change their code so
that the build environment can run
effectively the final stage is to
actually execute specific test scripts
and these test scripts can be written in
selenium amp so it's probably good to
mention here that both mavin and
selenium are plugins that run in the
Jenkins environment so before we were
talking about how Jenkins can be
extended with plugins Maven and selenium
are just two very popular examples of
how you can extend the Jenkins
environment the goal to go through this
whole process again it's an automated
process is to get your code from the
developer to the production server as
quickly as possible have it fully tested
and have no errors so it's probably
important at this point to mention uh
one piece of information around the
Jenkins environment that if you have
different code builds that need to be
managed and distributed this will
require that you need to have multiple
builds being managed Jenkins itself
doesn't allow for multiple files and
builds to be executed on a single server
you need to have a multiple server
environment with running different
versions of Jenkins for that to be able
to happen so let's talk a little bit
about the Master Slave architecture
within Jenkins so what we have here is
an overview of the Master Slave
architecture within Jenkins on the left
hand side is the remote source code
repository and that remote source code
repository could be GitHub or it could
be a team Foundation services or the new
Azure devops code repository or it could
be your own git repository the Jenkins
server acts as the master environment on
the left hand side and that Master
environment can then push out to
multiple other Jenkins slave
environments to distribute the workload
so it allows you to run multiple builds
and tests and production environments
simultaneously across your entire
architecture so Jenkins slaves can be
running the different build versions of
the code for different operating systems
and the server Master is controlling how
each of those builds operate so let's
step into a quick story of a company
that has used Jenkins very successfully
so here's a use case scenario um over
the last 10 or 15 years there has been a
significant shift within the automotive
industry where manufacturers have
shifted from creating complex Hardware
to actually creating software we've seen
that with companies such as Tesla where
they are creating a software to manage
their cars we see the same thing with
companies such as General Motors with
their OnStar program and Ford just
recently have rebranded themselves as a
technology company rather than just a
automotive company what this means
though is that the software within these
cars is becoming more complex and
requires more testing to allow more
capabilities and enhancements to be
added to the core software so Bosch is a
company that specifically ran into this
problem and their challenge was that
they wanted to be able to streamline the
increase increasingly complex Automotive
software by adopting continuous
integration and continuous delivery best
practices with the goal of being able to
delight and exceed the customer
expectations of the end user so Bosch
has actually used Cloud bees which is
the Enterprise Jenkins environment so to
be able to reduce the number of manual
steps such as building deploying and
testing Bosch has introduced the use of
cloud bees from Jenkins and this is part
of the Enterprise Jenkins platform it
has significantly helped improve the
efficiencies throughout the whole
software development cycle from
automation stability and transparency
because Jenkins becomes a self-auditing
environment now the results have been
tangible previously it took three days
before a build process could be done and
now it's taken that same three-day
process and reduced it to less than
three hours that is significant
large-scale deployments are now kept on
track and have expert support and there
is clear visibility and transparency
across the whole operations through
using the Jenkins tools implement the ca
process is over here now what is the
purpose of Jenkins here now Jenkins is
normally a kind of a CI tool which we
use for performing the build automations
and the test cases automation there it's
one of the open source tool which is
available there and one of the most
popular CI tool also available into
their Market
now this tool makes it easier for the
developers to integrate the changes to
the project here so we can easily
integrate the changes and whatever the
modifications we want to manage we will
be able to do that with the help of
Jenkins
now Jenkins also achieves The Continuous
integration with the help of couple of
plugins each and every tool which you
want to integrate have its own plugins
which is available there for example you
want to integrate Maven we have a maven
plugin in Jenkins which you can install
you can configure in that case you will
be able to use the maven there now you
can deploy the mavener to build tool
onto the Jenkins server and then you can
prepare or you can configure any number
of Maven jobs in case of Jenkins
so uh what exactly the maven or the
Jenkins really do is the mavenwen
integrates with Jenkins through the
particular plugin so you can able to
automate the pills because for
automation the build you require some
integration with the maven and that
integration is what we are getting from
the maven plugin so in Jenkins you have
to install the maven plugin and once the
plugin is installed so what you can do
is that you can proceed with the
configurations you can proceed with the
setup and this a particular plugin can
help you to build out some of the Java
based projects which is available there
in the git repositories and once that is
done you will be able to go ahead and
you will be able to process a complete
integration of Maven within Jenkins
right so let's see that how we can go
for the integration now I have already
installed the maven onto the Linux
virtual machine which we are using so
using the app utility or using the Yum
utility you can actually download the
Jenkins package and the maven package
onto the server onto the virtual machine
and now I'm going to proceed further
with the plugin installation and the
configuration of a maven project so I
have a GitHub repository which is having
a maven project Maven uh source code and
the mavenized test cases over there so
let's see let's log into the uh Jenkins
and see that how it works
so this is the Jenkins interface which
we have over here now in this one what
we can do is that we can create some
Maven jobs over here and once those jobs
are created we will be able to do a
custom build onto these Jenkins so first
of all we have to install the uh
particular plugin here
for that we have to go to the manage
shankings in manage Jenkins you have the
manage plugins option there so you have
to click on that now here you will be
having different tabs like updates
available installed Advanced all these
different tabs are available there so
what you can do is that you can click on
the available one when you go to the
available tab so what will happen that
here you can actually put up that what
exactly uh plugin you want to fetch here
so I can put a plugin called maven
now you can see that the very first one
the maven integration tool is available
so I'm going to select that particular
plugin and click on download now and
install after restart
now once that is done so what will
happen that the plugin will be
downloaded but in order to reflect the
changes we have to do a couple of
restart now for that you don't have to
go to the virtual machine you have the
option here itself that will allow you
to do the restart over here when you
click on this button so you check this
option and say that restart Jenkins when
the installation is done so what will
happen that the installation will be
automatically attempted whenever the
particular plugin installation is
completed here so you just have to
refresh the page again and you will be
able to see that the particular Jenkins
is being processed as such here
right so you can see that the screen is
coming up that Jenkins is restarting so
it will take a couple of five to six
seconds to do the restart and the login
screen to come up again over there
you can do the refresh also if you feel
automatically it will be reloaded once
the Jenkins is ready but sometimes we
have to refresh it so that we can get
the screen over there so once the login
is done so my Maven integration is done
so next thing which I will be doing is
that I will be creating a maven related
project so I'm going to put the admin
user
and the password so whatever the user
and password you have created you are
going to put that so that you will be
able to log into the Jenkins portal now
this is the Jenkins which is available
here so all you have to do is that you
have to click on create a new job or new
item so both the option is pretty much
same only
so here you will be able to see a maven
project here so I'm going to select like
maven build that's the name which I'm
going to give here and the maven project
I'm going to select here
and then press ok
now here you will be providing the first
of all the repository from which you
will be checking out the source code now
I can have a discard old builds over
here so if I feel that I want to have
like low rotation so all the previous
builds should be deleted so I'm just
saying that dates to keep a build should
be 10 over here and the number of bills
which I need to keep over here is 20.
you can adjust these settings according
to your requirement but uh over here we
are you know doing a kind of
configurations which we are trying to do
a lot of configurations and settings
same so these are the particular
settings which we are looking forward as
such over here so now we are going to
have the log rotation here so we can
have it like how many days we want to
keep and how many number of bills we
want to keep here so both the values we
are providing over here and then now I'm
going to put the get uh integration here
like the repo URL so I have this
repository here in which I have the Java
source code and some uh particular uh
genuine test cases and all I also have
the particular source code and it's kind
of a maven project so that's what I'm
trying to clone over here with the help
of this plugin so this plugin will
download this repository it will clone
it on to the Jenkins server and then
depending on our integration with mav1
the maven build will be triggered here
so now I'm going to process with the uh
Maven here so you can see here that it's
saying that uh Jenkins needs to know
that where the maven is installed
because that Maven version it needs to
configure it needs to process on that
part so I'll just do the save over here
and or I can click on this tool
configuration so I'll just save or do
the apply click on this tool
configuration here
now here you have the options like where
you can have the jdk installation but
what happens that thing Jenkins is
running there so jdk is automatically
installed so in the tools configuration
you don't have to put the jdk
configuration but at least for the maven
configurations you have to provide that
where exactly the maven is available
there so I'm just saying that maven
three I want to process and the latest
moving Apache web server I want to
configure here so I just want to have
like I just want to save this settings
so that it will be automatically
download the latest version Apache 3.6.3
version there and that same should be
utilized over here in this case now I'm
just going to the maven Builder
configuration here and click on the
configure part so these git repository
is available here and in the build step
it automatically builds up that uh what
a maven environment you want to select
so you see that previously since I did
not configure my Maven environment so it
was throwing an error but once I have
configured that I have to download it
during the build process or before the
build that utility should be downloaded
so instead of doing the physical
installation of Maven on the server what
I have chosen over here is that I have
selected the particular version like I
have selected that a particular
3.6.3 version should be installed for
the maven purposes over here now once
the that is done I'm going to put the
particular
steps over here you can have it like
clean install you can have clean compile
test clean test or test alone you can
give it's just a part of the setup or
the goals which you want to configure
here it by default says that pom.xml
file is the current one in the current
directory you need to refer you need to
pick on that one what it's up to you
only that how you want to configure and
how you want to process as such this
information so according to your
requirement you can say that okay I'm
just want to go for these particular
goals and you can say like save over
here the particular configuration will
be saved now you can just click on the
build now and you will be able to see
that the first of all the git clone will
happen and then the desired Mi 1
executable will be uh the build tool
will be configured and according to that
it will be processed here so you can see
here that uh the maven is uh getting
downloaded it's getting configured here
and once it's configured because I have
explained over there that 3.6.3 version
I have to select so that specific
version will be configured and will be
picked up over here now even if you
don't have the maven installed on the
physical machine on which the Jenkins is
running still you will be able to do the
processing using this particular
component here so you can see here that
we have some particular test cases
executed and in the end we are able to
get a particular artifices also there
since I did not call upon the package or
install goal that's the reason why the
particular artifacts was not generated
War file or jar file whatever the
packaging mode is available at Palm
level but still what happens that my
test cases do gets executed and that's
what I have got over here in this case
so this is a kind of a mechanism where
we feel that how we can configure a git
repository once the git repository is
configured you are going to integrate
the maven plugin in the maven plugin you
are going to configure in the tools
configuration that this and so and so
version I want to configure to run my
build and once that is done after that
you just have to trigger the build and
click on the bill now option and once
that is done you will be able to get a
particular full-fledged build or
compilation happened onto the Jenkins
and this log will give you the complete
details that what are the different
steps which has happened on this one so
let's take a slow scenario of a
developer and a tester before you had
the world of Docker a developer would
actually build the code and then they
send it to the tester but then the code
wouldn't work on their system coders
will work on the other system due to the
differences in computer environments so
what could be the solution to this well
you could go ahead and create a virtual
machine to be the same of the solution
in both areas we think Docker is an even
better solution so let's kind of break
out what the main big differences are
between Docker and virtual machines as
you can see between the left and the
right hand side both look to be very
similar what you'll see however is that
on the docker side what you'll see as a
big difference is that the guest OS for
each container has been eliminated
Docker is inherently more lightweight
but provides the same functionality as a
virtual machine so let's step through
some of the pros and cons of a virtual
machine versus Docker so first of all a
virtual machine occupies a lot more
memory space on the host machine in
contrast Docker occupies significantly
less memory space the boot up time
between both is very different Docker
just boots up faster the performance of
the docker environment is actually
better and more consistent than the
virtual machine Docker is also very easy
to set up and very easy to scale the
efficiencies therefore are much higher
with a Docker environment versus a
virtual machine environment and you'll
find it is easier to Port Docker across
multiple platforms than a virtual
machine finally the space allocation
between Docker and a virtual machine is
significant when you don't have to
include the guest OS you're eliminating
a significant amount of space and the
dock environment is just inherently
smaller so after Docker as a developer
you can build out your solution and send
it to a tester as long as we're all
running in the docker environment
everything will work just great so let's
step through what we're going to cover
in this presentation we're going to look
at the devops tools and where Docker
fits within that space we'll examine
what Docker actually is and how Docker
works and then finally we'll step
through the different components of the
docker environment so what is devops
devops is a collaboration between the
development team the operation team
allowing you to continuously deliver
Solutions and application and services
that both delayed and improved the
efficiency of your customers if you look
at the Venn diagram that we have here on
the left hand side we have development
on the right hand side we have operation
and then there's a crossover in the
middle and that's where the devops team
sits if we look at the areas of
integration between both groups
developers are really interested in
planning code building and testing and
operations want to be able to
efficiently deploy operate a monitor
when you can have both groups
interacting with each other on these
seven key and elements then you can have
the efficiencies of an excellent devops
team so planning and code base we use
tools like jit and gear for building we
use Gradle and Maven testing we use
selenium the integration between Dev and
Ops is through tools such as Jenkins and
then the deployment operation is done
with tools such as Docker and Chef
finally nagis is used to monitor the
entire environment so let's step deeper
into what Docker actually is so Docker
is a tool which is used to automate the
deployment of applications in a
lightweight container so the application
can work efficiently in different
environments now it's important to note
that the container is actually a
software package that consists of all
the dependencies required to run the
application so multiple containers can
run on the same Hardware the containers
are maintained in isolated environments
they're highly productive and they're
quick and easy to configure so let's
take an example of what Docker is by
using a house that may be rented for
someone using Airbnb so in the house
there are three rooms and only one
cupboard and kitchen and the problem we
have is that none of the guests are
really ready to share the cupboard and
kitchen because every individual has a
different preference when it comes to
how the cupboard should be stocked and
how the kitchen should be used this is
very similar to how we run software
applications today each of the
applications could end up using
different Frameworks so you may have a
framework such as rails perfect and
flask and you may want to have them
running for different applications for
different situations this is where
Docker will help you run the
applications with the suitable
Frameworks so let's go back to our
Airbnb example so we have three rooms
and a kitchen and cupboard how do we
resolve this issue well we put a kitchen
and cupboard in each room we can do the
same thing for computers Docker provides
the suitable Frameworks for each
different application and since every
application has a framework with a
suitable version this space could also
then be utilized for putting in software
and applications that are long and since
every application has its own framework
and suitable version the area that we
had previously stored for a framework
can be used for something else now we
can create a new application and this
instance a fourth application that uses
its own resources you know what with
these kinds of abilities to be able to
free up space on the computer it's no
wonder Docker is the right choice so
let's take a closer look to how Docker
actually works so when we look at Docker
and we call something Docker we're
actually referring to the base engine
which actually is installed on the host
machine that has all the different
components that run your Docker
environment and if we look at the image
on the left hand side of the screen
you'll see that Docker has a client
server relationship there's a client
installed on the hardware there is a
client that contains the docker product
and then there is a server which
controls how that Docker client is
created the communication that goes back
and forth to be able to share the
knowledge on that Docker client
relationship is done through a rest API
this is fantastic news because that
means that you can actually interface
and program that API so we look here in
the animation we see that the docker
client is constantly communicating back
to the server information about the
infrastructure and it's using this rest
API as that Communication channel the
docker server then will check out the
requests and the interaction necessary
for it to be the docker Daemon which
runs on the server itself well then
check out the interaction and the
necessary operating system pieces needed
to be able to run the container okay so
that's just an overview of the docker
engine which is probably where you're
going to spend most of your time but
there are some other components that
form the infrastructure for Docker let's
dig into those a little bit deeper as
well so what we're going to do now is
break out the four main components that
comprise of the docker environment the
full components are as follows the
docker client and server which we've
already done a deeper dive on Docker
images Docker containers and the docker
registry so if we look at the structure
that we have here on the left hand side
you see the relationship between the
docker client and the docker server and
then we have the rest API in between now
if we start digging into that rest API
particularly the relationship with the
docker Daemon on the server we actually
have our other elements that form the
different components of the docker
ecosystem so the docker client is
accessed from your terminal window so if
you are using Windows this can be
Powershell on Mac it's going to be your
terminal window and it allows you to run
the docker Daemon and the registry
service when you have your terminal
window open so you can actually use your
terminal window to create instructions
on how to build and run your Docker
images and containers if we look at the
images part of our registry here we
actually see that the image is really
just a template with the instructions
used for creating the containers which
you use within Docker the docker image
is built using a file called the docker
file and then once you've created that
Docker file you'll store that image in
the docker Hub or registry and that
allows other people to be able to access
the same structure of a Docker
environment that you've created the
syntax of creating the image is fairly
simple it's something that you'll be
able to get your arms around very
quickly and essentially what you're
doing is you're creating the option of a
new container you're identifying what
the image will look like what are the
commands that are needed and the
arguments for within those commands and
once you've done that you have a
definition for what your image will look
like so if we look here at what the
container itself looks like is that the
container is a standalone executable
package which includes applications and
their dependencies it's see instructions
for what your environment will look like
so you can be consistent in how that
environment is shared between multiple
developers testing units and other
people within your devops team now the
thing that's great about working with
Docker is that it's so lightweight that
you can actually run multiple Docker
containers in the same infrastructure
and share the same operating system this
is its strength it allows you to be able
to create those multiple environments
that you need for multiple projects that
you're working on interestingly though
within each container that container
creates an isolated area for the
applications to run so while you can run
multiple containers in an infrastructure
each of those containers are completely
isolated they're protected so that you
can actually control how your Solutions
work there now as a team you may start
off with one or two developers on your
team but when a project starts becoming
more important and you start adding in
more people to your team you may have
have 15 people that are offshore you may
have 10 people that are local you may
have 15 Consultants that are working on
your project you have a need for each of
those developers or each person on your
team to have access to that Docker image
and to get access to that image we use a
Docker registry which is an Open Source
server-side service for hosting and
distributing the images that you have to
find you can also use Docker itself as
its own default Retreat and Docker Hub
now something that has to be bear in
mind though is that for publicly shared
images you may want to have your own
private images in which case you would
do that through your own registry so
once again public repositories can be
used to host the docker images which can
be accessed by anyone and I really
encourage you to go out to Docker and
see the other Docker images that have
been created because there may be tools
there that you can use to speed up your
own development environments now you
will also get to a point where you start
creating environments that are very
specific to the solutions that you are
building and when you get to that point
you'll likely want to create a private
repository so you're not sharing that
knowledge with the world in general now
the way in which you connect with the
docker registry is through simple pull
and push commands that you run through
terminal window to be able to get the
latest information so if you want to be
able to build your own container what
you'll start doing is using the pull
commands to actually pull the image from
the docker repository and the command
line for that is fairly simple in
terminal window you would write Docker
pull and then you put in the image name
and any tags associated with that image
and use the command pause so in your
terminal window you would actually use a
simple line of command once you've
actually connected to your Docker
environment and that command will be
Docker pull with the image name and any
Associated tags around that image what
that will then do is pull the image from
the docker repository whether that's a
public repository or a private one now
in Reverse if you want to be able to
update the docker image with new
information you do a push command where
you take the script that you've written
about the docker container that you
define and push it to the repository and
as you can imagine the commands for that
are also fairly simple in terminal
window you would write Docker push the
image name any Associated tags and then
that would then push that image to the
docker repository again either a public
or a private repository so if we recap
the docker file creates a Docker image
that's using the build commands Docker
image then contains all of the
information necessary for you to be able
to execute the project using the docker
image any user can run the code in order
to create a Docker container and once a
Docker image is built it's uploaded to a
registry or to a Docker Hub where it can
be shared across your entire team and
from the docker Hub users can get access
to the docker image and build their own
new containers so let's have a look at
what we have in our current environment
so today when you actually have your
standard machine you have the
infrastructure you have the host
operating system and you have your
applications and then when you create a
virtual environment what you're actually
doing is you're actually creating
virtual machines but those virtual
machines actually are now sitting within
a hypervisor solution that sits still on
top of your host operating system and
infrastructure and with a Docker engine
what we're able to do is we're able to
actually reduce significantly the
different elements that you would
normally have within a virtualized
environment so we're able to get rid of
the the bins and the so we're able to
get rid of the guest OS and we're able
to eliminate the hypervisor environment
now this is really important as we
actually start working and creating
environments that are consistent because
we want to be able to make it so it's
really easy and stable for the
environment that you have within your
Dev and Ops environment now critical is
getting rid of that hypervisor element
it's just a lot of overhead so let's
have a look at a container as an example
so here we actually have a couple of
examples on the right hand side we have
different containers we have one
container that's running Apache Tomcat
in it with Java a second container is
running SQL server and microsoft.net
environment the third container is
running python with mySQL these are all
running just fine within the docker
engine and sitting on top of a host OS
which could be Linux it really could be
any host OS within a consistent
infrastructure and you're able to have a
solution that can be shared easily
amongst your teams so let's have a look
at an example that you'd have today if a
company is doing a traditional Java
application so you have your developers
working in JBoss on his system and he's
coding away and he has to get that code
over to a tester now what will happen is
that tester will then typically in your
traditional environment then have to
install JBoss on their machine and get
everything running and cool and
hopefully set up identically to the
developer chances are they probably
won't have it exactly the same but
they're trying to get it as close as
possible and then at some point you want
to be able to test this within your
production environment so you send it
over to a system administrator who would
then also have to install JBoss on their
environment as well yeah this just seems
to be a whole lot of duplication so why
go through the problem of installing
JBoss three times and this is where
things get really interesting because
the challenge you have today is that
it's very difficult to almost impossible
to have identical environments if you're
just installing software locally on
devices the developers probably got a
whole bunch of development software they
could be conflicting with the JBoss
environment the tester has similar
testing software but probably doesn't
have all the development software and
certainly the system administrator won't
have have all the tools of the developer
and test to have their own tools and so
what you want to be able to do is kind
of get away from The Challenge you have
of having to do local installations on
three different computers and in
addition what you see is that this uses
up a lot of effort because when you're
having to install software over and over
again you just keep repeating doing
really basic foundational challenges so
this is where Docker comes in and Docker
is the tool that allows you to be able
to share environments from one group to
another group without having to install
software locally on a device you install
all of the code into your Docker
container and simply share the container
so in this presentation we're going to
go through a few things we're going to
cover what Docker actually is and then
we're going to dig into the actual
architecture of Docker and kind of go
through what Docker container is and how
to create a Docker container and then
we'll go at through the benefits of
using Docker containers and then the
commands and finalize everything up with
a brief demo so what is darker so darker
is as you'd expect because all the
software that we cover in this series is
an open source solution and it is a
container solution that allows you to be
able to containerize all of the
necessary files and applications needed
to run the solution you're building so
you can share it from different people
in your team whether it's a developer a
tester or system administrator and this
allows you to have a consistent
environment from one group to the next
so let's kind of dig into the
architecture so you understand why
Docker runs effectively so the docker
architecture itself is built up of two
key elements there is the docker client
and then there is a rest API connection
to a Docker Daemon which actually hosts
the entire environment within the docker
host and the docker demo you have your
different containers and each one has a
link to a Docker register three the
docker client itself is a rest service
so as you'd expect a rest API and that
sends command line to the docker Daemon
through a terminal window or command
line interface window and we'll go
through some of these demos later on so
you can actually see how you can
actually interact with Docker the docker
Dem then checks the request against
um the docking components and then
performs the service that you're
requesting now the docker image itself
all it really is a collection of
instructions used to create a container
and again this is consistent with all
the devops tools that we have the devops
tools that we're looking to use
throughout this series of videos are all
environments that can be scripted and
this is really important because it
allows you to be able to duplicate and
scale the environments that you want to
be able to build very quickly and
effectively the agile container itself
has all of the applications and the
dependencies of those applications in
one package you kind of think of it as a
really effective and efficient zip file
it's a little bit more than that but
it's one file that actually has
everything you need to be able to run
all of your Solutions the actual Docker
registry itself is an environment for
being able to host and distribute
different Docker images among your team
so say for instance you had a team of
developers that were working on multiple
different solutions so say you have a
team of developers and you have 50
developers and they're working on five
different applications you can actually
have the applications themselves the
containers shared in the docker registry
so each of those teams at any time check
out and have the latest container of
that latest image of the code that
you're working on so let's dig into what
actually is in a container so the
important part of a docking container is
that it has everything you need to be
able to run the application it's like a
virtualized environment it has all your
Frameworks and your libraries and it
allow allows the teams to be able to
build out and run exactly the right
environment that the developer intended
what's interesting though is the actual
applications then will run in isolation
so they're not impacting other
applications that using dependencies on
other libraries or files outside of the
container because of the architecture it
really uses a lot less space and because
it's using less space it's a much more
lightweight architecture so the file
isn't the actual folder itself is much
smaller it's very secure highly portable
and the boot up time is incredibly fast
so let's actually get into how you would
actually create a Docker container so
the docker container itself is actually
built through command line and it's
built of a file and Docker image so the
actual Docker file is a text file that
contains all the instructions that you
would need to be able to create that
Docker image and then will actually then
create all of the project code with
inside of that image then the image
becomes the item that you would share
through the docker registry you would
then use the command line and we'll do
this later on select Docker run and then
the name of the image to be able to
easily and effectively run that image
locally and again once you've created
the document you can store that in the
docker registry making it available to
anybody within your network so something
to bear in mind is that Docker itself
has its own registry called Docker Hub
and that is a public restrict so you can
actually go out and see other Docker
images that have been created and access
those images as your own company you may
want to have your own private repository
so you want to be able to go ahead and
either do that locally through your own
repository or you can actually get a
licensed version of Docker Hardware you
can actually then share those files now
something that's also very interesting
to know is that you can have multiple
versions of a Docker image so if you
have a different version control
different release versions and you want
to be able to test and write code for
those different release versions because
you may have different setups you can
certainly do that within your Docker
registry environment okay so let's go
ahead and we're going to create a Docker
image using some of our basic Docker
commands and so there are essentially
really you know kind of just two
commands that you're going to be looking
for one is the build command another one
is to actually put it into your registry
which is a push command so if you want
to get a image from a Docker registry
then you want to use the pull command
and a pull command simply pulls the
image from the registry and in this
example using nginx as our registry and
we can actually then pull the image down
to our test environment on our local
machine so we're actually running the
container within our Docker application
on a local machine we're able to then
have the image run exactly as it would
in production and then you can actually
use the Run command to actually use the
docker image on your local machine so
just a you know a few interesting
tidbits about the docking container once
the container is created a new layer is
formed on top of the docker image layer
is called the container layout each
container has a separate read write
container layer and any changes made in
that docking container is then reflected
upon that particular container layout
and if you want to delete the container
layer the container layer also gets
deleted as well so you know why with
using Docker and containers be of
benefit to you well you know some of the
things that are useful is that
containers have no external dependencies
for the applications they run once you
actually have the container running
locally it has everything it needs to be
able to run the application so there's
no having to install additional pieces
of software such as the example we gave
with JBoss at the beginning of the
presentation now the containers are
really lightweight so it makes it very
easy to share them containers amongst
your teams whether it's a developer
whether it's a tester whether it's
somebody on your operations environment
it's really easy to share those
containers amongst your entire team
different data volumes can be easily
reused and shared among multiple
containers and again this is another big
benefit and this is a reflection of the
lightweight nature of your containers
the container itself also runs in
isolation which means that it is not
impacted by any dependencies you may
have on your own local environment so
it's a completely sandboxed environment
so some of the questions you might ask
us you know can you run multiple
containers together without the need to
start each one individually and you know
what yes you can with Docker compose
Dock and compose allows you to run
multiple containers in a single service
and again this is a reflection on the
lightweight nature of containers within
the docker environment so we're going to
end our presentation by looking at some
of the basic commands that you'd have
within Docker so we have here on the
left hand side we have a Docker
container and then the command for each
item we're actually going to go ahead
and use some of these commands and the
demo that we're going to do after this
presentation you'll see that in a moment
but just you know some of the basic
commands we have are committing the
docker image into the Container kill is
a you know standard kill command to you
know terminate one or more of the
running containers so they stop working
then restart those containers but
suddenly you can look at all the image
all of the commands here and try them
out for yourself so we're going to go
ahead and start a demo of how to use the
basic commands to run Docker so to do
this we're going to open up terminal
window or command line now depending
whether you're running Linux PC or Mac
and we're going to go ahead and first
thing we want to do is see what our
Docker image lists are so we can go sudo
Docker images and this will give us well
first we'll enter in our password so
let's go enter that in and this will now
give us a list of our Docker images and
here are the docker images I have
already been created in the system and
we can actually go ahead and actually
see the processes that are actually
running so I'm going to go ahead and
open up this window a little bit more
but this will show you the actual
processes and the containers that we
actually have and so on the far left
hand side you'll see under names we have
learn simply learn be unscore cool these
are all just different ones that we've
been working on so let's go ahead and
create a Docker image so I'm going to do
sudo
docker
run
Dash Dash p
0.0.0.0 go on 80 colon 80.
Ubuntu and this will allow us to go
ahead and run an Ubuntu image and this
will run the latest image and what we
have here is a hash number and this hash
number is a unique name that defines the
container that we've just created and we
can go ahead and we can check to make
sure that the container actually is
present so we're going to do
pseudo.co.ps and this actually show us
down there so it's not in a running
state right now but that doesn't mean
that we don't have it so let's list out
all the containers that are both running
and in the exit state so let's do sucker
PS Dash a and this lists all the
containers that I have running on my
machine
and this shows all the ones that have
been in the running State and in the
exit State and here we see one that we
just created about a minute ago and it's
called learn
and these are all running Ubuntu and
this is the one that we had created just
a few seconds ago
let's open it up and
there we go so let's change that to that
new Doc container to a running state so
scroll down and we're going to type sudo
docker
run
Dash it Dash Dash
name my
um so this is going to be the new
container name it's going to be my
Docker so this is how we name our Docker
environment
and we'll put in the image name which is
Ubuntu
and dash bin Dash Bash
and it's now in our root and we'll exit
out of that
so now we're going to go ahead and start
the new my Docker container so sudo
docker
start
my
and we'll get the container image which
will be my docker
my docker
return and that started that Docker
image and let's go ahead and check
against the other running Docker images
to make sure it's running correctly so
sudo docker
PS
and there we are underneath name on the
right hand side you have to see my
Docker along with the other Docker
images that we created and it's been
running for 13 seconds
quite fast so we want to rename the
container let's use the command sudo
docker
rename we can take another Docker image
this to scrub this one and we'll put it
in rename
and we'll rename and put in the old
container name which is image and then
we'll put in the new container name and
let's call it
purple
so now the container image that had
previously been called image is now
called Purple
so do sudo Docker PS
to list all of our Docker images
and if we scroll up and there there we
go purple
how easy is that to rename an image
and we can go ahead and use this command
if we want to stop container so we're
going to write sudo docker
stop
and then we'll have to put in the
container name
and we'll put in my Docker the container
that we originally created
and that image has now stopped
and let's go ahead and prove that we're
going to list out all the docker images
and what you see is that it's not listed
in the active images it's uh not on the
list on the far right hand side
but if we go ahead and we can list out
all of the docker images so you actually
see it's still there as an image it's
just not in an active stages what's
known as in an exit state
so here we go
and there's my Docker it's in an exited
state so that happened 27 seconds ago
so if we want to to remove a container
we can use the following command
so sudo docker
RM
we'll remove
my docker and that will remove it from
the extent state
and we're going to go ahead and we're
going to double check that
and yep
yep it's not not listed there under exit
State anymore
it's gone
and there we go
there that's where it used to be all
right let's go back
so if we want to exit a container in the
running state so we do sudo kill and
then the name of the container
I think one of them is called yellow
let's just check and see if that's going
to kill it
oops no I guess we don't have one called
yellow so let's find out name of
container that we actually have
so sudo Docker kill oh we're going to
list out of the ones that are running oh
okay there we go now yellow isn't in
that list so let's take I know let's
take simply learn and so we can actually
go ahead and let's write sudo Docker
kill simply learn
and that will actually kill an active
Docker container
boom there we go
and we list out all the active
containers you can actually see now
that's they simply learn container is
not active anymore
and these are all the basic commands for
Docker container so if you are ready to
embrace this exciting career path look
no further then simply learns Caltech
postgraduate program in devops this
comprehensive program will equip you
with the knowledge and skills needed to
master the day of principles tools and
practices dive deep into
containerization orchestration and
automation Frameworks like Docker
kubernetes and genkets gain hands-on
experience with Cloud platforms and
learn how to Leverage The Power of
infrastructure as code don't miss out on
this chance to transform your career and
become an invaluable asset to any
organization click the link in the
description to discover more about this
day offs course we're going to break up
this presentation into four key areas
we're going to talk about life before
kubernetes which some of you are
probably experiencing right now what is
kubernetes the benefits that kubernetes
brings to you particularly if you are
using containers in a devops environment
and then finally we're going to break
down the architecture and working
infrastructure manager for kubernetes so
you understand what's happening and why
the actions are happening the way that
they are so let's jump into our first
section of life before kubernetes so the
way that you have done work in the past
or you may be doing work right now is
really building out and deploying
Solutions into two distinct areas one is
a traditional deployment where you're
pushing out code to physical servers in
a Data Center and you're managing the
operating system and the code that's
actually running on each of those
servers another environment that you may
potentially be using is to Blind code
out to Virtual machines so let's go
through and look at the two different
types of deployments that you may be
experiencing when you have applications
running on multiple machines you run
into the potential risk that the setup
and configuration of each of those
machines isn't going to be consistent
and your code isn't going to work
effectively and there may be issues with
uptime and errors within the
infrastructure of your entire
environment there's going to be problems
with resource allocation and you're
going to have error issues where
applications may be running effectively
and not not effectively and not load
balance effectively across the
environment the problem that you have
with this kind of infrastructure is that
it gets very expensive you can only
install one piece of software one
service on one piece of Hardware so your
Hardware is being massively
underutilized this is where virtual
machines have become really popular with
a virtual machine you're able to have
better resource utilization and
scalability a much less cost and this
allows you to be able to run multiple
virtual machines on a single piece of
Hardware the problem is is that VMS of
for virtual machines are not perfect
either some of the challenges you run
with VMS is that the actual hardware and
software need needed to manage the VM
environment can be expensive there are
security risks with virtual with VMS
there are security risks with VMS there
have been data breaches recorded about
solutions that run in virtualized
environments you also run into an issue
of availability and this is largely
because you can only have a finite
number of virtual machines running on a
piece of hardware and this results in
limitations and restrictions in the
types of environment you want to be
running and then finally setting up and
managing a virtualized environment is
time consuming it can take a lot of time
and it can also get very expensive so
how about kubernetes well kubernetes is
a tool that allows you to manage
containerized deployment of solutions
and inherently kubernetes is a tool that
is really a Next Level maturity of
deployment so if you can think of your
maturity curve as deploying code in
directly to Hardware in a Data Center
and then deploying your solutions to
Virtual machines the next evolution of
that deployment is to use containers and
kubernetes so let's kind of go through
and look at the differences between a
virtual machine and kubernetes I've got
a few here that we want to highlight and
you'll get an understanding of what the
differences are between the two so first
of all with virtual machines there is
inherently security risks and what
you'll find as we get dig through the
architecture later in the presentation
is that kubernetes is inherently secure
and this is largely because of the
Legacy code the legacy of kubernetes and
where it came from and we'll talk about
that in just a moment but kubernetes is
inherently secure virtual machines are
not easily portable now with that said
they they are technically portable
they're just not very easily portable
whereas with kubernetes it's working
with Docker container Solutions it is
extremely portable that means that you
can actually spin up and spin down and
manage your infrastructure exactly the
way that you want it to be managed and
scale it on the demands of the customers
as they're coming in to use the solution
from a time consuming point of view
kubernetes is much less time consuming
than with a virtual machine a few other
areas that we want to kind of highlight
from differences virtual machines use
much less isolation when building out
the encapsulated environment then
kubernetes does for instance with a
virtual machine you have to run
hypervisor on top of the OS and hardware
and then inside of the virtual machine
you also have to have the operating
system as well whereas in contrast on a
kubernetes environment because it's
leveraging a Docker container and or
container-like Technologies it only has
to have the OS and the hardware and then
inside of each container it doesn't need
to have that additional OS layer it's
able to inherit what it needs to be able
to run the application this may makes
the whole solution much more flexible
and allows you to run many more
containers on a piece of Hardware than
versus running virtual machines on a
single piece of Hardware so as we
highlighted here VMS are not as portable
as kubernetes and kubernetes is portable
directly related to the use of
containerization and because kubernetes
is built on top of containers it is much
less time consuming because you can
actually script and automatically
allocate resource to nodes within your
kubernetes environment because it allows
the infrastructure to run much more
effectively and much more efficiently so
this is why if we look at our evolution
of the land of time before kubernetes
while we are running into a solution
where kubernetes had to come about
because the demand for having more
highly scalable solutions that are more
efficient was just really a natural
evolution of this software deployment
model that started with pushing out code
to physical hardware and then pushing
code out to virtual machines and then
needing to have a solution much more
sophisticated kubernetes would have come
about at some point in time I'm just
really glad it came back when it did so
what is kubernetes let's dig into the
history of kubernetes and how it came
about so in essence uh kubernetes is an
open source platform that allows you to
manage and deploy and maintain groups of
containers and a container is something
like Docker and if you're a developing
code you're probably already using
Docker today consider kubernetes as the
tool that manages multiple Docker
environments together now we talk a lot
about Docker and as a container solution
with kubernetes the reality is is that
kubernetes can actually use other
container tools out there but Docker
just simply is the most popular
container out there both these tools are
open source that's one that is so
popular and they just allow you to be
able to have flexibility in being able
to scale up your Solutions and they will
Design and for the post-digital world
that we live and exist in today so a
little bit of background a little bit of
trivia around kubernetes so kubernetes
was originally a successor to a project
at Google and the original project was
Google book um Google Borg it does
exactly what kubernetes Dan does today
but kubernetes was Rewritten from the
ground up and then released as an open
source project in 2014 so that people
outside of Google could take advantage
of the power of kubernetes
containerization management tools and
today it is managed by the cloud native
Computing foundation and there are many
many companies that support and manage
kubernetes so for instance if you're
signing up for Microsoft Azure AWS
Google Cloud all of them will leverage
kubernetes and it's just become the the
de facto tool for managing large groups
of containers so let's kind of Step
through some of the key benefits that
you'd experienced from kubernetes so we
have nine key benefits the first it is
highly portable it is 100 open source
code and this means that you can
actually go ahead and contribute to this
code project if you want to through
GitHub uh the ability to scale up the
solution is incredible
um what's um the the history of
kubernetes being part of a Google
project for managing the Google network
and infrastructure kind of really sets
the groundwork for having a surgeon that
is highly scalable the out of the high
scalability also comes the need for high
availability and this is the desire to
be able to have a highly efficient and
highly energized environment that also
you can really rely on so if you're
building outer kubernetes management
environment you know that it's going to
be available for the solutions that
you're maintaining and it's really
designed for deployment so you can
script out the environment and actually
have it as part of your devops model so
you could scale up and meet the demands
of your customer then what you'll find
is that the load balancing is extremely
efficient and it allows you to
distribute the load efficiently across
your entire network so your network
remains stable and then also the tool
allows you to manage the orchestration
of your storage so you can have local
storage such as an SSD on the hardware
that the kubernetes is maintaining or if
the kubernetes environment is pulling
storage from a public Cloud such as
Azure or AWS you can actually go ahead
and make that available to your entire
system and you can inherit the security
that goes back and forth between the
cloud environments and one of the things
you'll find consistent with kubernetes
is that it is designed for a cloud-first
environment and kubernetes as well is
that it's it's really a self-healing
environment so if something happens or
something fails kubernetes will detect
that failure and then either restart the
process kill the process or replace it
and then because of that you also have
automated rollouts and rollbacks in case
you need to be able to manage the state
of the environment and then finally you
have automatic bin packaging so you can
actually specify the compute power
that's being used from CPU and RAM for
each container so let's dig into the
final area which is the actual
kubernetes architecture and we're going
to cover this at a high level there's
actually another video that you can that
simply learn has developed which digs
deeper into the kubernetes architecture
and so the kubernetes architecture is a
clustered based architecture and it's
really about two key areas you have the
kubernetes master which actually
controls
um and all of the activities within your
entire kubernetes infrastructure and
then you have nodes that actually are
running on Linux machines out that are
controlled by the master so let's kind
of go through some of these areas so
when look at the kubernetes master to
begin with and we'll start with
Etc is this is a tool that allows for
the configuration and the information
and the management of nodes within your
cluster and one of the key features that
you'll find with all of the tools that
are managed within either a the master
environment or within a node is that
they are all accessible via the API
server and what's interesting about the
API server is that it's a restful based
infrastructure which means that you can
actually secure each connection with SSL
and other security models to ensure that
your entire infrastructure and the
communication going back and forth
across your infrastructure is tightly
secured schedule it goes ahead and
actually as you'd expect it actually
manages the schedule of activities
within the actual cluster and then you
have the control and the controller is a
Daemon server that actually manages and
pushes out the instructions to all of
your nodes so the other tool is really
the the infrastructure and you can
consider them the administration site
I'm of the master whereas controller is
the management it actually pushes out
all of the controls via the API server
so let's actually dig into one of the
actual nodes themselves and there are
three key areas of the nodes one is the
doctor environment which actually helps
and manage and maintain the container
that's actually inside of the node and
then you have the Kubler which is
responsible for information that goes
back and forth and it's going to do most
of the conversation with the API server
on the actual health of that node and
then you have the actual kubernetes
proxy which actually runs the services
actually inside of the node so as you
see all of these infrastructures are
extremely lightweight and designed to be
very efficient and very available for
your infrastructure and so here's a
quick recap of the different tools that
are available and it really breaks down
into two key areas you have your
kubernetes master and and the kubernetes
node now the kubernetes Mazda has the
instructions of what's going to happen
within your kubernetes infrastructure
and then it's going to push out those
instructors to an indefinite number of
nodes that will allow you to be able to
scale up and scale down your solution in
a dynamic way let's have an overview of
the kubernetes architecture so
kubernetes has really broken up into
three key areas you have your
workstation where you develop your
commands and you push out those commands
to your master and the master is
comprised of four key areas which
essentially control all of your nodes
and the node contains multiple pods and
each pod has your Docker container built
into it so consider that you could have
a really almost an infinite number of
PODS I'm sorry infinite number of nodes
are being managed by the master
environment so you have your cluster
which is a collection of servers that
maintain the availability and the
compute power such as RAM CPU and disk
utilization and you have the my Master
which is really components that control
and schedule the activities of your
network and then you have the node which
actually hosts the actual Docker virtual
machine itself and be able to actually
control and communicate back to the
master the health of that pod and we'll
get into more detail now later in the
presentation so you know you keep
hearing me talk about containers but
they really are the center of the work
that you're doing with kubernetes and
can the concept around kubernetes and
containers is really just a natural
evolution of where we've been with
internet and digital Technologies over
the last uh 10 15 years so before
kubernetes you had tools where you
either running virtual machines or
you're running data centers that had to
maintain and manage and notify you of
any interruptions in your network
kubernetes is the tool that actually
comes in and helps address those
interruptions and manages them for you
so the solution to this is the use of
containers so you can think of
containers as that Natural Evolution
from you know uh 15 20 years ago you
would have written your code and posted
it to a Data Center and more recently
you've probably posted your code to a
virtual machine and then move the
virtual machine and now you actually
just work directly into a container and
everything is self-contained and can be
pushed out to your environment and the
thing that's great about containers
they're they're isolated environments
very easy for developers to work on them
but it's also really easy for operations
teams to be able to move a container
into production so let's kind of step
and back and look at a competing product
to kubernetes which is Docker swarm now
one of the things we have to remember is
that darker containers which are
extremely popular
um built by the company Docker and made
open source and Docker actually has
other products one of those other
products is docker warm and Docker swarm
is a tool that allows you to be able to
manage multiple containers so if we look
at some of the uh the benefits of using
Docker swarm versus kubernetes now one
thing that you'll find is that both
tools have strengths and weaknesses but
it's really good that they're both out
there because it helps keep they really
kind of justifies the importance of
having these kind of tools so kubernetes
was designed originally from the ground
up to be Auto scaling whereas duka swarm
isn't the load balancing is automatic on
Docker Swan whereas with kubernetes you
have to manually configure load
balancing across your nodes the
installation for Docker swarm is really
fast and easy I mean you can be up and
running within minutes kubernetes takes
a little bit more time is a little bit
more complicated eventually you'll get
there um I mean it's not like it's going
to take you days and weeks but it's it
is a tool that's a when you compare the
two Docker swarms much easier to get up
and running now what's interesting is
that kubernetes is incredibly scalable
and it's you know that's its real
strength is its ability to have strong
clusters whereas with Docker swarm it's
cluster stream isn't um as strong when
compared to kubernetes now you compare
it to anything else on the market it's
really good um so this is kind of a
splitting hairs kind of comparison but
kubernetes really does have the
advantage here if you're looking at the
two compared to each other for
scalability I mean kubernetes was
designed for by Google to scale up and
support Google Cloud Network
infrastructure they both allow you to be
able to share um storage volumes with
Docking you can actually do it with any
container with that is managed by the
docker swamp whereas with kubernetes it
manages the storage with the pods and a
product can have multiple containers
within it but you can't take it down to
the level of the container interestingly
uh kubernetes does have a graphical user
interface for being able to control and
manage the environment the reality
however is that you're likely to be
using terminal to actually make the
controls and Commands to control your
either Docker swarm or kubernetes
environment and it's great that it has a
GUI and to get you started but once
you're up and running you're going to be
using terminal window for those fast
quick administrative controls that you
need to make so let's look at the
hardware components for kubernetes so
what's interesting is that kubernetes is
extremely light out of all the systems
that we're looking at it's extremely
lightweight
um it allows you to have you compare it
to like a virtual machine which is very
heavy at you know kubernetes is
extremely lightweight and other uses any
resources at all interesting enough
though is that if you are looking at the
usage of CPU it's better to actually
take it for
the cluster as a whole rather than
individual nodes because the nodes will
actually combined together to give you
that whole computer power again this is
why kubernetes works really well in the
cloud where you can do that kind of
activity rather than if you're running
in your own data center
um so you can have persistent volumes um
such as a local SSD or you can actually
attach to a cloud data storage again
kubernetes is really designed for the
cloud I would encourage you to use cloud
storage wherever possible rather than
relying on physical storage the reason
being is that if you connect to cloud
storage and you need to flex your your
storage the cloud will do that for you I
mean that's just an inherent part of why
you'd have cloud storage whereas if
you're connecting to physical storage
you're always restricted to the
limitations of the physical Hardware so
that's um kind of pivot and look at the
software components as compared to the
hardware components so the main part of
the components is the actual container
and all of the software running in the
container runs on Linux so if you have
documents stored as a developer on your
machine it's actually running inside of
when
um that's what makes it so lightweight
and really one of the things that you'll
find is that most data centers and Cloud
providers now are running predominantly
on Linux inside of the um the the
container itself is then managed inside
of a pod and a part is really just a
group of containers bundles together and
the kubernetes scheduler and proxy
server then actually manage what how the
pods are actually pushed out into your
kubernetes environment the the pods
themselves can achieve and share
resources both networking and storage so
pods aren't um pushed out manually
they're actually managed through a layer
of abstraction and part of their
deployment and and this is the strength
of kubernetes you use to find your
infrastructure and then kubernetes will
then manage it for you and there isn't
that problem of manual management of
PODS if you have to manage the
deployment of them if you that's simply
taken away and it's completely automated
and and the the final area of software
Services is on Ingress and this is
really the secure way of being able to
have communication from outside of the
cluster and passing of information into
that cluster and again this is done
security through SSL layers and allows
you to ensure that security is at the
center of the work that you have within
your kubernetes environment so let's
dive now into the actual architecture
before we start looking at a use case of
how kubernetes is being employed so
kubernetes again is we looked at this uh
diagram at the beginning of the
presentation and there are really three
key areas there's the workstation where
you develop your commands and then you
have your master environment which
controls the scheduling the
communication and the actual commands
that you have created and pushes those
out and manages the health of your
entire node Network and each node has
various pods so we like break this down
so the masternode is the most vital
component with the master you have four
key controls you have Etc the controller
manager schedule an API server the
cluster store Etc this actually manages
the details and values that you've
developed on your local workstation and
then we'll work with the out the control
schedule and API server to communicate
that out those instructions of how your
infrastructure should look like to your
entire network the control manager is
really an API server and again this is
all about security so we use wrestle
apis which can be packaged in SSL to
communicate back and forth across your
pods and the master and indeed the
services within each of them as well so
at every single layer of abstraction the
communication is secure the schedule as
the name would imply release schedules
when tasks get sent out to the actual
nodes themselves the nodes themselves
are done nodes they just have the
applications running on them the master
and the scam is really doing all of the
work to make sure that your entire
network is running efficiently and then
you have the API server which has your
rest commands and the communication and
back and forth across your networks that
is secure and efficient so your node
environment is where all the work that
you do with your containers gets pushed
out too so um a work is really a it's a
combination of containers and each
container will then logically run
together on that node so you'd have a
collection of containers on a node that
all make logical sense to have together
within each node you have Docker and
this is your isolated environment for
running your container you have your
cubelet which is a service for conveying
information back and forth to the
service about the actual health of the
kubernetes node itself and then finally
you have the proxy server and the proxy
server is able to manage the nodes the
volumes the the creation of new
containers and actually helps pass the
community the the health of the
container back up to the master to see
whether or not the containers should be
either killed stop started or updated so
finally let's look at see where
kubernetes is being used by other
companies so you know kubernetes is
being used by a lot of companies and
they're really using it to help manage
complex existing systems so that they
can have greater performance and with
the end goal of being able to Delight
the customer increase value to the
customer and enhance increased value and
revenue into the organization so an
example of this is a company called
BlackRock where they actually went
through the process of implementing
kubernetes so they could so BlackRock
had a chance where they needed to be
able to have much more Dynamic access to
their resources and they were running
complex installations on people's
desktops and it was just really really
difficult to be able to manage their
entire infrastructure so they actually
he went and pivoted to using kubernetes
and this allowed them to be able to be
much more scalable and expansive in the
management of their infrastructure and
as you can imagine kubernetes was then
hooked into their entire existing system
and has really become a key part of the
success that BlackRock is now
experiencing of a very stable
infrastructure and the bottom line is
that BlackRock is now able to have
confidence in their infrastructure and
be able to give their confidence as back
to their customers through the
implementation and more rapid deployment
of additional features and services as
one of the key tools that you would have
within your devops environment so the
things that we're going to go through
today is we're going to cover why you
would want to use a product like ansible
what ansible really is and how it's of
value to you in your organization the
differences between ansible and other
products that are similar to it on the
market and what makes ansible a
compelling product and then dig into the
architecture for ansible we're going to
look at how you would create a Playbook
how you would manage your inventory of
your server environments and then what
is the actual workings of ansible
there's a little extra we're going to
also throw an ansible Tower one of the
secret Source solutions that you can use
for improving the Speed and Performance
of how you create your ansible
environments and finally we're going to
go through a use Case by looking at
HootSuite social media management
company and how they use ansible to
really improve the efficiency within
their organizations so let's jump into
this so the big question is why ansible
so you have to think of ansible as
another tool that you have within your
devops environment for helping manage
the servers and this definitely falls on
the operations side of the devops
equation so if we look here we have a
picture of Sam and like yourselves Sam
is a system administrator and he is
responsible for maintaining the
infrastructure for all the different
servers within his company so some of
the servers that he may have that he has
to maintain could be web servers running
Apache they could be database servers
running MySQL and if you only have a few
servers then that's fairly easy to
maintain I mean if you have three web
servers and two database servers and
let's face it will we all love just to
have one or two servers to manage it
would be really easy to maintain the
trick however is as we start increasing
the number of servers and this is a
reality of the environments that we live
and operate in it becomes increasingly
difficult to create consistent setup of
different infrastructures such as web
servers and databases for the simple
reason that we're all human as if we had
to update and maintain all of those
servers by hand there's a good chance
that we would not set up each server
identically now this is where ansible
really comes to the rescue and helps you
become an efficient efficient operations
team ansible like other system Solutions
such as chef and puppet uses code that
you can write and describe the
installation and set up of your servers
so you can actually repeat it and deploy
those servers consistently into multiple
areas so now you don't have to have one
person redoing and re-following setup
procedures you just write one script and
then each script can be executed and
have a consistent environment so we've
gone through why you'd want to use
ansible let's step through what ansible
really is so yeah this is all great but
you know how do we actually use these
tools in our environment so ansible is a
tool that really allows you to create
and control three key areas that you
would have within your operations
environment first of all there's it
automation so you can actually write
instructions that automate the it setup
that you would tip typically do manually
in the past the second is the
configuration and having consistent
configuration imagine setting up
hundreds of Apache servers and being
able to guarantee with Precision that
each of those Apache servers is set up
identically and then finally you want to
be able to automate the deployment so
that as you scale up your server
environment you can just push out
instructions they can deploy
automatically different servers the
bottom line is you want to be able to
speed up and make your operations team
more efficient so let's talk a little
bit about pull configuration and how it
works with ansible so there are two
different ways of being able to set up
different environments for Server Farms
one is to have a key server that has all
the instructions on and then on each of
the servers that connect to that main
Master server you would have a piece of
software known as a client installed on
each of those servers that would
communicate to the main Master server
and then would periodically either
update or change the configuration of
the slave server this is known as a pull
configuration an alternative is a push
configuration and the push configuration
is slightly different the main
difference is as with a pull
configuration you have a master server
where you actually put up the
instructions but unlike the pull
configuration where you have a client
installed on each of the services with a
push configuration you actually have no
client installed on the remote servers
you simply are pushing out the
configuration to those servers and
forcing a restructure or a fresh clean
installation in that environment so
ansible is one of those second
environments where it's a push
configuration server and this contrasts
with other popular products like chef
and puppet which have a master's slave
architecture texture with a master
server connecting with a client on a
remote slave environment where you would
then be pushing out the updates with
ansible you're pushing out the service
and the structure of the server to
remote hardware and you are just putting
it onto the hardware irrelevant of the
structure that's out there and there are
some significant advantages that you
have in that in that you're not having
to have the extra overhead weight of a
client installed on those remote servers
having to constantly communicate back to
the master environment so let's step
through the architecture that you would
have for an ansible environment
so when you're setting up an ansible
environment the first thing you want to
do is have a local machine and the local
machine is where you're going to have
all of your instruction and really the
power of the control that you'd be
pushing out to the remote server so the
local machine is where you're going to
be starting and doing all of your work
connect it from the local machine are
all the different nodes pushing out the
different configurations that you would
set up on the local machine the
configurations that you would write and
you would write those in code within a
module so you do this on your local
machine for creating these modules and
each of these modules is actually
consistent playbooks the local machine
also has a second job and that job is to
manage the inventory of the nodes that
you have in your environment the local
machine is able to connect to each of
the different nodes that you would have
in your Hardware Network through SSH
clients so a secure client let's dig
into some of the different elements
within that architecture and we're going
to take a first look at playbooks that
you would write and create for the
ansible environments so the core of
ansible is the Playbook this is where
you create the instructions that you
write to define the architecture of your
Hardware so the Playbook is really just
a set of instructions that configure the
different nodes that you have and each
of those set of instructions is written
in a language called yamo and this is a
standard language used for configuration
server environments to you know that
yaml actually stands for yaml a markup
language that's just a little tidbit to
hide behind your ear so let's have a
look or one of these playbooks it looks
like and here we have a sample yaml
script that we've written so you start
off your yammo script with three dashes
and that integrates the start of a
script and then the script itself is
actually consistent of two distinct
plays at the top we have play one and
below that we have play two within each
of those plays we Define which nodes are
we targeting so here we have a web
server in the top play and in the second
play we have a database server that
we're targeting and then within each of
those server environments we have the
specific tasks that we're looking to
execute so let's step through some of
these tasks we have an install patchy
task we have a starter patchy task and
we have it installed my SQL task and
when we do that we're going to execute a
specific set of instructions and those
instructions can include installing
Apache and then setting the state of the
Apache environment or starting the
Apache environment and setting up and
running the MySQL environment so this
really isn't too complicated and that's
the really good thing about working with
yaml is it's really designed to make it
easy for you as an operations lead to be
able to configure the environments that
you want to consistently create so let's
take a step back though we have two
hosts we have web server and database
server why do these names come from well
this takes us into our next stage and
the second part of working with ansible
which is the inventory management part
of ansible so the inventory part of
ansible is where we maintain the
structure of our Network environment so
what we do here is part of the structure
in creating different nodes is we've had
to create two two different nodes here
we have a web server node and a database
server node and under web server node we
actually have the names that were
actually pointed to specific machines
within that environment so now when we
actually write our scripts all we have
to do is refer to either web server or
database server and the different
servers will have the instructions from
the yamascript executed on them this
makes it really easy for you to be able
to just point to new services without
having to write out complex instructions
so let's have a look at how ansible
actually works in real world so the real
world environment is that you'd have the
ansible software installed on a local
machine and then it connects to
different nodes within your network on
the local machine you'll have your first
your playbook which is the set of
instructions for how to set up the
remote nodes and then to identify how
you're going to connect to those nodes
you'll have an inventory we use secure
SSH connections to each of the servers
so we are encrypting the communication
to those servers we're able to grab some
basic facts on each server so we
understand how we can then push out the
Playbook to each server and configure
that server remotely the end goal is to
have an environment that is consistent
so this asks you a simple question what
are the major opportunities that ansible
has over chef and puppet really like to
hear your answers in the comments below
pop them in there and we'll get back to
you and really want to hear how you feel
that ansible is a stronger product or
maybe you think it's a weaker product as
it compares to other similar products in
the market here's the bonus we're going
to talk a little bit about ansible Tower
so ansible Tower is an extra product
that red hat created it that really kind
of puts the cherry on the top of the ice
cream or is the icing on your cake add
ible by itself is a command line tool
however anselor Tower is a framework
that was designed to access ansible and
through the ansible tower framework we
now have an easy to use GUI this really
makes it easy for non-developers to be
able to create the environment that they
want to be able to manage in their
devops plan without having to constantly
work with the command prompt window so
instead of opening up terminal window or
a command window and writing out complex
instructions only in text you can now
use drag and drop and mouse click
actions to be able to create your
appropriate playbooks inventories and
pushes for your nodes alright so we've
talked a lot about ansible let's take a
look at a specific company that's using
ansible today and in this example we're
going to look at HootSuite now HootSuite
if you've not already used their
products and they have a great product
HootSuite is a social media management
system they are able to help with you
managing your pushes of social media
content across all of the popular social
media platforms they're able to provide
the analytics they're able to provide
the tools that marketing and sales teams
can use to be able to assess a sentiment
analysis of the messages that are being
pushed out really great tool and very
popular but part of their popularity
drove a specific problem straight to
HootSuite the chance they had at
HootSuite is that they had to constantly
go back and rebuild their server
environment and they couldn't do this
continuously and be consistent there was
no standard documentation and they had
to rely on your memory to be able to do
this consistently imagine how complex
this could get as you're scaling up with
a popular product that now has tens of
thousands to hundreds of thousands of
users this is where ansible came in and
really helped the folks over at
HootSuite today the devops team at
HootSuite write out playbooks that have
Specific Instructions that Define the
architecture and structure of their
Hardware nodes and environments and are
able to do that as a standard product
instead of it being a problem in scaling
up their environment they now are able
to rebuild and create new servers in a
matter of seconds the bottom line is
ansible has been able to provide
HootSuite with it automation consistent
configuration and free up time from the
operations team so that instead of
managing servers they're able to provide
additional new value to the company
going to be six things that we're going
to stamp through the first is really
looking at what is the world that many
of you already work in today how do you
administer your networks and your
systems today then we're going to look
at how Chef can be the tool that really
helps improve the efficiencies of
creating a consistent operations
environment we're going to look at the
tools and the components and the
architecture that takes to construct
chef and then finally we're going to
take a use case of one of the many
companies that is using Chef today to
improve the efficiency of their
operations environments so let's take a
step back at what life was like before
Chef this may well be the life that
you're experiencing right now so as a
system administrator typically the way
that you work is that you are
responsible for the uptime of all of the
systems within your network and you may
have many many systems that you are
responsible for now if one system goes
down that's not a problem because as a
system administrator you can get
notifications and you can jump right
onto the problem and solve it when
things start getting really difficult
however is when you have multiple
systems and you are not able as a single
person to get to all those systems and
solve them as quickly as possible the
problem that you start having is that
your environments are not in sync if
only this could all be automated well
this is where Chef comes to the rescue
for you so let me take some time and
introduce you to the concepts behind
chef and why it's a tool that you would
probably want to be using in your
environment so what actually is Chef so
Chef is a way in which you can use code
to fix your physical Hardware
environment and so what does that look
like well the way that we write out
scripts in Chef is that you can actually
code your entire environment so you
don't have to actually be managing your
environment on a hardware by Hardware
basis but actually use scripts called
recipes that will actually manage the
environment for you so Chef will
actually ensure that every system is
automatically set to the right states
that meet the requirements you have
defined in your code so you don't have
any issues where Hardware starts to fail
because it references the code to reset
the state within that environment and
that makes you a happy system
administrator so let's go through the
mechanics of how this actually works
with Chef so Chef is ultimately an
automation to tool that converts
infrastructure to code and you'll often
hear the term infrastructure as code or
IAC and this really starts as a output
of the work that Chef has done because
you take the policies that you as an
organization have created and you
convert that into a scripted language
that can then be implemented
automatically this enables Chef to be
able to manage multiple systems very
very easily and considering how broad
and deep our systems become it's very
easy to see how this can help you as a
system administrator the code is then
tested continuously and deployed by Chef
so that you're able to guarantee that
your standards and appropriate state for
your hardware and operating environment
are always maintained so let's step
through the different components that
are available in Chef so the first one
is actually creating a the actual
recipes for chefs so you actually work
on these on a workstation and you'll
write out your code which is referred to
as a recipe and that recipe will be
written using a language called Ruby
good thing for you is that there are
lots of examples of Ruby's recipes that
have been created by the open source
Community a lot of examples knife is the
command tool that you use that
communicates between the recipes and the
server so your recipe is the instruction
a knife is the tool that makes that
instruction work and sets the
appropriate State on the server
environment when you create more than
one recipe you start creating a
collection those collections are
referred to as cookbooks and you can
make take an environment such as a large
Network where you have multiple nodes
potentially hundreds of servers that
have to all have a consistent and equal
State environment and you would put that
cook book and use that cookbook to
ensure with Chef that the state of each
of those nodes is consistent and then
Ojai fetches the current state of your
nodes and then the chef client
configures the nodes as defined within
your cookbook so let's step through the
architecture and working environment of
Chef so for an administrator a systems
administrator you only have to allow
work from your workstation and from that
workstation you can then configure all
the nodes of your environment you can
use different recipes that you create
and those recipes can be compiled into a
cookbook that can be then applied to a
node the thing that's of value is that
as you change the environment As you
move and mature as an organization and
your nodes need to mature consistently
and quickly with your organization all
you have to do is roll out a different
recipe to your nodes and then the nodes
we'll use the tools within Chef to make
the updates so you can create a second
recipe or even a third recipe to be
deployed out to your server environment
knife is going to be the tool that does
the hard work of doing the updates on
the server with the server you have your
collection of recipes in a single
cookbook however understanding the state
of the individual nodes is important to
ensure that the information is broadcast
from the server with the instructions on
how to set up the network within those
nodes in this instance we use Ojai and
Ojai will fetch the state of the nodes
and send that back to the server and get
the information from The cookbook using
the chef client in your Chef node
Network there is the potential at times
that maybe one of your node fails if
there are two failed nodes then a
request is sent back to the server and
information on the latest node structure
that is identified and defined in the
cookbook will be sent to the failed
nodes if that fails to reset the nodes
then the workstation is notified and you
as the administrator will receive a
notification to manually reset those
nodes it should be noted that this is a
very rare occasion within the chef
network setup so let's step through a
use case in this instance we're going to
look at herpolium and how they used Chef
so her poem is a bank and it's the
largest bank in Israel and they have a
mixture of Linux and windows servers
that they have to maintain and as you
can imagine this requires a lot of
constant configuration and work and this
has led to issues in the past so the
challenge that the team Napoleon faced
was cratings and hardening software that
ran the major tasks and was repeatable
on those servers that it didn't matter
who the people were forming the jobs
that there was consistency in the work
that was being done and addressed the
problem that the servers including the
hard run the servers was not consistent
throughout the organization Chef
addresses those specific problems and
became the go-to product for herpolium
they were able to write the recipes and
the cookbook that could be deployed out
to the network effectively and it didn't
matter what the system was the recipes
reflected the standards that the system
administration team put together and
they were able to ensure and test each
script and use standard testing tools
for those scripts it didn't matter what
kind of environment that herpolium had
if certain ports need to be closed or if
a firewall needs to be installed or
modified or if custom strong passwords
had to be created all of this could be
done using the recipes that Chef offers
and once those recipes had been
finalized they could be packaged into a
common cookbook that could then be
deployed to the entire network ensuring
a consistency in results for the company
and also it didn't matter whether the
cookbooks were being deployed to Linux
or Windows machine because the scripts
were being put together in the recipes
and were written in Ruby you could
actually go through and update and
modify those scripts depending on the
environment so if you needed to make a
modification for Linux you could make
those modifications so they were
consistent across all the Linux servers
or all the windows servers and this
really drove in the ability to harden
and secure the environment in the entire
network within a matter of minutes using
Chef versus the days and weeks that it
previously took so if we take a before
and after scenario using a chef before
Chef tasks were repeated multiple times
and it was just hard to keep track of
all the people doing the work and the
reality is because so many people were
touching all the different systems
specific standards the banks had
established was simply not being met
after Chef all of the tasks could be
scripted using Ruby and using the recipe
and cookbook model that Chef has created
the chef tool was able to deploy out to
the entire network specific cookbooks
that provided a consistent experience
and consistent setup and operation
environment for all the hardware the end
result is that the system administration
team could guarantee that the standards
of the bank were being met consistently
so in this session what we can do is
we're going to cover what and why you
would use puppets what are the different
elements and components of puppet and
how does it actually work and then we'll
look into the companies that are
adopting public and what are the
advantages that they have now received
by having puppet within their
organization and finally we'll wrap
things up by reviewing how you can
actually write a manifest in puppet so
let's get started so why puppet so here
is a scenario that as an administrator
you may already be familiar with you as
an administrator have multiple servers
that you have to work with and manage so
what happens when a server goes down
it's not a problem you can jump onto
that server and you can fix it but what
if the scenario changes and you have
multiple servers going down so here is
where public shows its strap with
puppets all you have to do is write a
simple script that can be written with
Ruby and write out and deploy to the
servers your settings for each of those
servers the code gets pushed out now to
the servers that are having problems and
then you can choose to either roll back
to those servers to their previous is
working States or set them to a new
state and do all of this in a matter of
seconds and it doesn't matter how large
your server environment is you can reach
to all of these servers your environment
is secure you're able to deploy your
software and you're able to do this all
through infrastructure as code which is
the advanced devops model for building
out Solutions so let's dig deeper into
what puppet actually is so puppet is a
configuration management tool maybe
similar tools like Chef that you may
already be familiar with it ensures that
all your systems are configured to a
desired and predictable state public can
also be used as a deployment tool for
software automatically you can deploy
your software to all of your systems or
to specific systems and this is all done
with code this means you can test the
environment and you can have a guarantee
that the environment you want is written
and deployed accurately so let's go
through those components of Puppets so
here we have a breakdown of the puppet
environment and on the top we have the
main server environment and then below
that we have the client environment that
would be installed on each of the
servers that would be running within
your network so if we look at the top
part of the screen we have here our
puppet master store which has and
contains our main configuration files
and those are comprised of manifests
that are actual codes for configuring
the clients we have templates that
combine our codes together to render a
final document and you have files that
will be deployed as content that could
be potentially downloaded by the clients
wrapping this all together is a module
of manifest templates and files you
would apply a certificate authority to
sign the actual documents so that the
clients actually know that they're
receiving the appropriate and authorized
modules outside of the master server
where you'd create your manifest
templates and files you would have
public clients as a piece of software
that is used to configure a specific
machine there are two parts to the
client one is the agent that constantly
interacts with the master server to
ensure that the certificates are being
updated appropriately and then you have
the fact that the current state of the
client that is used and communicated
back to through the agent so let's step
through the workings of puppet so the
puppet environment is a master sleeve
architecture the clients themselves are
distributed across your network and they
are constantly communicating back to a
Master server environment where you have
your puppet modules the client agent
sends a certificate with the ID of that
server back to the master and then the
master will then sign that certificate
and send it back to the client and this
authentication allows for a secure and
verifiable communication between claims
and master the factor then collects the
state of the client and sends that to
the master based on the facts sent back
the master then compiles manifests into
the catalogs and those catalogs are sent
back to the clients and an agent on the
client will then initiate the catalog a
report is generated by the client that
describes any changes that have been
made and sends that back to the master
with the goal here of ensuring that the
master has full understanding of the
hardware running software in your
network this process is repeated at
regular infos ensuring all client
systems are up to date so let's have a
look at companies that are using puppet
today there are a number of companies
that have adopted puppet as a way to
manage their infrastructure so companies
that are using puppy today include
Spotify Google ATT so why are these
companies choosing to use puppet as
their main configuration management talk
the answer can be seen if we look at a
specific company Staples so Staples
chose to take and use puppet for their
configuration management tool and use it
within their own private Cloud the
results were dramatic the amount of time
that the it organization was able to
save in deploying and managing their
infrastructure through using puppets
enabled them to open up time to allow
them to experiment with other and new
projects and assignments a real tangible
benefit to a company so let's look at
how you write a manifest in puppets so
so manifests are designed for writing
out in code how you would configure a
specific node in your server environment
the manifests are compiled into catalogs
which are then executed on the client
each of the manifests are written in the
language of Ruby without dot PP
extension now if we step through the
five key steps for writing a manifest
they are one create your manifest and
that is written by the system
administrator two compile your manifest
it's compiled into a catalog three
deploy the catalog is then deployed onto
the clients for execute the catalogs are
run on the client by the agent and then
five and clients are configured to a
specific and desired state if we
actually look into how manifest is
written it's written with a very common
syntax if you've done any work with Ruby
or really configuration of systems in
the past this may look very familiar to
you so we break out the work that we
have here you start off with a package
file or service as your resource type
and then you give it a name and then you
look at the features that need to be set
such as IP address then you're actually
looking to have a command for instance
such as present or start the Manifest
can contain multiple resource types if
we continue to write our manifest and
puppets the default keyword applies a
manifest to all clients so an example
would be to create a file path that
creates a folder called sample in a main
folder called Etc the specified content
is written into a file that is then
posted into that folder and then we're
going to say we want to be able to
trigger an Apache service and then
ensure that that Apache service is
installed on a node so we write the
Manifest and we deploy it to a client
machine on that client machine a new
folder will be created with a file in
that folder and an Apache server will be
installed you can do this to any machine
and you'll have exactly the same results
on those machines so if you're ready to
embrace this exciting career path look
no further than simply learns Caltech
postgraduate program in devops this
comprehensive program will equip you
with the knowledge and skills needed to
master the day of principles tools and
practices dive deep into
containerization orchestration and
automation Frameworks like Docker
kubernetes and genkets gain hands-on
experience with Cloud platforms and
learn how to Leverage The Power of
infrastructure as code don't miss out on
is chance to transform your career and
become an invaluable asset to any
organization click the link in the
description to discover more about this
devops course before you start
understanding any automation tool it's
good to look back into what manual
testing is all about what are its
challenges and how an automation tool
overcomes these challenges challenges
are always overcome by inventing
something new so let's see how selenium
came into existence and how did it
evolve to become one of the most popular
web application automation tool selenium
Suite of tools selenium is not a single
tool it has multiple components and we
will be looking into each of them as you
know every automation tool has its own
advantages and limitations so we will be
understanding the advantages and
limitations of selenium and how do we
work around it selenium jobs and salary
very important isn't it you are here to
learn selenium you definitely want to
know what's the market demand on the
skill what worth you will be once you
are skilled on this automation too so
what we will do we'll just discuss some
stats on the job market related to test
Automation and selenium in particular
all right so let's get started manual
testing and its challenges what is
manual testing manual testing mainly
inverse physical execution of test cases
against various application and to do
what to detect bugs and errors so as a
definition if you can say it is one of
the Primitive methods of testing a
software execution of test cases without
using any automation tools it does not
require the knowledge of any automation
tools and can practically test any
application this could be a simple a
definition can be let's do one thing
let's take an example right before we
start understanding why what are the
challenges of manual testing and why we
have to really look into any automation
tools let's take a simple example say we
have a simple use case as a user I want
to log into my web application using
username and password a very common use
case isn't it and this is a common use
case for any kind of web application
under test now let's say for example I
have a web application here say I have
this community.simplilearn.com for me to
actually enter into this application
what I need to do is first login into
this application so this is a simple
login page which accepts a user ID or an
email ID a password and you click on the
login now as a tester you would write
multiple test cases to test this use
case for example you will have multiple
data test cases like valid credentials
invalid credentials null value and so on
you will also test multiple links on
this page probably the forget password
link the sign up now so whatever links
are made available you're going to test
that then you will say it as this check
box so you will check the remember me
functionality and when you log in back
again you want to validate if all your
previous data are filter you will also
check for error messages on an invalid
login you will do accessibility testing
you will also do a performance on this
page like say for example response time
after you click on login button and so
on so while you are doing a manual
testing literally you can do any kind of
test you want so this is what you do in
the first place and then you start
executing this test cases one by one you
will find bugs your developers are going
to fix them and you will need to rerun
all these test cases again and again
until all your bugs are fixed and your
application is ready to ship now this is
a very simple use case now let's look at
a little complex use case now say you
have an account creation form like this
so I have this online pan application
form right this is still a simpler form
I would say now look at the amount of
data you need to fill here each and
every field you need to test and you
need to test this with hundreds of
transactions because you're going to
have those kind of data sets with you
and then you're going to repeat them
again and again until you are satisfied
that your application is working as per
it is designed so now if you go back to
our presentation now this brings us to
the first demerit of the manual testing
and what it is this is very time
consuming process and hence it's going
to get very boring for any manual tester
also it is more error Pro the risk of
error is pretty high why because it is
done manually and human mistakes are
bound to happen requires presence of
tester all the time since its manual
execution testers presence is required
on the the time one needs to keep doing
manual steps over and over again and
hence he needs to be sitting in front of
his PC manual creation of logs and
repositories creating manual reports
grouping them formatting them so that
they are like a good looking reports
sending those reports manually to all
the stakeholders say collecting logs
from the various machines after you
execute the test cases consolidating all
those results and logs right creating
repositories maintaining them wow this
is a huge long list of mundane tasks one
needs to do and this you need to do a do
again and again until your testing is
completed right so this is going to be a
manual process and it's going to be very
tedious and as I said it was also error
Pro limited scope the scope of manual
testing is very limited for example say
regression testing ideally you would
want to run all the test cases which you
have written but since it's a manual
process you would not have the luxury of
time to execute all of them and hence
what you do you will pick up choose your
test cases and then execute that way you
are kind of limiting the scope of
testing also say working with large
amount of data which is manually pretty
impractical what about performance
testing so you want to say collect
certain metrics on various performance
managers as a part of your performance
testing you want to simulate certain
multiple loads on your application which
is under test hence performing this
manually is definitely not feasible and
to top it all the say you're working in
an agile model where your code is being
churned out by developers say every week
testers are building the test and they
are executing them as and when the bills
are made available to them for testing
now this happens iteratively and hence
you will need to run all the test cases
which you have created multiple times
during your development process and
doing this manually is going to become
definitely very tedious and boring and
is this really effective way of doing it
not at all so what do we do we automate
it so this tells us why we want to
automate one one is for the faster
execution two to be less error prone and
the most important to help you run your
test cases frequently as many times as
you want so there were many tools
available in the market today for
automation so one such tool is selenium
Advent of selenium much before selenium
there were various tools in the market
like rft qtp just a number few popular
ones Selena was introduced by a
gentleman called Jason Huggins way back
in 2004. he was an engineer at
thoughtworks he was working on a web
application which needed frequent
testing and he realized the inefficiency
manual testing this application
repeatedly so what he did was he wrote a
JavaScript program that automatically
controlled the browser actions and he
named it as JavaScript test Runner he
also made this open source which later
was renamed as selenium core and this is
how selenium came into existence since
then selenium has become one of the most
powerful tool to test web application so
what is selenium selenium is the most
popular and widely used automation tool
for automating your web application it
enables us to test web application on
all kind of browsers like IE Chrome
Firefox Safari Edge Opera and even the
Headless browser selenium is an open
source and it's platform independent the
biggest reason why people are preferring
this tool is because it is free of cost
whereas the qtp and rft which I
mentioned earlier are the chargeable
tools selenium is also a set of tools
and libraries to facilitate automation
of web application it is not a single
tool it has multiple components in it
which we will be seeing in detail in
some time and all these tools together
they help us test the web application
you can run selenium scripts on any
platform it is platform independent as
and why it is platform independent
because it is developed in JavaScript
it's very common for manual testers not
to have in-depth programming knowledge
so the selenium has a record and replay
back capability and this tool is called
as selenium ID which can be used to
create a set of actions as a script and
then you just have to replay that script
so mainly this is used for demo purposes
because selenium itself is such a
powerful tool and it offers so rich
functionality you would want to be in a
position to take the advantage of all
its future however if you have testers
who do not have enough of programming
knowledge to start with there is still
this ID which is a record replay tool
which you can use to create your test
scripts it also provides support for
different programming languages like
Java python C sharp Ruby and so on so
you can literally write your test
scripts in any language you like or
which you know of so one need not again
know a complete in-depth or an advanced
knowledge of any of these languages
selenium supports different operating
systems and browsers it supports Windows
Max Linux and say if you want to run
your test on Ubuntu machines yes that is
also possible it's on any platform of
your choice selenium you can run the
test cases selenium Suite of tools so
now let's go little more deeper into the
selenium so as I said it is a suite of
tools and these are the major components
of the selenium selenium IDE selenium
remote control which is in short called
as RC selenium web driver and selenium
grid from selenium 3.0 version onwards
you will not find the remote control
anymore because it has been deprecated
but you still have your ID Webdriver and
grid components now let's take a deeper
look into each of these components and
understand how they function in this
Suite of tools selenium integrated
development environment which is called
as selenium IDE so it is the most
simplest tool in the suite of selenium
it's an integrated development
environment it has a record and Playback
functionality and it's a very very
simple and easy to use too however it is
mainly used as a prototyping and not
used for creating the actual automation
for your real-time projects why because
of its own limitations since it's just a
record and replay to it so this IDE is
available as a Firefox plugin and a
Chrome extension so either you can use
Firefox or the Chrome browser to record
your test script set the commands
created in the scripts where you are
going to record are these commands are
called selenis commands and these are
the commands which interact with your
browser you can also write some of these
commands manually while you are
recording your scripts and we'll see how
to do that when we talk about the ID in
detail and it is again a platform
independent so you can deploy ID on any
platform of your choice so you can have
it on your windows Linux and Mac OS
however the only requirement is you
should have either Chrome or Firefox
browser support on that particular
platform earlier selenium IDE was only a
Firefox extension it died with the
Firefox version 54 or I55 I guess so ID
was not supported from the 55 version
onwards and this was sometime during
2017 however very recently all new brand
selenium IDE has been launched by apply
tools and they have made it cross
browser that is it is a available both
on Chrome and Firefox so you can install
this extension as a Chrome extension or
as an add-on on your Firefox browser
it's a completely revamped version which
was done by apply tools they literally
rewrote the complete code and now they
have made it available on the GitHub
under the Apache 2.0 license and it also
comes with many new features now like
reusability of test cases it has a
better debugging technology now and most
importantly it supports parallel test
execution it has introduced a utility
called selenium side Runner that allows
your test cases to run on any browser so
you can create your automation using
your ID on Chrome or Firefox however
through this side runner from your
command prompt you can execute any of
your test cases on any browser of your
choice and thus by achieving your cross
browser testing you can also run
parallel testing using grid without
writing any specific code to do this
again this can be done using the
selenium site Runner and having the grid
installation done so let's look at how
ID looks like and what are its
components are and how do we work with
ite let's go to the Firefox browser now
and launch the ID and see how the ID
looks so let me go to my Firefox now on
this browser if you can see here there
is an icon here which says selenium ID
and this appears after you go ahead and
install the selenium ID plugin so now
I'll just go click on this and then I
already have a project created so this
is the IDE window which gets open for I
have a pre-recorded script here and what
the script does is I just open amazon.in
website and I am searching for the
product iPhone also I have an assertion
statement here which asserts for the web
page title and then I'm closing the
browser so let's first execute the
script and see how it works and then we
will get into the details of what are
the different components available on
this ID so for me to execute what I will
do is it a ID has a very beautiful
feature here to control the speed of the
execution so what I'll do is I'll reduce
the speed a little bit it so that you
should be able to see the complete
execution step by step now to execute
the script I'll just go here and say run
current test so this opens the Firefox
window and the amazon.in website it
should search for iPhone product now
then it is going to do the title
assertion and close the browser so if
you look here all the statements look in
green now so that means every step has
been executed successfully and there are
no errors so now let's look at the
components of this ID so the first
component which you need to understand
is how do you record the test so there
is a start recording button made
Available to You clicking on which you
can launch any website of the
application under test and you can start
recording your steps and once you have
recorded you can use this play button
which is nothing but your run current
test button to execute your test so if
you want to execute multiple tests then
you have an option to do so here which
is run or test this particular window
what you're seeing here right so this is
the window where you have all your
commands which gets recorded when you
record your script and these are pretty
simple commands and they're very
self-explanatory like say for example
open so it says open and the target is
amazon.in that means it is giving
instruction to your browser to open this
particular website setting the window
size clicking on a button so this is the
search button which we have and then
typing what is the product which you're
searching for and then you have multiple
assert statements made available to you
on this idle so these are all called as
the selenis command and as simple as a
close browser command and now if you
look here in this window this is log
window so every step which gets executed
here there is a log generated showing
you the status of the execution of that
step so if at all there is a failure you
would see a red failed Mark against that
particular step and the test case will
not say completed successfully it will
say the failure occurred okay so it is
very simple and very user friendly kind
of an ID so let's go back to our
presentation so we saw the play button
here and we saw how to record it I also
showed you the commands the command
window what we have so these are the
selenium commands and then we saw the
log which indicated the execution of
each of this command now saving the
script so let me go back to the ID again
so once you have created your test
scripts you have run it successfully you
need to save your project so for you to
save the project when you click on the
save project every ID project it is
saved with an extension dot Sid which is
nothing but selenium ID so you just go
ahead and click on Save since I've
already saved this project I'm going to
just say cancel so that's the x file
extension it uses to save your project
for more details on how to use IDE you
can watch our video on selenium ID on
simplylearn.com selenium RC this is
selenium remote control so this was
developed by Paul Hammond and he
refactored the code and was also
credited with Json as a co-creator of
selenium RC is used to write web
application test in different
programming language the server itself
is written in Java language it supports
multiple programming languages like Java
C sharp Perl Python and PHP it interacts
with the browser with the help of RC
server it uses a simple HTTP get and
post request for communication this RC
server is also a selenium 1.0 version
but it got deprecated in selenium 2.0
and was completely removed in 3.2 and it
got replaced by selenium Webdriver we
will see why this happened when we talk
about the selenium Webdriver this is the
architecture of selenium remote control
so when Jason Huggins introduced
selenium the tool was JavaScript program
and he called it as JavaScript test
Runner and it is also called a selenium
core so every HTML has this JavaScript
statements which are executed by this
web browser there is a JavaScript engine
which helps in executing this command
now this had a major issue of something
called as same origin policy now what is
the same origin policy same origin
policy it prohibits your JavaScript from
accessing elements or interacting with
the scripts from the domain different
from where it is launched and this was
for the security measure for example
every web page and say google.com domain
like say your search email Google Drive
can be accessible but nothing outside
this domain like say if you want to
access Yahoo or an Amazon that was not
possible due to security reasons
obviously so the testers had to install
the selenium core and the web server
which contained your web application
which you are basically testing on the
same system and this was not feasible
and effective all the time now to
overcome this to overcome the same
origin policy what Paul Hammond did was
he created something called as this
remote server which is selenium remote
control server and it acted like a proxy
to trick the browser in believing that
your core which is your selenium core
and your web application under test are
on the same domain this thus by
eliminating your same origin policy and
this was called as your selenium remote
control which was a version one of
selenium now if you look at the simple
architecture of this what are the things
it has so one is your script right first
you need to write your scripts using any
supported language let's say PHP Java
Python and Ruby as I mentioned now
before we start the testing what is the
requirement of the selenium one version
is you need to install the RC server now
this RC server is a separate application
so once you install this this selenium
server is responsible for receiving the
Salinas commands now these are the
selenis commands which you have written
in a scripts it interprets them and it
executes it on your browser and the
results are sent back to your test
scripts and this browser interaction
happens only through this RC server
again as I said using a simple HTTP get
and post request to communicate now upon
receiving these instructions right from
the selenium core the response is sent
back to this RC server from your browser
and back to your your test scripts so
the cycle starts from your test scripts
the selenius commands are interpreted by
your RC server those are sent to the
browser and it's a simple JavaScript
which executed on your browser and the
results after the execution are sent
back to your test scripts through your
RC server this is a simple cycle now
this cycle is repeated for every command
written in your test script until your
complete test case is executed okay so
this is how the RC worked now this RC
had few shortcomings and what are those
one is the architecture of this RC
itself was complicated why because of
this intermediate RC server which we
have between the browser and your test
script and this RC server had to be
installed before running any test
scripts so that means this became an
additional setup and it acts as a
mediator between your selenium commands
and your browser also the execution of
commands takes a very long time it is
pretty slow why because every command it
needs to make a full trip from your test
script to your RC server to the Core add
to the browser execute the script and
then again back in the same line which
makes your overall test execution very
slow and now because of this shortfalls
Webdriver was introduced so selenium
Webdriver selenium webdrivers were
introduced by gentleman called Simon
Steward and it was developed in the year
2006. it is the first cross-platform
testing framework it is a programming
interface to create and run test cases
it makes provision to act on
developments so basically there are
multiple locating techniques which
selenium Webdriver provides us for
finding those elements and to perform
operations on those web elements
Webdriver does not require a core engine
like RC and it interacts natively with
the browser application so there is a
direct interaction with your browser
application and it eliminates the need
of having that intermediate RC server
and that makes the architecture pretty
simple this Webdriver again it supports
multiple languages which we already
talked about and also it has integration
with multiple Frameworks so once you
understand how the web driver Works what
are its functionalities and what are the
drawbacks you will appreciate the fact
that so how does selenium Webdriver
overcomes this even in Webdriver we have
test scripts we also have a web browser
here and then we have a web driver
however what we do not have this
additional server which RC required and
this makes the architecture of the web
driver simpler the browser is controlled
directly from the operating system level
now due to this direct communication the
speed of test execution is much faster
than RC now how does this web driver
work so we have something called as
client libraries or the language
bindings so we know now that selenium
supports multiple languages and you're
free to use any supported language to
create your automation script and where
are these libraries available so if you
go to
seleniumhq.org so let me go to the
browser and show you that so in the
selenium hq.r if you go to the download
words so here you will see something
called as selenium client and Webdriver
language binding and depending on the
language which you want to work with or
which you want to create test scripts
with you can download this client
libraries so if you're working with Java
download the client library for Java so
if you're working with python you have a
corresponding link which you through
which you can download your client
libraries and then selenium provides a
lot of apis for us to interact with the
browser and these are called as a rest
apis so then we have the browser drivers
through which we interact with the
browser now what are these browser
drivers so there are drivers specific to
each browser so let me show you that on
our
seleniumhq.org so here if you just
scroll down you have something called as
third party drivers bindings and plugins
for every supported browser of selenium
you have a corresponding driver file
available now say for example if we want
to work with Firefox Mozilla gecko
driver is what you need to download and
these driver files so if you go and
click here you'll get an executable
driver file for any of the platforms
which you are working so if you're
working on Windows Mac OS or Linux you
can just download the corresponding
driver files here so these drivers see
these driver files are the ones which
interact with the actual browser and how
does this happen this driver establishes
the connection with the real browser
without revealing the internal logic of
the browser functionality now what does
that mean say we want to execute a test
script and how does this test script
gets executed first is it it sends an
HTTP request which is generated and sent
to this browser driver which we just saw
for each of the command in your scripts
and then this driver receives the
request through your HTTP server and
this HP HTTP server does all the
execution directly on the browser and
then the execution status the complete
status is sent back to your test script
so let me show you a simple use case how
it works with the web driver so for that
let's go to my Eclipse here so I already
have a test script written here in
Webdriver and what this basically does
is it launches amazon.com amazon.in
website it uses the Firefox browser and
then I'm also fetching the page title
and printing it on the console and then
I'm closing the browser this is a simple
functionality which I'm doing here so if
you look here so the eclipse is the ID
which I'm using I've already installed
selenium here and I've written a simple
Java class and this Java class we have
multiple methods one is to launch
browser one is to fetch the title of the
page and one is to close the browser now
while launching the browser I set every
browser has its own driver files so
somewhere in this Java class I need to
tell selenium as which browser to use
and how is that done that is done by
this first two statements here where I
am doing a system.set property and where
I'm indicating that this is my driver
file which is gecko driver to be used
and using this I want the selenium to
launch the Firefox browser and after
after this all the operations any
operations which you want to perform on
your web application under test you can
do it here now let's go ahead and
execute this and see how it works so
simply what I need to do is I'll just
say execute this as a Java program so
the Firefox browser is getting launched
so you can see amazon.in being launched
here and since we are not doing any
operations here what I am simply doing
is printing the title of the page and
closing the browser that is it so if you
see here you can see the page title is
online shopping site so and so and this
is what I'm printing through this method
which is fetch page title a very simple
way of writing a automation script using
Webdriver so now you saw how we write a
small test script using the Webdriver
function now say if I want to run the
script on multiple browsers and on
multiple operating system and I do not
have just one test case I say say I've
got hundreds of test cases now with this
approach what you can achieved your best
is you can sequin Chile execute on every
browser available and all your 100 or
200 test cases whatever you have now
this is going to take quite a long time
to execute and hence to reduce the time
taken for the execution selenium has
something called as grid selenium grid
the main objective of the grid is to
minimize your test execution time this
grid is designed to distribute commands
to different machines simultaneously in
real time environment you always have
the need to run test cases in a
distributed environment for example a
web application for which say you have
automated thousands of test cases and
you want to run them as a regression
Suite across multiple browsers and Os
what you need to do is you need to run
them parallely and not really
sequentially so grid allows this
parallel execution of tests on different
browsers and operating system you also
want an integration with reporting tools
so that you can get a Consolidated
report after the tests have been
executed on multiple machines so grid
can be integrated with such many other
tools how does the grid work so grid has
this Hub and node concept which helps in
achieving the parallel execution let's
take an example say again your
application supports all browsers and it
also supports different OS like the one
which you're seeing here like Windows
Mac and Linux and your requirement is to
run tests on all the supported browsers
and operating systems like this one so
what you do here is you configure a
Master machine that is this which is
also called as Hub by running something
called a selenium Standalone server and
this selenium Standalone server can be
downloaded downloaded from your selenium
HQ website so let me show you that let's
go to my browser and in the downloads
here if you see selenium Standalone
server version 3.1.4.5 and this is what
you need to download so using the server
you create a hub configuration like this
once you do this you also need to create
nodes specific to your requirement like
a Windows 10 like a Mac machine with
Firefox with the Windows 8 with a chrome
and Mac say with Safari browser and the
node configuration is also done using
the same server which is the selenium
Standalone server so you create a node
configuration on this node machines so
once you have the setup created through
your master that is your Hub you can
control on which nodes you want to
execute which test say for example here
say you want to run some test on your
Mac machine with Safari browser role so
you have a kind of smoked a switch you
need to run now through the master you
can control this complete execution so
what Master does it it selects this
particular node depending on the
configuration which you have set on the
Master machine also you could have
multiple possible combinations like say
I want to just run smoke tests on all my
nodes I want to run specific component
tests on say only Windows 10 with an
edge browser or I can say I want to just
run my complete regression Suite but
only on Mac machine and so on and all
this is controlled through your master
so what happens on the Node once it gets
the command from the master the node
actually runs the specified test on the
browser so you can check out our
detailed video on selenium grid for you
to understand how the grid works and how
do we do this configuration and execute
the test to summarize the selenium
versions selenium version 1 comprised of
an ID RC and grid as we discussed
earlier RC was on its path of
deprecation and Webdriver was taking
birth so selenium 2 had an earlier
version of Webdriver and also RC existed
from 3.0 onwards RC was completely
remote and web driver took its place
also there's a four dot version around
the corner it has more features and
enhancements so some of the features
which is expected are like w3c Webdriver
standardization they have an improved
idea and also I believe there are some
improvements done to the grid advantages
of selenium speed and accuracy since
there is no manual execution your test
can run faster and with grid it helps in
parallel execution so you can actually
execute large volume of test cases
within a short frame for example let's
say before your release of product
during your last lap of testing you find
a bug and and it has to be fixed so the
developer fixes the bug and what you
want to do is you want to run all the
test cases pertaining to that area can
you imagine the amount of time which
will be required if you have to do this
manually but by automation you can
achieve this in a short duration of time
and you can also enable releasing your
product in time and with less or no
human error the second Advantage is it's
an open source so it's available free of
cost and anyone can download and start
using it supports a wide spectrum of
programming languages it is not
restricted to any particular programming
language whatever language you are
comfortable with you have an option to
use that and to write your selenium
scripts selenium has support for almost
all browsers and operating system
including a headless browser so this
helps you to create test cases once and
run them across all browsers and
operating system and thus saving your
time for manual execution it also helps
in achieving a broad test coverage it's
a pretty easy to use tool you can check
out all our videos and you can just Just
Master the skill of writing or admission
scripts reusability and add-ons of
course like any programming languages it
provides you a mechanism to reuse your
code and avoid redundancy limitations so
like any other tool selenium also has
certain limitations so let's look at
what they are and see if there are any
workarounds also since selenium is an
open source you do not have much
technical support available however
there are loads of documentation
materials forums available which you can
refer to this tool is for automating web
application it cannot handle mobile
application or desktop application
automation however what selenium
provides is the support for integrating
tools like APM tool for mobile testing
selenium is not very good with image
testing since especially it is designed
for web application automation but then
we again have many other tools which can
be integrated with selenium like Auto it
and securely selenium again has a
limited reporting capability it can help
create basic reports but we definitely
definitely need more so it provides
supports for tools like test NG report
NG extent reports which you can
integrate and generate beautiful reports
powerful isn't it of course it does not
have a test management capability a tool
need not have all these capabilities but
again you can integrate with the test
management tool used in your
organization since selenium supports
multiple programming language the
developer of the automation script need
to have some basic knowledge of any
programming language to write effective
automation scripts of course a deep
Advanced knowledge is not required a
basic knowledge is more than sufficient
so if you look at all the selenium still
provides a complete solution for your
automation need and that's the beauty of
selenium and that is what makes it the
most popular today most popular tool
today for automation selenium jobs and
salary a very interesting topic so the
market trends for test automation are
continuously changing and selenium has
seen an exponential growth in a very
short time period this selenium makes it
convenient for clients to receive an
hour UI much faster and how does that
happen by reducing the number of test
Cycles involved in testing their product
in prison scenario selenium is a perfect
automation for any of your web web
application testing an automated web
application testing and it is the first
choice of your automation tester as well
as the companies also so as the
softwares are becoming more and more
complex so is the test Automation and
selenium is a key skill set for any test
automation engineer to have but of
course it's not the only skill set one
needs to have so if you look at a
typical job description here so go on
other days when the engineers were hired
as a manual tester every company today
wants to hire Engineers who knows how to
do Automation and since selenium is the
most popular web application automation
tool companies look specifically for
selenium automation engineers and this
is the typical job description of any
automation engineer to develop test
cases to detect bugs and errors
automation framework designing it's not
just about automating your test cases
one should have the ability to design
the company the framework related to the
project to improve and automate test
practices to participate in
communicating best practices defining
test strategy test manuals for tracking
and fixing your software issues let's
look at the compensation part of it so
compensation varies from company to
company and it also depends on your
experience level so according to the
stats if you look at the U.S market it
can vary anywhere from twenty one
thousand dollars to one fifty thousand
dollars average could be of course
somewhere around ninety thousand dollars
and if you look at the Indian market
range it ranges anywhere from 2 to 10
lakhs per annum for a startup with an
average of 5x so the more and more
you're going to get experience the
better is going to be a compensation
package and the market has always been
very lucrative for automation Engineers
so since selenium is so powerful web
automation tools there are some of the
top-end companies who use selenium in
their various project they want to
provide their customers the best quality
software and help them reduce the time
to Market and hence they go in for
selling so a lot of these companies have
selenium deployments here we wrap up
with devops Basics if you like this
video hit the like button and consider
subscribing to our Channel and hit the
Bell icon to never miss an update from
Simply learn till then keep watching and
keep learning
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
foreign