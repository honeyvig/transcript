welcome to the devops complete course
this course is your ticket to the
Fantastic career in technology we'll
explain why devops is a great choice for
future and break down what a DAV Ops
engineer does in simple terms if you're
curious about the potential earnings we
will cover that to we will provide a
road map like a step-by-step guide so
you know where to start and how to
progress you will learn about important
tools like git which helps to manage
code Maven for building projects
selenium for testing and genkins for
automation these tools are the building
blocks of devops and we will make sure
you understand them clearly so in this
video we are here to help you gain the
skills and knowledge needed for a
successful devops career it is an
exciting Journey that is accessible to
anyone interested in technology so get
ready to explore the world of devops
with us and you will be prepared to
thrive in this Dynamic and rewarding
field if you aspire to become a devops
engineer simply learns professional
certificate program in cloud computing
and devops is your ideal Launchpad in
partnership with E and ICT Academy I
guti this program equips with the skills
needed to excel in cloud computing and
devops learn to deploy robust Microsoft
aure and AWS applications through a
blend of live virtual class Hands-On
Labs self-based videos and collaborative
peer interactions don't miss this
opportunity to master the art of cloud
and devops enroll today and pave your
way to becoming a cloud
expert meet Tim Tim builds a robot in
his lab a climate controlled and
pollution free
environment once he's done he drops the
robot off at his project partner Mia's
house Mia takes it out to her backyard
to ensure that the robot meets the
requirements but here is where the
problem arises the change in the
environment causes the robot to
malfunction Mia is now really annoyed
and she has a lot to correct and it
seems to her as though Tim didn't really
do much this wall between them leaves
the poor robot to bite the dust well
what if we broke this wall Tim and M
work together in a common space Tim
develops each block of functionality of
the robot which is then immediately
checked by Mia both are now working
simultaneously instead of waiting on the
other to finish their task as in when a
feature is ready for use they are put
together to build the final product they
develop a common mindset and share ideas
to further speed up the process they use
several tools which can automate every
stage this means that the robot is now
ready sooner with less iterations and
manual work from an organization
perspective Tim would be the dev
developer while Mia the operations their
Union is the core of the devops approach
devops has several stages and set of
tools to automate each of these stages
let's have a look at these Tim first
puts down a plan in terms of a software
this could mean deciding on the modules
and the algorithms to use once he has
the plan he now codes the plan with
tools such as git Tim has a repository
for storing all the codes and their
different versions this is called
Version Control next this code is
fetched and made executable this is the
build stage tools such as gradal and
Maven will sort this out now before
deployment the product is tested to
catch any bugs the most popular tool
automating testing is selenium once the
products are tested Mia must deploy it
the deployed product is then
continuously configured to the desired
State answerable puppet and Docker are
some of the most common tools used that
automate these stages now every product
is continuously monitored in its working
environment nagios is one such tool that
automates this phace and the feedback is
fed back to the planning stage and
finally we have the core of the devops
life cycle the integration stage tool
such as Jenkins is responsible for
sending the code for build and test if
the code pass passes the tests it's
further sent for deployment this is
called continuous integration let's now
have a look at an organization that has
adopted the devops approach which of the
below sequence of steps would they
follow to develop a software leave your
answers in the comment section keep an
eye out for the right answer on the
comment section or our YouTube
Community Giants such as Amazon Netflix
Target Etsy and Walmart have all adopted
devops and seen a considerable increase
in delivery and quality in 2014 an R of
downtime for Netflix would cost it
$200,000 it became absolutely crucial
that Netflix prepared themselves for any
sort of failure and so they took to the
devops approach and implemented it in
the most unique way they developed a
tool called the Simeon Army this tool
created failures and automatically
deployed them in an environment that did
not affect the users the team would
troublesho these failures and this gave
them enough experience to deal with any
degree of collapse with everything being
automated and happening simultaneously
organizations can now deliver at a much
faster pace so considering the benefits
of devops and its Divergence from the
traditional methods would devops be the
future let us know what you think in the
com comment section below through a
number of key elements today the first
two will be reviewing models that you're
already probably using for delivering
Solutions into your company and the most
popular one is waterfall followed by
agile then we'll look at devops and how
Dev Ops differs from the two models and
how it also borrows and leverages the
best of those models we'll go through
each of the phases that are used in
typical Dev Ops delivery and then the
tools used within those phases to really
improve the efficiencies within devops
finally we'll summarize the advantages
that devops brings to you and your teams
so let's go through waterfall so
waterfall is a traditional delivery
model that's been used for many decades
for delivering Solutions not just IT
solutions and digital Solutions but even
way before that it has its history goes
back to World War II so waterfall is a
model that is used to capture
requirements and then Cascade each key
deliverable through a series of
different stage Gates that is used for
building out the solution so let's take
you through each of those stage Gates
the first that you may have done is
requirements analysis and this is where
you sit down with the actual client and
you understand specifically what they
actually do and what they're looking for
in the software that you're going to
build and then from that requirements
analysis you'll build build out a
project plan so you have an
understanding of what the level of work
is needed to be able to be successful in
delivering the solution after that you
got your plan then you start doing the
development and that means that the
programmers start coding out their
solution they build out their
applications to build out the websites
and this can take weeks or even months
to actually do all the work when you've
done your coding and development then
you send it to another group that does
testing and they'll do full regression
testing of your application against the
systems and databases that integrate
with your application you'll test it
against the actual code you'll do manual
testing you do UI testing and then after
you've delivered the solution you go
into maintenance mode which is just kind
of making sure that the application
keeps working there's any security risks
that you address those security risks
now the problem you have though is that
there are some challenges however that
you have with the waterfall model the
cascading deliveries and those complete
and separated stage Gates means that
it's very difficult for any new
requirements from the client to be
integrated into the project so if a
client comes back and it's the project
has been running for 6 months and
they've gone hey we need to change
something that means that we have to
almost restart the whole project it's
very expensive and it's very time
consuming also if you spend weeks and
months away from your client and you
deliver a solution that they are only
just getting to see after you spend a
lot of time working on it they could be
pointing out things that are in the
actual final application that they don't
want or are not implemented correctly or
lead to just general unhappiness the
challenge you then have is if you want
to add back in the client's feedback to
restart the whole waterfall cycle again
so the client will come back to you with
a list of changes and then you go back
and you have to start your programming
and you have to then start your testing
process again and just you're really
adding in lots of additional time into
the project so using the waterall model
companies have soon come to realize that
you know the clients just aren't able to
get their feedback in quickly
effectively it's very expensive to make
changes once the teams have started
working and the requirement in today's
digital world is that Solutions simply
must be delivered faster and this has
led for a specific change in agile and
we start implementing the agile model so
the agile model allows programmers to
create prototypes and get those
prototypes to the client with the
requirements faster and the client is
able to then send the requirements back
to the programmer with feedback this
allows us to create what we call a
feedback loop where we're able to get
information to the client and the client
can get back to the development team
much faster typically when we're
actually going through this process
we're looking at the engagement cycle
being about 2 weeks and so it's much
faster than the traditional waterfall
approach and so we can look at each
feedback loop as comprising of four key
elements we have the planning where we
actually sit down with the client and
understand what they're looking for we
then have coding and testing that is
building out the code and the solution
that is needed for the client and then
we review with the client the changes
that have happened but we do all this in
a much tighter cycle that we call a
Sprint and that typically a Sprint will
last for about 2 weeks some companies
run sprints every week some run every
four weeks it's up to you as a team to
decide how long you want to actually run
a Sprint but typically it's two weeks
and so every two weeks the client is
able to provide feedback into that Loop
and so you were able to move quickly
through iterations and so if we get to
the end of Sprint 2 and the client says
hey you know what we need to make a
change you can make those changes
quickly and effectively for spring three
what we have here is a breakdown of the
ceremonies and the approach that you
bring to Agile so typically what will
happen is that a product leader will
build out a backlog of products and what
we call product backlog and this will be
just a whole bunch of different features
and they may be small features or bug
fixes all the way up to large features
that may actually span over multiple
Sprints but when you go through the
Sprint planning you want to actually
break break out the work that you're
doing so the team has a mixture of small
medium and large solutions that they can
actually Implement successfully into
their Sprint plan and then once you
actually start running your Sprint again
it's a two-e activity you meet every
single day to with the actual Sprint
team to ensure that everybody is staying
on track and if there's any blockers
that those blockers are being addressed
effectively and immediately the goal at
the end of the two weeks is to have a
deliverable product that you can put in
front of the C customer and the customer
can then do a review the key advantages
you have of running a Sprint with agile
is that the client requirements are
better understood because the client is
really integrated into the scrum team
they're there all the time and the
product is delivered much faster than
with a traditional waterfall model
you're delivering features at the end of
each Sprint versus waiting weeks months
or in some cases years for a waterful
project to be completed Ed however there
are also some distinct disadvantages the
product itself really doesn't get tested
in a production environment it's only
being tested on the developer computers
and it's really hard when you're
actually running agile for the Sprint
team to actually build out a solution
easily and effectively on their
computers to mimic the production
environment and the developers and the
operations team are running in separate
silos so you have your development team
running their Sprint and actually
working to build out the features but
then when they're done at the end of
their Sprint and they want to do a
release they kind of fling it over the
wall at the operations team and then
it's the operations team job to actually
install the software and make sure that
the environment is running in a stable
fashion that is really difficult to do
when you have the two teams really not
working together so here we have is a
breakdown of that process with the
developers submitting their work to the
operations team for deployment and then
the operations team may submit their
work to the production servers but what
if there is an error what if there was a
setup configuration error with the
developer test environment that doesn't
match the production environment there
may be a dependency that isn't there
there may be a link to an API that
doesn't exist in production and so you
have these challenges that the
operations team are constantly faced
with and their challenge is that they
don't know how the code works so this is
where devops really comes in and let's
dig into how devops which is developers
and operators working together is the
key for successful continuous delivery
so devops is is an evolution of the
agile model the agile model really is
great for Gathering requirements and for
developing and testing out your
Solutions and what we want to be able to
do is kind of address that challenge and
that gap between the Ops Team and the
dev team and so with Dev Ops what we're
doing is bringing together the
operations team and the development team
into a single team and they are able to
then work more seamlessly together
because they are integrated to be able
to build out solutions that are being
tested in a production like environment
so that when we actually deploy we know
that the code itself will work the
operations team is then able to focus on
what they're really good at which is
analyzing the production environment and
being able to provide feedback to the
developers on what is being successful
so we're able to make adjustments in our
code that is based on data so let's step
through the different phases of a devops
team so typically you'll see that the
devops team will actually have eight
phases now this is somewhat similar to
Agile and what I'd like to point out at
time is that again agile and devops are
very closely related that agile and
devops are closely related delivery
models that you can use with devops it's
really just extending that model with
the key phases that we have here so
let's step through each of these key
phases so the first phase is planning
and this is where we actually sit down
with a business team and we go through
and understand what their goals are the
second stage is as you can imagine and
this is where it's all very similar to
Agile is that the coders actually start
coding but they typically they'll start
using tools such as git which is a
distributed Version Control Control
software it makes it easier for
developers to all be working on the same
code base rather than bits of the code
that is rather than them working on bits
of the code that they are responsible
for so the goal with using tools such as
git is that each developer always has
the current and latest version of the
code you then use tools such as mavin
and gradal as a way to consistently
build out your environment and then we
also use tools to actually automate our
testing now what's interesting is when
we use tools like selenium and junit is
that we're moving into a world where our
testing is scripted the same as our
build environment and the same as using
our G environment we can start scripting
out these environments and so we
actually have scripted production
environments that we're moving towards
Jenkins is the integration phase that we
use for our tools and another Point here
is that the tools that we're listing
here these are all open-source tools
these are tools that any team can start
using we want to have tools that control
and manage the deployment of code into
the production environments and then
finally tools such as anible and Chef
will actually operate and manage those
production environments so that when
code comes to them that that code is
compliant with the production
environment so that when the code is
then deployed to the many different
production servers that the expected
results of those servers which which as
you want them to continue running is
received and then finally you monitor
the entire environment so you can
actually zero in on spikes and issues
that are relevant to either the code or
changing consumer habits on the site so
let's step through some of those tools
that we have in the devops environment
so here we have is a breakdown of the
devops tools that we have and again one
of the things I want to point out is
that these tools are open-source tools
tools there are also many other tools
this is just really a selection of some
of the more popular tools that are being
used but it's quite likely that you
already using some of these tools today
you may already be using Jenkins you may
already be using git but some of the
other tools really help you create a
fully scriptable environment so that you
can actually start scripting out your
entire devops tool set this really helps
when it comes to speeding up your
delivery because the more you can
actually script out the work that you're
doing the more effective you can be at
running automation against those scripts
and the more effective you can be at
having a consistent experience so let's
step through this devops process so we
go through and we have our continuous
delivery which is our plan code build
and test environment so what happens if
you want to make a release well the
first thing you want to do is send out
your files to the build environment and
you want to be able to test the code
that you've been created because we're
scripting everything in our code from
the actual unit testing being done to
the all the way through to the
production environment because we're
testing all of that we can very quickly
identify whether or not there are any
defects within the code if there are
defects we can send that code right back
to the developer with a message saying
what the defect is and the developer can
then fix that with information that is
real on the either the code or the
production environment if however your
code passes the the scripting test it
can then be deployed and once it's out
to deployment you can then start
monitoring that environment what this
provides you is the opportunity to speed
up your delivery so you go from the
waterfall model which is weeks months or
even years between releases to Agile
which is 2 weeks or 4 weeks depending on
your Sprint Cadence to where you are
today with devops where you can actually
be doing multiple releases every single
day so there are some significant
advantages and there are companies out
there that are really zeroing in on
those advantages if we take any one of
these companies such as Google Google
any given day will actually process 50
to 100 new releases on their website
through their Dev Ops teams in fact they
have some great videos on YouTube that
you can find out on how their Dev op
teams work Netflix is also a similar
environment now what's interesting with
Netflix is that Netflix have really
fully embraced Dev Ops within their
development team and so they have a
devops team and Netflix is a completely
digital company so they have software on
phones on Smart TVs on computers and on
websites interestingly though the devops
team for Netflix is only 70 people and
when you consider that a third of all
internet traffic on any given day is
from Netflix it's really a reflection on
how effective devops can be when you can
actually manage that entire business
with just 70 people so there are some
key advantages that devops has it's the
actual time to create and deliver
software is dramatically reduced
particularly compared to Waterfall
complexity of maintenance is also
reduced because your automating and
scripting out your entire environment uh
you're improving the communication
between all your teams so teams don't
feel like they're in separate silos but
that are actually working cohesively
together and that there is continuous
integration and continuous delivery so
that your consumer your customer is
constantly being
delighted the field of devops has grown
exponentially over the past few years
and it's no surprise why devops is a set
of practices and tools that aims to
break down the SOS between development
and operations team and streamline the
software delivery process by doing so
devops enables organizations to deliver
high quality software faster and more
efficiently than ever before so if
you're interested in pursuing a career
in devops then this video on how to
choose devops as a career is for you
with the proper road map on how to get
started with it also do not forget to
subscribe to our YouTube channel and hit
the Bell icon to to never miss an update
from Simply learn so without any further
Ado let's get started first is to
understand the role of a devops
professional before diving into the
specifics of how to become a devops
professional it's essential to
understand the roles and
responsibilities of a devops
professional a devops professional is
typically responsible for Designing
implementing and maintaining the
infrastructure automation tools and
processes required for the efficient
delivery of software this includes
collaborating with development teams to
integrate automation into the software
development process setting up and
maintaining the infrastructure required
for the software delivery pipeline such
as servers databases and network systems
developing and implementing automation
scripts to improve the efficiency of
software delivery process continuously
monitoring and analyzing the software
delivery process to identify and resolve
bottlenecks and improve
efficiency ensuring that the software
delivery process aderes to Industry best
practice practices and regulatory
requirements so now that you understand
the roles and responsibilities of a
devops professional let's take a look at
the road map to become a devops
professional so first is to learn the
basics of software development to Be an
Effective devops professional it's
essential to have a good understanding
of software development processes
including programming languages software
development methodologies and Version
Control tools you can start by learning
programming languages like python Java
or Ruby well a devops engineer course
will prepare you for a career in devops
Technologies through this devops
engineer course you will learn to review
deployment methodologies cicd pipelines
observability and use devop tools like
gate Docker and genkins with this devops
engineer certification you can check out
the link for this course in the
description following that learn about
infrastructure and operations as a
devops professional you will be
responsible for managing infrastructure
including including servers databases
and network system so it's essential to
have a good understanding of
infrastructure and operations this
include learning about servers operating
systems Network protocols and storage
systems then explore about automation
tools and cloud services automation is a
critical part of devops learning
automation tools such as anible puppet
Ori can help you streamline the software
delivery process and improve efficiency
most organizations today use cloud
services to deliver software learning
cloud services such as AWS Azure or gcp
can help you design and Implement Cloud
architecture that are efficient and
scalable well to help you with learning
devops and cloud services simply learn
has to offer as your devop solution
expert master's program to help you
become an industry ready professional in
this course you will learn to plan
smarter collaborate better and ship
faster with a set of modern development
services the link for this course is
mentioned in the description do check
them out once done with Skilling you
need to gain experience and get
certified as discussed before the best
way to gain experience in devops is to
work on real world projects this can be
achieved through internships
volunteering for open-source projects or
even taking on small projects on your
own this will allow you to put your
theoretical knowledge into practice and
gain hands-on experience along with it
there are various certification programs
available such as the devops Institute
certification program AWS certification
devops engineer and Microsoft Asha devop
Solutions certification once you have
gained experience and acquired the
necessary skills it's time to start
looking for job opportunities there are
various job titles in devops such as
devops engineer site reliability
engineer automation engineer and release
engineer in top hiring companies in the
field include Amazon Google Microsoft
IBM and many others well a postgraduate
program in devops in collaboration with
calch ctm is a professional development
option that will Square your skills with
industry standards in this course you
will learn how to formalize and document
development processes and create a
self-documenting system devop
certification course will also cover
Advanced tools like puppet solt stack
and anible that will help
self-governance and automated management
at scale
link is mentioned in the description do
check it out if you're into the tech
industry or just curious about the role
of devops and software development you
have come to the right place so what
exactly is devops in simple terms it's a
set of practices and tools that help
developers and operational team work
better together releasing software
faster with higher quality at its score
devops is about breaking down barriers
between development and operations and
creating a culture of collaboration that
focuses on delivering value to customers
as quickly as and efficiently as
possible of course this is a vast
oversimplification and there are many
different aspects to devops that we
could spend hard diving into but for now
let's focus on some of the key
responsibilities of a doops engineer who
is the person responsible for
implementing and overseeing devops
practices and processes but before we
begin if you're new to the channel and
haven't subscribed already consider
getting subscribed to Simply learn to
stay updated with all the latest
Technologies and hit that Bell icon to
never miss an update from us having said
that the demand for devops professionals
has overgrown in recent years as more
and more companies adapt devops
practices to improve their software
development and delivery processes so
are you ready to advance your
professional career to the next level
taking our step-by-step simply learns
postgraduate program in devops in
collaboration with IBM will help you
start your devops journey that will
prepare you for the devops engineer role
in order to match your skills with
market demand this devops training
program is designed in collaboration
with Caltech ctme our Cutting Edge
Blended learning combines live online
devop certification classes with
interactive labs to give you practical
experience this postgraduate program
covers topics including git GitHub
Docker cicd practices using genkins
kubernets and much more so what else can
you expect from this program well this
devops training program will cover
skills like devops methodology
continuous integration devops on cloud
deployment Automation and you'll also
get hands-on experience with the latest
tools and techniques including terraform
Maven and emble junkins Docker junit and
many more this program will cover
industry projects like dockerizing
junkins pipeline deploy angular
application and Docker container
branching development model and many
more exciting projects so if you're
looking to pursue your career as a
devops engineer and acquire skills that
will prepare you for your job consider
enrolling in this intensive training
program we will leave the link in the
description box below make sure to check
that out so without any further Ado
let's get started with today's topic
firstly let us understand what is devops
now devops is a software development
approach that emphasizes collaboration
Automation and communication between
development and operations team it aims
to streamline the entire software
development life cycle by integrating
and optimizing processes tools and
methodologies it encourages a culture of
shared responsibility where developers
and operations team work together
closely throughout the entire software
development life cycle from planning and
coding to testing deployment and
monitoring now the question is who is a
devops engineer well you got it right a
devops engineer is a professional who
combines software development expertise
with operations knowledge to facilitate
collaboration steamline processes and
improve software delivery and
infrastructure management within an
organization a devops engineer role is
to bridge the gap between development
and operations team enabling efficient
and reliable software development and
deployment
practices but the question is how to
become a devops engineer what are are
the skills that you need to possess to
become a good devops engineer well a
devops engineer possess a wide range of
skills including Proficiency in
scripting and programming languages
knowledge of various tools and
Technologies expertise in system
administration Cloud platforms and
conization Technologies as well as
strong problem solving and communication
skills are necessary firstly having a
good coding knowledge well tools like
Confluence jira git these tools can
support and enhance collaboration and
project management Within a devops
environment next having a good knowledge
on deployment tools are also necessary
now tools like dcos provides alization
capabilities for distributed
applications Docker enables
containerization for consistent and
scalable deployments and AWS offers a
broad range of cloud services for
infrastructure provisioning scalability
and manage services next you need to
have a good knowledge on operations
tools as well now chef and anemal focus
on infrastructure Automation and
configuration management while kubernets
specializes in container oration and
management these tools are utilized in
devops to automate various aspects of
software development life cycles
including infrastructure provisioning
configuration management application
deployment and scaling moving ahead you
need to have a strong grip on monitoring
tools nagios Splunk and data dog are
three commonly used tools in the field
of monitoring and observability now each
tool serves a specific purpose in
monitoring and managing system and
applications nagio specializes in
infrastructure and application
monitoring spun focuses on log analysis
and data visualization data dog provides
comprehensive monitoring and analytic
capabilities in Cloud environments these
tools play a crucial role in maintaining
the health and performance of systems
and applications moving ahead you need
to have a good knowledge on genkins and
cod chip now genkin and cod chips are
both essential Tools in Deo practices
genkins is a flexible and extensible
automation server that supports
continuous integration testing and
deployment on the other hand cor ship is
is a cloud-based cicd platform that
offers Simplicity and ease of use
particularly for cloud native
applications both these tools contribute
to improving development productivity
and code quality and finally having a
good knowledge on testing tools like
selenium junit are necessary for a doops
engineer junit is primarily focused on
unit testing and automated testing of
java code while selenium is geared
towards functional testing and
automation of web applications both
these tools play critical roles in
devops workflow contributing to faster
feedback Cycles improved code quality
and Reliable Software releases so these
are some of the main and important
skills that you need to possess as a
devops engineer before moving ahead
let's take a minute and listen to the
experiences of our Learners who have
enrolled in the devops pgp program which
has proven to be highly beneficial for
many aspiring engineers and
professionals leading them to achieve
New Heights in the field of devops I
started my I Journey with Accenture 3
years ago I joined their as a cloud
architect there I worked with AWS and
Azure Technologies looking for higher
paying jobs and devops seemed the right
career choice so I decided to go with
the postgraduate programing devops in
collaboration with Caltech ctme the
course was divided into modules and we
had
assignments I was really impressed by
how many job interviews I landed after I
added the certification to my portfolio
and now I am earning 40% more than my
previous job it didn't only boost my
career but also my confidence well now
comes the main part what exactly are the
day-to-day roles and responsibilities of
a devops engineer now a devops engineer
play a crucial role in Bridging the Gap
between development and operations team
as we discussed earlier so here are some
of the top five roles and
responsibilities of a doops engineer in
detail first on the list we have
collaboration and communication now deop
engineer facilate effective
communication and collaboration with
development and operations team they
actively participate in meetings and
discussions to align goals and
expectations now as a devops engineer
you need to engage in regular meetings
and discussions regular engagement
ensures that they are up to dat with
ongoing projects challenges and goals
enabling them to better align their
efforts and contribute effectively
regular engagement ensures that they are
up to date with ongoing projects
challenges and goals enabling them to
better align their efforts and cont
contribute effectively actively listen
and understand the requirement concerns
and feedback now when engaging with
development and operations team de offs
Engineers practice active listening they
pay close attention to the requirements
concerns and feedback expressed by team
members from both the sides by
understanding the perspectives pain
points and suggestions deop Engineers
can better assess the needs of their
teams and collaborate to find suitable
Solutions facilitate effective
communication channels now deop
Engineers take the initiative to
establish and maintain effective
communic iation within the organization
this often involves settling up
dedicated chat platforms like slack
Microsoft teams or collaboration tools
like jira to Foster better collaboration
and ensure that information flows
smoothly between the teams and finally
encourage cause functional collaboration
now deop Engineers recognize the value
of cross functional collaboration and
knowledge sharing among the team members
they actively encourage them from
development and operations team to
collaborate exchange ideas and share
their expertise second on the list we
have infrastructure Automation and
configuration management now deop
Engineers focus on automating
infrastructure provisioning and managing
configurations using certain tools they
Define infrastructure as code enabling
efficient deployment and scaling of
resources now as a devops engineer you
have to identify the infrastructure
requirements effectively now devops
Engineers work closely with development
teams to understand the infrastructure
requirements of the application this
involves analyzing the needs of
applications in terms of Computer
Resources Storage security networking
and scalability by gathering all these
requirements devops engineer can Ure
that infrastructure is provisioned and
configured to meet the application
specific need and future growth write
infrastructure automation scripts and
templates now once the infrastructure
requirements are identified devops
Engineers use automation tools and
techniques to define the desired state
of infrastructure components the right
scripts and templates that specify how
the infrastructure should be provisioned
configured and managed automate the
provisioning configuration and
management of servers well deop
Engineers leverage infrastructure as
code or in short IAC principles to
automate the provisioning configuration
and management of servers networks and
other infrastructure resources they use
tools like an symol Chef or puppet to
automate the deployment and
configuration of infrastructure
components and finally regularly update
infrastructure code now devops engineer
uses Version Control Systems like git
track changes collaborate with team
members and manage different versions of
infrastructure code by regularly
updating and versioning infrastructure
code devops Engineers can easily track
and rever changes whenever necessary now
third on the list we have continuous
integration and continuous deployment or
cicd in short now deop Engineers are
responsible for establishing and
maintaining cicd pipelines which enable
developers to integrate code changes
seamlessly and deploy applications
rapidly so for that they have to set up
a version control system now Version
Control System like git play a crucial
role in devops by providing a
centralized repository for managing code
and tracking changes setting up a
Version Control System involves creating
a repository initializing it with a code
and defining branching and merging
strategies configure a built server now
a built server automates the process of
compiling testing and packaging
application code tools like genkin and
gitlab cic CD allows you to Define build
pipelines that specify the steps to be
executed these pipelines typically
involve tasks such as pulling code from
repository compiling source code
generating artifacts and packaging the
application next automate the deployment
process now automation of the deployment
process is crucial for achieving rapid
and consistent software releases
containerization tools like Docker
provide a lightweight and portable way
to package application and their
dependencies Docker containers can be
created and deployed consistently across
different environments ensuring
consistency between development testing
and production Define and enforce
quality Gates and monitor cicd kpis
quality Gates ensure that the code meets
predefined quality standards before it
is promoted to the next stage of the
cicd pipeline automated testing
including unit test integrated test and
end to end test integration test and end
to end test help catch bugs and validate
the functionality of the managing
applications and finally measuring KPS
of the cicd pipeline provide insights
into its performance and help identify
areas for improvement well next we have
monitoring and performance optimization
now doops Engineers monitor system
performance identify and optimize the
infrastructure and application Stacks
whenever necessary they Implement
monitoring tools to collect and analyze
metrics logs and traces so for that
select and configure monitoring tools
now monitoring tools like Prometheus or
graph can be used to collect and
visualize these metrics allowing teams
to identify bottlenecks or optimize
processes and enhance the overall
efficiency of the cicd
pipeline also they have to collaborate
with development and operations team to
fine tune application
performance so continuously optimizing
the infrastructure which will ensure
High availability scalability and
reliability of that application and
finally we have security and compliance
now do engineers play a critical role in
implementing security measures and
ensuring compliance with industry
standards and regulations they work
closely with security teams to Define
and Implement security controls
throughout the software delivery
pipeline so they have to continuously
collaborate with the security teams to
identify and Define security
requirements and controls and Implement
security measures such as monab scanning
access management and secure
configuration they have to continuously
integrate security testing and code
analysis into the cicd pipeline and
monitor for any sort of potential
security risk or breaches and respond
promptly to mitigate any identified
vulnerabilities so these were some of
the main or top five roles and
responsibilities of a devops engineer
talking about the salary figures of a
senior devops engineer according to
glasto a senior devops engineer working
in the United States earns a whooping
salary of$
17836 to the same senior devops engineer
in India earns 18 lakh rupees annually
to sum it up as you progress from entry
level to midlevel and eventually to
experience devop engineer your roles and
responsibilities evolve
significantly each level presents unique
challenges and opportunities for growth
all contributing to your journey as a
successful devops professional so
excited about the opportunities devops
offers great now let's talk about the
skills you will need to become a
successful devops and genu coding and
scripting strong knowledge of
programming Lang languages like python
Ruby or JavaScript and scripting skills
are essential for Automation and Tool
development system administration
familiarity with Linux Unix and Windows
systems including configuration and
troubleshooting cloud computing
Proficiency in Cloud platforms like AWS
Azure or Google Cloud to deploy and
manage applications in the cloud
containerization and
orchestration understanding container
Technologies like Docker and container
or cration tools like kubernetes is a
must continuous integration or
deployment experience with cicd tools
such as jenin gitlab Ci or Circle CI to
automate the development workflow
infrastructure as code knowledge of IAC
tools like terraform or insensible to
manage infrastructure
programmatically monitoring and logging
familiarity with monitoring tools like
promp the grafana and logging Solutions
like elk stack acquiring these skills
will not not only make you a valuable
devops engineer but will also open doors
to exciting job opportunities so to
enroll in the postgraduate program in
devops today click the link mentioned in
the description box below don't miss
this fantastic opportunity to invest in
your future let's take a minute to hear
it out from our Learners who have
experienced massive success in their
career through a post-graduate program
doops Engineers are one of the highest
paed Professionals in the tech industry
today and if you are watching this video
most likely you are someone who wants to
get into devops or want to learn devops
but with so many different tools and
Technologies like tform an Linux AWS
genkin stalker kubernetes and so much
more learning devops can be very time
consuming and confusing and that is why
in this video I'm going to share with
you a complete devops road map that you
get follow to learn devops from scratch
and also an excellent resource you can
use to learn all the devops tools at one
place so watch this video till the end
before we start with the video and look
into the devops road map let me
introduce myself my name is NASA Chri
also goes by Cloud champ on YouTube and
I work as a freelance devops engineer
for multiple companies the road map
shared in this video is the exact road
map I followed to learn devops from
scratch and I'm pretty sure if you watch
this video till the end you will have a
clear idea on what things to learn and
what not to learn and get jobs faster a
deop engor is responsible for deploying
application and automating the manual
process but how can you automate a
manual process process if you don't know
how is an application created so first
thing you need to know is to understand
the concepts of software development
what is a build what is software
deployment so generally try to
understand the whole software developer
life cycle from idea to code and to
releasing the application to end users
after you understand the software
developer life cycle and now you have an
idea of how is an application created
second thing you need to know is Linux
Linux is very very very important a
doops engineer should have a good
Hands-On Lage with Linux need to know
all the important commands because every
Dev tools you look at it's a an
terraform kubernetes stalker all of them
work on commands and you can only manage
to work with them when you have good
hands on knowledge with Linux so learn
Linux in Linux you can learn things like
shell commands leux file systems and
permissions SSH Key Management
virtualization some part of networking
like L bances how to set up firewalls
how IP addresses work and much more
Linux is very important so it's also
called as operating system of the cloud
so spend time learning leux now most of
you might be confused and you might ask
let see I want to learn all this tools
but where should I find the resources
you can learn this from YouTube some of
these from blog some from documentation
but they are all scattered which will
waste your time so rather than that I
will suggest you checking out an
excellent devops program by simply learn
this postgraduate devops program by
simply learn is in collaboration with
IBM which will teach you all the
important devops tools that you require
in order to become a devops engineer
like terraform MAV anci Jenkins stalker
kubernetes kit and a lot more along with
industry projects that will provide you
hands on experience and can help you
become theop engineer faster and also a
certification by Caltech which will
validate your learning in devops so you
can check out the learning part here and
reviews by previous learner wow all of
them are five star reviews so click on
the link in the description and click on
apply now for this program you don't
require any prior work experience you
require a bachelor degree with an
average of 50% or higher marks you can
be from programming or non-programming
background which makes this program for
everyone so click on the link in the
description click on apply now fill in
all your details and proceed to start
learning devops with simply learn next
important thing after learning leadex is
going to be get because every company is
going to store their code online on git
repositories like GitHub gitlab or bit
bucket so doops engineers to know how to
work with Git to clone and push the code
from local machine to git repositories
or from G repositories to local machine
also understand what is branching how
does branching books what is M request
what is pull request and a lot more so
DS engineer should have a good
understanding of get and also know all
the most used get commands once you have
cleared your Basics now is the time to
learn Cloud because D Engineers need to
know how to create servers databases
storage virtual VCS and lot more on the
cloud you can can choose any cloud like
AWS Azure gcp IBM Oracle or anything but
the Ops Engineers need to know how to
create infrastructure to deploy their
applications and software on the cloud
once you have learned how to create
infrastructure and deploy your
applications on the cloud manually you
need to automate this because Toops is
all about automation so you can automate
this using infrastructure as code tools
like terraform anible chef perpet and a
lot more but I would suggest you
learning only these two tools which are
terraform and an that are highly used in
the industry today learning anable and
terraform can provide you with so many
job opportunities so start with learning
terraform and anible after you have
mastered creating an application on the
cloud manually due to rise in demand and
application stability companies are
shifting from servers to Containers
which is why you need to learn Docker
and kubernetes for Docker you need to
understand the concepts of
virtualization and concept of
containerization also you need to know
how to containerize an application and
and WR it on a server or a kubernetes
cluster you need to know commands on how
to create Docker file how to create
Docker image how to run containers
networking in Docker and some of the
parts for kubernetes you need to
understand the kubernetes architecture
what is deployment what is replica set
what is POD what is node and how to
properly manage your ciz application
using kubernetes cluster either on eks
AKs or GK which is going to this engine
so it is very important for you to
understand and know the command to
properly work through containerization
which is a leading and a popular tool
right now in the market next very
important thing for a doops engineer to
learn is going to be cicd or continuous
integration and continuous deployment
because every company wants to deploy
their application automatically in
devops all code changes like new
features or buck fixes should be
integrated in the existing application
and provided to the end user in an
automated fashion and you can only do
this by using cicd so ad option you
should know how to set up cicd server
how to integrate code repositories to
trigger pipeline automatically when
there is a change and to fix B faster
and to provide better quality software
to the end user very fast in automated
fashion some popular CD tool includes
genkins gitlab Circle CI Travis CI so
learning cicd will help you land your
job very very fast and it's very
important also known as hard of Dem Ops
so you should master
cicd all the tools that have mentioned
till now will help you deploy an
application on the cloud or on
containers anywhere but once the
software or your application is in
production it is very important for you
to monitor it to track performance to
see if there are any issues check system
resources like CPU or Ram or anything so
one of the responsibilities of Dos
engineer is to set up software
monitoring infrastructure monitoring
collect logs and to visualize data to
check if there are any issues or if
system has n resources or not so you you
need to learn tools like Prometheus
grafana also login tools like Cloud
watch or elk stack which is very
important to make sure that the
application is running without any
issues and without any problems
congratulations now you know all the
tools and Technologies you require to
become a devops engineer but devops is
all about Automation and there is always
going to be something that you can
automate so to automate this you will
require scripting language and some of
you might argue that scripting is not
required or you don't need any
programming language in devops coding is
not a thing in devops but that's not
actually true you will require knowledge
of one programming language or a
scripting language because you need to
automate the manual process so some
popular scripting language can be bash
Powershell which are all OS related so
you can use bash in Linux and Powershell
in Windows and nonos liated our python
go Ruby uh which is what I would suggest
I would suggest you trying to Learn
Python which will help you automate all
the manual process like uh rotating
passwords of your databases like
starting the deployment starting to
build or or clearing case anything that
you are doing manually can be automated
through scripting languages using this
python or go or bash my suggestion would
be to start with python and if you love
python you will be unstoppable and you
will have more value as a devops
engineer and you don't need to learn any
programming language at a software
engineer level you just need to know in
the python or enough go just to write
scripts which can automate your things
you don't need to go deep in DSA and all
those things which are very common
questions that I get so just focus on
learning python in a way where you can
automate things by writing scripts so
just the basics not too Advance only to
automate stuff so there you have it a
complete devop s map to learn things
from scratch along with this make sure
you have your LinkedIn perfect you
attend meetups and also enroll to the
simply learn devops program thank you
where Innovation meets efficiency to
transform software development and
operations into seamless and agile
process in this video on devops
engineering road map 2024 we will be
covering what is devops devops
engineering road map responsibilities of
devops engineer salary and Company's
hiring let me ask you a quick question
so which of the following is a key
objective of devops option A is slowing
down software development
option b is increasing the gap between
development and operations option C is
automating and streamlining software
delivery option D is reducing
collaboration among teams now you can
pause this video and answer in the
comment section below all right so now
let's start with the first topic which
is what is devops so devops is a
collaborative approach that brings
together software development that is
Dev and it operations that is op steams
to Streamlight the software delivery
process it emphas izes communication
collaboration automation to enable
faster and more Reliable Software
releases devops aims to break down the
wall between teams allowing for seamless
integration and continuous feedback
throughout the development life cycle by
automating tasks such as good testing
deployment and infrastructure management
devops promot and stability ultimately
devops Fosters a culture of
collaboration Innovation and continuous
Improvement enabling organizations to
deliver high quality software at faster
Pace to meet customer demands if you
inspire to become a devops engineer
simply learn professional certificate
program in cloud computing and devops is
your ideal choice in partnership with en
ICT Academy IAT goti this programs
equips you with the skills needed to
excel in cloud computing and devops
learn to deploy robust Microsoft Azure
and AWS applications through a blend of
live virtual class Hands-On Labs
self-based videos and collaborative peer
interactions don't miss this opportunity
to master the art of cloud and devops
enroll today and pave your way to become
a cloud expert all right now let's move
on and understand the devops engineering
road map so the devops engineers need to
possess a diverse set of skills so let's
explore the importance of key skills in
the devops engineer road map first we
have is Linux Linux is an open source
operating systems widely used in the
devops world devops Engineers need to be
familiar with Linux as it is commonly
used for Server management scripting and
automation understanding Linux commands
and system administrations helps in
effectively managing
infrastructure next skill is Jenkins
cicd continuous integration and
continuous delivery deployment that is
cicd are essential practices in devops
Jenkins is a popular tool for automating
the cicd pipeline allowing for frequent
code integration testing and deployment
devops Engineers need to understand
genkins to enable faster and more
Reliable Software relas releases the
next skill that you must possess in
order to become a devops engineer is
Docker and Docker swamp containerization
with Docker allows for consistent and
portable application deployments devops
Engineers use Dockers to package
applications and their dependencies into
containers making them easier to manage
and deploy Docker swarm helps in oriz
and scaling containerization
applications ensuring High availability
and efficient resource utilization
coming to the next one which is
kubernetes kubernetes is crucial for
managing containerization application at
a scale it automates the deployment
scaling and management of containers
making it easier to manage complex
environments devops Engineers need to
understand kubernets to ensure efficient
resource allocations High availability
and seamless application scaling all
right now moving on to the next skill
which is cloud platforms so Cloud
platforms provide on demand resources
and services enabling scalability and
flexibility devops Engineers need to be
proficient in Cloud platforms like AWS
AA or Google Cloud to deploy and manage
applications in the cloud this allows
for efficient resource provisioning
scalability and cost optimization next
up we have is version control system so
Version Control Systems like git are
essential for collaboration and managing
code changes devops Engineers use
Version Control Systems to track changes
collaborate with team members and ensure
code Integrity it enables efficient code
management versioning and collaboration
across teams following this there is
monitoring and logging monitoring and
logging tools are crucial for tracking
applications and infrastructure
performance D Ops Engineers set of
monitoring systems to proactively
identify issues troubleshoot problems
and ensure Optimal Performance and
availability logging helps in capturing
and analyzing applications logs for
debugging and performance optimization
each of these skills plays a vital role
in enabling collaboration Automation
scalability and reliability in the
devops workflow they help devops
Engineers streamline process improve
efficiency and deliver high quality
software at a faster Pace now let's talk
about the responsibilities of a devops
engineer so as a devops engineer your
responsibilities revolve around Bridging
the Gap between development and
operation teams to ensure smooth and
effic ient software delivery here are
some key responsibilities explained in
the understandable manner first is
collaboration ha collaboration means you
work closely with development operations
and other cross functional teams to
Foster collaborations and streamline
communication then and performance here
Monitor and performance means you set up
monitoring systems to track the
performance and health of applications
and infrastructure this helps in
identifying and resolving issues proac L
then third is release Management in this
you manage the release process
coordinating with different teams to
plan and execute software releases all
right next is continuous integration and
deployment in this you automate the
process of integration code changes from
developers and deploying them to the
production environments next is
troubleshooting and support in
troubleshooting and support you
investigate and resolve issues related
to application or infrastructure
performance working closely with
development and operation teams then
there is continuous Improvement in
continuous Improvement you continuously
evaluate and improve existing processes
tools and infrastructure to enhance
efficiency and scalability right then
there is security and compliance in this
you implement security measures and best
practices to protect applications and
infrastructure from potential threats
all right so this was about the
responsibilities of a devops engineer
here now let's talk about the
interesting part which is the salary all
right so on an average a devops engineer
in the United States can expect to earn
between
$138,000 per positions may start around
$70 to $90,000 per year it's important
to note that these figures are
approximate and can vary significantly
based on individual circumstances all
right so this was the salary now let's
talk about the company's hiring for
devops engineer so first of all there is
Mercedes-Benz research and development
India a company based in bangaluru India
that requires devops Engineers with
skills in AWS kubernetes Docker Jenkins
and python the next big company is IBM a
multinational company that has openings
for devops engineer then there is
Flipkart Flipkart a leading e-commerce
company that is looking for devops
engineers with expertise in Linux git
terraform and monitoring tools next
company is koru
a remote company that offers full-time
positions for devops engineers with
experience in Integrations updates fixes
and Technical Support then there is
Accenture Accenture is currently seeking
devops Engineers to optimize their it
operations and software development
processes then Oracle Oracle is actively
recruiting devops Engineers to enhance
their ID operations and streamlined
software development at last we have is
sap sap is looking for devops engineers
to improve their it operations and
software development practices as you
follow this devops road map keep in mind
that it's not just a path but an ongoing
Journey embrace the idea of always
learning things easier with automation
as you hit each Milestone you will not
only make things run smoother but also
create a culture of teamwork and
Improvement in your workplace if you
aspire to become a devops engineer
simply learns professional certificate
program in cloud computing and devops is
your ideal launch pad in partnership
with with E and ICT Academy I guti
androll today and pave your way to
becoming a cloud expert little scenario
before using G one of the challenges
that a lot of developers and development
teams would have had is developers would
be working on different types of code
whether it was database code whether it
was python or whether it's Java or net
and they would have a central server
that they would be pushing all of their
source code in but there was little or
no communication that was actually going
on in between all the Developers you
know the challenge you had with that
scenario is that when people would be
checking in code you could be like you
know there'll be conflicts and you'd
have to roll back different code
versions and this is really kind of the
challenge that git addresses git is a
tool that allows all of the developers
no matter what stack they're working in
to have access to all of the code and it
makes it much more effective of being
able to have development teams work on
small medium or even massive
applications and some of the biggest
applications out there are managed
through a git distributed server
environment so let's jump into what
we'll be covering so you have a clear
understanding of the value that you're
going to get out of watching this
presentation so we're going to go
through devops and the tools have
available for devops we'll talk about
what version control means within a
devops environment and cover the two
different types of Version Control
centralized and distributed and then
we're going to zero in on git which is a
really fantastic distributed version
control system and we're going to go
through and understand the the features
workflow branches and commands in git
we're going to give you a demo and then
summarize all of the activities at the
end of the the presentation so what is
devops this is one of my favorite
questions I love the idea of what devops
is so devops really is a culture of
being able to deliver Solutions faster
where your development teams and your
operation teams work effectively
together the idea is to be able to
continuously build out Solutions and
have testing codes that's built into
your Solutions so that no matter where
they are in the stage of the integration
and deployment life cycle they're always
being tested and you can always have
code being get ready to be released out
the idea is that instead of having big
releases that you would have maybe once
every two weeks or once a month that you
would actually have a continuous stream
of releases because you always have code
that is being tested you have your
network that's been tested you have your
environment being tested and you'll be
able to provide feedback directly to the
appropriate person whether they're in
devs or whether they're in Ops to be
able to be successful at being able to
deliver Solutions faster the bottom line
your Dev team thinks like operations and
your operations team begin to feel like
think like developers it's really a
fantastic way of being able to speed up
delivery and have your team just think
and feel and act in a different cultural
environment so let's have a look at some
of the tools that are available to you
in devops environment if we look at
devops it's really kind of split into
two areas the dev side you have building
code planning and testing and then on
your op side you have release deployment
operating and monitoring but the tools
really all interact with each other and
what's really great is that the tools
are either open source or very very low
in cost in fact most of the tools that
you're seeing in front of you right now
are open source tools which means
there's no licensing to you and you can
actually effectively manage them
Implement them within your team right
now so let's have a look at some of the
dev tools that you have for doing code
versioning and so some of the tools that
you may have used in the past uh include
subversion team Foundation service um
and git these are all different types of
Version Control software that you have
out there one of the oldest ones is um
subversion it's a centralized Version
Control System it's one of the tools I
used years ago it really is good at
doing what it's supposed to do which
which is just a very simple version
controlled solution it's open source so
it's free there's no licensing that
there are challenges with working with
it because it is a centralized tool
rather than a distributed tool we'll get
into that in a little bit but for a very
basic Version Control System you know it
does what you wanted to do team
Foundation Service Solutions something
that many of you in Microsoft world may
been using for a long time and the thing
that's great about TFS is it's built
right into the Microsoft environment
it's the server side the services side
of building out your Solutions with
Microsoft um just recently in the last
couple of weeks at the beginning of
September 2018 Microsoft actually just
rebranded TFS as Azure Dev Ops and so
you have Azure pipelines and there are
five now Azure tools that actually
replace TFS but these are all tools that
are very similar in concept the old TFS
tools are very similar to the
centralized SN tools whereas the new
tools that are part of azure are
actually much more aligned with Git so
what you're seeing is Microsoft moving
from the centralized server to a
distributed server with their new Azure
Dev Ops tools which is fantastic news
for every developer listening to this
and then we have git and get is a
distributed Version Control System it is
again open source which means that you
can start using it right now without
fear of having to have any costs or any
penalties and the thing that's U really
good about it is that it's really you
can use it for almost any kind of
digital project and what it's good at is
being able to create that historical
record and versioning of your source
code whether you're doing a web
application a mobile application or
you're actually building a python script
for a machine learning solution so let's
dig into what version control systems
are and how they can be of value to you
so the role of version control system is
to allow developers to be able to check
in their files into a repository and
here you actually have see how we can
check in three files into a repository
and that repository then becomes a
snapshot or a historical record of the
files that we're working on and you want
to be able to have it so that the
repository is flexible enough so that as
you want to be able to scale and add in
new files or new versions of a file that
you can actually do that easily within
your VCS environment and the goal is to
be able to constantly have each of these
versions available so that anybody that
wants to be able to check out the file
can actually have the latest version or
see a history of how that file became
what it is today let's look at
centralized Version Control Systems and
then we'll compare it against
distributed Version Control Systems so a
centralized version control system has a
central server where all the files are
stored and everybody has to check in and
check out from that centralized server
and all of the versions are managed
within that server environment the
problem is is that that if that Central
server crashes which doesn't happen but
very often but it can happen you end up
losing all of your files and I was
actually on a project where that
actually happened to us and we lost 6
months of history of how the application
was created fortunately for us we
actually had a backup but it didn't have
all the historical data so we were able
to at least continue working but we just
lost a lot of files and it took a while
to get back into place so let's now look
at and compare this to distributed
version control system with a
distributed Version Control System you
still have a centralized server that
manages the files but the difference is
is that as a developer you check out all
of the files for a project so you can
actually manage the whole project
locally on your development Ma sheine
and make your changes and then you can
just check in and out the changes that
you've made to the centralized server
the opportunity you have here is that
the server itself if it goes down you're
not going to be in a situation where you
lose all of the history because
everybody that's working on the actual
application has all of the versions of
the code locally on their machine now
one of the more popular distributed
Version Control Systems that out there
is git I mean it really is probably the
most popular buyer margin compared to a
system out there so you know let's dig
into you know what actually is git so
git is a virsion control system for
managing files and as a developer you
have a get client on your machine and
you're able to make changes and create
local repositories of a program on your
computer and then you can sync up with a
remote service such as GitHub or gitlab
or the new Azure Labs environment where
you can actually store files remotely
and then allow remote teams to have
access to those changes in the actual
applic you made so you can track your
changes easily you have it's the the
tool is inherently distributed so it
makes it very easy to manage the code
for large teams and you can bring people
in very quickly and you can have people
come in as Specialists and spin them up
quickly and have them work on a piece of
the code and then drop them out of the
project if you want to have a check on
how this works go and start contributing
to um any of the the many thousands of
Open Source projects at GitHub where you
can go in with your get tools and just
contribute to those projects and then
finally one of the things that I really
like with Git is that it's not a linear
development approach it's not starting
from a and going to Z it's a nonlinear
approach which allows you to have
branches that people are working on in
parallel to your master Branch where
people are actually doing the delivery
of the production code so let's step
through some of these features uh so get
as you can imagine the first thing it
does is track history that's probably
the most important part of having a
Version Control System it's free open
source you can actually go and use it
right now there are no costs for
actually using it it is a nonlinear
environment that allows people to be
able to build out new features in
parallel to the master Branch so that
you can be constantly having the master
code out there without and adding in new
features without breaking the code and
you can automatically create backups by
because each developer has a version of
the code remotely it's incredibly
scalable some of the biggest projects
out there now are being managed through
git and because of this scale the
collaboration becomes a byproduct of
teams working together and branching
just is so much easier within gate so as
you can see this whole thing leads to a
very effective distributed environment
so let's step through some of the the
workflow that we have available for git
so the way that git works is that you as
an operator as a developer you have the
git client on your local development
machine whether it's a Linux machine a
Mac or a PC and you connect to a remote
server and you pull down the latest
working copy and then you're able to
make all of your changes uh locally and
you can modify the the codes you can
review changes and you could commit
those codes to your git repository that
you have locally and then you can push
those changes back up to the remote
server and as soon as you push them into
the remote server those changes then
become available to everybody else
working on the project so they can be
constantly keeping everybody updated on
all the work that is happening so you
know typically the way that it would
work is you know you would start off
with having the working file working
directory that you have locally and then
you would stage those files into a
staging area and then you get those
files ready to be committed to your git
repository and these are all actions
that you would do locally and then
connect to a remote server and then
later when you go back into work on your
project the first thing you should
always do is check out that code so that
you always have the latest version of
the code and everybody's kept in sync so
let's talk about branching in G so the
way that branches work is that if
imagine you're working on a project and
projects always have a tendency of
getting bigger than you imagine and what
you want to be able to do is keep that
main product working and keeping it
working effectively but if you want to
be able to add a feature to that product
you may have two or three different
groups that are working on multiple
features
simultaneously and what you want to be
able to do is give them the freedom to
work on those individual features and
the way you do it in get is that you can
create branches of the master and while
you're um of the the main branch which
is called the master and so each person
could be working on their own separate
branch and then when they want to they
can then later merge those branches back
into the main master and all this time
the main Master still works and still
able to produce the right code for the
customer but at the same time it allows
the developers the freedom to be able to
write in their new features and so we
can look at this in a little bit more
visually with the master branch and we
can put in small features and large
features and we can then merge them back
as we need to so this kind of covers
some of the commands that we have in git
so the First Command you'd want to use
is called get in it and that's allowing
you to go into a folder on your local PC
and or your local Mac and convert that
file that folder into a local git
repository and that creates that
repository then you want to be able to
make changes to that repository and the
commands you'd be using in this instance
would be add commit or status and you
want to be able to sync your
repositories with a remote server so be
able to put the code that you have on
your computer on the remote server use
push you want to get the code from the
remote server you'd use pull and then
add r
and then if you want to do parallel
development from the master Branch the
main branch you would use Branch merge
or rease as ways to be able to do
parallel development and now we're going
to go through and do a demo on git so
we're going to do a demo on git so you
can feel comfortable running the
commands in the interface and you're
able to get git running correctly in
your environment so the first thing
we're going to do is we're going to set
up which version of git that you have
and then we're going to see if we can
establish some Global configurations
within your git environment and we're
going to do sand deep. Das as the name
the email address is sandep dos
simplylearn and then we're going to do a
list of the different configuration
settings we have so to check the version
of git that we have we do uh g- Dash
version in our terminal or command line
window you can also use Powershell that
will actually get you running in the
same environment as well all of these
tools are going to be command line tools
and then the next thing we're going to
do is we're going to assign some Global
username names and a global uh email
address so that we can actually access
the git account itself locally on the
device and so we're going to Dash and
we're going to put in config d-g Global
user.name Sandeep and then git config D-
Global user. email and sand deep. DS
simply learn.net and now we're going to
check the list of usernames and email
IDs in our configuration environment
with Dash list and you'll see that we
have everything in there correctly here
is our username right here and our name
and our email ID so if you need any help
when you're actually doing any of your
work in get all you have to do is do get
help config or get config d-el and this
will allow you to actually get access to
the help screens so let's just go and
type that in get help config return and
this takes us to the help page that's
running locally on your device and here
we have a breakdown of all the different
commands and what those commands
actually do within a git
configuration and we can go ahead and we
can do git config d-el and that will
actually take us to the same page so two
ways of doing the same thing so we're
going to go ahead and we're going to
create a test repository on our local
system and we're going to do that with
make directory called test and then
we're going to move our cursor within
command window terminal window to um the
test folder and then we're going to
initialize that folder to become a git
repository so let's go ahead and do that
so we're going to make a new directory
and we're going to call it test and
we're going to move the cursor so it's
actually in the test folder we do CD
change directory to test and you can see
now we're actually in the test folder
and now what we have to do is initialize
this folder as a git repository and so
we're going to do this as git in it and
it's now a new git instance that we can
actually now use for managing our git
environment so we're going to create a
new file and we're going to call it
info.txt and we're going to put some
content in it and we're actually going
to put it in the folder that we've just
created so you can actually see what
it's like to test out the G environment
with a file that hasn't been checked
into it yet so let's go ahead and do
that so we open up our folder directory
and here you can see we have the test
folder and we're just going to go ahead
and create a default test file so right
click Text new document info.txt open
that and we'll just you can put really
whatever you want in here we're just
going to put in this information right
now name equals Sam registration number
uh one 12 479 and save that file close
it out now if we actually go ahead and
run a git status command you'll see that
the info.txt file is in red that's
because that file has not actually been
checked into the project it's a new file
that we've added but it hasn't been
committed yet into the git repository so
let's go ahead and see what it takes to
actually add the file to the G
repository that we just created so we're
going to do that by doing Gad info.txt
and then we're going to commit that into
the history of that git repository so we
can do so it now has a Version Control
and we can start doing uh forking and
other uh kind of similar activities when
we connect later to our GitHub git
environment so we do git add info.txt
and remember we're actually still in the
test folder so it's going to look for
that file in that folder and just add it
in and now if we go ahead and do get
status we'll actually be able to see
that the info.txt file is is in an
active color first of all we have to
commit it so we go get commit DM and
then we're going to commit and we add in
some quick text you can write you want
committing the text file it's now
actually committed into the git
repository and it's still running
locally on your PC so we're going to go
make some changes to the file and we're
going to save it and we want to be able
to see how we can use git to compare the
differences between the two files so
let's go over and we're going to make
some changes to the info. TX T XT file
here we are here's info.txt which is
going to add in a new line we going add
in the line address and we going save
that file and now when we go over to get
we're going to be able to review the
difference so we're going to do get div
and here you'll actually see that we've
added in a new address file so now you
can actually see this is the difference
and the difference between the two
documents as you can imagine when you
have code this would become more
elaborate where actually will show you
code that's been pulled out if you
reduced any of eliminated any code or if
you added in new code really useful so
we're going to go ahead and add our git
username to a remote location so we can
actually start testing out the uh file
that we've created into a remote
repository in this case we're going to
use GitHub and you'll be able to see how
we can actually then save that file into
a git repository that somebody else
could then access and be able to make
edits too so let's just go ahead and
make those changes so get config D
global uh user. username and we do
simply learn and that will associate it
with this uh git repository that we just
created locally so simply learn D GitHub
so simply learn GitHub is the username
that we use on GitHub so we're going to
go over to GitHub here we are in GitHub
and then create new repository and then
as you can see we're simp so we're in
simply learn Das GitHub and we're going
to call the new repository test we're
going to make it public so anybody can
access it and you'll see that it
automatically adds a read me file and we
have our test environment set up
correctly so we'll go ahead and copy the
URL link U which is github.com
simplylearn
GitHub test. copy that and we'll go back
over to our command line window and so
now we're going to add the username to
the GitHub configuration so what we're
doing is we're connecting the local
repository to the remote repository so
we type get remote add origin and then
we'll paste in the address and this then
allows us to connect the local
repository that we created in the test
folder with the remote repository and
now we're connected so now that we've
connected we can actually push the file
that you have in the local repository on
your PC or Mac to the remote repository
in the GitHub server environment so
let's go ahead and do that so we're
going to do get push and push is the
command to push the documents from the
origin which is the local file to master
and Master's a remote file hey and there
we are success so what we have now is
the file that we just um created in our
local git repository is actually now in
through remote repository as well so
anybody can access it so that's going
have a look at that so we're going to
refresh the GitHub page and hey there we
are there's info.txt has been uploaded
and we're ready to go that's great
that's uh that's what you should be
seeing so refreshing your web page on
the remote GitHub folder and it could be
any git service we just happen to be
using GitHub because it's free to use
you'll be able to see that the local
file has been installed and is now part
of the git repository in the remote
server so what we're going to do now is
we're going to create three more files
and we're just going to call them
info.txt info 2 and info 3 and then
we're going to push them out to the
remote server and we're going to merge
everything together into a single
environment so let's go ahead and create
those three files and we're just going
to add those into the folder so let's go
ahead create new text document info one
and then we're going to create info two
and then
info three get my typing right here okay
here we go there we are okay I'm opening
info 3 and I'm just going to enter some
text in here and what we're going to do
is illustrate how we can do branching
and each of these files I'm going to
save and then close out and now I can go
over and so let's create a new branch
and we're going to call this one first
uncore branch and we go get Branch first
uncore branch and this will be a new
branch of the code environment that
we're creating and so hit return and
that allows us to create a new branch
and so what we do is we're going to move
to the new Branch so we're going to do
get checkout first uncore branch and
that allows us to move into that branch
and you see that info. text um is
already there which is good that's what
we want to see now what we need to do is
um add in the new documents so here we
have the different steps we've taken we
created a branch we moved to the new
branch and now we're going to go ahead
and add the info 3 txt to that Branch so
you can actually see everything coming
together and we're going to go get add
info 3.txt so we're just adding a file
like we were previously with our initial
our first initial git folder and that
file has now been added to the first
Branch so we're going to go ahead and
commit the file to the first branch and
then we're going to merge the uh
documents into the master Branch so we
have everything together so let's go
ahead and do those steps we're going to
do the commit first and then we're going
to merge everything together all right
so we do get commit - M and we put in
we're just going to say uh made changes
to First branch and that's just a
documentation so that we know what we've
done and there we are we've uh committed
the file so we had that file info 3 txt
committed and now we're going to go
ahead and merge the files into the main
master and uh so we can actually have
everything together in one consistent
environment and we do that by typing in
so this list out what we have in our
Master file so we just do LS and that
shows everything in the folder and we
have info.txt info 1 txt info 2 txt and
info 3 txt are now all in the first
Branch so let's go ahead and we're going
to merge okay so actually um before we
merge we have to check out the master
Branch so we do get checkout master so
it gets us into the master branch and
you'll see that we have just one file
there info.txt so we're going to list
out the files in the master Master
branch and so so what you'll see is
info. text info one text and info 2 text
are there but info. info3 text isn't
there because that was created in a
separate Branch so let's go ahead and
merge the first Branch into the master
Branch so we do get merge and we do
first underscore branch and there we are
we've merged it in and excellent that
looks good so let's go ahead and list
out the files that we have in the master
branch and we do LS and then and you'll
see that we have four files including
the info 3. text that we had in a
separate Branch now all in the master
Branch so what are we going to cover
today so we're going to introduce the
concept of Version Control that you will
use within your Dev Ops environment then
we'll talk about the different tools are
available in a distributed Version
Control System we'll highlight a product
called git which is typically used for
Version Control today and you'll also go
through what are the differ between git
and GitHub you may have used GitHub in
the past or other products like gitlab
and we'll explain what are the
differences between git and git and
services such as GitHub and gitlab we'll
break out the architecture of what a git
process looks like um how do you go
through and create forks and clones how
do you have collaborators being added
into your projects how do you go through
the process of branching merging and
rebasing your project and what are the
list of commands that are available to
you in git finally I'll take you through
a demo on how you can actually run git
yourself and in this instance use the
software of git against a public service
such as GitHub all right let's talk a
little bit about Version Control Systems
so you may have already been using a
virion control system within your
environment today you may have used
tools such as Microsoft's team
Foundation services but essentially the
use of a virsion control system allows
people to be able to have files
that are all stored in a single
repository so if you're working on
developing a new program such as a
website or an application uh you would
store all of your Version Control
software in a single repository now what
happens is that if somebody wants to
make changes to the code they would
check out all of the code in the
repository to make the changes and then
they would be an addendum added to that
so um there will be the the version one
changes that you had then the person
would then
later on check out that code and then be
a version two um added to that code and
so you keep adding on versions of that
code the bottom line is that eventually
you'll have people being able to use
your code and that your code will be um
stored in a centralized location however
the challenge you're running is that
it's very difficult for large groups to
work simultaneously within a project the
benefits of a VCS system a Version
Control System should demonstrates that
your you're able to store multiple
versions of a solution in a single
repository now let's take a step at some
of the challenges that you have with
traditional Version Control Systems and
see how they can be addressed with
distributed Version Control so in a
distributed Version Control environment
what we're looking at is being able to
have the code shared across a team of
developers so if there are two or more
people working on a software package
they need to be ble to effectively share
that code amongst themselves so that
they constantly are working on the
latest um piece of code so a key part of
a distributed Version Control System
that's different to just a traditional
version control system is that all
developers have the entire code on their
local systems and they try and keep it
updated all the time it is the role of
the distributed VCS server to ensure
that each client and we have a developer
here and developer here and developer
here and each of those are clients have
the latest version of the software and
then that each person can then share the
software in a peer-to-peer like approach
so that as changes are being made into
the server of changes to the code then
those changes are then being
redistributed to all of the development
team the tool to be able to do an
effective distributed VCS environment is
gig now you may remember that we
actually covered G in a previous video
and we'll reference that video for you
so we start off with our remote git
repository and people are making updates
to the copy of their code into a local
environment that local environment can
be updated manually and then
periodically pushed out to the git
repository so you're always pushing out
the latest code that youve code changes
you made into the repository and then
from the repository you a able to pull
back the latest updates and so your G
repository becomes the kind of the
center of the universe for you and then
updates are able to be pushed up and
pulled back from there what this allows
you to be able to accomplish is that
each person will always have the latest
version of the code so what is git git
is a distributed Version Control tool
used for source code management so
GitHub is the remote server for that
source code management and your
development team can connect their get
client to that remote Hub server uh G is
used to track the changes of the source
code and allows large teams to work
simultaneously with each other it
supports a nonlinear development because
of thousands of parallel branches and
has the ability to handle large projects
efficiently so let's talk a little bit
about get versus GitHub so get is a
software tool whereas GitHub is a
service and I'll show you how those two
look in a moment you install the
software tool for G locally on your
system whereas GitHub because it is a
service it's actually hosted on a
website git is actually the software
that used to manage different versions
of source code whereas GitHub is used to
have a copy of the local repository
stored on the service on the website
itself
git provides command line tools that
allow you to interact with your files
whereas GI help has a graphical
interface that allows you to check in
and check out files so let me just show
you the two tools here so here I am at
the git website and this is the website
you would go to to download the latest
version of git and again git is a
software package that you install on
your computer that allows you to be able
to do Version Control in a peer-to-peer
environment for that peer-to-peer
environment to be successful however you
need to be able to store your files in a
server somewhere and typically a lot of
companies will use a service such as
GitHub as a way to be able to store your
files so git can communicate effectively
with GitHub there are actually many
different companies that provide similar
service to GitHub get lab is another
popular service but you also find that
development tools such as Microsoft
Visual Studio are also incorporating
giit commands into their tools so the
latest version of Visual Studio team
Services also provides this same ability
but GitHub it has to be remembered is a
place where we actually store our files
and can very easily create public and
sharable is a place where we can store
our files and create public sharable
projects you can come to GitHub and you
can do a search on projects you can see
at the moment I'm doing a lot of work on
blockchain but you can actually search
on the many hundreds of projects here in
fact I think there's something like over
a 100,000 projects being managed on G
Hub at the moment that number is
probably actually much larger than that
and so if you are working on a project I
would certainly encourage you to start
at GitHub to see if somebody's already
maybe done a prototype that they're
sharing or they have an open- source
project that they want to share that's
already available um in GitHub certainly
if you're doing anything with um Azure
you'll find that there are thousands
45,000 Azure projects currently being
worked on interestingly enough GitHub
was recently acquired by Microsoft and
Microsoft is fully embracing open-source
Technologies so that's essentially the
difference between get and GitHub one is
a piece of software and that's git and
one is a service that supports the
ability of using the software and that's
GitHub so let's dig deeper into the
actual git architecture itself so the
working directory is the folder where
you are currently working on your git
project and we'll do a demo later on
where you can actually see how we can
actually simulate each of these steps so
you start off with your working
directory where you store your files and
then you add your files to a staging
area where you are getting ready to
commit your files back to the main
branch on your git project you will want
to push out all of your changes to a
local repository after you've made your
changes and these will commit those
files and get them ready for
synchronization with the service and
will then push your services out to the
remote repository an example of a remote
repository would be GitHub later when
you want to update your code before you
write any more code you would pull the
latest changes from the remote repos
repository so that your copy of your
local software is always the latest
version of the software that the rest of
the team is working on one of the things
that you can do is as you're working on
new features within your project you can
create branches you can merge your
branches with the mainline code you can
do lots of really creative things that
ensure the that a the code remains at
very high quality and B that you're able
to seamlessly add in new features
without breaking the core code so let's
step through some of the concepts that
we have available in G so let's talk
about forking and cloning in kit so both
of these terms are quite old terms when
it comes to development but forking is
certainly a term that goes way way way
back um long before uh we had
distributed CVS systems such as ones
that we're using with Git to Fork a
piece of software is a particular open
source project you would take the
project and create a copy of that
project and but then you would then
associate a new team and new people
around that project so it becomes a
separate project in entirety a clone and
this is important when it comes to
working with g a clone is identical with
the same teams and same structuring as
the main project itself so when you
download the code you're downloading
exact copy of that code with all the
same security and access rights as the
main code and then you can then check
that code back in and potentially your
code because it is identical could
potentially become the mainline code uh
in the future now that typically doesn't
happen your changes are the ones that
merged into the main branch but also but
you do have that potential where your
code could become the main code with Git
You can also add collaborators that can
work on the project which is essential
for projects where particularly where
you have large teams this works really
well when you have product teams where
the teams themselves are self-empowered
you can do what a concept what's called
branching in git and so say for instance
you are working on a new feature that
new feature and the main version of the
project have to still work
simultaneously so what you can do is you
can create a branch of your code so you
can actually work on the new feature
whereas the rest of the team continue to
work on the main branch of the the
project itself and then later you can
merge merge the two together pull from
remote is the concept of being able to
pull in Services software the team is
working on from a remote server and get
rebase is the concept of being able to
take a project and reestablish a new
start from the project so you may be
working in a project where there have
been many branches and the team has been
working for quite some time on different
areas and maybe kind of losing control
of what the true main branch is you may
choose to rebase your project and what
that means though is that anybody that's
working on a separate Branch will not be
able to Branch their code back into the
mainline Branch so going through the
process of a get rebase essentially
allows you to create a new start for
where you're working on your project so
let's go through forks and clones so you
want to go through the process so you
want to go ahead and Fork the code that
you're working on so this's use a
scenario that one of your team wants to
go ahead and add a new change to the
project the team member may say yeah go
ahead and you know create a separate
Fork of the actual project so what was
that look like so when you actually go
ahead and create a fork of the
repository you actually go and you can
take the version of the mainline Branch
but then you take it completely offline
into a local repository for you to be
able to work from and you can take the
mainline code and you can then work on a
local version of the code separate from
the Mainland branch is now a separate
Fork collaborators is the ability to
have team members working on a project
together so if you know someone is
working on a piece of code and they see
some errors in the code that you've
created none of us are perfect at
writing code I know I've suddenly made
errors in my code it's great to have
other team members that have your bag
and can come in and check and see what
they can do to improve the code so to do
that you have to then add them as a
collaborator now you would do that uh in
GitHub you can give them permission
within GitHub itself it's really easy to
do super visual um interface that allows
you to do the work quickly and easily
and depending on the type of permissions
you want to give them sometimes it could
be very limited permissions it may be uh
just to be able to read the files
sometimes it's being able to go in and
make all the changes you can go through
all the different permission settings on
GitHub to actually see what you can do
but you'll be able to make changes so
that people can actually have access to
your your repository and then you as a
team can then start working together on
the same code let's step through
branching in git so suppose you're
working on an application but you want
to add in a new feature and this is very
typical within a Dev Ops environment so
to do that you can create a new branch
and build a new feature on that Branch
so here you have your main application
on what's known as the master branch and
then you can then create a sub branch
that runs in parallel which has your
feature you can then develop your
feature and then merge it back into the
master Branch at a later point in time
now the benefit you have here is that by
default we're all working on the master
Branch so we always have the latest code
the circles that we have here on the
screen show various different commits
that have been made so that we can keep
track of the master branch and then the
branches that have come off which have
the new features and there can be many
branches in git so git keeps you the new
features you're working on in separate
branches until you're ready to merge
them back in with the main branch so
let's talk a little bit about that merge
process so you're starting with the
master branch which is the blue line
here and then here we have a separate
parallel Branch uh which has the new
features so if we to look at this
process the base commit of feature B is
the branch f is what's going to merge
back into the master branch and it has
to be said there can be so many
Divergent branches but eventually you
want to have everything merg back into
the master Branch let's step through get
rebase so again we have a similar
situation where we have a branch that's
being worked in parallel to the master
branch and we want to do a get rebase so
we're at stage C and what we've decided
is that we want to reset the project so
that everything from here on out out
with along the master branch is the
standard product however this means that
any work that's been done in parallel as
a separate Branch will be adding in new
features along this new rebased
environment now the benefit you have by
going through the rebase process is that
you're reducing the amount of storage
space that's required for when you have
so many branches it's a great way to
just reduce your total footprint for
your entire project
so get rebase is the process of
combining a sequence of commits to form
a new base commit and the primary reason
for rebasing is to maintain a linear
project history when you rebas you
unplug a branch and replug it in on the
tip of another branch and usually you do
that on the master branch and that will
then become the new Master Branch the
goal of rebasing is to take all the
commits from a feature branch and put it
together in a single Master Branch it
makes it the project itself much easier
to manage let's talk a little bit about
pull from remote Suppose there are two
developers working together on
application the concept of having a
remote repository allows the code to the
two developers will be actually then
checking in their code into a remote
repository that becomes a centralized
location for them to be able to store
their code it enables them to stay
updated on the recent changes to the
repository because they'll be able to
pull the latest changes from that remote
repository so that they are ensuring
that as developers they're always
working on the latest code so you can
pull any changes that you have made to
your fault remote repository to your
local repository the command to be able
to do that is written here and we'll go
through a demo of how to actually do
that command in a little bit good news
is if there are no changes you'll get a
notification saying that you're already
up to date and if there is a change it
will merge those changes to your local
repository and you get a list of the
changes that have been be made remotely
so let's step through some of the
commands that we have in git so git in
it initializes a local git repository on
your hard drive get add adds one or more
files to your staging area get commit DM
commit message is a commit changes the
git command commits changes to head up
so the git command commits changes to
your local staging area gets status
checks the status of your current
repository and lists the files you have
changed get log provides a list of all
the commits made on your current Branch
get diff views the changes that you've
made to the file so you can actually
have files next to each other you can
actually see the differences between the
two files uh get push origin Branch name
so the name of your branch command will
push the branch to the remote repository
so that others can use it and this is
what you do at the end of your project
get config D global username will tell
get Who You Are by configuring the
author name and we'll go through that in
a moment get config Global user email
will tell get the author of by the email
ID get clone creates a g repository copy
from a remote Source get remote add
origin server connects the local
repository to the remote server and adds
the server to be able to push to it get
branch and then the branch name will
create a new Branch for you to create a
new Fe feature that you may be working
on uh get checkout and then the branch
name will allow you to switch from one
branch to another Branch get merge
Branch name Will merge a branch into the
active Branch so if you're working on a
new feature you can then merge that into
the main branch a get rebase will
reapply commits on top of another base
tip and get rebase will reapply commits
on top of another base tip and these are
just some of the popular git commands
there are some more but you can
certainly dig into those as you're
working through using G so let's go
ahead and run a demo using G so now we
are going to do a demo using get on our
local machine and GitHub as the remote
repository for this to work I'm going to
be using a couple of tools first I'll
have the deck open as we've been using
up to this point uh the second is I'm
going to have my terminal window also
available and let me bring that over so
you can actually see this and the
terminal window is actually running get
bash as the software in the background
which you'll need to download and
install you can also run git bash
locally on your Windows computer as well
and in addition I'll also have the
GitHub repository that we're using
simply learn uh already set up and ready
to go all right so let's get started so
the first thing we want to do is create
a local repository so let's go ahead and
do exactly that so the local repository
is going to reside in my development
folder uh that I have on my local
computer and for me to be able to do
that I need need to create a drive in
that folder so I'm going to go ahead and
change the directory so I'm actually
going to be in that folder before I
actually create make the new folder so
I'm going to go ahead and change
directory and now I'm in the development
directory I'm going to go ahead and
create a new
folder and that's gone ahead and created
a new folder called hello
world I'm going to move my cursor so
that I'm actually in the hello world
folder and now that I'm in the hello
world folder I can now initialize this
folder as a git repository so I'm going
to use the git command in it to
initialize and let's going ahead and
initialize that folder so let's see
what's happened so here I have my hello
allall folder that I've created and
you'll now see that we have a hidden
folder in there which is called doget
and we expand that we can actually see
all of the different subfolders that g
repository will create so let's just
move that over a little bit so that we
can see the rest of the work and now if
we check on our folder here we actually
see this is users Matthew uh development
hello world. git and that matches up
with hidden folder
here so we're going to go ahead and
create a file called readme.txt in our
folder so here is our hello world folder
and I'm going to go ahead and using my
text editor which happens to be
Sublime I'm going to create a file and
it's going to have in there the text
hello world and I'm going to call this
one
readme.txt
if I go to my Hello World folder you'll
see that we have the readme.txt file
actually in the folder what's
interesting is if I select the get
status command what it'll actually show
me is that this file file has not yet
been added to the commits yet for this
project so even though the file is
actually in the folder it doesn't mean
that it's actually part of the project
for us to do that we actually have to go
and
select
for us to actually commit the file we
have to go into our terminal window and
we can use the git status to actually
read the files that we have there so
let's go ahead and use the get status
command and it's going to tell us that
this file has not been committed you can
use this with any folder to see which
files and subfolders haven't been
committed and what we can now do is we
can go and actually add the readme file
so let's go ahead and we just going to S
add git add so the git command is ADD
readme.txt so that then adds that file
into our main um project and we want to
then commit those files into the main
repositories history and so to that do
that we'll hit the the get command
commit and we'll do a message in that
commit and this one will
be first
commit and it has committed that project
what's interesting is we can now go back
into readme file and I can change this
so we can go hello git git is a very
popular version control
solution and
we'll we'll save that now what we can do
is we can actually go and see if we have
made differences to the read me meex so
to do that we'll use the diff command
for get so we do get
diff and it gives us two um releases the
first is what the original text was
which is hello world and then what we
have afterwards is what is now the new
text in green which has replaced the
original
text so what we're going to do now is
you want to go ahead and create an
account on GitHub we already have one
and so what we're going to do is we're
going to match the account from GitHub
with our local account so to do that
we're going to go ahead and S get
config and we're going to do Dash and
it's going to be a global user. name and
we going to put in our username that we
use for GitHub in this instance we're
using the simply
[Music]
learn Dash GitHub account name
and under the GitHub account you can go
ahead and create a new repository name
in this instance we called the
repository uh
hello-world and what we want to do is
connect the local GitHub account with
the remote hello world doget account and
we do that by using this command uh from
get which is our remote connection and
so let's go ahead and type that in open
this up so we can see the whole thing so
we're going to type in get remote add
origin
https back slashback
slash
github.com
slash simply
learn Das GitHub and you have to get
this typed in correctly when you're
typing in the location hello- world
doget that creates the connection to
your hello world account and now what we
want to do is we want to push the files
to the remote location using the get
push command commit get push
origin
master so we're going to go ahead and
connect to our local remote GitHub so
I'm just going to bring up my terminal
window again and so let's select git
remote add
origin and we'll connect to the remote
location
github.com
SLS simplylearn
Das GitHub
slash
hello-world
doget oh we actually have already
connected so we're connected to that
successfully and now we're going to push
the master Gish so get push
origin master and everything is
connected and
successful and if we go out to GitHub
now we can actually see that our file
was updated just a few minutes
ago so what we can actually do now is we
can go and Fork a project from GitHub
and clone it locally so we're going to
use the um fork tool that's actually
available on GitHub let me show you
where that is located and here is our
branching tool it's actually changed
more recently with a new UI
interface and once complete we'll be
able to then pull a copy of that to our
account using the fork new h HTP URL
address so let's go ahead and do
that so we're going to go ahead and
create a fork of our project now to do
that you would normally go in when you
go into your project you'll see that
there are Fork options in the top right
hand corner of the screen now right now
I'm actually logged in with the default
primary count for this project so I
can't actually Fork the project as I'm
working on the main branch however if I
come in with a separate ID and here I am
I have a different ID and so I'm
actually pretending I'm somebody else I
can actually come in and select the fork
option and create a fork of this project
and this will take just a few seconds to
actually create the
fork and there we are we have gone ahead
and uh created the
fork so you want to S clone or download
with this and so this is the I select
they'll actually give me the web address
I can actually show you what that looks
like I'll open up my text
editor that's not
correct
guess that is correct so I'm going to
copy
that and I can Fork the project locally
and clone it locally I can change the
directory so I can create a new
directory that I'm going to put my files
in and then post in that content into
that fileer so I can now actually have
multiple versions of the same code
running on my
computer I can can then go into the for
content and use the patchwork command to
actually so I can create a copy of that
code that we've just created and we call
it that's a a clone and we can create a
new folder that we're actually putting
the work in and we could for whatever
reason we wanted to we could call this
folder Patchwork and that would be maybe
new feature and then we can then paste
in the URL of the new uh directory that
would have has the fork work in it and
now at this point we've now pulled in
and created a clone of the original
content and so this allows us to go
ahead and Fork out all of the work for
our project onto our computer so we can
then diver our work
separately so now what we can actually
do is we can actually create a branch of
the fork that we've actually pulled in
onto our computer so we can actually
then create our own code that runs in
that separate branch and so we want to
check out um the uh the branch and then
push the origin Branch uh down to our
computer this will give us the
opportunity to then add our
collaborators so we can actually then go
over to GitHub and we can actually come
in and add in our
collaborators and we'll do that under
settings and select
collaborators and here we can actually
see we have different collaborators that
have been added into the project and you
can actually then request people to be
added via their GitHub name or by email
address or by their full
name one of the things that you want to
be able to do is ensure that you're
always keeping the code that you're
working on fully up toate by pulling in
all the changes from your
collaborators you can create a new
branch and then make changes and merge
it into the master Branch now to do that
you would create a folder and then that
folder in this instance would be called
test we would then move our cursor into
the folder called test and then
initialize that folder so let's go ahead
and do that so let's call um create a
new folder and we're going to first of
all change our root folder and we're
going to go
to
development and we're going to create a
new
folder
call it test and we're going to move
into the test folder and we will
initialize that
folder and we're going to move some
files into that test
folder
call this one test
one and then we're going to do file save
as and this one's going to be test
two and now we're going to commit those
files kit
add kit add and then we'll use the dot
to pull in all
files and then git
commit
DM
files
committed
make sure I'm in the right folder here I
don't think I
was and now that I'm in the correct
folder let's go ahead
and and get
commit and it's going ahead and added
those files and so we can see the two
files that were created have been add it
into the
master and we can now go ahead and
create a new branch and call this one G
Branch
testore
branch and let's go ahead and create a
third file to go into that
folder this
is file three do file save as we'll call
this one test 3.
text and we'll go ahead and
[Music]
add that file and do get ADD test
3.txt
and we're going to move from the master
Branch to the test
Branch get check out
testore
branch and it switched to the test
branch and we'll be able to list out all
of the files that are in the that Branch
now and we want to go through and merge
the files into one area so let's go
ahead and we'll do get merge
testore
branch and it's well we've already
updated everything so that's good
otherwise it would tell us what we would
be
merging and now all the files are merged
successfully into the master
Branch there we go all merg together
fantastic and so what we're going to do
now is move from Master Branch to test
Branch so get
checkout
testore
branch and we can modify the files the
test three file that we took out and
pull that file
up and we
can now
modified
and we can
then
commit that
file back in and we've actually been
able to then commit the file with one
changes and and we see it's the text R
change that was
made and we can now go through the
process of checking the file back in
switching back to the master branch and
ensuring that everything is in sync
correctly we may at one point want to
rebase all of the work is kind of a hard
thing you want to do but it will allow
you to allow for managing for changes in
the future so let's switch to it back to
our test branch which I think we're
actually on we're going to create two
more
files let's go to our folder here and
let's go copy
those and that's
created we'll rename those tests
four
and
five and so we now have additional
files and we're going to add those into
our branch that we're working on so
we're going to go in and select get add-
a and we're going to commit those files
get
commit D
a-m
adding to new
files and it's added in the two new
files so we have all of our files now we
can actually list them out and we have
all the files that are in the
branch and we'll switch then to our
Master Branch we want to rebase the
master so we to get
rebase
master and that will then give us the
command that everything is now
completely up to
dat we can go get
checkout Master to switch to the master
account if you aspired to become a d Ops
engineer simply learns professional
certificate program in cloud computing
in devops is your idle launch pad in
partnership with E and ICT Academy I
guti enroll today and pave your way to
becoming a cloud expert is
Maven if we talk in the literal sense
Maven means accumulator of knowledge
Maven is a very powerful project
management tool or we can call it a
build tool that helps building
documenting and managing a project but
before we move forward and dive deep
into the basics of Maven let's
understand understand what is meant by
the term build
tool a build tool takes care of
everything for building a project it
generates a source code generates
documentation from a source code it even
compiles the source code and packages
the compiled codes into jar of zip files
along with that the bill tool also
installs the packaged code in local
repository server repository or
repository coming back to Maven it is
written in Java or C and it is based on
Project object model or
pom again let's have a pause and
understand what is meant by this term
project object
model a project object model or p is a
building block in Maven it is an XML
file that contains information about the
project and configuration a details used
by Maven to build a project this file
resides in the base directory of the
project as p. XML file the PM contains
information about the project and
various configuration details it also
includes the goals and plugins used by
Maven in a
project Maven looks for the p in the
current directory while executing a task
or a goal it reads the P gets the needed
configuration information and then runs
the
goal coming back MAV is used to building
and managing any Java based
project it simplifies the day-to-day
work of Java developers and helps them
in their
projects now when we know the basics of
Maven let's have a look at some reasons
to know why is Maven so popular and why
are we even talking about it so let's
have a look at at the need for
Maven Maven as by now we know is
popularly used for Java based projects
it helps in downloading libraries or jav
files used in the
project to understand the part of why do
we use Maven or the need of Maven let's
have a look at some problems that Maven
solved the first problem is getting the
jar files in a project getting the right
jar files is a difficult task where
there could be conflicts in the versions
of the two separate packages however it
makes sure all the jar files are present
in its repositories and avoid any such
conflicting
scenarios the next problem it sorted was
downloading
dependencies we needed to visit the
official website of different software
which could be a TDS task now instead of
visiting individual websites we could
visit mvn repository. which is a central
Repository of the jar
files then Maven plays a vital role in
the creation of the right project
structure in seret struts Etc otherwise
it won't be
executed then Maven also helps to build
and deploy the project so that it may
work
properly so the next point is what
exactly Maven
does
it makes the building of the project
easy the task of downloading the
dependencies and jar files that were to
be done manually can now be done
automatically all the information that
is required to build the project is
readily available
now finally M helps manage all the
processes such as building documenting
releasing and other methods that play an
integral part in managing a
project now when we know everything
about Maven let's look at some companies
that use
Maven there are over 2,000 companies
that use Maven today the companies that
use Maven are mostly located in the
United States and in the computer
science Industry Maven is also used in
Industries other than computer science
like information technology and services
financial service banking hospital and
care and much more some of the biggest
corpor operations that use Maven are as
follows first we have vavo then comes
Accenture followed by JP Morgan Chase
and Company then comes craft base and
finally we have red hat CR will tool
here in this one we are going to talk
about what exactly is a Gren build tool
and how we can use it and in this one uh
we will be also working on some demos
and some Hands-On to understand that how
we can make use of Gradle for performing
the build activity so let's begin with
uh the first understanding that what
exactly is in griddle All About Now
griddle is an kind of a build tool which
can be used for the uh build automation
performance and uh it can be used for
various programming languages primary
it's being used for the uh Java base
applications it's a kind of build tool
which can help you to see that how
exactly automatically you can prepare
the builds you can perform the
automations earlier we used to do the
build activity from the eclipse and uh
we used to do it manually right but with
the help of this this build tool we are
going to do it like automatically
without any manual efforts as such here
there are like lot of activities which
we will be doing during the build
process primary there are different
activities like compilations linkage
packaging these are the different uh
tasks which we perform during the build
process so that we can understand that
how the build can be done and we can
perform the
automations uh this uh process also it's
kind of a standardized because again if
you want to automate something standards
or a standard process is something which
we require for that before being going
ahead with that part so that's the
reason why we are getting this build
tool because this build tool helps us to
do an standardization process to see
that how the standards can be met and
how we can proceed further with that
part also it's something which can be
used to variety of languages programming
languages Java is the primary language
for which we use the Cradle but again
other languages like Scala Android cc++
try these are some of the languages for
which we can use the same tool now it's
actually using like it's referring to as
an Cy based domain specific language
rather than XML because ant and MAV
these are the XML based build tools but
this one is not that uh dependent on XML
it's using the gry based domain specific
language DSL language is being used here
right now um again uh it's something
which can be used to do the build uh it
can further on used to perform the test
cases Auto also there and then further
on you can deploy to the artifactry also
that okay I want to push the AR to the
artifactry so that also that part also
you can get it done over here so primary
this tool is known for doing the build
automations for the big and large
projects the projects in which the
source code the amount of source code
and uh the uh efforts is more so in that
case this particular tool makes sense
now griddle includes both the pros of
Maven and uh ant but it removes the
drawbacks or whatever the uh issues
which we face during these two build
tools so it's helping us to remove all
the cons which we face during the
implementation of ant and Maven and
again again all the pros of ant and
Maven is implemented with this crle tool
now let's see that why exactly this
griddle is used because that's a very
valid question that what is the activity
like what is the reason why we use the
gradal because um the first one is that
it resolves issues faced on other build
tools that's a primary reason because we
all already having the tools like Maven
and ant which is available there but
primary this gdle tool is something
which is is removing all the issues
which we are facing with the
implementation of other tools so these
issues are getting uh removed as such
second one is that it focuses on
maintainability performance and uh
flexibility so it's giving the focus on
that how exactly we can manage the big
large projects and uh we can have
flexibility that what different kind of
projects I want to build today I want to
build in different ways tomorrow the
source code modifies gets added up so I
have the flexibility that I can change
this Bild scripts I can perform the
automations so a lot of flexibility is
available which is being supported by
this tool and then the last one is like
uh it provides a lot of features lot of
plugins now this is one of the uh
benefit which we get in the case of M
also that we get a lot of features but
again when we talk about cradle then it
provides a lot of plugins like let's say
that normally in a build process we do
the compilation of the source code but
sometimes let's say that we want to
build an angular or a nodejs application
now in that case we maybe in involved in
running some command line executions
some command line commands just to make
sure that yes we are running the
commands and we are getting the output
so there are a lot of features which we
can use like uh there are a lot of
plugins which is available there and we
will be using those uh plugins in order
to go ahead and in order to execute
those build process and doing the
automations now let's talk about the
cradle and MAV because again when we
talk about MAV like it was like
something which was primary used for the
Java but again when we are talking about
cradle so again is just uh being used
primary for the Java here but what is
the reason that we prefer Gradle over
the CR uh MAV so what are the different
uh reason for that let's talk about that
part because this is very important we
need to understand that what is the
reason that Gradle is preferred as an
better tool for the Java as compared to
mavan when we talk about for the build
automation here now the first one is
that the uh gridle using the groupy DSL
language domain specific language
whereas the maven is considered as in
project management tool which is
creating the palms or XML format files
so it's being used for the Java project
but XML format is being used here and on
the other hand griddle is something
which is not using the XML formats and
uh whatever the build scripts you are
creating that is something which is
there in the groupy based uh DSL
language and on the other hand in the
pal we have to create the xmls
dependencies whatever the attributes you
putting up in the May that's something
which is available there in the format
of XML the overall goal of the gdle is
to add function it to a project whereas
the goal of uh the maven is to you know
to complete a project phase like to work
on different different project phase
like compilation test executions uh then
uh packaging so uh then deploying to
artifa so these are all different phases
which is available there into the maven
but on the other hand griddle is all
about adding the functionality that how
you want to have some particular
features added up into the build scripts
in gridle there are like we usually
specify that what are the different
different tasks we want to manage so
different different tasks we can add up
into the case of griddle and we can
override those tasks also in case of
Maven it's all about the different
phases which is being happening over
here and it's in sequence manner so
these phases happens in the sequence
order that how exactly you can uh build
up the sequence there but in case of
griddle you can have your own tasks
custom tasks also and you can disrup the
sequence and you can see that how the
different steps can be executed in a
different order so Maven is something
which is a phase mechanism there but
griddle is something which is according
to the features or the flexibilities now
griddle works on the tasks whatever the
task you want to perform you uh it works
directly on those tasks there on the
other hand uh MAV is something does not
have any kind of inbuilt cache so every
time you running the build so separate
uh things or the plugins and all these
information gets loaded up which takes
definitely a lot of time on the other
hand gdle is something which is using
its internal cach so that it can make
the uh builds a little bit faster
because it's not something which is
doing the things from the scratch
whatever the uh things is already being
available in the cach it just pick that
part and from there it will proceed
further on the build Automation and
that's the reason why gradal performance
is much faster as compared to Maven
because it uses some kind of a cach in
there and then helps to improve the
overall performance now let's talk about
the gridle installation because this is
an very important aspect to be done
because when we are doing the
installation we have to download the
Cradle executables right so let's see
that what are the different steps is
involved in the process of the Gradle
installation so when we talk about the
Gradle installation so there are primary
four steps which is available the very
first one is that you have to check if
the Java is installed now if the uh Java
is not installed so you can go to the
open jdk uh or you can go for the Oracle
Java so you can do the installation of
the jdk on your system so jdk8 uh is is
something you can uh most commonly used
nowadays so you can install that once
the Java is downloaded and installed
then you have to do the Gradle uh
download Gradle there now once the
Gradle boundaries are executable uh or
the user file gets downloaded so you can
add the environment variables and then
you can validate if the Gradle
installation is working fine as expected
or not so we will be doing the Gradle
installation into our local systems and
uh into the windows platform and we'll
see that how exactly we can go for the
installation of Gradle and we'll see
that what are the different version we
are going to install here so let's go
back to the system and see that how we
can go for the Gradle installation so
this is the website of the jdk of java
orle Java now here you have different
jdk so from there you can do whatever
the uh option you want to select you can
go with that so jdk8 is something which
is most commonly used nowadays like it's
most comfortable or compatible version
which is available so um in case you
want to see that if the jdk is installed
into your system all you have to do is
that you have to just say like Java hun
version and that will give you the uh
output at whether the Java is installed
into your system or not so in case my
system the Java is installed but if you
really want to do the installation you
have to download the jdk installer from
this website from this article website
and then you can proceed further on that
part now once the jdk is installed so
you have to go for the Cradle
installation because Gradle is something
the which will be performing the build
automations and all that stuff so you
have to download the boundaries like uh
the Z file prop in which we have the
execut tables and all and then we have
to have have some particular environment
variables configured so that we will be
able to have the System modified over
there so right now we have got like the
prequests as in Java version installed
now the next thing is that we have to
install or download the execute tables
so uh in order to download the latest
gradel distribution so you have to click
on this one right now over here there
are different options like uh you want
to go for 6.7 now it's having like
binary only or complete we'll go for the
the binary only is because we don't want
to have the source we just want the
binaries and the executables now it's
getting downloaded it's around close to
100 MB of the installer which is there
now we have to just extract into a
directory and then the same uh path we
need to configure into the environment
variable so that in that way we will be
able to see that how the uh gridle
executables will be running and uh it
will give the uh complete output to us
over here in this case so it may take
some time and once the uh particular
modifications and the download is done
then we have to extract it and once the
extraction is done so we will be able to
go back and uh have some particular
version or have the configurations
established over there so let's just
wait for some time and then we will be
continuing with the environment
variables like this one so once the
installation and the extraction is done
now we just have to go to the downloads
where this one is downloaded we have to
extract it now extraction is required so
that we can have the setup like we can
set up this path into our environment
variables and once the path is
configured and established we will be
able to start further on that part on
the execution so meanwhile these files
are getting extracted let's see so we
already got the folder structure over
here and uh we will see like we will
give this path here there is two
environment variables we have to
configure one is the crore home and one
is the um in the path variable so we'll
copy this path here so meanwhile this is
getting uh extracted we can save our
time and we can go to the environment
variable so we can WR click on this one
properties in there we have to go for
the advanced systems settings then
environment
variables now here we have to give it
like
Gradle uncore home now in this one we
will not be going giving it till the bin
directory so that only needs to be there
where the gridle is extracted so we'll
say okay and uh then we have to go for
the path variable where we will be
adding up a new entry in this one we
will be putting up till the pin
directory here because the crle
executable should be there when I'm
running the crle command so these two
variables I have to configure then okay
okay and okay so this one is done so now
you have to just open the command prompt
and see that whether the execution or
the uh commands which you're running is
is completely successful or not so
meanwhile it's extracting all the
executables and all those things it will
help us to understand that how the whole
build process or how the build tools can
be integrated over there now once the
extraction is done so you have to run
like
CMD Java I version to check the version
of the Java and then the
Cradle underscore version is what you're
going to see check the version of the
Cradle which is installed and now you
can see that show that 6.7 version is
being installed over here in this case
so that's a way that how we are going to
have the crle installation performed
into our particular system so let's go
back to the content let's talk about the
crle core Concepts here now in this one
we are going to talk about what are the
different Core Concepts of credal all
about the very first one is the projects
here now a project uh represents a item
to be performed over here to be done
like uh deploying an application to a
staging environment performing some
build so gridle is something which is
required uh the projects um the gridle
project which you prepare is having
multiple tasks which is available there
which is configured and all these tasks
all these different tasks needs to be
executed into a sequence now sequence is
again is a very important part because
again if the sequence is not me properly
then the uh execution will not be done
in a proper order so that's the very
important aspect here tasks is the one
in which is a kind of an identity in
which we will be performing a series of
steps these tasks may be like
compilation of a source code preparing a
jar file preparing a web application
archive file or ER file also we can have
like in some task we can even publish
our artifact to the artifactory so that
we can store those artifacts into a Shar
location so there are different ways in
which we can have this uh particular
tasks
executed now build scripts is the one in
which we will be storing all this
information what are the dependencies
what are the different tasks we want to
refer it's all going to be present in
the build. Gradle file there build. grd
file will be having the information
related to what are the different
dependencies you want to download and
you want to store there so all these
things will be a part of the build
scripts now let's talk about the
features of cradle what are the
different features which we can uh use
in case of cradle here there are
different type of uh features which is
available there so let's talk about them
one by one so the very first one over
here is the high performance then u high
performance is something which we can
see that we already discussed that in
case you are using a large projects so
griddle is something which is in better
approach as compared to Maven because of
the high performance which we are
getting it uses an internal cache which
makes sure that you are using like you
are doing the builds faster and that can
give you a higher performance over there
second one is the support it provides
the support so it yes definitely
provides a lot of uh support on how you
can perform the builds and it's being a
latest tool which is available there so
the support is also quite good in terms
of how you want to prepare the build how
you want to download the plugins
different plugin supports and the
dependencies uh information also there
next one is multi project build software
so using this one you can have multiple
project s in case in your repository you
have multiple projects here so all of
them can be easily built up with the
help of this particular tool so it
supports multiple project to be built up
using the same gridle project and uh
gridle scripts so that support is also
available with this Gradle build Tool uh
incremental builds are also something
which you can do with the help of cradle
so if you have uh done only the
incremental changes and you want to
perform only the incremental build so
that can also be possible with the help
of a griddle here here the uh build
scans so we can also perform the build
scans so we can use some uh Integrations
with sonar Cube and all where we can
have the uh scans done to the source
code on understand on how the build
happens or how the source code really
happens on there so that code scan or
the build scans can also be performed
with this one and then uh it's a
familarity with Java so for Java it's
something which is uh considered as in
by default not even Java in fact Android
which is also using the Java programming
language is using the uh particular
cradle over here so that the build can
be done and it can gain uh benefits out
of that so in in all the manners in all
the different ways it's basically
helping us to see that how uh we can
make sure that this tool can help us in
providing a lot of features and that can
help us to make a reliable build tool
for our Java Base projects or any other
programming based project here right now
let's see that how we can un a Java
project with a Gradle here and uh for
that we have to go back and cradle is
something something which is already
installed we just have to create a
directory where we can have like how we
can perform some executions we can
prepare some build scripts and we can
have a particular execution of a gdle
build happened over there so let's go
back to the machine okay so we are going
to open the terminal here and we'll see
that how we can create it so first of
all I have to create a directory
structure let's say that we'll say like
cradle hyphen project now once the
project is created so we can go inside
this direction so to uh create some uh
Gradle related projects and preparing
the files now uh in this one we let's
first create a particular one so we will
be saying like VI
build. cradle so in this one we are
going to put like uh two plugins we are
going to use so we are going to say like
apply
plugin Java and uh then we are going to
say like
apply plugin
application so these two plugins we are
going to use and when we got this file
over here in this one so it shows like
build. griddle which is available there
in this case so two these files are
available now if you want to learn like
you know what are the different tasks
you can run like griddle tasks command
over there so griddle task will help you
know that what are the different tasks
which is available over here by
processing the build scripts and all so
um this will definitely help help you to
understand on giving you the output so
here all the different tasks are being
given and it will help you to understand
that what are different tasks you can
configure and you can work over here
just like jar files clean and all that
stuff build compile then uh init is
there then all these different uh
executions assemble then Java dog then
build then check test all these
different tasks are there and if you
really want to run the gridle build so
you can run like gdle clean to perform
the clean activity because right now you
are doing like if a build so before that
you can have a clean and then you can
run a a specific command or you can run
The Griddle clean build which will
perform the cleanup also and it will at
the same time will have the build
process also performed over there so
build and CLE up both will be executed
over here and what is the status whether
it's a success or a failure that will be
given back to you now in this case in
the previous one if you see that when
you ran the clean the crle clean it was
only running one task but when you go
for the uh build uh process when you run
the gradal clean build it's going to
give you much more information in fact
you can also give me uh further
information like you can have the hyphen
info flag also there so that if you want
to get the details about the uh
different uh tasks which we which is
being executed over here so that also
you're going to get over here in this
one so you just have to put like hyph
iPhone Info and then all these steps
will be given back to you that how these
uh tasks will be executed Ed and the
response will be there so that's a way
that how you can create a pretty much
simple straightforward project in form
of cradle which can definitely help you
to run some couple of cradle commands
and then you can understand that what
are the basic commands you can run and
how the configurations really works on
there right let's go back to the main
content right now let's move on to the
next one so in the next one we are going
to see that how we can prepare a griddle
build project in case of eclipse now we
are not using the local system we are
not directly creating the folders and
the files here we are actually using the
eclipse for performing the creating a
new credle project over here so let's
move on that part okay so now the
eclipse is open and uh I have opened in
this one the very first thing is that we
have to do the Gradle plug-in
installation so that we can create new
projects on Gradle and uh then we have
to uh configure the path that how the
Cradle plugin can be configured on the
pref uh preferences and all that stuff
and then then we will be doing the build
process so the very first thing is that
we have to go to the eclipse Marketplace
in there we have to search for
gridle so once the search is
done it will show us the plugins related
to Gradle so we have to go for build
ship Gradle integration so we'll click
on the install it will proceed with
installation it will download it in some
cases maybe it's part of the eclipse as
in uh in the ID so you can go to the
installed Tab and you can see that also
that if this plugin is already installed
or not but in this case we are
installing it and uh once the
installation is done we just have to
restart the uh specific uh ones we have
to restart this uh Eclipse so that the
changes can be
reflected so it's
downloading it's downloading the Cradle
here and once that is installed we will
be able to use it over here in this case
is in this scenario so we have to just
wait for that part so still downloading
the jar files so once the jar file is
done it's now over theas and download it
so after that we will be able to proceed
further on that download aspect so it's
going to take some time to download it
and once it's done we will be able to
proceed further now once the progress is
done so it's asking us for the restart
now so uh before that uh we just have to
click on restart now and then the
eclipse will be restarted all together
again here so you can do it manually or
you can go for that options it just
require a restart so that the new
changes can be reflected over here so
the plugins can be activated and can be
referenced here now we have to just uh
put up like the uh you know the
configuration where we can have the
system so we can go for the gridle
configuration so we can go for Windows
and then
preferences now in this case we have to
go for the uh for the ones in which the
Cradle option is available there so
cradle is what we are going to select
now user home the gdle user home is what
we need to use right so you want to go
for the gdle you want to go for local
installation so so all these options you
can use you can if if you go for the
gridle rapper then it will be
downloading The Griddle locally and it
is going to use the gridle W or gr W.B
file but if you already have an
installation locally so you can prefer
that also right now uh in the previous
demo we have already got the uh griddle
uh extracted so we just have to go for
the downloads in the downloads already
gdle is available so we are going to
select that part here so this is what we
are going to
select right so this represents that
this is the directory structure in which
we are having the uh mechanism so you
can either go for the build scan so you
can select the build scan also so once
this uh is enabled then all the projects
will be scanned and will be you know
published and uh it's in kind of
additional option which is available if
you really want to disable it you can
disable it also and you can go with this
configuration so so uh this is where the
uh particular gridle folder is being put
over here in this case and uh then we
have to just click on
apply and we just have to click on apply
and close so with this one the
particular execution is done now we will
be going for the project creation so you
can right click over here or you can go
to the file also so here we going to go
for the CH project and in this we are
going to have a Gradle project so Gradle
project is what we are going to create
here and
next so we are going to say like
gradal
project and then
next so once that is done so
finish so uh with this one when you
create the project so what will happen
that uh automatically there will be a f
structure will be available there right
and uh there are some uh Gradle scripts
which will also be created there so we
will be doing the modifications there
and we'll see that how the uh particular
gridle build script looks like and how
we can we will be adding some couple of
uh selenium related dependencies and
we'll see that how we can have more and
more dependencies added and what will be
the impact of those dependencies on the
overall project so that also it's very
important aspect to be considered so let
this processing be happened over there
just creating and uh some plug and
boundaries are getting installed and
getting downloaded so we'll see that
once the project is uh imported
completely executed over here and got
created we can extract that now if you
see here the particular option is
available about The Griddle tasks so you
can extract it also and you will be able
to know that what are the different
tasks which is available there let's see
that in the build they are running like
build these are the different tasks
which is happening inside the build
process so G griddle executions will be
also available over here in this case
and gridle task will be different will
be represented over here in this one so
you just have to extract on the gdle
project okay this is the library which
is available now uh what happens that uh
you will be able to have like settings.
grd in this one you will be able to have
like okay Gradle hyphen project is
something which is available there in
this one so that's what being referring
then we have over here as in these
folder structures which is created like
Source main Java this is the one source
test Java is the one which is available
as the folder structure and Source test
resources are also available here so the
main source main resources are also
available now in this case what happens
that these are the dependencies project
and external these are the different
dependencies are available there so
let's see let's add an dependency over
here in this one in the build. gridle
script and see that how we can do that
if we open build. gridle file so you can
see that these dependencies are there
like test implementation junit is
available there right and then we have a
implementations of this one which is
available now these jar files when you
put up it will automatically be added up
as in part of this one as in part of the
uh particular uh dependencies over here
and uh which means that you don't have
to store them as an within the
repository and automatically they can be
happened over there so let's open a
dependency page so we will be going to
mvn repository where we will be opening
a dependency link so this is the
dependency link here so slum hyph Java
is available and it can give you the uh
for all the different options now we
have for Maven this is the one and for
GLE this is the one here so we have to
just copy this one and uh we have to use
it as a dependency so this is the group
and this is the name and the version
which we are using here now we have
copied this one so we will go back to
the eclipse so here we have to just put
that
dependency and uh we have to just save
it so uh this is something which is
providing like selenium dependencies
which is available so now we have to
just refresh the project so so right
click over here then you will be able to
see the options in the Cradle saying
that refresh cradle project now once
they moment you do that so you will be
able to do like for the first time maybe
it will take some time to download all
the dependencies which is related to
selenium but after that you will be able
to see like the dependencies will be
simply added up over here in this case
so you can see that all the selenium
related dependencies are added up for
any reason if you comment these ones and
you say like synchronize
again so you will see that all the
dependencies which you are adding up
from the selenium represent uh from the
selenium perspective will be gone back
again so this is the way that how you
can keep on adding the dependencies
which is required for preparing your
build for your source code and from
there you will be able to proceed
further on the execution part so that's
the best part about this uh griddle here
so that's a way that how we are going to
prepare a griddle project within the
eclipse and now you can keep on adding
like the source code in this one and
that's a way that how the code base will
be added up over here right so that's
the way that how the uh particular uh
executions or this gridle project is
being prepared in case of eclipse so
when we talk about the gdel installation
so there are primary four steps which is
available the very first one is that you
have to check if the Java is installed
now if the uh Java is not installed so
you can go to the open jdk uh or you can
go for the Oracle Java so you can do the
installation of the jdk on your system
so jdk8 is something you can uh most
commonly used nowadays so you can inst
inst that once the Java is downloaded
and installed then you have to do the
Gradle uh download Gradle there now once
the Gradle boundaries are executable uh
or theer file gets downloaded so you can
add the environment variables and then
you can validate if the gral
installation is working fine as expected
not so we will be doing the Gradle
installation into our local systems and
uh into the windows platform and we'll
see that how exactly we can go for the
installation of cradle and we'll see
that what are the different version we
are going to install here so let's go
back to the system and see that how we
can go for the Gradle installation so
this is the website of uh the jdk of
java orle Java now here you have
different jdk so from there you can do
whatever the uh option you want to
select you can go with that so jdk8 is
something which is most commonly used
nowadays like it's most comfortable or
compatible version which is available so
um in case you want to see that if the
jdk is installed into your system all
you have to do is that you have to just
say like Java hyphone version and that
will give give you the uh output that
whether the Java is installed into your
system or not so in case my system the
Java is installed but if you really want
to do the installation you have to
download the jdk installer from this
website from this article website and
then you can proceed further on that
part now once the jdk is installed so
you have to go for the Cradle
installation because cradle is something
the which will be performing the build
automations and all that stuff so you
have to download the bindar like uh the
Z file probably in which we have the
executables and all and then we have to
have have some particular environment
variables configured so that we will be
able to have the System modified over
there so right now we have got like the
prequests as in Java version installed
now the next thing is that we have to
install or download the executables so
uh in order to download the latest
gradal distribution so you have to click
on this one right now over here there
are different options like uh you want
to go for 6.7 now it's they having like
binary only or complete we'll go for the
binary only because we don't want to
have the source we just want the
binaries and the executables now it's
getting downloaded it's around close to
100 MB of the installer which is there
now we have to just extract into a
directory and then the same uh path we
need to configure into the environment
variable so that in that way we will be
able to see that how the uh gridle
executables will be running and uh it
will give the uh complete output to us
over here in this case so it may take
some time and once the uh particular
modifications and the download is done
then we have to extract it and once the
extraction is done so we we will be able
to go back and uh have some particular
version or have the configurations
established over there so let's just
wait for some time and then we will be
continuing with the environment
variables like this one so once the
installation and the extraction is done
now we just have to go to the downloads
where this one is downloaded we have to
extract it now extraction is required so
that we can have the setup like we can
set up this path into our environment
variables and once the path is
configured and established we will be
able to start further on that part on
the execution so meanwhile these files
are getting extracted let's see so we
already got the folder structure over
here and uh we will see like we will
give this path here there is two
environment variables we have to
configure one is the grer lore home and
one is the um in the path variable so
we'll copy this path here so meanwhile
this is getting uh extracted we can save
our time and we can go to the
environment variable so we can right
click on this one
properties in there we have to go for
the advanced systems settings then
environment
variables now here we have to give it
like
Gradle underscore home now in this one
we will not be going giving it till the
bin directory so that only needs to be
there where the gridle is extracted so
we'll say okay and uh then we have to go
for the path variable where we will be
adding up a new entry in this one we
will be putting up till the pin
directory here because the Cal
executable should be there when I'm run
running the grle command so these two
variables I have to configure then okay
okay and okay so this one is done so now
you have to just open the command prompt
and see that whether the execution or
the uh commands which you're running is
is completely successful or not so
meanwhile it's extracting all the
executables and all those things it will
help us to understand that how the whole
build process or how the build tools can
be integrated over there now once the
extraction is done so you have to run
like CMD
Java iPhone version to check the version
of the Java and then the
Cradle underscore version is what you're
going to see check the version of the
Cradle which is installed and now you
can see that it shows that 6.7 version
is being installed over here in this
case so that's a way that how we are
going to have the crle installation
performed into our particular system
started with the first topic that is the
birth of
selenium selenium was primarily created
by Jason Huggins in 2004 Jason an
engineer at thought workk was working on
a web application that needed to be
tested frequently he realized that the
repeated manual testing of the
application is becoming inefficient so
he created a JavaScript language that
automatically controlled the browser's
actions this program was named
JavaScript test Runner after he realized
that his idea of automating the web
applications has a lot of potential he
made the JavaScript test Runner open
source and it was later renamed as
selenium
core so we know that Jason was a person
who initially created selenium but then
we must also know that selenium is a
collection of different tools and since
there are different tools there will be
several developers too to be exact about
the number of different tools there are
four different tools that have their own
creators let's have a look at all of
them the first tool is the selenium
remote control or the selenium RC that
was created by Paul
Hammond then comes selenium grid that
was developed by Patrick
lightbody the third tool is the selenium
IDE that was created by shinya katani
and the fourth tool that is the selenium
web driver was created by Simon
stward we shall be learning about all
these tools in great detail As you move
forth in our video first let's have a
look at what is selenium selenium is a
very popular open source tool that is
used for automating the test carried on
the web browsers there may be various
programming languages like Java C python
Etc that could be used to create
selenium test
scripts this testing that is done using
selenium tool is referred to as selenium
testing we must understand that selenium
allows the testing of web applications
only we can neither test any computer
software nor any mobile application
using
selenium selenium is composed of several
tools with each having its own specific
role and serving its own testing
needs moving forth let's have a look at
the features of s lenium which will help
us understand the reason behind its
widespread popularity so here we have a
set of features of
selenium selenium is an open- source and
portable framework that has a playback
and record feature it is one of the best
cloud-based testing platform and
supports various OS and languages it can
be integrated with several testing
Frameworks and supports parallel test
execution
we will be talking about all these
features in detail as we move forward so
here the first feature that we have is
op source and portable framework this
feature states that selenium is an open-
source and portable framework for
testing web applications in addition to
that selenium commands are categorized
in terms of different classes which make
it easier to understand and
implement the second feature is the
playback and record feature the feature
states that the tests can be authorized
without learning a test scripting
language with the help of playback and
record
features the next feature says that
selenium is a cloud-based testing
platform selenium is a leading
cloud-based testing platform that allows
testers to record their actions and
Export them as a reusable script with a
simple to understand and easy to use
interface
moving forth the next feature states
that selenium supports various operating
systems browsers and programming
languages the tool supports programming
languages such as cop Java python PHP
Ruby Pearl and JavaScript if we talk
about the operating systems then
selenium supports operating systems like
Android iOS Windows Linux Mac and
Solaris and the tool also supports
various browsers like Google Chrome
Mozilla Firefox Internet Explorer Edge
Opera Safari Etc then the next feature
we have is the integration with testing
Frameworks selenium can be well
integrated with testing Frameworks like
test NG for application testing and
generating reports and also selenium can
be integrated with Frameworks like ant
and Maven for source code compilation
the last feature in our list is the
parallel test execution selenium enables
parallel testing which reduces time and
increases the efficiency of the test
selenium requires fewer resources as
compared to other automation testing
tools now let's move further and get to
know different selenium tools by now we
know that there are four different tools
that come under the selenium suit the
four tools are selenium remote control
selenium grid selenium IDE and selenium
web
driver we shall have a look at these
four Tools in detail one after the
another beginning with selenium remote
control selenium remote control enables
the writing of automated web
applications in languages such as Java C
Pearl Python and PHP to build complex
tests such as reading and writing files
quering a database and and emailing the
test results selenium RC was a
sustainable project for a very long time
before selenium web driver came into
existence and hence selenium RC is
hardly in use today as the web driver
offers more powerful
functionalities the second tool we shall
see is the selenium
grid selenium grid is a smart proxy
server that enables the running of test
in parallel on several
machines this is is made possible when
the commands are rooted to the remote
web browser instances and one server
acts as the Hub The Hub here is
responsible for conducting several test
on multiple machines selenium grid makes
cross browser testing easy as a single
test can be carried on multiple machines
and browsers all together making it easy
to analyze and compare the
results here there are two main
components of selenium grid Hub and the
node Hub is a server that accepts the
access request from the web driver
client rooting the Json test commands to
the remote drivers on nodes and here the
node refers to the Remote device that
consist of a native OS and a remote web
driver it receives the request from the
Hub in the form of a Json test commands
and executes them using the web driver
moving forth the third tool is the
selenium IDE if you want to begin with
the selenium IDE it needs no additional
setup except installing the extension of
your
browser it provides an easy to ous tool
that gives instant feedbacks selenium
IDE records multiple locators for each
element it interacts with if one locator
fails during the playback the others
will be tried until one is successful
the IDE
makes the test debugging easier with
features like setting breakpoints and
pausing
exceptions through the use of the Run
command you can reuse one test case
inside one another and also selenium IDE
can be extended through the use of
plugins they can introduce new commands
to the IDE or integrate with a third
party service the last and the fourth
tool in the selenium suit is the
selenium web driver s selenium web
driver is the most critical component of
the selenium tools suit that allows
cross browser compatibility
testing the web driver supports various
operating systems like Windows Mac Linux
Unix Etc it provides compatibility with
a range of languages including python
Java and pearl along with it it provides
supports different browsers like Chrome
Firefox Opera Safari and Internet
Explorer the selenium web driver
completes the execution of test scripts
faster when compared to other tools and
also it provides compatibility with
iPhone driver HTML unit driver and
Android driver now after you know about
the tools the question arises which tool
to choose so in the next topic we shall
see different factors on the basis of
which we may decide which tool would be
more more suitable for us here we shall
have a look at the reasons why one
should choose that particular tool we
shall begin with selenium remote control
selenium remote control or selenium RC
should be chosen to design a test using
a more expressive language than
selenis Selen is a set of selenium
commands that are used to test or web
applications then the selenium RC might
be chosen to run tests against different
browsers on different operating systems
the RC may be chosen to deploy test
across multiple environments using
selenium grid it helps in testing the
applications against a new browser that
supports JavaScript and web applications
with complex Ajax based
scenarios now the second tool for which
we shall see the reasons are selenium
grid selenium grid as we learned in the
reasons for selenium RC is used to run
selenium RC scripts in multiple browsers
and operating systems
simultaneously the grid is used to run a
huge test suit that needs to be
completed at the earliest possible time
now comes the third tool the third tool
is selenium IDE selenium IDE is used to
learn about Concepts on automated
testing and selenium these Concepts
include SEL commands such as type Open
click and wait assert verify
Etc the concepts also include locators
such as ID name xath CSS selector
Etc selenium IDE enables the execution
of customized JavaScript code using
runscript and exporting test cases in
several different formats the IDE is
used to create tests with a limited
amount of knowledge in programming and
these test cases and test suits can be
exported later to RC or web driver now
finally let's have a look at the reasons
to choose a last tool that is selenium
web driver selenium web driver uses a
certain programming language in
designing a test case the web driver is
used to test applications that are rich
in Ajax based functionalities execute
tests on HTML unit browser and create
customized test results so this is what
we're going to be covering in this
session we're going to cover what life
is like before using Jenkins and the
issues that Jenkins specifically
addresses then we'll get into what
Jenkins is about and how it applies to
continuous integration and the other
continuous integration tools that you
need in your devops team then
specifically we'll Deep dive into
features of Jenkins and the Jenkins
architecture and we'll give you a case
study of a company that's using Jenkins
today to actually transform how their it
organization is operating so let's talk
a little bit about life before Jenkins
let's see this scenario I think it's
something that maybe all of you can
relate to as developers we all write
code and we all submit that code into a
code repository and we all keep working
away writing our unit tests and
hopefully we're running our unit tests
but the problem is that the actual
commits that actually get sent to the
code repository aren't consistent you as
developer may be based in India you may
have another developer that's based in
the Philippines and you may have another
team lead that's based in the UK and
another development team that's based in
North America so you're all working at
different times and you have different
amounts of code going into the code
repository There's issues with
integration and you're kind of running
into a situation that we like to call
development hell where things just
aren't working out and there's just lots
of delays being added into the project
and the bugs just keep mounting up the
bottom line is the project is delayed
and in the past what we would have to do
is we'd have to wait until the entire
software code was built and tested
before we could even begin checking for
errors and this just really kind of
increased the amount of problems that
you'd have in your project the actual
process of delivering software was slow
there was no way that you could actually
iterate on your software and you just
ended up with just a big headache with
teams pointing fingers at each other and
blaming each other so let's jump into
Jenkins and see what Jenkins is and how
it can address these problems so Jenkins
is a product that comes out of the
concept of continuous integration that
you may have heard of as power
developers where you'd have two
developers sitting next to each other
coding against the same piece of
information what they were able to do is
to continuously develop their code and
test the code and move on to new
sections of code Jenkins is a product
that allows you to expand on that
capacity to your entire team so you're
able to submit your codes consistently
into a source code environment so there
are two ways in which you can do
continuous delivery one is through
nightly builds and one is through
continuous so the approach that you can
look at continuous delivery is modifying
the Legacy approach to building out
Solutions so what we used to do is we
would wait for nightly builds and the
way that our nightly builds would work
and operate is that as co-developers we
would all run and have a cut off time at
the end of the day um that was
consistent around the world that we
would put our codes into a single
repository and at night all of that code
would be run and operated and tested to
see if there were any changes and a new
build would be created that would be
referred to as the nightly build with
continuous integration we're able to go
one step further we're able to not only
commit our changes into our source code
but we can actually do this continuously
there's no need to race and have a team
get all of their code in at arbitrary
time you can actually do a continuous
release because what you're doing is
you're putting your tests and your
verification Services into the build
environment so you're always running
Cycles to test against your code this is
the power that Jenkins provides in
continuous integration so let's dig
deeper into to continuous integration so
the concept of continuous integration is
that as a developer you're able to pull
from a repository the code that you're
working on and then you'll be able to
then at any time submit the code that
you're working on into a continuous
integration server and the goal of that
continuous integration server is that it
actually goes ahead and validates and
passes any tests that a tester may have
created now if on the continuous
integration server a test isn't passed
then then that code gets sent back to
the developer and the developer can then
make their changes it allows the
developer to actually do a couple of
things it allows the developer not to
break the build and we all don't want to
break the builds that are being created
but it also allows the developer not to
actually have to run all the tests
locally on their computer running tests
particularly if you have a large number
of tests can take up a lot of time so if
you can push that service off to another
environment like a continuous
integration server it really improves
the productivity of your developer
what's also good is that if there are
any code errors that have come up that
maybe Beyond just the standard CI test
so maybe there's a Code the way that you
write your code isn't consistent those
errors can then be passed on easily from
the tester back to the developer too the
goal from doing all of this testing is
that you're able to release and deploy
and your customer is able to get new
code faster and when they get that code
it simply just works so let's talk a
little bit about some of the tools that
you may have in your continuous
integration environment so the cool
thing with working with continuous
integration tools is that they are all
open source at least the ones that we
have listed here are open source there
are some that are private but typically
you'll get started with open source
tools and it gives you the opportunity
to understand how you can accelerate
your environment quickly so bamboo is a
continuous integration tool that
specifically runs multiple builds in
parallel for faster compilation so if
you have multiple versions of your
software that runs on multiple platforms
this is a tool that really allows you to
get that up and running super fast so
that your teams can actually test how
those different builds would work for
different environments and this has
integration with andt and mavin and
other similar tools so one of the tools
you're going to need is a tool that
allows you to automate the the software
build test and release process and
buildbot is that open-source product for
you again it's an open-source tool so
there's no license associated with this
so you can actually go in and you can
actually get the environment up and
running and you can then test for and
build your environment and create
releases very quickly so billbo is also
written in Python and it does support
power execution jobs across multiple
platforms if you're working specifically
on Java project projects that need to be
built and test then Apache Gump is the
tool for you it makes all of those
projects really easy it makes all the
Java projects easier for you to be able
to test with API level and functionality
level testing so one of the popular
places to actually store code and create
a versioning of your code is GitHub and
it's a service that's available on the
web just recently acquired by Microsoft
if you are storing your projects in
GitHub then you'll be to use Travis
continuous integration or Travis CI and
it's a tool designed specifically for
hosted GitHub projects and so finally
we're covering Jenkins and Jenkins is a
central tool for automation for all of
your projects now when you're working
with Jenkins sometimes you'll find
there's documentation that refers to a
product called Hudson Hudson is actually
the original version of the product that
finally became Jenkins and it was
acquired by orac
when that acquisition happened the team
behind um Hudson was a little concerned
about the direction that Oracle May
potentially go with Hudson and so they
created a hard Fork of Hudson that they
renamed Jenkins and Jenkins has now
become that op source project it is one
of the most popular and continuously
contributed projects that's available as
open source so you're always getting new
features being added to it it's a tool
that really becomes the center for your
CI environment environment so let's jump
into some of those really great features
that are available in Jenkins so Jenkins
itself is really comprised of five key
areas around easy installation easy
configuration plugins extensibility and
distribution so as I mentioned for the
easy installation Jenkins is a
self-contained Java program and that
allows it to run on most popular
operating systems including Windows Mac
OS and Unix you'd even run it on Linux
it really isn't too bad to set up it
used to be much harder than it is today
the setup process has really improved
the web interface makes it really easy
for you to uh check for any errors in
addition you have great buil-in help one
of the things that makes tools like
Jenkins really powerful for developers
and continuous integration teams and
your devops teams as a whole when you
have plugins that you can then add in to
extend the base function functionality
of the product Jenkins has hundreds of
plugins and you can go and visit the
update Center and see which other
plugins that would be good for your Dev
Ops environment suddenly check it out
there's just lots of stuff out there in
addition to the plug-in architecture
Jenkins is also extremely extensible the
opportunity for you to be able to
configure Jenkins to fit in your
environment it's almost endless now it's
really important to remember that you
are EXT sending Jenkins not creating a
custom version of Jenkins and that's a
great differentiation because the core
Foundation remains as the core Jenkins
product the extensibility can then be
continued with newer releases of Jenkins
so you're always having the latest
version of Jenkins and your extensions
mature with those core foundations the
distribution and the nature of Jenkins
makes it really easy for you to be able
to have it available across your entire
network it really will become the the
center of your CI environment and it's
certainly one of the easier tools and
more effective tools for devops so
this's jump into the standard Jenkins
pipeline so when you're doing
development you start off and you're
coding away on your computer the first
thing you have to do when you're working
in the Jenkins pipeline is to actually
commit your code now as a developer this
is something that you already doing or
least you should be doing you're
committing your code to a git server um
or to an SVN server or similar type of
service so in this instance you'll be
using Jenkins as the place for you to
commit your code Jenkins will then
create a build of your code and part of
that build process is actually going
through and running through tests and as
again as a developer you're already
comfortable with running unit tests and
writing those tests to validate your
code but there may be additional tests
that Jenkins is running so for instance
as a team you may have a standard set of
tests for how you actually write out
your code so that each team member can
understand the code that's been written
and those tests can also be included in
the testing process within the Jenkins
environment assuming everything past the
the tests you can then get everything
placed in a stage and release ready
environment within Jenkins and finally
you're getting ready to deploy or
deliver your code to a production
environment Jenkins is going to be the
tool that helps you with your server
environment to be able to deploy your
code to the production environment and
the result is that you're able to move
from a developer to production code
really quickly this whole process can be
automated rather than having to wait for
people to actually test your codes or
going through a nightly build you're
looking at being able to commit your
code and go through this testing process
and release process continuously as an
example companies Etsy will release up
to 50 different versions of their
website every single day so let's talk
about the architecture within Jenkin
that allows you to be so effective at
applying a continuous delivery devops
environment so the server architecture
really is broken up into two sections on
the left hand side of section you have
the code the developers are doing and
submitting that code to a source code
repository and then from then Jenkins is
your continuous integration server and
it will then pull any code that's been
sent to the source code repository and
will run tests against it it will use a
build server such as mavin to actually
then build the code and every single
stage that we have that Jenkins manages
there are constant tests so for instance
if a build fails that feedback is sent
right back to the developers so that
they can then change their code so that
the build environment can run
effectively the final stage is to
actually execute specific test Scripts
and these test scripts can be written in
selenium so it's probably good to
mention here that both mavin and
selenium are plugins that run in the
Jenkins environment so before we were
talking about how Jenkins can be
extended with plugins mavin and selenium
are just two very popular examples of
how you can extend the Jenkins
environment the goal to go through this
whole process again it's an automated
process is to get your code from the
developer to the production server as
quickly as possible have it fully tested
and have no errors so it's probably
important at this point to mention uh
one piece of information around the
Jenkins environment that if you have
different code builds that need to be
managed and distributed this will
require that you need to have multiple
builds being managed Jenkins itself
doesn't allow for multiple files and
builds to be executed on a single server
you need to have a multiple server
environment with running different
versions of Jenkins for that to be able
to happen so let's talk a little bit
about the Master Slave architecture
within Jenkins so what we have here is
an overview of the Master Slave
architecture within Jenkins on the left
hand side is the remote source code
repository and that remote source code
repository could be GitHub or it could
be uh Team Foundation services or the
new Azure Dev Ops Code repository or it
could be your own get repository the
Jenkins server acts as the master
environment on the left hand side and
that Master environment can then push
out to multiple other Jenkin slave
environments to distribute the workload
so it allows you to run multiple builds
and tests and production environments
simultaneously across your entire
architecture so Jenkins slaves can be
running the different build versions of
the code for different operating systems
and the server Master is controlling how
each of those builds operate so let's
step into a quick story of a company
that has used Jenkins very successfully
so here's a use case scenario um over
the last 10 or 15 years there has been a
significant shift within the automotive
industry where manufacturers have
shifted from creating complex Hardware
to actually creating software we've seen
that with companies such as Tesla where
they are creating software to manage
their cars we see the same thing with
companies such as General Motors with
their OnStar program and Ford just
recently have rebranded themselves as a
technology company rather than just a
automotive company what this means
though is that the software within these
cars is becoming more complex and
requires more testing to allow more
capabilities and enhancements to be
added to the core software so Bosch is a
company that specifically ran into this
problem and their challenge was that
they wanted to be able to streamline the
increasingly complex Automotive software
by adopting continuous integration and
continuous delivery best practices with
the goal of being able to delight and
exceed the customer expectations of the
end user so Bosch has actually used
Cloud bees which is the Enterprise
Jenkins environment so to be able to
reduce them the number of manual steps
such as building deploying and testing
Bosch has introduced the use of cloud
bees from Jenkins and this is part of
the Enterprise Jenkins platform it has
significantly helped improve the
efficiencies throughout the whole
software development cycle from
automation stability and transparency
because Jenkins becomes a self- auditing
environment now the results have been
tangible previously it took 3 days
before a Bild process could be done and
now it's taken that same 3-day process
and reduced it to Less Than 3 hours that
is significant large scale deployments
are now kept on track and have expert
support and there is clear visibility
and transparency across the whole
operations through using the Jenkins
tools if you aspired to become a devops
engineer simply learns professional
certificate program in cloud computing
and devops is your ideal launch pad in
partnership with E and ICT Academy I
guti enroll today and pave your way to
becoming a cloud expert implement the CI
processes over here now what is the
purpose of Jenkins here now Jenkins is
normally a kind of a CI tool which we
use for performing the build automations
and the test cases automation there it's
one of the open source tool which is
available there and one of the most
popular CI tool also available into the
market now this tool makes it easier for
the developers to integrate the changes
to the project here so we can easily
integrate the changes and whatever the
modifications we want to manage we will
be able to do that with the help of
Jenkins now Jenkins also achieves The
Continuous integration with the help of
couple of uh plugins each and every tool
which you want to integrate have its own
plugins which is is available there for
example you want to integrate Maven we
have a maven plugin in Jenkins which you
can install you can configure in that
case you will be able to use the maven
there now you can uh deploy the maven to
build tool onto the Jenkin server and
then you can prepare or you can
configure any number of Maven jobs in
case of
chenkin so uh what exactly the MAV or
the Jenkins really do is the MAV when
integrates with Jenkins through the
particular plugin so you can able to
automate the builds because for
automation the build you require some
integration with the MAV and that
integration is what we are getting from
the uh Maven plug-in so in Jenkins you
have to install the maven plugin and
once the plugin is install so what you
can do is that you can proceed with the
configurations you can proceed with the
setup and this uh particular plug-in can
help you to build out some of the Java
Base projects which is available there
in the kit repositories and once that is
done you will be able to go ahead and
you will be able to process a complete
integration of Maven within
jins right so let's see that how we can
go for the integration now I have
already installed the may1 onto the uh
Linux virtual machine uh which we are
using so using the app utility or using
the Yum utility you can actually
download the Jenkins package and the
maven package onto the uh server onto
the virtual machine and now I'm going to
proceed further with the plug-in
installation and the configuration of a
maven project so I have a GitHub
repository which is having a maven
project Maven uh uh source code and the
maviz test cases over there so let's see
let's log into the uh Jenkins and see
that how it works so this is the Jenkins
interface which we have over here now in
this one what we can do is that we can
create some Maven jobs over here and
once those jobs are created we will be
able to do a custom build onto this
Jenkins so first of all we have to
install the uh particular plugin here
for that we have to go to the manage
shenin in manage Jenkins you have the
manage plugins option there so you have
to click on that now here you will be
having different tabs like updates
available installed Advanced all these
different tabs are available there so
what you can do is that you can click on
the available one when you go to the
available tab so what will happen that
here you can actually put up that what
exactly uh plugin you want to fetch here
so I can put a plugin called
mavan now you can see that the very
first one the M integration tool is
available so I'm going to select that
particular plugin and click on download
now and install after restart now once
that is done so what will happen that
the plugin will be downloaded but in
order to reflect the changes we have to
do a couple of restart now for that you
don't have to go to the uh virtual
machine you have the option here itself
that uh will allow you to do the restart
over here when you click on this button
so you check this option say that
restart Jenkins when the installation is
done so what will happen that the
installation will be automatically
attempted whenever the uh particular
plug-in installation is completed here
so you just have to refresh the page
again and uh you will be able to see
that uh the particular Jenkins is being
processed as such
here right so you can see that the
screen is coming up that Jenkins is
restarting so it will take a couple of 5
to six seconds to do the rest start and
uh the login screen to come up again
over
there you can do the refresh also if you
feel automatically it will be reloaded
once the Jenkins is ready but sometimes
we have to refresh it so that we can get
the screen over there so once the login
is done so my Maven integration is done
so next thing which I will be doing is
that I will be creating a maven related
project so I'm going to put the admin
user and the password so whatever the
user and password you have created you
are going to to put that so that you
will be able to log to the Jenkins
portal now this is the Jenkins which is
available here so all you have to do is
that you have to click on create a new
job or new item so both the option is
pretty much same
only so here you will be able to see a
maven project here so I'm going to
select like Maven build that's the name
which I'm going to give here and the
maven project I'm going to select here
and then press
okay now here you will be providing the
first of all the a repository from which
you will be checking out the source code
now I can have a discard old builds over
here so if I feel that I want to have
like log loation so all the previous uh
builds should be deleted so I'm just
saying that dat keep build should be 10
over here and uh the number of builds
which I need to keep over here is 20 you
can adjust these settings according to
your requirement but uh over here we are
you know doing a kind of configurations
which we are trying to do a lot of
configurations and set here so these are
the uh particular settings which we are
looking forward as such over here so now
we are going to have the uh log rotation
here so we can have it like how many
days we want to keep and how many number
of builds we want to keep here so both
the values we are providing over here
and then now I'm going to put the git uh
integration here like the repo URL so I
have this repository here in which I
have the Java source code and some uh
particular uh J Test cases and all I
also have the
particular source code and it's kind of
a mov project so that's what I'm trying
to clone over here with the help of this
plugin so this plug-in will download
this repository it will clone it onto
the Jenkin server and then depending on
our integration with mavan the mavan
build will be triggered here so now I'm
going to process with the uh MAV here so
you can see here that it's saying that
uh Jenkins needs to know that where the
maven is installed because that MAV
version it needs to configure it needs
to process on that part so I'll just do
the save over here and uh or I can click
on this uh tool configuration so I'll
just save or do the apply click on this
uh tool configuration
here now here you have the options like
where you can have the jdk installation
but what happens that same chins is
running there so jdk is automatically
installed so in the tools configuration
you don't have to put the jdk
configuration but at least for the MAV
configurations you have to provide that
where exactly the MAV is available there
so I'm just saying that MAV
three I want to process and the latest M
Apache web server I want to configure
here so I just want to have like I just
want to save this settings so that it
will be automatically download the
latest version Apache 3.6.3 version
there and that same should be utilized
over here in this case now I'm just
going to the maven build uh
configuration here and click on the
configure part so these get repository
is available here and uh in the build
step it automatically builds up that uh
what Maven environment you want to
select so you see that previously since
I did not configur my MAV environment so
it was throwing an error but once I have
configured that uh I have to download it
during the build process or before the
build that utility should be downloaded
so instead of doing the physical
installation of Maven on the server what
I have chosen over here is that I have
selected the particular version like I
have selected that uh particle
3.6.3 version should be installed for
the maven purposes over here now now
once that is done I'm going to put the
particular steps over here you can have
it like clean install you can have clean
compile test clean test or test alone
you can give it's just a part of the uh
setup or the goals which you want to
configure here it by default says that
pom.xml file is the current one in the
current directory you need to refer you
need to pick on that one what it's up to
you only that how you want to configure
and how you want to process as such
these information so according to your
requirement you can say that okay I just
want to go for these particular goals
and uh you can say like save over here
the particular configuration will be
saved now you can just click on the
build now and you will be able to see
that the first of all the git clone will
happen and then the desired M executable
will be uh the build tool will be
configured and according to that it will
be processed here so you can see here
that uh the maven is uh getting
downloaded it's getting configured here
and once it's configured because I have
explained over there that 3.6.3 version
I have to select so that specific
version will be configured and will be
picked up over here now even if you
don't have the maven installed on the
physical machine on which the jenin is
running still you will be able to do the
processing using this particular
component here so you can see here that
we have some particular test cases
executed and in the end we are able to
get a particular artifacts also there
since I did not uh call upon the package
or install goal that's the reason why
the particular artifacts was not
generated v file or jar file whatever
the packaging mode is available at Palm
level but still what happens that my
test cases do gets executed and that's
what I have got over here in this case
so this is a kind of a mechanism where
we feel that how we can configure a git
repository once the git repository is
configured you are going to integrate
the maven plug-in in the maven plugin
you are going to configure in the tools
configuration that this and so and so
version I want to configure to run run
my build and once that is done after
that you just have to trigger the build
and uh click on the build now option and
once that is done you will be able to
get a particular full-fledged build or
compilation happened onto the Jenkins
and this log will give you the complete
details that what are the different
steps which has happened on this one so
let's take a little scenario of a
developer and a tester before you had
the world of Docker a developer would
actually build their code and then then
send it to the tester but then the code
wouldn't work on their system the code
doesn't work on the other system due to
the differences in computer environments
so what could be the solution to this
well you could go ahead and create a
virtual machine to be the same of the
solution in both areas we think Docker
is an even better solution so let's kind
of break out what the main big
differences are between Docker and
virtual machines as you can see between
the left and the right hand side both
look to be very similar what you'll see
however is that on the docker side what
you'll see as a big difference is that
the guest OS for each container has been
eliminated Docker is inherently more
lightweight but provides the same
functionality as a virtual machine so
let's step through some of the pros and
cons of a virtual machine versus Docker
so first of all a virtual machine
occupies a lot more memory space on the
host machine in contrast Docker occupies
significantly less memory space the boot
up time between both is very different
Docker just boots up faster the
performance of the docker environment is
actually better and more consistent than
the virtual machine Docker is also very
easy to set up and very easy to scale
the efficiencies therefore are much High
higher with a Docker environment versus
a virtual machine environment and you'll
find it is easier to Port Docker across
multiple platforms than a virtual
machine finally the space allocation
between Docker and a virtual machine is
significant when you don't have to
include the guest OS you're eliminating
a significant amount of space and the
docker environment is just inherently
smaller so after Docker as a developer
you can build out your solution and send
it to a tester and as long as we're all
running in the docker environment
everything will work just great so let's
step through what we're going to cover
in this presentation we're going to look
at the devops tools and where Docker
fits within that space we'll examine
what Docker actually is and how Docker
works and then finally we'll step
through the different components of the
docker environment so what is devops
devops is a collaboration between the
devel velopment team the operation team
allowing you to continuously deliver
Solutions and applications and services
that both delight and improve the
efficiency of your customers if you look
at the vend diagram that we have here on
the left hand side we have development
on the right hand side we have operation
and then there's a crossover in the
middle and that's where the devops team
sits if we look at the areas of
integration between both groups
developers are really interested in
planning code building and testing and
operations want to be able to
efficiently deploy operate and monitor
when you can have both groups
interacting with each other on these
seven key elements then you can have the
efficiencies of an excellent devops team
so planning and co-base we use tools
like jit and geara for building we use
gradal and Maven testing we use selenium
the integration between Dev and Ops is
through tools such as Jen
and then the deployment operation is
done with tools such as Docker and Chef
finally nagas is used to monitor the
entire environment so let's step deeper
into what Docker actually is so Docker
is a tool which is used to automate the
deployment of applications in a
lightweight container so the application
can work efficiently in different
environments now it's important to note
that the container is actually a
software package that consists of all
the dependen required to run the
application so multiple containers can
run on the same Hardware the containers
are maintained in isolated environments
they're highly productive and they're
quick and easy to configure so let's
take an example of what Docker is by
using a house that may be rented for
someone using Airbnb so in the house
there are three rooms and only one
cupboard and kitchen and the problem we
have is that none of the guests are
really ready to share the cupboard and
kitchen because every individual has a
different preference when it comes to
how the cupboard should be stocked and
how the kitchen should be used this is
very similar to how we run software
applications today each of the
applications could end up using
different Frameworks so you may have a
framework such as rails perfect and
flask and you may want to have them
running for different applications for
different situations this is where will
help you run the applications with the
suitable Frameworks so let's go back to
our ABNB example so we have three rooms
and a kitchen and cupboard how do we
resolve this issue well we put a kitchen
and cupboard in each room we can do the
same thing for computers Docker provides
the suitable Frameworks for each
different application and since every
application has a framework with a
suitable version this space could also
then be utilized for putting in software
and applications that along and since
every application has its own framework
and suitable version the area that we
had previously stored for a framework
can be used for something else now we
can create a new application in this
instance a fourth application that uses
its own resources you know what with
these kinds of abilities to be able to
free up space on the computer there no
wonder Docker is the right choice so
let's take a closer look to how Docker
actually works so when we look at Docker
and we call something Docker we're
actually referring to the base engine
which actually is installed on the host
machine that has all the different
components that run your Docker
environment and if we look at the image
on the left hand side of the screen
you'll see that Docker has a client
server relationship there's a client
installed on the hardware there is a
client that contains the docker product
and then there is a server which
controls how that Docker client is
created the communication that goes back
and forth to be able to share the
knowledge on that Docker client
relationship is done through a rest API
and this is fantastic news because that
means that you can actually interface
and program that API so we look here in
the animation we see that the docker
client is constantly communicating back
to the server information about the
infrastructure and it's using this rest
API as that Communication channel the
docker server then will check out the
requests and the interaction necessary
for it to be the docker demon which runs
on the server itself will then check out
the interaction and the necessary
operating system pieces needed to be
able to run the container okay so that's
just an overview of the docker engine
which is probably where you're going to
spend most of your time but there are
some other components that form the
infastructure for Dockers let's dig into
those a little bit deeper as well so
what we're going to do now is break out
the the four main components that
comprise of the docker environment the
four components are as follows the
docker client and server which we've
already done a deeper dive on Docker
images Docker containers and the docker
registry so if we look at the structure
that we have here on the left hand side
you see the relationship between the
docker client and the docker server and
then we have the rest API in between now
if we start digging into that rest API
particularly the relationship with the
docker Damon on the server we actually
have our other elements that form the
different components of the docker
ecosystem so the docker client is
accessed from your terminal window so if
you are using Windows this can be
Powershell on Mac it's going to be your
terminal window and it allows you to run
the docker demon and the registry
service when you have your terminal
window open so you can actually use your
terminal window to create instructions
on how to build and run run your Docker
images and containers if we look at the
images part of our registry here we
actually see that the image is really
just a template with the instructions
used for creating the containers which
you use within Docker the docker imag is
built using a file called the docker
file and then once you've created that
Docker file you'll store that image in
the docker Hub or R Street and that
allows other people to be able to access
the same structure of a Docker
environment that you've created the
syntax of creating the image is fairly
simple it's something that you'll be
able to get your arms around very
quickly and essentially what you're
doing is you're creating the option of a
new container you're identifying what
the image will look like what are the
commands that are needed and the
arguments for within those commands and
once you've done that you have a
definition for what your image will look
like so if we look here at what the
container itself looks like is that the
container is a standalone executable
package which includes applications and
their dependencies it's the instructions
for what your environment will look like
so you can be consistent in how that
environment is shared between multiple
developers testing units and other
people within your devops team now the
thing that's great about working with
Docker is that it's so lightweight that
you can actually run multiple Docker
containers in the same infrastructure
and share the same operating system this
is its strength and allows you to be
able to create those multiple
environments that you need for multiple
projects that you're working on
interestingly though within each
container that container creates an
isolated area for the applications to
run so while you can run multiple
containers in an infrastructure each of
those containers are completely isolated
they're protected so that you can
actually control how your Solutions work
there now as a team you may start off
with one or two developers on your team
but when a project starts becoming more
important and you start adding in more
people to your team you may have 15
people that are offshore you may have 10
people that are local you may have 15
Consultants that are working on your
project you have a need for each of
those developers or each person on your
team to have access to that Docker image
and to get access to that image we use a
docka registry which is an open-source
serviz service for hosting and
distributing the images that you have
defined you can also use Docker itself
as its own default registry and Docker
Hub now something that has to be bear in
mind though is that for publicly shared
images you may want to have your own
private images in which case you would
do that through your own registry so
once again public repositories can be
used to host the docket images which can
be accessed by anyone and I really
encourage you to go out to Docker and
see the other Docker images that have
been created because there may be tools
there that you can use to speed up your
own development environments now you
will also get to a point where you start
creating environments that are very
specific to the solutions that you are
building and when you get to that point
you'll likely want to create a private
repository so you're not sharing that
knowledge with the world in general now
the way in which you connect with the
docker registry is through simple pull
and push commands that you run through
terminal window to be able to get the
latest information so if you want to be
able to build your own container what
you'll start doing is using the pull
commands to actually pull the image from
the docker repository and the command
line for that is fairly simple in
terminal window you would write Docker
pull and then you put in the image name
and any tags associated with that image
and use the command pulls so in your
terminal window you would actually use a
simple line of command once you've
actually connected to your Docker
environment and that command will be
Docker pull with the image name and any
Associated tags around that image what
that will then do is pull the image from
the docker repository whether that's a
public repository or a private one now
in Reverse if you want to be able to
update the docker image with a new
information You' do a push command where
you would take the script that you've
written about the docker container that
you defined and push it to the
repository and as you can imagine the
commands for that are also fairly simple
in terminal window you would write do
push the image name any Associated tags
and then that would then push that image
to the docker repository again either a
public or a private repository so if we
recap the docker file creates a Docker
image that's using the build commands a
dog image then contains all the
information necessary for you to be able
to execute the project using the dogget
image any user can run the code in order
to create a Docker container and once a
Docker image is built it's uploaded to a
registry or to a Docker Hub where it can
be shared across your entire team and
from the docker Hub users can get access
to the docker image and build their own
new containers so let's have a look at
what we have in our current environment
so today when you actually have your
standard machine you have the
infrastructure you have the host
operating system and you have your
applications and then when you create a
virtual environment what you're actually
doing is you're actually creating
virtual machines but those virtual
machines actually are now sitting with
with in a hypervisor solution that sits
still on top of your host operating
system and infrastructure and with a
Docker engine what we're able to do is
we're able to actually reduce
significantly the different elements
that you would normally have within a
virtualized environment so we're able to
get rid of the the bins and the so we're
able to get rid of the guest OS and
we're able to eliminate the hypervisor
environment and this is really important
as we actually start working and
creating environments that are
consistent because we want to be able to
make it so it's really easy and stable
for the environment that you have within
your Dev and Ops environment now
critical is getting rid of that
hypervisor element it's just a lot of
overhead so let's have a look at a
container as an example so here we
actually have a couple of examples on
the right hand side we have different
containers we have one container running
Apache Tomcat in a with Java a second
container is running SQL server in a
microsoft.net environment third
container is running python with mySQL
these are all running just fine within
the docker engine and sitting on top of
a host OS which could be Linux it really
could be any host OS within a consistent
infrastructure and you're able to have a
solution that can be shared easily
amongst your teams so let's have a look
at an example that you'd have today if a
company is doing a traditional Java
application so you have your developers
working in Jos on his system and he's
coding away and he has to get that code
over to a tester now what will happen is
that tester will then typically in your
traditional environment then have to
install J boss on their machine and get
everything running and Co and hopefully
set up identically to the developer
chances are they probably won't have it
exactly the same but they'll try to get
it as close as possible and then at some
point you want to be able to test this
within your production environment so
you send it over to a system
administrator who would then also have
to install j boss on their environment
as well yeah this just seems to be a
whole lot of duplication so why go
through the problem of installing JBoss
three times and this is where it things
get really interesting because the
challenge you have today is that it's
very difficult to almost impossible to
have identical environment if you're
just installing software locally on
devices the developer probably got a
whole bunch of development software that
could be conflicting with the Joss
environment the tester has similar
testing software but probably doesn't
have all the development software and
certainly the system administrator won't
have all the tools that the developer
and test to have their own tools and so
what you want to be able to do is kind
of get away from The Challenge you have
of having to do local installations on
three different computers and in
addition what you see is that this uses
up a lot of effort because when you're
having to install software over and over
again you just keep repeating doing
really basic foundational challenges so
this is where Docker comes in and Docker
is the tool that allows you to be able
to share environments from one group to
another group without having to install
software locally on a device you install
all of the code into your Docker
container and simply share the container
so in this presentation we're going to
go through a few things we're going to
cover what Docker actually is and then
we're going to dig into the actual
architecture of Docker and kind of go
through what Docker container is and how
to create a Docker container and then
we'll go go at through the benefits of
using Docker containers and then the
commands and finalize everything out
with a brief demo so what is doer so
Docker is as you'd expect because all
the software that we cover in this
series is an OP Source solution and it
is a container solution that allows you
to be able to containerize all of the
necessary files and applications needed
to run the solution that you're building
so you can share it from from different
people in your team whether it's a
developer tester or system administrator
and this allows you to have a consistent
environment from one group to the next
so let's kind of dig into the
architecture so you understand why
Docker runs effectively so the docker
architecture itself is built up of uh
two key elements there is the docker
client and then there is a rest API
connection to a Docker Damon which
actually hosts the entire environment
within the docker host and the docker
Damon you have your different containers
and each one has a link to a Docker
registry the docker client itself is a
rest service so as you'd expect a rest
API and that sends command line to the
docker Damon through a terminal window
or command line interface window and
we'll go through some of these demos
later on so you can actually see how you
can actually interact with Docker the
docker Damon then checks the request
against um the docker components and
then performs the service that you're
requesting now the docket image itself
all it really is a collection of
instructions used to create a container
and again this is consistent with all
the devops tools that we have the devops
tools that we're looking to use
throughout this series of videos are all
environments that can be scripted and
this is really important because it
allows you to be able to duplicate and
scale the environments that you want to
be able to build very quickly and
effectively the actual container itself
has all of the the applications and the
dependencies of those applications in
one package you kind of think of it as a
really effective and efficient zip file
it's a little bit more than that but
it's one file that actually has
everything you need to be able to run
all of your Solutions the actual Docker
rry itself is an environment for being
able to host and distribute different
Docker images among your team so say for
instance you had a team of developers
that were working on multiple different
solutions so say you have a team of
developers and you have 50 developers
and they're working on five different
applications you can actually have the
applications themselves the containers
shared in the docker registry so each of
those teams at any time check out and
have the latest container of that latest
image of the code that you're working on
so let's dig into what actually is in a
container so the important part of a
duckin container is that it has
everything you need to to be able to run
the application it's like a virtualized
environment it has all your Frameworks
and your libraries and it allows the
teams to be able to build out and run
exactly the right environment that the
developer intended what's interesting
though is the actual applications then
will run in isolation so they're not
impacting other applications that's
using dependencies on other libraries or
files outside of the container because
of the architecture it really uses a lot
less space and because it's using less
space it's a much more lightweight
architecture so the files the actual
folder itself is much smaller it's very
secure highly portable and the boot up
time is incredibly fast so let's
actually get into how you would actually
create a Docker container so the docker
container itself is actually built
through command line and it's built of a
file and Docker image so the actual um
Docker file is a text file that contains
all the instructions that you would need
to be able to create that Docker image
and then we'll actually then create all
of the project code with inside of that
image then the image becomes the item
that you would share through the docker
registry you would then use the command
line and we'll do this later on select
docket run and then the name of the
image to be able to easily and
effectively run that image locally and
again once you've created the dock image
you can store that in the docker
registry making it available to any
anybody within your network so something
to bear in mind is that Docker itself
has its own registry called dockerhub
and that is a public registry so you can
actually go out and see other doc images
that have been created and access those
images as your own company you may want
to have your own private um repository
so you want to be able to go ahead and
either do that locally through your own
uh repository or you can actually get a
licensed version of Dr Hub where you can
actually then share those files now
something that's also very interesting
to know is that you can have multiple
versions of a docket image so if you
have a different version control
different release versions and you want
to be able to test and write code for
those different release versions because
you may have different setups you can
certainly do that within your Docker
registry environment okay so let's go
ahead and we're going to create a Docker
image using some of our basic Docker
commands and so there are essentially
really you know kind of just two
commands that you're going to be looking
for one is a build command and another
one is to actually put it into your
registry which is a push command so if
you want to get a image from a Docker
registry then you want to use the pull
command and a pull command simply pulls
the image from the registry um in this
example using uh NG as our registry and
we can actually then pull the image down
to our test environment on our local
machine so we're actually running the
container within our docker application
on um a local machine we're able to then
have the image run exactly as it would
in production and then you can actually
use the Run command to actually use the
docker image on your local machine so
just a you know a few interesting
tidbits about the docker container once
the container is created a new layer is
formed on top of the dock image layers
called the container layer each
container has a separate read WR
container layer and any changes made in
that docking container is then reflected
upon that particular container layer
and if you want to delete the container
layer the container layer also gets
deleted as well so you know why would
using Docker and containers be of
benefit to you well you know some of the
things that are useful is that
containers have no external dependencies
for the applications they run once you
actually have the container running
locally it has everything it needs to be
able to run the application so there's
no having to install additional pieces
of software such as the example we gave
with J boss at at the beginning of the
presentation now the containers are
really lightweight so it makes it very
easy to share the containers amongst
your teams whether it's a developer
whether it's a tester whether it's
somebody on your operations environment
it's really easy to share those
containers amongst your entire team
different data volumes can be easily
reused and shared among multiple
containers and again this is another big
benefit and this is a reflection of the
lightweight nature of your contain
containers the container itself also
runs in isolation um which means that it
is not impacted by any dependencies you
may have on your own local environment
so it's a completely sandboxed
environment so some of the questions you
might ask is you know can you run
multiple containers together without the
need to start each one individually and
you know what yes you can with Dock and
compose Dock and compose allows you to
run multiple contain containers in a
single service and again this is a
reflection on the lightweight nature of
containers within the docker environment
so we're going to end our presentation
by looking at some of the basic commands
that you'd have within Docker so we have
here on the left hand side we have a
Docker container and then the command
for each item we're actually going to go
ahead and use some of these commands in
the demo that we're going to do after
this presentation you'll see that in a
moment but just you know some of the
basic commands we have are committing
the docket image into the Container kill
is a you know standard kill command to
you know terminate one or more of the
running containers so they stop working
then restart those containers but
suddenly you can look at all the image
all of the commands here and try them
out for yourself so we're going to go
ahead and start a demo of how to use the
basic commands to run Docker so to do
this we're going to open up terminal
window or command line depending whether
you're running Linux PC or Mac and we're
going to go ahead and the first thing we
want to do is see what our Docker image
lists are so we can go pseudo Docker
images and this will give us well first
we're entering our password so let's go
enter that in and this will now giv us a
list of our Docker images and here are
the docker images have already been
created in the system and we can
actually go ahead and actually see the
processes that are actually running so
I'm going to go ahead and open up this
window a little bit more but this will
show you the actual processes and the
containers that we actually have and so
on the far left hand side you see under
names we have learn simply learn bore
cool these are all just different ones
that we've been working on so let's go
ahead and create a Docker image so we
going to do
pseudo
Docker
run-
d-p
0.0.0.0 go call on 80 callon
80 obuntu and this will allow us to go
ahead and run
anonto image and this around the latest
image and what we have here is a hash
number and this hash number is a unique
name that defines the container that
we've just created and we can go ahead
and we can check to make sure that the
container actually is present so we're
going to do so pseudo doer. PS and this
actually show us on there so it's not in
a running state right now but that
doesn't mean that we don't have it so
let's list out all the containers that
are both running and in the exit state
so it's do sucker ps- a and this lists
all the containers that I have running
on my
machine and this shows all the ones that
have been in the running State and in
the exit State and here we see one that
we just created about a minute ago and
it's called
learn and and these are all running
Ubuntu and this is the one that we had
created just a few seconds
ago let's open it up and there we go so
let's change that to that new Doc
container to a running state so scroll
down and we're going to type PSE
sudo
[Music]
Docker
run
dasit Das Das
name
my um so this is going to be the new
container name it's going to be my
Docker so this is how we name our Docker
environment and we'll put in the image
name which is
ub2 and dash bin Dash
bash and it's now in our root and we'll
exit out of
that so so now we're going to go ahead
and start the new my Docker container so
pseudo
Docker
start
my and we'll get the container image
which will be my Docker my
Docker return and that started that
Docker image and let's go ahead and
check against the other running Docker
images to make sure it's running
correctly so pseudo Docker
PS and there we are underneath on the
right hand side you actually see my
Docker along with the other Docker
images that we created and it's been
running for 13 seconds quite fast so we
want to rename the container let's use
the command pseudo
Docker rename we can take another Docker
image this grab this one and we'll put
it in rename and we'll rename and we'll
put in the old container name which is
image and then we'll put in the new
container name and let's call it
purple so now the container image that
had previously been called image is now
called Purple so we do pseudo Docker
PS to list all of our Docker
images and if we scroll up and there
there we go purple how easy is that to
rename an
image and we can go ahead and use this
command if we want to stop a container
so we're going to write pseudo
Docker
stop and then we have to put it in the
container
name and we'll put in my Docker the
container that we originally
created and that image has now
stopped and let's go ahead and prove
that we're going to list out all the
docket images and what you see is that
it's not listed in the active images
it's uh not on the list on the far right
hand side but if we go ahead and we can
list out all of the dock images so you
actually see it's still there as an
image it's just not in an active stage
it's what's known as in an exit state so
here we
go and there's my Docker it's in an
exited state so that happened 27 seconds
ago so if we want to to remove a
container we can use the following
command so P sudo
Docker
RM we
remove my Docker and that will remove it
from the exited
State and we're going to go ahead and
we're going to double check
that and
yep
yep it's not not listed there under exit
State
anymore it's
gone there we go there that's where it
used to be all right let's go
back so if we want to exit a container
in the running state so we do pseudo
kill and then the name of the
container I think one of them is called
yellow let's just check and see if
that's going to kill
it oops no I guess we don't have one
called yellow so let's find out name of
container that we actually have so PSE
sudo Docker kill oh we're going to list
out the ones are running oh okay there
we go yellow isn't in that list so let's
take I know let's take simply learn and
so we can actually go ahead and let's
write pseudo Docker kill simply learn
and that will actually kill an active
Docker
container boom there we
go and we list out all the active
containers you can actually see now
that's the simply learn container is not
active
anymore and these are all the basic
commands for dock
container we're going to break up this
presentation into four key areas we're
going to talk about life before
kubernetes which some of you are
probably experiencing right now what is
kubernetes the benefits that kubernetes
brings to you particularly if you are
using containers in a Dev Ops
environment and then finally we're going
to break down the architecture and
working infrastructure for kubernetes so
you understand what's happening and why
the actions are happening the way that
they are so let's jump into our first
section of life before kubernetes so the
way that you have done work in the past
or you may be doing work right now is
really building out and deploying
Solutions into two distinct areas one is
a traditional deployment where you're
pushing out code to physical servers in
a Data Center and you're managing the
operating system and the code that's
actually running on each of those
servers another environment that you may
potentially be using is deploying code
out to Virtual machines so let's go
through and look at the two different
types of deployment that you may be
experiencing when you have applications
running on multiple machines you run
into the potential risk that the setup
and configuration of each of those
machines isn't going to be consistent
and your code isn't going to work
effectively and there may be issues with
uptime and errors within the
infrastructure of your entire
environment there's going to be problems
with resource allocation and you're
going to have eror issues where
applications may be running effectively
and not not effectively and not load
balanced um effectively across the
environment the problem that you have
with this kind of infrastructure is that
it gets very expensive uh you can only
install one piece of software one
service on one piece of Hardware so your
Hardware is being massively
underutilized this is where virtual
machines have become really popular with
a virtual machine you're able to have
better resource utilization and
scalability at much less cost and this
allows you to be able to run multiple
virtual machines maches on a single
piece of Hardware the problem is is that
VMS or for virtual machines are not
perfect either some of the challenges
you run with VMS is that the actual
hardware and software need needed to
manage the VM environment can be
expensive there are security risks with
virtual with VMS there are security
risks with VMS there have been data
breaches recorded about solutions that
run in virtualized environments you also
run into an issue of availability and
this is largely because you can only
have a finite number of virtual machines
running on a piece of hardware and this
results in limitations and restrictions
in the types of environment you want to
be running and then finally setting up
and managing a virtualized environment
is time consuming uh it can take a lot
of time and it can also get very
expensive so how about kubernetes well
kubernetes is a tool that allows you to
manage containerized deployment of
solutions and inherently kubernetes is a
tool that is really a Next Level
maturity of deployment so if you can
think of your maturity curve as
deploying code in directly to Hardware
in a Data Center and then deploying your
solutions to Virtual machines the next
evolution of that deployment is to use
containers and kubernetes so let's kind
of go through and look at the
differences between a virtual machine
and kubernetes and we've got a few here
that we want to highlight and you'll get
an understanding of what the differences
are between the two so first of all with
virtual machines there is inherently
security risks and what you'll find as
we get dig through the architecture
later in the presentation is that
kubernetes is inherently secure um and
this is largely because of the Legacy
code uh the legacy of kubernetes and
where it came from we'll talk about that
in just a moment but kubernetes is
inherent secure uh virtual machines are
not easily portable now with that said
they they are technically portable
they're just not very easily portable
whereas with kubernetes it's working
with Docker container Solutions it is
extremely portable that means that you
can actually spin up and spin down and
manage your infrastructure exactly the
way that you want it to be managed and
scale it on the demands of the customers
as they're coming in to use the solution
from a timec consuming point of view
kubernetes is much less time consuming
than with a virtual machine a few other
areas that we want to kind of um
highlight differences virtual machines
use much less isolation when than
building out the encapsulated
environment than kubernetes does uh for
instance with a virtual machine you have
to run hypervisor on top of the OS and
hardware and then inside of the virtual
machine you also have to have the
operating system as well whereas in
contrast on a kubernetes environment
because it's leveraging darker container
and or container like Technologies it
only has to have the OS and the hardware
and then inside of each container it
doesn't need to have that additional OS
layer it's able to inherit what it needs
to be able to run the application this
makes the whole solution much more
flexible and allows you to run many more
containers on a piece of Hardware than
versus running virtual machines on a
single piece of high rare So as we um
highlighted here VMS are not as portable
as kubernetes and kubernetes is portable
directly related to the use of
containerization and because kubernetes
is built on top of containers it is much
less time consuming because you can
actually script and and automatically
allocate resource to nodes within your
kubernetes environment this allows the
infrastructure to run much more
effectively and much more efficiently so
this is why if we look at our evolution
of the land of time before kubernetes
while we are running into a solution
where kubernetes had to come about
because the demand for having more
highly scalable solutions that are more
efficient was just really a natural
evolution of this software deployment
model that started with pushing out code
to physical hardware and then pushing
code out to Virtual machines and then
needing to have a solution much more
sophisticated kubernetes would have come
about at some point in time I'm just
really glad it came back when it did so
what is kubet this let's dig into the
history of kubernetes and how it came
about so in essence kubernetes is an
open-source platform that is allows you
to manage and deploy and maintain groups
of containers and a container is
something like Docker and if you're
developing code you're probably already
using Docker today consider kubernetes
as the tool that manages multiple Docker
environments together now we talk a lot
about doer as a container solution with
kubernetes the real is is that
kubernetes can actually use other
container tools out there but Docker
just simply is the most popular
container out there both these tools are
open source that's why they're so
popular and they just allow you to be
able to have flexibility in being able
to scale up your Solutions and they were
designed for the postd digital world
that we live and exist in today so a
little bit of background a little bit of
trivia around kubernetes so kubernetes
was originally a successor to a project
at Google and the original project was
Google Bor um Google Bor it does exactly
what kubernetes done does today but
kubernetes was Rewritten from the ground
up and then released as an open-source
project in 2014 so that people outside
of Google could take advantage of the
power of kubernetes containerization
management tools and today it is managed
by the cloud native Computing foundation
and there are many many companies that
support to manage kubernetes so for
instance if you're signing up for
Microsoft Azure AWS Google Cloud all of
them will leverage kubernetes and it's
just become the the de facto tool for
managing large groups of containers so
let's kind of Step through some of the
key benefits that you'd experience from
kubernetes and so we have nine key
benefits and the first it is highly
portable and is 100% open source code
and this means that you can actually go
ahead and contribute to this code
project if you want want to through
GitHub uh the ability to scale up the
solution is incredible um what's um the
the history of kubernetes being part of
a Google project for managing the Google
network and infrastructure uh kind of
really sets a groundwork for having a
solution that is highly scalable the out
of the high scalability also comes the
need for high availability and this is
the desire to be able to have a highly
efficient and highly energ environment
that also you can really rely on so if
you're building out a kubernetes
management um environment you know that
it's going to be um available for the
solutions that you're maintaining and
it's really designed for deployment so
you can script out the environment and
actually have it as part of your devops
model so you can scale up and meet the
demands of your customer then what
you'll find is that the um load
balancing is extremely efficient and it
allows you to distribute the load
efficiently across your entire network
so your network remains stable and then
also the tool um allows you to U manage
the orchestration of your storage so you
can have local storage such as an SSD on
the hardware that the kubernetes is M
maintaining or if the kubernetes
environment is pulling storage from a
public Cloud such as Azure or AWS you
can actually go ahead and make that
available to your entire system and you
can inherit the security that goes back
and forth between the cloud environments
and one of the things you'll find
consistent with kubernetes is that it is
designed for a cloud first environment
um kubernetes as well is that it's it's
really a self-healing environment so if
something happens or something fails U
kubernetes will detect that failure and
then either restart the process kill the
process or replace it and then because
of that you also have automated roll
outs and roll backs uh in case you need
to be able to manage the state of the
environment and then finally uh you have
automatic bin packaging so you can
actually specify the compute power
that's being used from CPU and RAM for
each container so this dig into the
final area which is the actual
kubernetes architecture and we're going
to cover this at a high level there's
actually another video uh that you can
that simply learn has developed which
digs deeper into the kubernetes
architecture and so the kubernetes
architecture is a clust based
architecture and it's really about two
key areas you have the kubernetes master
which
controls um um all of the activities
within your entire kubernetes
infrastructure and then you have nodes
um that actually are running on Linux
machines um out that are controlled by
the master so let's kind of go through
some of these um areas so if we look at
the kubernetes master uh to begin with
um then we'll start with accet is this
is a tool that allows for the
configuration of information and the
management of nodes Within in your
cluster and one of the key features that
you'll find with all of the tools that
are managed within either a the master
environment or within a node is that
they are all accessible via the API
server um and what's interesting about
the API server is that it's a restful
based infrastructure which means that
you can actually secure each connection
with SSL um and other um security models
to ensure that your entire
infrastructure and the communication
going back and forth across your
infrastructure is tightly secured
scheduler goes ahead and actually as
you'd expect actually um manages the
schedule of activities within the actual
cluster and then you have the controller
and the controller is a Damon server
that actually manages and pushes out the
instructions to all of your nodes so uh
the other tools really are the uh the
infrastructure and you can consider them
the administration site um of the master
whereas controller is the management it
actually pushes out all of the controls
via the API server so so let's actually
dig into um one of the actual nodes
themselves and there are three key areas
of the nodes one is the do environment
which actually helps and manage and
maintain the container that's actually
inside um of the node and then you have
the kuet which is responsible for
information that goes back and forth and
it's going to do most of the
conversation with the API server on the
actual health of that node and then you
have the actual kubernetes proxy which
actually runs the services actually
inside of the node so as you see all of
these infrastructures are extremely
lightweight and designed to be very
efficient and very available for your
infrastructure and so here's a quick
recap of the different tools that are
available and it really breaks down into
two key areas you have your kubernetes
master and the kubernetes node uh the
kubernetes master has the instructions
of what's going to happen within your
kubernetes infrastructure and then it's
going to push out those infrastructure
to an indefinite number of nodes that
will allow you to be able to scale up
and scale down your solution in a
dynamic way let's have an overview of
the cetes architecture so kubernetes is
really broken up into three key areas
you have your workstation where you
develop your commands and you push out
those commands to your master and the
master is comprised of um four key areas
um which essentially control all of your
nodes and and the node contains multiple
pods and each pod has your Docker
container built into it so consider that
you could have really almost an infinite
number of PODS sorry infinite number of
nodes being managed by the master
environment so you have your cluster
which is a collection of servers that
maintain the Ava availability and the
compute power such as RAM CPU and dis
utilization um you have the master which
is really components that control and
schedule uh the activities of your
network and then you have the node which
actually hosts the actual Docker virtual
machine itself um and be able to
actually control and communicate back to
the master the health of that pod and
we'll get into more detail in the
architecture later in the presentation
so you know you keep hearing me talk
about um containers but they really are
the center of the work that you're doing
with kubernetes and the concept around
kubernetes and containers is really just
a natural evolution of where we've been
with internet and digital Technologies
over the last uh 10 15 years so before
kubernetes um you had tools where you
either running virtual machines or
you're running uh data centers that had
to maintain and and manage and notify
you of any interruptions in your network
kubernetes is the tool that actually
comes in and helps address those
interruptions and manages them for you
so the solution to this is the use of
containers so uh you can think of
containers as that Natural Evolution
from you know uh 15 20 years ago you
would have written your code and posted
it to a data center uh more recently you
probably post your code to a virtual
machine and then move the virtual
machine and now you actually just work
directly into a container and everything
is self-contained and can be pushed out
to your um environments and the thing
that's great about containers they're
they're isolated environments very easy
for developers to work in them but it's
also really easy for uh operations teams
to be able to move a container into
production so let's kind of step and and
look at a competing product to
kubernetes which is Docker swarm now one
of the things that we have to remember
is that Docker um containers which are
extremely popular um built by the
company Docker and made open source and
Docker actually has other products one
of those other products uh is Docker
swarm and Docker swarm is a tool that
allows you to be able to manage multiple
containers uh so if we look at some of
the uh the benefits of using Docker
swarm versus kubernetes and one of the
things that you'll find is that both
tools have strengths and weaknesses but
it's really good that they're both out
there U because it helps keep that it
really kind of justifies uh the
importance of having these kind of tools
so kubernetes was designed originally
from the ground up to be autoscaling
whereas Docker swarm isn't the load
balancing is automatic on Docker swarm
whereas with kubernetes you have to
manually configure load balancing across
your nodes the installation for Dr swarm
is really fast and easy I mean you can
be up and running within minutes
kubernetes takes a little bit more time
is a little bit more complicated
eventually you'll get there um I mean
it's not like it's going to take you
days and weeks but it's it is a tool
that's a when you compare the two dror
swarm's much easier to get up and
running now what's interesting is that
kubernetes is incredibly scalable and
it's you know that's its real strength
is its ability to have strong clusters
whereas with doer swarm it's cluster
strength isn't um as strong when
compared to kubernetes now you compare
it to anything else on the market it's
really good um so this is kind of a
splitting hairs kind of comparison um
but kubernetes really does have the
advantage here if you're looking at the
two compared to each other for
scalability I mean kubernetes was
designed for by Google to scale up and
support uh Google Cloud Network
infrastructure they uh both allow you to
be able to share um uh storage volumes
with do you can actually do it um with
um any container U with that is managed
by the docker swarm whereas with
kubernetes it manages the storage with
the Pod and a pod can have multiple
containers within it but you can't take
it down to the level of the container
interestingly uh kubernetes does have a
graphical user interface um for being
able to control and manage uh the
environment the reality however is that
you're likely to be using terminal uh to
actually make the controls and Commands
to control your um either do swarm or
kubernetes environment um it's great
that it has a a gooey and to get you
started but once you're up and running
you're going to be using terminal window
for those fast quick administrative
controls that you need to make so let's
look at the hardware components for uh
kubernetes so U what's interesting is
that kubernetes is extremely light of
all the systems that we're looking at
it's extremely lightweight um it's
allows you to have if you compare it to
like a virtual machine which is very
heavy you know um cetes is extremely
lightweight and how uses any resources
at all interesting enough though is that
if you are looking at the usage of CPU
it's it's better to actually take it for
um uh the cluster as a whole rather than
individual nodes U because uh the nodes
will actually combine together to give
you that whole compute power again this
is why kubernetes works really well in
the cloud where you can do that kind of
activity rather than if you're running
in your own data center um so you can
have persistent volumes um such as a
local SSD um or you can actually attach
to a cloud data storage again kubernetes
is really designed for the cloud I would
encourage you to use cloud storage
wherever possible rather than relying on
physical storage uh the reason being is
that if you connect to cloud storage and
you need to flex your your storage the
cloud will do that for you I mean that's
just an inherent part of why you would
have cloud storage whereas if you're
connecting to physical storage you're
always restricted to the limitations of
the physical Hardware so let's um kind
of pivot and look at the software
components as compared to the hardware
components so the uh main part of the um
components is the actual container and
all of the software running in the
container runs on Linux so if you um
have Docker installed as a developer on
your machine it's actually running
inside of Linux um that's what makes it
so lightweight and really one of the
things that you'll find is that most
data centers and Cloud providers now are
running predominantly on Linux inside of
the the the container itself is then
managed inside of a pod and a pod is
really just a group of containers
bundled together and um the kubernetes
scheduler and proxy server then actually
manage what um how the pods are actually
pushed out uh into your kubernetes
environment the the pods themselves can
actually then share resources for both
networking and storage so pods aren't um
pushed out manually they're actually um
managed through a layer of abstraction
and part of their deployment and and
this is the strength of kubernetes you
you you um Define your um infrastructure
and then kubernetes will then manage it
for you and there isn't that problem of
um manual management of PODS uh if you
have to manage the deployment of them
you that's simply taken away and it's
completely automated and the the final
area of software Services is on Ingress
and this is really the secure way of
being able to have communication from
outside of the cluster and passing of
information in into that cluster um and
again this is done securely through SSL
layers and allows you to ensure that
security is at the center of the work
that you have within your kubernetes
environment so let's dive now into the
actual architecture before we start
looking at a use case of how kubernetes
is being employed so kubernetes again is
um we looked at this uh diagram at the
beginning of the presentation and there
are really three key areas there's the
workstation where you develop your
commands and then you have your master
environment which uh controls the uh
scheduling the communication um and the
actual um commands that you have created
and pushes those out and manages the
health of your entire node Network and
each node has uh various pods so we like
break this down uh so the master node is
the most vital component um with the
master uh you have 4 key controls you
have Etc controller manager schedule and
API server the clust store Etc this
actually manages the details and values
that you've developed on your local
workstation um and then we work with the
out the control schedule and API server
to communicate that out those
instructions of how your infrastructure
should look like to your entire network
uh the control manager is really an API
server and again um this is all about
security so we use restful apis um which
can be packaged in SSL to communicate
back and forth um across your uh part
and the master and indeed the services
within each of them as well uh so at
every single layer of extraction uh the
communication is secure uh the schedule
as the name would imply really schedules
um when tasks get sent out to the actual
nodes themselves the nodes themselves
are are dumb nodes they just have uh the
applications um running on them the
master in the is really doing all of the
work uh to make sure that your entire
network is running um efficiently and
then you have the API server which has
your rest commands and the communication
um back and forth across your network
that is secure and efficient so your
node environment is where all the work
that you do with your containers gets
pushed out too so um a a work is really
a it's a combination of containers and
each container will then logically run
together on that node so you'd have a
collection of containers uh on a node
that all make logical sense to have
together uh within each node um you have
a Docker and this is your isolated
environment for running your container
uh you have your cuet which is a service
for conveying information back and forth
um to the service about the actual
health of the kubernetes node itself and
then finally you have the proxy server
and the proxy server is able to manage
the nodes the volumes the the creation
of new containers um and actually helps
pass the communic the the health of the
container back up to the master to see
whether or not the container should be
either killed stop started or um updated
so finally let's look at see where
kubernetes is being used by other
companies so you know kubernetes is
being used by a lot of companies and
they're really using it to help manage
complex existing systems so that they
can have uh greater performance and with
the end goal of being able to Delight
the customer increase value to the
customer and hence increase value and
revenue into the organization so example
of this is a company called Black Rock
uh where they actually went through the
process of implementing KU etes uh so
they could so Black Rock had a challenge
where they needed to be able to have
much more Dynamic access to their
resources uh they were running complex
installations on people's desktops and
it was just really really difficult to
be able to manage their entire
infrastructure and so they actually then
um pivoted to using cetes and this
allowed them to be able to be much more
scalable and expansive in the the
management of their uh infrastructure
and as you can imagine kubernetes was
then hooked into their entire existing
system and has really become a key part
of the success that Black Rock is now
experiencing of a very stable
infrastructure um and the bottom line is
that U Black Rock is now able to have
confidence in their infrastructure and
be able to give that confidence um as
back to their customers through the
implementation and more rapid deployment
of additional features if you aspired to
become a devops engineer simply learn
professional certificate program in
cloud computing and devops is your ideal
launch pad in partnership with E and ICT
Academy i guti and r today and pave your
way to becoming a cloud expert going to
be six things that we're going to step
through the first is really looking at
what is the world that many of you
already work in today how do you
administer your networks and your
systems today then we're going to look
at how Chef can be the tool that really
helps improve the efficiencies of
creating a consistent distant operations
environment we're going to look at the
tools and the components and the
architecture that takes to construct
chef and then finally we're going to
take a use case of one of the many
companies that is using Chef today to
improve the efficiency of their
operations environment so let's take a
step back at what life was like before
Chef this may well be the life that
you're experiencing right now so as a
system administrator typically the way
that you work is that you are
responsible for the uptime of all of the
systems within your network and you may
have many many systems that you are
responsible for now if one system goes
down that's not a problem because as a
system administrator you can get
notifications and you can jump right
onto the problem and solve it when
things start getting really difficult
however is when you have multiple
systems and you are not able as a single
person to get to all those systems and
Sol them as quickly as possible the
problem that you start having is that
your environments are not in sync if
only this could all be automated well
this is where Chef comes to the rescue
for you so let me take some time and
introduce you to the concepts behind
chef and why it's a tool that you would
probably want to be using in your
environment so what actually is Chef so
Chef is a way in which you can use code
to fix your physical Hardware
environment and so what does that look
like well the way that we write out
scripts in Chef is that you can actually
code your entire environment so you
don't have to actually be managing your
environment on a hardware by Hardware
basis but actually use scripts called
recipes that will actually manage the
environment for you so Chef will
actually ensure that every system is
automatically set to the right states
that meet the requirements that you have
defined in your code so you don't have
any issues where Hardware starts to fail
because it references the code to reset
the state within that environment and
that makes you a happy system
administrator so let's go through the
mechanics of how this actually works
with Chef so Chef is ultimately an
automation tool that converts
infrastructure to code and you'll often
hear the term infrastructure as code or
IAC and this really starts as a output
of the work that Chef has done cuz you
take the policies that you as an
organization have created and you
convert that into a scripted language
that can then be implemented
automatically this enables Chef to be
able to manage multiple systems very
very easily and considering how broad
and deep our systems become it's very
easy to see how this can help you as a
system administrator the code is then
tested continuously and deployed by Chef
so that you're able to guarantee that
your standards and appropriate States
for your hardware and operating
environment are always maintained so
let's step through the different
components that are available in Chef so
the first one is actually creating uh
the actual recipes for Chef so you
actually work on these on a workstation
and you'll write out your code which is
referred to as a recipe and that recipe
will be written using a language called
Ruby good thing for you is that there
are lots of examples of Ruby recipes
that have been created by the
open-source Community a lot of examples
knife is the command tool that you use
that communicates between the recipes
and the server so your recipe is the
instruction and knife is the tool that
makes that instruction work and sets the
appropriate State on the server
environment when you create more than
one recipe you start creating a
collection those collections are
referred to as cookbooks and you can
make take an environment such as a large
Network where you have multiple codes
potentially hundreds of servers that
have to all have a consistent and equal
State environment and you would put that
cookbook and use that cookbook to ensure
with Chef that the state of each of
those noes is consistent and then ohigh
fetches the current state of your noes
no and then the chef client configures
the nodes as defined within your
cookbook so let's step through the
architecture and working environment of
Chef so for an administrator a systems
administrator you only have to now work
from your workstation and from that
workstation you can then configure all
the nodes of your environment you can
use uh different recipes that you create
and those recipes can be compiled into a
cookbook that can be then applied to to
a node the thing that's of value is that
as you change the environment As you
move and mature as an organization and
your nodes need to mature consistently
and quickly with your organization all
you have to do is roll out a different
recipe to your nodes and then the nodes
will use the tools within Chef to make
the updates so you can create a second
recipe or even a third recipe to be
deployed out to your server environment
knife is going to be the tool that does
the hard work of doing the updates on
the server with the server you have your
collection of recipes in a single
cookbook however understanding the state
of the individual nodes is important to
ensure that the information is broadcast
from the server with the instructions on
how to set up the network within those
nodes in this instance we use ohigh and
ohigh will fetch the state of the nodes
and send that back to to the server and
get the information from The cookbook
using the chef client in your Chef node
Network there is the potential at times
that maybe one of your node fails if
there are two failed nodes then a
request is sent back to the server and
information on the latest node structure
that is identified and defined in the
cookbook will be sent to the failed
nodes if that fails to reset the nodes
then the station is notified and you as
the administrator will receive a
notification to manually reset those
nodes it should be noted that this is a
very rare occasion within the chef
network setup so let's step through a
use case and this instance we're going
to look at holum and how they used Chef
so um holum is a bank and it's the
largest bank in Israel and they have a
mixture of Linux and windows servers
that they have to maintain and as you
can imagine this requires a lot of
constant configuration and work and this
has led to issues in the past so the
challenge that the team polium faced was
creating and hardening software that ran
the major tasks and was repeatable on
those servers that it didn't matter who
the people were performing the jobs that
there was consistency in the work that
was being done and address the problem
that the servers including the hardware
on the servers was not consistent
throughout the organization Chef
addresses those specific problems and
became the go to product for holum they
were able to write the recipes and the
cookbooks that could be deployed out to
the network effectively and it didn't
matter what the system was the recipes
reflected the standards that the system
administration team put together and
they were able to ensure and test each
script and use standard testing tools
for those scripts it didn't matter what
kind of environment that holum had if
certain ports need to be closed or if a
firewall needs to be uh installed or
modified or if custom strong passwords
had to be created all of this could be
done using the recipes that Chef offers
and once those recipes had been
finalized they could be packaged into a
common cookbook that could then be
deployed to the entire network and sure
a consistency in results for the company
and also it didn't matter whether the
cookbooks were being deployed to Linux
or Windows machine because the scripts
were being put together in the recipes
and were written in Ruby you could
actually go through and update and
modify those scripts depending on the
environment so if you needed to make a
modification for Linux you could make
those modifications so they were
consistent across all the Linux servers
or all the windows servers and this
really drove in the ability to harden
and secure the environment in the entire
network within a matter of minutes using
Chef versus the days and weeks that it
previously took so if we take a before
and after scenario using uh Chef before
Chef tasks were repeated multiple times
and it was just hard to keep track of
all the people doing the work and and
the reality is because so many people
were touching all the different systems
specific standards the banks had
established were simply not being met
after Chef all of the tasks could be
scripted using Ruby and using the recipe
and cookbook model that Chef has created
the chef tool was able to deploy out to
the entire network specific cookbooks
that provided a consistent experience
and consistent setup and operation
environment for all the hardware where
the end result is that the system
administration team could guarantee that
the standards of the bank were being met
cons puppet so in this session what
we're going to do is we're going to
cover what and why you would use puppet
what are the different elements and
components of puppet and how does it
actually work and then we'll look into
the companies that are adopting puppet
and what are the advantages that they
have now received by having puppet
within their organization and finally
we'll wrap things up by reviewing how
you can write a manifest in puppet so
let's get started so why puppet so here
is a scenario that as an administrator
you may already be familiar with you as
an administrator have multiple servers
that you have to work with and manage so
what happens when a server goes down
it's not a problem you can jump onto
that server and you can fix it but what
if the scenario changes and you have
multiple servers going down so here is
where puppet shows its strength with
puppet all you have to do is write a
simple script that can be written with
Ruby and write out and deploy to the
servers your settings for each of those
servers the code gets pushed out to the
servers that are having problems and
then you can choose to either roll back
to those servers to their previous
working States or set them to a new
state and do all of this in a matter of
seconds and it doesn't matter how large
your server environment is you can reach
to all all of these servers your
environment is secure you're able to
deploy your software and you're able to
do this all through infrastructure as
code which is the advanced Dev Ops model
for building out Solutions so let's dig
deeper into what puppet actually is so
puppet is a configuration management
tool maybe similar tools like Chef that
you may already be familiar with it
ensures that all your systems are
configured to a desired and predictable
State pu can also be used used as a
deployment tool for software
automatically you can deploy your
software to all of your systems or to
specific systems and this is all done
with code this means you can test the
environment and you can have a guarantee
that the environment you want is written
and deployed accurately so let's go
through those components of puppet so
here we have a breakdown of the puppet
environment and on the top we have the
main server environment and then below
that we have the client environment that
would be installed on each of the
servers that would be running within
your network so if we look at the top
part of the screen we have here our
puppet master store which has and
contains our main configuration files
and those are comprised of manifests
that are actual codes for configuring
the clients we have templates that
combine our codes together to render a
final document and you have files that
would be deployed as content that could
be potentially downloaded by the clients
wrapping this all together is a module
of manifest templates and files you
would apply a certificate authority to
sign the actual documents so that the
clients actually know that they're
receiving the appropriate and authorized
modules outside of the master server
where You' create your manifest
templates and files you would have
public client is a piece of software
that is used to configure a specific
machine there are two parts to the c one
is the agent that con L interacts with
the master server to ensure that the
certificates are being updated
appropriately and then you have the fact
that the current state of the client
that is used and communicated back to
through the agent so let's step through
the workings of puppet so the puppet
environment is a Master Slave
architecture the clients themselves are
distributed across your network and they
are constantly communicating back to a
Master server environment where you have
your puppet modules the client agent
sends a certificate with the ID of that
server back to the master and then the
master will then sign that certificate
and send it back to the client and this
authentication allows for a secure and
verifiable communication between client
and master the factor then collects the
state of the client and sends that to
the master based on the facts sent back
the master then compiles manifests into
the catalogs and those cataloges are
sent back to the clients and an agent on
the client will then initiate the
catalog a report is generated by the
client that describes any changes that
have been made and sends that back to
the master with the goal here of
ensuring that the master has full
understanding of the hardware running
software in your network this process is
repeated at regular intervals ensuring
all client systems are up to dat so
let's have a look at companies that are
using puppet today there are a number of
companies that have adopted puppet as a
way to manage their infrastructure so
companies that are using puppet today
include Spotify Google AT&T so why are
these companies choosing to use puppet
as their main configuration management
tool the answer can be seen if we look
at a specific company Staples so Staples
chose to take and use puppet for their
configuration management tour and use it
within their own private Cloud the
results were dramatic the amount of time
that the it organization was able to
save in deploying and managing their
infrastructure through using puppets
enable them to open up time to allow
them to experiment with other and new
projects and assignments a real tangible
benefit to a company so let's look at
how you write a manifest in puppet so so
manifests are designed for writing out
in code how you would configure a
specific node in your server environment
the manifests are compiled into
cataloges which are then then executed
on the client each of the manifests are
written in the language of Ruby with a
PP extension and if we step through the
five key steps for writing a manifest
they are one create your manifest and
that is written by the system
administrator two compile your manifest
and it's compiled into a catalog three
deploy the catalog is then deployed onto
the clients for execute the catalogs are
run on the client by the agent and then
five and clients are configured to a
specific and desired state if we
actually look into how manifest is
written it's written with a very common
syntax if you've done any work with Ruby
or really configuration of systems in
the past this may look very familiar to
you so we break out the work that we
have here you start off with a package
file or service as your resource type
and then you give it a name and then you
look at the features that need to be set
such as IP address then you're actually
looking to to have a command written
such as present or start the Manifest
can contain multiple resource types if
we continue to write our manifest and
puppet the default keyword applies a
manifest to all clients so an example
would be to create a file path that
creates a folder called sample in a main
folder called Etc the specified content
is written into a file that is then
posted into that folder and then we're
going to say we want to be able to
trigger an Apache service and then
ensure that that Apache service is
installed on a node so we write the
Manifest and we deploy it to a client
machine on that client machine a new
folder will be created with a file in
that folder and an Apache server will be
installed you can do this to any machine
and you'll have exactly the same results
on those machines of devops staying
abast of the top tools is essential for
professionals seeking to enhance their
productivity and efficiency so deop
tools discussed in this tutorial will
will cover a broad spectrum of
functionalities ranging from Automation
and containerization to monitoring and
project management familiarity with
these tools enables professionals to
streamline the development and
operations processes improve
collaboration and Achieve faster more
Reliable Software releases and much more
so without any further Ado let's get
started with today's topic now in a
world driven by rapid technological
advancements businesses are constantly
seeking ways to deliver software faster
and more efficiently enter devops the
dynamic philosophy that has
revolutionize the software development
and operations landscape devops is not
just a busz word it's a game-changing
approach that Bridges the gap between
developers and operations team forer
collaboration and enhancing the entire
software development life cycle now at
its core devops embodies a cultural
shift towards seamless integration and
continuous delivery it promotes a
collaborative environment where
developers system administrators quality
assurance professionals and other
stakeholders work together harmoniously
and promoting cross functional
communication devops empowers teams to
streamline processes automated workflows
and ultimately deliver high quality
software at an accelerated Pace having
said that the demand for devops
professionals has overgrown in recent
years as more and more companies adap
devops practices to improve their
software development and delivery
processes so are you ready to advance
your professional career to the next
level taking our step-by-step simply
learns postgraduate program in devops in
collaboration with IBM will help you
start your devops journey that will
prepare you for the devops engineer role
so in order to match your skills with
market demand this day ofs training
program is designed in collaboration
with calch ctme and Cutting Edge Blended
learning combines live online day of
certification classes with interactive
labs to give you practical experience
this deop training program will cover
skills like devops methodology
continuous integration de Ops on cloud
infrastructure deployment Automation and
many more and along with that you'll
also get hands-on experience with latest
tools and techniques including terraform
Maven anible Docker Jenkins and much
more so if you're looking to pursue your
career as devops engineer and acquire
these skills that will prepare you for
your job consider enrolling in this
intensive training program today we will
leave the link in the description box
below so make sure you check that out so
let us move ahead and proceed with our
topic today so the main question is what
fuels is deop Revolution now it's a
powerful collection of tools
specifically designed to support and
amplify its principles so these tools
access a catalyst empowering teams to
automate task manage infrastructure
efficiently and monitor application
performs l so without any further delay
let us directly jump into the top nine
noteworthy tools highlighting their
features benefits and significant that
have become synonymous with the devops
ecosystem in no particular order so
first on the list we have genkin now
genkin is an open source automation
server known for its extensive plug-in
ecosystem making it highly versatile and
customizable it facilitates continuous
integration and continuous delivery or
in short cacd pipelines automating the
built test and deploy deployment
processes junkins enables team to
integrate code frequently ensuring early
detection of issues and faster software
releases so let us now look at some of
the key features of the genkin firstly
easy installation and configuration
genkin has a userfriendly web interface
that simplifies installation
configuration and management of build
jobs and pipelines distributed builds
for scalability now it's supposed for a
vast ecosystem of plugins for seamless
integration and it can distribute buil
tasks across multiple nodes which helps
in paralyzing builds and reducing
overall build time especially in large
projects and finally extensible through
scripting languages like groovy now
Jenkins provides accessibility through
scripting languages like groy which is a
powerful and verstile language that runs
on the Java virtual machines or in short
jbm so by leveraging groy scripting
capabilities genkins becomes highly
adaptable and customizable to meet the
specific requirements of different
development teams and projects so those
are some of the key features of genkins
let us now move ahead and discuss some
of of the benefits or advantages of
using genkins so firstly we have enables
faster feedback and quicker time to
market now genkins continuous
integration capabilities enable
developers to merge their code changes
into a shared repository regularly by
doing so Jenkins automatically triggers
buildt and testing processes to validate
any kind of changes this automates
feedback loop allows developers to
receive quick feedback on the quality of
their code so as a result issues and
bugs can be identified early in the
development cycle prevent the
accumulation of defects and reducing the
time required to fix them as discussed
earlier it also facilitates early issue
detection and reduce bug turnaround time
now with genkins running automated
builds and test every time new code is
committed potential issues and bugs are
CAU early in the development process so
by detecting issues at an early stage
developers can address them promptly
before they propagate further into the
code base and finally automates
repeative task same time and effort now
junit automates various tasks involved
in the software development life cycle
this includes tasks such as building the
code running tests and deploying
applications by automating these
repetitive and timec consuming task
genkin saves developers and operations
team significant amount of time and
efforts so that is what genkin is all
about it's a widely adopted in the
devops landscape due to its flexibility
and extensive plug-in support and
learning it will add a great value to
your array of skills so it's ability to
automate CSD processes and integrate
with various tools makes it a crucial
component of modern software development
and delivery pipelines second on the
list we have Docker Docker has
revolutionized application deployment
with its containerization approach it
allows developers to package
applications and their dependencies into
lightweight isolated containers Docker
contains provide consistency across
different environments ensuring that
applications turn consistently
regardless of the underlying
infrastructure this portability along
with the rapid startup times and
efficient resource utilization has made
Docker a foundational tool in De off
practices so let us now some discuss
some of the the key features of Docker
first one is packaging applications and
dependencies into containers now Dockers
allows developer to package their
applications and all their dependencies
into a self-contained units called
containers and the process is known as
containerization now these containers
encapsulate the application code its
runtime various libraries and system
tools required to run the application by
doing so Docker ensures consistency
across different environments secondly
efficient resource utilization through
containerization now this lightweight
nature of containers allows for more
efficient resource utilization multiple
containers can run on a single physical
machine without the need for individual
OS instances it means that you can host
more applications and services on the
same Hardware reducing the number of
servers needed and finally easy scaling
and management of containers Now
containerization simplifies application
scaling and management so when you need
to handle increased application load you
can quickly scale by running more
instances on the containerized
application on additional servers or
with within a container orchestration
platform like
kubernets so let us now talk about some
of the benefits of the docker tool now
firstly rapid deployment and scalability
Docker enables rapid deployment of
applications du its lightweight and
containerized approach when using
Dockers developers package their
applications and dependencies into
containers which encapsulate everything
needed to run the application so these
containers are portable and can be
easily moved from one environment to the
other and next we have the isolation of
applications and dependencies for
improved security Docker utilizes
calization to isolate applications and
the dependencies from the ecosystem and
other containers each containers operate
in its own user space separate from
other containers providing a strong
level of isolation this isolation
prevents applications from affecting
each other and helps contain potential
security breaches within the confines of
the container and finally simplify
development to production workflow now
Docker streamlines the development to
production workflow by providing
consistency between different
environments with Docker developers can
create containers that run the same way
in development testing and production
environments thereby reducing the
chances of unexpected issues arising
during the deployment so Docker has
become a Cornerstone of modern day of
practices its updated to streamline
application deployment and improved
resource utilization has transformed
software development and operations
enabling faster and more reliable
application delivery so that was all
about Docker so let us now move ahe and
discuss the next tool which is kubernets
so next on the list we have kubernets
kubernets is an open source container
orchestration platform that automates
the deployment scaling and management of
containerized applications it provides a
robers infrastructure for running and
coordinating containers across clusters
of machine making it easier to manage
large scale deployments deop streams can
easily deploy update and scale
applications facilitating faster and
more Reliable Software delivery while
promoting collaboration and consistency
across development and operations life
cycle through kubernets so let us look
at some of the key features of it
automatic container deployment and
scaling now container orchestration
platforms like kubernets provide
automatic container deployment and
scaling capabilities when deploying
applications on kubernets you can Define
the desired state of your application
using yl files on declarative
configuration kubernets then take cares
of this ensuring that specified number
of container replicas is running at all
times service Discovery and load
balancing now kuber net enables faster
feedback and quicker time to Market in a
cized environment multiple instances of
an application may be running across
different containers making it
challenging for clients or other
services to know the IP addresses of all
the running instances and detecting and
keeping tracking of the locations of
various services within the container
cluster and kubernets for example
provides a built-in service Discovery
mechanism next we have selfhealing and a
set of containers now container
orchestration platform provides
selfhealing capabilities to ensure that
applications are always available and
responsive if a container fails due to
an application crash or any other issues
the orchestration platform detes the
failure and automatically restarts the
fail
container so let us now move ahead and
discuss some of the benefits of using
kubernets tool firstly it is scalable
and highly available application and it
provides a seaming repeat and provides a
seamless scaling of resources based on
demand next it has a simplified
management of conization applications
across various clusters and finally
automated deployment and updates
reducing manual Intervention which
further improves resource utilization
and Co optimization overall kubernets
has emerged as the industry standard for
container orchestration its ability to
automate application deployment scaling
and management simplifies the
complexities of running contz
applications at scale making it a
crucial tool for devops
practitioners moving ahead let us
discuss our next tool which is anible
anible is a powerful automation tool
that's simplifies configuration
management application deployment and
orchestration it employs a declarative
language to Define desired State
configuration making it easy to manage
an automate infrastructure task anible
follows an agentless architecture
allowing it to work efficiently across a
wide range of systems and environments
so let us now look look at it some of
its features firstly it's a declarative
language for defining infrastructure
configuration and it is also an
agentless architecture for easy
deployment and management as it is
discussed earlier now this is done
through a Playbook driven automation for
orchestration where extensive library of
modules for a wide range of task are
employed so let us now discuss some of
its benefits so firstly it's simplified
infrastructure management through
automation which increases the
operational efficiency with reduced
manual task and its ENT nature ensures
that it has consistently and
predictability overall and finally it
supports a wide range of infrastructure
automation use cases and with its
agentless architecture it allows easy
integration with various systems and
environments so in a natural anable
Simplicity flexibility and EAS of use
have made it a popular choice for
automating infrastructure and
application deployment task approach and
agentless architecture contribute to
efficient and streamline development
Workforce so moving ah the next tool on
our list is git is a distributed version
control system that has become a
fundamental tool for modern software
development practices it allows
developers to track changes collaborate
effectively and manage codebase
efficiently gets decentralized
architecture ensures that developers can
work offline and merge changes
seamlessly across branches let us now
discuss some of its key features now its
distributed version control system has
efficient collaboration with various uh
tools within the management of the
ecosystem of devops and it also has an
integration with various code hosting
platforms which is the branching and
merging capabilities for concurrent
development and finally it support for
code reviews and collaboration workflows
which also gives a commit based tracking
of changes so these are some of the key
features of git so let us now discuss
some of the advantages or like benefits
of using git so firstly it's easy
tracking and management of code changes
which ensures efficient collaboration
and concurrent development within the
resource files and its ability to work
offline and merge changes seamlessly
will benefit a lot of doops Engineers
and finally its easy integration with
other devops tools makes it a wonderful
to use to manage code versions track
changes and enable effusion
collaboration which has revolutionized
the software development so it has
become a essential tool for Version
Control facilitating effective
collaboration and enabling streamlined
devops
workflows so moving ahead the next tool
on our list is terraform now terraform
is an infrastructured code or in short I
AC tool that allows teams to Define and
provision infrastructure resources in a
declarative manner it supports multiple
Cloud providers and enables consistent
and reproducible infrastructure
deployment terraforms declarative syntax
and State Management capabilities
simplify infrastructure provisioning and
configuration let us now look at some of
its key features firstly it provides
multicloud support for provisioning
resources across different providers its
infrastructure as Cod approach for
consistent and reproducible deployments
makes it declarative Syntax for defining
infrastructure configurations and
finally automated resource provisioning
and dependency
management so some of its benefits or
the advantages of using terraform is its
simplified resource provisioning and
dependency management now terraforms
infrastructure as code approach supports
for multiple Cloud providers makes it an
essential tool for managing
infrastructure codes and collaboration
and Version Control for infrastructure
configurations as well and finally in
State Management for TR tracking
infrastructure changes which makes it a
beneficial tool for devops
engineers now nagos is used to Monitor
and manage the health and performance of
the infrastructure application and
network resources it's a comprehensive
monitoring solution to identify and
resolve issues proactively ensuring High
availability and reliability of systems
so let us look some of its key features
firstly its monitoring capabilities
secondly it's centralized fun figure
management and finally its event
handling and escalation tool so learning
nagos is also crucial and beneficial if
you're just starting on De op's
Journey so moving ahead let us now
discuss our next tool on our list which
is Elk stack now elk stack offers a
comprehensive solution for centralizing
logs from various applications and
systems making it easier for deop teams
to monitor troubleshoot and gain
valuable insights from their log data
the elk stack comprising elastic search
log statch and kibana provides a
comprehensive log management and
Analysis platform it access a
distribution search and analytics engine
while log stack collects processes and
transform locks and finally kibana
offers a userfriendly interface for
visualizing and exploring
data so some of its key features
including realtime monitoring alerting
scalable and efficient log storage and
retrieval and distributed search and
analytics through data visualization and
Exploration with
kabana let us look at some of its
benefits so firstly it's realtime
monitoring for proactive detect is
repeat it's realtime log monitoring for
proactive issue detection with the help
of centralized log management and
Analysis Advanced log analytics for
double shooting and performance
optimization making it a scalable
architecture for handling large scale
volumes of data and finally efficient
log storage and retrieval for compliance
and auditing purposes in a nutshell elk
stack has gained immense popularity for
its ability to handle log management and
Analysis it skill it empow deop teams
with realtime insights into application
and infrastructure logs facilitating
effective trouble shooting and
performance
optimization well finally on the list we
have jira software jira is a widely used
project management tool that supports
aile development methodologies it offers
robust features for planning tracking
and managing task issues and workflows J
customizable boards backlogs and
workflows Empire teams to collaborate
effectively visualize and progress to
gain transparency into projects statuses
with integration to various devops tools
jira facilitate seamless tracking of
development activities enabling
continuous Improvement and efficient
project management some of its key
features are customizable boards
workflows for project management agile
planning and estimation features issue
tracking and management
capabilities and some of its benefits
include efficient task management and
tracking enhanced collaboration and
visibility across various teams and
integration with various devops tools
for streamline work workflows it also
helps in reporting and analytics for
project insights and performance
measurement finally jira has become a
go-to tool for agile project management
in the doops ecosystem It's ability to
support aile methodologies track task
and integrate with other doops tools
makes it a valuable assets for teams
seeking efficient project management and
continuous Improvement so learning it
will add a great value to your Aro
skills again so these are some of the
top nine de Ops rules that you must know
which will help you enhance and exited
your career in devops now let's
understand what is devops devops is like
a teamwork approach for making computer
programs better and faster it combines
the work of software developers and
operation team the goal is to help them
work together and use tools that speed
up the process and make fewer mistakes
they also keep an eye on the programs
and fix problems early This Way
businesses can release programs faster
with few errors and everyone works
better together if you want to learn
more about this then check postgraduate
program in devops to understand from the
basics two advanced concepts this
postgraduate program in devops is
crafted in partnership with Caltech ctme
this comprehensive course Alliance your
expertise with industry benchmarks
experience are Innovative Blended
learning merging live online daop
certification sessions with immersive
labs for practical Mastery Advance your
career with Hands-On training that meets
industry demands all right now let's
move on on to the first question of
devops interview question which is what
do you know about devops so think of a
devops like teamwork in the IT world
it's become really important because it
helps teams work together smoothly to
make computer programs faster and with
fewer mistakes imagine a soccer team
everyone works together to win the game
devops is similar it's when computer
developers and operations people team up
to make software better they start
working together from the beginning when
they they plan what the software will be
like until they finish and put it out
for people to use this Teamwork Makes
sure things go well and the software
works great so this was about devops now
moving on to the second question which
is how is devops different from agile
methodology devops is like a teamwork
culture where the people who create the
software and the people who make it work
smoothly join forces this helps in
always improving and updating the
software without any big breaks agile is
a way of making softwares that's like
taking small steps towards instead of
big jumps it's about releasing small
parts of the software quickly and
getting feedback from the users this
helps in solving any issues or
differences between what users want and
what developers are making so you can
answer this question in this way all
right moving on to the third question
which is what are the ways in which a
build can be scheduled SL run and
genkins so as you can see there are four
ways by source code management commits
second is after the completion of other
builds third is schedule to run at a
specified time and fourth one is manual
belt requests so if the interviewer asks
then you can answer these four ways in
which a belt can be scheduled in genkins
all right now the fourth question is
what are the different phases in devops
so the various phases of devops life
cycle are as follows so as you can see
first is plan so initially there should
be a plan for the type of application
that needs to be developed getting a
rough picture of the development process
is always a good idea then code the
application is coded as per the end user
requirements then there is build build
the application by integrating various
codes formed in previous steps after
that there is test this is the most
crucial step of the application
development test the application and
rebuild if necessary then there is
integrate so multiple codes from
different programmers are integrated
into one after integrate there is deploy
so code is deployed into a cloud
environment for further usage it is
ensure that any new changes do not
affect the functioning of a hight
traffic website after that there is
operate so operations are performed on
the code if required then there is
Monitor application performance is
monitored changes are made to meet the
end user requirements so these all were
the different phases of devops and here
we have explained each one of these you
can go through it and if the interviewer
have asked you this question you can
answer it in a similar way all right now
moving on to the next question which is
mention some of the core benefits of
devops so the core benefits of devops
are as follows first of all we'll see
technical benefits first technical
benefit of devops is continuous software
delivery then second is less complex
problems to manage third is early
detection and faster correction of
defects then here comes the business
benefits the first benefit of devops
that is business benefit of devops is
faster delivery of features so it allows
faster delivery of features then there
is stable operation environment then
third one is improved communication and
collaboration between the teams so these
were the business benefits so here we
have discussed about technical benefits
and business benefits all right now the
next question is how will you approach a
project that needs to implement devops
so here are the simpler terms here's how
we can bring devops into a specific
project using these Steps step one would
be first we look at how things are
currently done and figure out where we
can make them better this makes about 2
to 3 weeks then we can plan for what
changes to make then step two would be
we create a small test to show that our
plan works when everyone agrees with it
we can start making the real changes and
put the plan into ction then third step
would be we are all set to actually use
devops we do things like keeping track
of different versions of our work
putting everything together testing it
and making it available to the users we
also watch How It's Working to make sure
everything goes smoothly by doing these
steps right the keeping track of changes
putting everything together testing and
watching how it's going we are all set
to use devops in our project so this is
how you can approach a project that
needs to implement in devops and this is
how you can answer this question all
right moving on to the question number
seven which is what is the difference
between continuous delivery and
continuous deployment all right so first
would be continuous delivery so it
ensures code can be safely deployed on
production whereas continuous deployment
here every change that passes the
automated test is deployed to production
automatically then in continues deliver
delivery it ensures business
applications and services functions as
expected whereas in continuous
deployment it makes software development
and the release process faster and more
robust in continuous delivery delivers
every change to a production like
environment through rigorous and
automated testing whereas in continuous
deployment there is no explicit approval
from a developer and requires a
developed culture of monitoring so these
were the three points that you can
highlight while differentiating between
continuous delivery and continuous
deployment all right so this was the
question number seven now moving on to
the question number eight which is name
three security mechanisms of Jenkins
uses to authenticate users so here we
have to name three security mechanisms
first one would be Jenkins uses an
internal database to store user data and
credentials second is genkins can use
the lightweight directory access
protocol that is ldap server to
authenticate users third one is genkins
can be configured to employ the
authentication mechanism that the
deployed application server uses so
these were the three mechanisms that
genin uses to authenticate users all
right now moving on to the question
number nine which is how does continuous
monitoring help you maintain the entire
architecture of the system so continuous
monitoring within devops involves the
ongoing identification detection and
reporting of any anomalies or security
risk across the entire system
infrastructure it guarantees the proper
functioning of services applications and
resources on servers by overseeing
server statuses it accesses the accuracy
of application operations this practice
also facilitates uninterpreted audits
transaction scrutiny and regulated
surveillance so this was about the
question number nine that was how does
continuous monitoring help you maintain
the entire architecture of the system
now moving on to the question number 10
which is what is the role of of AWS in
devops so in the realm of devops AWS
assumes several rules first one would be
adaptable services so it offers
adaptable pre-configured Services
eliminating the necessity for software
installation or configuration second one
is designed for expansion whether
managing one instance or expanding 2,000
AWS Services accommodate seamless
scalability then there is automated
operations AWS empowers task and process
automation free up valuable time for
inventing Pursuits the next one is
enhanced security AWS identity and
accesses management that is I am allows
precise user permissions and policy
establishment then the last one is
extensive partner Network so AWS Fosters
a vast partner Network that integrates
with and enhances its service offerings
so these were the role of awfs in the
realm of devops all right now moving on
to the question number 11 name three
important dups kpis the three important
kpis are as follows first one is
meantime to failure recovery this is the
average time taken to recover from a
failure second kPa is deployment
frequency the frequency in which the
deployment occurs is called deployment
frequency third one is percentage of
failed deployments the number of times
the deployment fails is called
percentage of failed deployments so
these were the three important devops
kpis this question can also be asked all
right moving on to question number 12
which is how is IAC implemented using
AWS so commence by discussing
traditional methods involving scripting
commands into files followed by testing
in isolated environments prior to
deployment notice how this practice is
giving way to infrastructure as code IAC
comparable to code for various Services
IAC aided by AWS empowers developers to
craft accesses and manage infrastructure
components descriptively utilizing
formats like Json or yaml this Fosters
streamlined development and expeditious
implementation of alterations in
infrastructure all right now moving on
to question number 13 which is describe
the branching strategies you have used
they'll ask you this question so to test
our knowledge the purpose of branching
and our experience of branching at a
past job this question is usually asked
so here we will disc discuss topics that
can help you answering this devops
interview question so release branching
we can clone the develop Branch to
create a release Branch once it has
enough functionality for a release this
Branch kicks off the next release cycle
does no new features can be contributed
Beyond this point the things that can be
contributed are documentation generation
bug fixing and other release related
tasks the release is merged into master
and given a version number once it is
ready to ship it should also be merged
back into the development branch which
may have evolved since the initial
release so this was the first one then
there is feature branching this
branching model maintains all
modifications for a specific feature
contained within a branch the branch
gets merged into Master once the feature
has been completely tested and approved
by using test that are automated then
the third branching is Task branching in
this branching model every task is
implemented in its respective Branch the
task key is mentioned in the branch name
we need to Simply look at the task key
in the branch name to discover which
code implements which tasks so these
were the branching strategies that you
can say that you have used or if you
have really used it otherwise you can
say something else all right moving on
to question number 14 which is can you
explain the shift left to reduce failure
Concept in devops so shifting left
within the devops framework is a concept
aimed to enhance security performance
and related aspects to illustrate
consider the entity of divorce processes
currently security valuations occur
before the deployment stage by employing
the left ship approach we can introduce
security measures during the earlier
development phase denoted as the left
this integration spans a multiple phases
encompassing pre-development and testing
not confined to development alone this
holistic integration is likely to
elevate security measures by ident
identifying well abilities at initial
stage leading to a more fortified
overall process now moving on to the
question number 15 which is what is
blue/ green deployment pattern so this
approach involves seamless deployment
aimed at minimizing downtime it entails
shifting traffic from one instance to
another requiring the replacement of
outdating code and a new version to
integrate fresh code the updated version
resides in a green environment while
while the older one remains in a blue
environment after modifying the existing
version a new instance is generated from
the old one to execute the updating
instance ensuring a smoother transition
so the main focus of this approach is
smooth
deployment all right so this was
question number 15 now moving on to the
question number 16 which is what is
continuous testing so continuous testing
involves the automated execution of
tests within the software delivery
pipeline
offerings immediate insights into
potential business risk within the
latest release by seamlessly integrating
testing into every stage of the software
develop lici I repeat by seamlessly
integrating testing into the every stage
of software delivery life cycle
continuous testing minimizes issues
during transition phases and empowers
development teams and instant
feedback this approach accelerates
developer efficiency by obtaining the
need to to rerun all test after each
update and project rebuild culminating
in notable gains in speed and
productivity so this was about
continuous
testing now moving on to question number
17 what are the benefits of automation
testing so some of the benefits of
automation testing includes first it
helps to save money and time second is
unattended execution can be easily done
third one is huge test matrices can be
easily tested the next one is parallel
execution is enabled then there is
reduced human generated errors which
results in improved accuracy and last
one is repeated test task execution is
supported so these are the benefits of
automation
testing coming to question number 18
which is what is a Docker file used for
so a Docker file is used for creating
Docker images using the build command
with a Docker image any user can run the
code to create Docker containers once
Docker image is built it's uploaded in
doer registry from the docker registry
users can get the docker image and build
new containers whenever they want so
this is what Docker file is used for
coming to question number 19 which is
what is the process for reverting a
commit that has already been pushed and
made public so there are two ways to do
that to revert a commit so first would
be remove or fix the bad file in a new
commit and push it to the remote
repository then commit it to the remote
repository using get commit minus M
commit
message again get commit minus M commit
message so this is the first way then
second is create a new commit that undos
all the changes that were made in the
bad commit then use the command get
rever commit ID as you can see in the
screen First Command also and the second
command also the second command says get
revert commit ID commit ID you'll have
to put the commit ID all right so
question number 19 this was now moving
on to the last question which is
question number 20 how do you find a
list of files that have been changed in
a particular comment so the answer would
be the command to get a list of files
that have been changed in a particular
commit is git diff tree minus r then
there is commit hash so this command as
you can see on the screen G diff3 minus
r commit hash you can put it then
examples also there like commit hash 87
e
6735 F2 1B so this is example as you can
see on the screen then there is minus r
flag instruct so the command to list
individual files and commit hash will
list all the files that were changed to
or added in that commit so there's
telling about the as we can see minus r
functionality minus r is a flag that
instructs the command to list individual
files and commit hash will list all the
files that were changed or added in that
cment so these are the top 20 devops
interview questions that you must
understand if you're planning to give a
devops interview so in this video we
have explored key Concepts methodologies
and best practices that are crucial and
fostering collaboration between
development and operation streams by
understanding the principles discussed
here you are well equipped to navigate
the dynamic landscape of devops and
drive successful software delivery
congratulations you have completed the
devops complete course and you are now
equipped with the skills and knowledge
to excel in the dynamic world of
technology devops offers endless
opportunities and you well prepared to
make the most of them whether you're
aiming for higher paying positions
smoother software development or a
fulfilling career your journey starts
here keep exploring inating and building
the future staying ahead in your career
requires continuous learning and
upskilling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in Cutting Edge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to ner up and get certified click
here