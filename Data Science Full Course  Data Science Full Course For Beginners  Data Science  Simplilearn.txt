hello everyone welcome to data science
full course by simply Le data science is
all about turning raw data into valuable
insights that help businesses make
better decisions it's a blend of
Statistics programming and domain
expertise that allows you to analyze
data uncover patterns and predict future
Trends in today's world where data is
everywhere data science has become one
of the most important and fastest
growing fields in 20 24 the demand for
data scientists is stronger than ever
companies across all Industries are
looking for professionals who can make
sense of the vast amounts of data they
collect this high demand means great
opportunities with data scientists
earning between $80,000 to $150,000 a
year depending on the experience and
expertise craving a career upgrade
subscribe like and comment
below dive into the link in the
description to FasTrack your
Ambitions whether you're making a switch
or aiming higher simply learn has your
back so if you're interested to make
your current machine learning and AI
unlock your potential in AI machine
learning with simply learns professional
certificate course in generative AI in
collaboration with ID kpur designed for
aspiring AI professionals this program
offers hands-on experience with machine
learning algorithms deep learning and
NLP Guided by industry experts and I
cardboard faculty network with community
of Learners and professionals and ear a
prestigious certificate upon completion
so enroll now and take your first step
towards your future in Ai and machine
learning you can find the course Link in
the description box and in the pin
comments so let's get started are you
one of the many who dreams of becoming a
data scientist keep watching this video
if you're passionate about data science
because we will tell you how does it
really work under the hood am is a data
scientist let's see how a day in her
life goes while she's working on a data
science project well it is very
important to understand the business
problem first in our meeting with the
clients Emma asks relevant questions
understands and defines objectives for
the problem that needs to be tackled
she's a curious Soul who asks a lot of
Vice one of the many traits of a good
data scientist now she Gees up for data
acquisition to gather and scrape data
from multiple sources like Feb servers
logs databases apis and online
repositories oh it seems like finding
the right data takes both time and
effort after the data is gathered comes
data preparation this step involves data
cleaning and data transformation data
cleaning is the most timec consuming
process as it involves handling many
complex scenarios here Emma deals with
inconsistent data types misspelled
attributes missing values duplicate
values and whatnot then in data
transformation she modifies the data
based on defined mapping rules in a
project ETL tools like talent and
Informatica are used to perform complex
Transformations that helps the team to
understand the data structure better
then understanding what you actually can
do with your data is very crucial for
that Emma does exploratory data analysis
with the help of Eda she defines and
refines the selection of feature
variables that will be used in the model
development but what if Emma skips this
step you might end up choosing the wrong
variables which will produce an
inaccurate model thus exploratory data
analysis becomes the most important step
now she proceeds to the core activity of
a data science project which is data
modeling she repetitively applies
diverse machine learning techniques like
Cann decision tree knif based to the
data to identify the model that best
fits the business requirement she trains
the models on the training data set and
tests them to select the best performing
model Emma prefers python for modeling
the data however it can also be done
using R and SAS well the trickiest part
is not yet over visualization and
communication Emma meets the clients
again to communicate the business
findings in a simple and effective
manner to convince the stakeholders she
uses tools like Tableau powerbi and
click view that can help her in creating
powerful reports and dashboards and then
finally she deploys and maintains the
model she she tests the selected model
in a pre-production environment before
deploying it in the production
environment which is the best practice
right after successfully deploying it
she uses reports and dashboards to get
realtime analytics further she also
monitors and maintains the Project's
performance well that's how Emma
completes the data science project we
have seen the daily routine of a data
scientist is a whole lot of fun has a
lot of interesting aspects and comes
with its own share of challenges now
let's see how data science is changing
the world data science techniques along
with genomic data provides a deeper
understanding of genetic issues and
reaction to particular drugs and
diseases logistic companies like DHL
FedEx have discover the best rules to
ship the best suited time to deliver the
best mode of transport to choose thus
leading to cost efficiency with data
science it is possible to not only
predict employee attrition but to also
understand the key way variables that
influence employee turnover also the
airline companies can now easily predict
flight delay and notify the passengers
beforehand to enhance their travel
experience well if you're wondering
there are various roles offered to a
data scientist like data analyst machine
learning engineer deep learning engineer
data engineer and of course data
scientist the median base salaries of a
data scientist can range from
$95,000 to $16 $5,000 so that was about
the data science are you ready to be a
data scientist if yes then start today
the world of data needs you are you
fascinated by data and want to turn it
into a fulfilling career today we are
going to talk about data science and how
you can become a data scientist whether
you're just starting out or looking to
improve your skills this guide will help
you get started on your journey in data
science firstly what is data science in
simple terms data science is a field
where you use programming math and
knowledge about a specific area to find
useful information in data data
scientists look at large sets of data to
find patterns Trends and connections
that can help companies make smart
decisions for example think about
Netflix have you noticed how it always
seems to know what show or movie you
want to watch next that's because
Netflix uses data science it looks at
what you have watched before what you
have liked and even what time of day you
watched by studying all this information
Netflix can suggest shows and movies it
thingss you will enjoy making it easy
for you to find something to watch and
Amazon does something similar when you
shop online when you search for items
buy something or even just add something
to your card Amazon keeps track of that
it then uses this data to recommend
other products you might like this helps
you discover things you might need and
it also helps Amazon sell more products
now that we have a clear idea of of what
data science is let's talk about how you
can actually become a data scientist
this might seem like a big task but
don't worry we'll break it down step by
step so you can see exactly what you
need to learn and why it's important so
the first step is to learn programming
language data scientists often use two
main languages Python and R python is
very popular because it's easy to learn
and has lots of libraries or tools that
help with data analysis and machine
learning our is also great especially
for statistics and creating graphs
learning these languages will help you
write code to work with data which is a
big part of being a data scientist so
once you have chosen your programming
language it's important to start with
the basics understand these fundamentals
will make everything else you learn much
easier so before you dive into complex
stuff you need to understand the basics
of python includes learning about
variables which are like containers that
store data and if else Loops which are
used to make decisions in your code
these Basics are like the ABCs of
programming and you'll use them all the
time when you start working on data
science projects so now that you have
the basics down it's time to move on to
some tools that will make working with
data much easier let's talk about dumpy
and pandas so once you're comfortable
with python Basics it's time to learn
about two important libraries numpy and
pandas so numpy helps you work with
numbers especially if you're dealing
with large amounts of data it's like a
superpowered calculator and pandas is
used for handling and organizing data in
tables making it easier to clean
transform and analyze data so these
tools are essential for any data
scientist now data science isn't just
about coding it also involves a lot of
math here's a quick rundown of the key
areas you need to focus
on statistics for beginners focus on
descriptive statistics which involves
understanding measures of central
tendency like mean median and mode as
well as measures of spread like variance
and standard deviation you'll also need
to grasp the basics of data distribution
such as normal and binomial
distributions and as you move on to
intermediate level statistics you will
explore inferential statistics which
involves drawing conclusions from data
samples through Concepts like sampling
confidence intervals and hypothesis
testing you'll also need to understand
probability distributions such as poison
and exponential distributions at the
advanced level delve into regression
analysis is learning techniques like
linear regression logistic regression
and multiple regression which are
crucial for making predictions Based on
data additionally understanding basent
statistics will be key especially
Concepts like base theorem priors and
likelihoods which are used in advanced
predictive
modeling next comes linear algebra so in
linear algebra start by understanding
vectors and scalars which are
fundamental in representing data so as
you progress study metrices and Metric
operations which which are crucial for
handling data in multiple Dimensions
finally at an advanced level learn about
Egan values and Egan vectors which are
important in many machine learning
algorithms such as principal component
analysis now calculus is another
important area though you don't need to
be an expert in it begin by
understanding derivatives which measure
how a function changes as its inputs
change this concept is essential when
you're optimizing machine learning
models and finally in probability start
with basic concepts like odds and
conditional probability which will help
you understand and manage uncertainty in
data as you advance explore base theorem
and learn about probability
distributions which are used extensively
in statistical modeling and machine
learning algorithms now once you
crunched the numbers the next step is to
make your findings easy to understand
that's where data visualization comes in
now data visualization is all about
presenting your data in a way that's
easy for others to understand so start
with matplot li a python libr library
that allows you to create simple plots
like line graphs and Scatter Plots this
will help you identify Trends and
patterns in your data and as you gain
confidence move on to cabon which Builds
on matplot lib and allows you to create
more complex and aesthetically pleasing
visualizations like heat maps that can
show correlations between
variables additionally it's important to
have a basic understanding of excel
especially if you'll be working in a
business environment Excel is widely
used for quick data analysis and knowing
how to create basic charts and pivot
tables will make it easier to share your
results with colleagues who may not be
familiar with python now to take your
data visualizations to the next level
you can learn tools like powerbi and
table powerbi a tool from Microsoft
allows you to create interactive
dashboards that are easy to share and
update in real time this makes it great
for businesses that need to monitor key
metrics on an ongoing basis tab is
another powerful tool for creating
interactive and shable visualizations so
it's particularly useful for working
with large data sets and Performing
complex analytics so both of these tools
are highly valued in the industry for
their ability to turn data into
actionable insights making your
visualizations even more impactful and
useful for decision making so now that
you can analyze and visualize data it's
time to explore one of the most exciting
parts of data science which is machine
learning machine learning is a powerful
tool that allows computers to learn from
data and make predictions on decisions
without being explicitly programmed
let's break down the different types of
machine number one supervised
learning so supervised learning is one
of the most common types of machine
learning in supervised learning you
train your model on a data set that
contains both the input features and the
corresponding correct outputs so this
means the data you use to train your
model is labeled and the goal is to
teach the model to make predictions
based on these labels for instance you
might model that predicts house prices
based on features like the number of
bedrooms location and size of the house
so popular algorithms and supervised
learning include linear regression for
predicting continuous values and
classification algorithms like logistic
regression decision trees and support
Vector machines for categorizing data
into classes next up is unsupervised
learning now unlike supervised learning
with data that doesn't have labels so
the goal here is to find the patterns or
structures within the data so for
example you might use unsupervised
learning to group customers into
different segments based on their
purchasing Behavior without knowing
beforehand what those segments should be
so clustering algorithms like K means
and hierarchical clustering are commonly
used in unsupervised learning to
identify these groupings additionally
you'll use techniques like principal
component analysis to reduce the
dimensionality of your data while
preserving its important features next
comes semi-supervised learning so
semisupervised learning combines
elements of both supervised and
unsupervised learning so in
semi-supervised learning you work with a
data set that is partially labeled this
approach is useful when labeling data is
expensive or time consuming the model
learns from the labeled portion of the
data and makes predictions on the
unlabeled portion so this technique can
be very effective in scenarios where you
have a small amount of labeled data and
a large amount of unlabelled data such
as in text classification or image
recognition tasks now finally let's talk
about reinfor en forcement learning in
this type of learning an agent which can
be a computer program or robot learns by
trying things out and getting feedback
so the agent makes decisions and based
on whether those decisions are good or
bad it gets Rewards or penalties so the
goal is to figure out which actions lead
to the best outcomes over time now with
a good grasp of programming math and
machine learning under your belt there's
one more essential skill you need to
master working with databases so let's
talk about SQL so SQL stands for
structured query language is a standard
language for managing and quering
relational databases it's crucial for
extracting and manipulating data from
large databases start by learning the
basics like how to write simple queries
to select insert update and delete data
understanding how to join tables is also
important as it allows you to combine
data from different sources into a
single query as you advance explore more
complex SQL features like subqueries
transactions and indexing which helps
improve the efficiency and performance
of your database queries the mastering
SQL will enable you to handle data
extractions and manipulation tasks more
effectively making you a more versatile
data scientist so now that you have got
a solid foundation it's time to put your
skills into practice so kaggle is a
fantastic platform to start applying
what you have learned it's a popular
website by data scientists and machine
learning enthusiasts can join
competitions and work with real data
it's a fantastic way to get hands-on
experience and build a portfol folio so
you can start by exploring the data sets
on kaggle and try solving some easy
problems to get used to working with
data you can also join competitions to
test your skills and see how you compare
to others so doing projects on kaggle
helps you practice what you have learned
and gives you results you can show to
Future employees so there you have it a
step-by-step guide on how to become a
data scientist remember it takes time
and effort but with dedication you can
definitely make it so let's first start
with understanding the basics so what
exactly is data science think of it as
Art and Science of extracting useful
information from the data it is a mix of
various Fields like computer science
statistics and domain specific knowledge
data science helps organizations make
better decisions predict future Trends
and improve their operations a data
scientist is like a detective who uses
and uncover hidden patterns and insight
they have to know programming understand
statistics and have a good domain
knowledge now basically statistics and
probability are your best friends here
you will need to understand Concepts
like mean median mode variance standard
deviations and probability distributions
these are the building blocks of data
analysis now next process is learning a
language now what programming language
you're going to choose so I'll refer you
to take on python python is a great
choice because of its Simplicity and
vast number of its Library available for
data science start with basics like
variables data types control structures
like FNL loops and functions once you
are comfortable in Python dive into
python libraries like numpy pandas and
Mt plot Li now let us discuss each of
these libraries one by one so basically
first one is numpy now numpy's primary
object is the homogeneous
multi-dimensional array an array that
can be indexed sliced and iterated over
similarly to list in Python but with
much greater efficiency you are going to
get vectorized operations numai is going
to allow you for element wise operations
and broadcasting which makes code more
concise and faster compared to python
Loops now the third one is mathematical
functions it also provides a wide array
of mathematical functions like linear
algebra operations random number
generation and forer
transforms next one we have is pandas
now we all know that pandas is a
powerful library for data manipulation
and Analysis it introduces two primary
data structures series and data frame
which are built on top of numpy if we
talk about series series is a
one-dimensional labeled array capable of
holding and in data type next one we
have data frame a two-dimensional label
data structure with Columns of
potentially different types similar to a
table in database or an Excel
spreadsheet you can also do data
manipulation pandas provide functions to
handle missing datas for filtering data
merge or join data sets and perform
group operations like aggregation
transformation and finally we have Matt
plotly Now Matt plotly is a powerful
plotting library in Python which is used
for creating static and animated and
also interactive visualizations it is
highly customizable and integrates well
with other libraries such as numai and
pandas the key features of this Library
are that it has versatile plotting
functions it creates a wide range of
plots including line plots Scatter Plots
bar charts histograms Etc next one it
can be used for customizations we can
customize every aspect of a plot such as
titles labels Legends colors and line
style so these were some of the benefits
of using matte plotly now let us move on
to the another programming language
which you can also choose that is R now
if you prefer R it is also a powerful
tool especially in the academics world
so when you are starting out learn the
basics of A and explore libraries like
dplr for data manipulation ggplot 2 for
data visualization and tiyr now next let
us focus on what are the mathematics
that is needed for data science so we
all know that mathematics is a
foundation of data science you don't
need to be a math genius but
understanding some key Concepts will be
very helpful the first one we are going
to choose that is linear algebra
learn about vectors matrices and their
operations this is crucial for
understanding how data is manipulated in
algorithm next you have to focus on
calculus focus on differentiation and
integration partial derivatives and
gradient descent these Concepts help in
understanding how optimization
algorithms work next one we have
statistics and probability get
comfortable with descriptive statistics
like mean median and mode and also
standard deviation
probability distributions hypothesis
testing and inferential statistics these
will help you to make a sense of data
and draw
conclusion now let's start with the next
part that is mastering data manipulation
and cleaning data is often messy so
learning how to clean and manipulate it
is very very essential use pandas and
python to read data from different
sources like CSP files or databases
clean the data by handling the missing
values removing duplicates and dealing
with the outliers next one you have to
transform the data as needed and create
a new features to improve your models
now after you have done this you need to
get Hands-On with data
visualization visualization is a key to
understanding data and communicating
your findings start with matplot lib and
cbon in Python to create basic plots
like line bar and Scatter Plots
customize your plots with titles labels
and legends to make them more
informative if you are using r then
ggplot 2 is your goto go Library it uses
the grammar of Graphics to create
beautiful and complex visualizations
with A's next one you have to go for Eda
which stands for exploratory data
analysis Eda is like a detective work
you have to explore the data to uncover
patterns spot anomalies and check
assumptions perform univariate and
bivariate Analysis look at correlations
and create profiles of the data this
step helps you to understand data better
and guides you to your next step the
next step we have is machine learning
now is the time to get into the heart of
data science that is machine learning
start with unsupervised learning which
includes regression predicting
continuous values or classification
which means predicting discrete
categories then explore unsupervised
learning techniques like clustering
which means grouping similar data points
and dimensionality reduction which means
reducing the number of features while
retaining important information
learn how to evaluate models using
metrics like accuracy precision recall
and F1 score cross validation is also a
technique to assess how well your model
generalizes to unseen data after this
you have completed move on to deep
learning now we all know that deep
learning is an advanced area of machine
learning involving neural networks start
with the basics of neural networks and
then explore more complex architectures
like CNN which means conal neural
network networks for image data and
recurrent neural networks for sequential
data popular Frameworks like tensorflow
and pytorch are great for implementing
deep learning models after you have
completed this move on to NLP now NLP is
all about teaching machines to
understand and generate human language
start with text pre-processing
techniques like tokenization stemming
and lionization then explore sentiment
analysis topic modeling and word
embeddings like word to and glob to
represent Text data in numerical form
after this you have done go on to
exploring Big Data Technologies now
sometimes data is so large that
traditional tools also cannot handle it
and that's where the big data technology
is come in learn about Hado ecosystem
and Apache spark for big data processing
these tools help you work with massive
data sets efficiently and after that
learn about data Science Project Life
Cycle a typical data science project for
follows a life cycle like this the first
phase is problem definition understand
the problem you are trying to solve The
Next Step you have to move to data
collection gather the relevant data
third step involves data cleaning
preparing the data for analysis at the
fourth stage you have to build the model
you have to choose and implement the
right models fifth is the model
evaluation assess the model performance
sixth one is deployment put the model
into production so so these are some of
the steps which are involved in data
Science Project Life Cycle now after you
have done all this now it's a time to
apply what you have learned so you have
to do real world projects and case
studies participate in competitions on
platform like kaggle where you can
tackle challenging problems and see how
you stack up against others study
successful data science case studies to
understand how different techniques are
applied in various Industries now I
would also recommend you some of the
additional resources then you can add on
while you are on your journey to become
a data scientist so certain of the books
like python for data analysis by wz mcky
is also recommended then Hands-On
machine learning with psychic learn
kiras and tens oflow by Orin gon and
finally deep learning by Ian goodflow
and yosua beningo are a very very good
choice for learning now you can also go
for online certification courses and
also watch YouTube exhaustively to
understand certain underlying Concepts
if you're excited about diving into the
world of data but aren't sure where to
start you are in the right place with
more data being generated every day
companies are eager to hire skilled
professionals who can turn this raw data
into actionable insights it's like
having a super power in today's world of
information overload so in today's video
we are breaking down a fast track road
map to learn data science in Just 2
months there are various roles within
this field like like data analysts who
focus on interpreting data data
Engineers who build and maintain data
pipelines and machine learning Engineers
who develop algorithms to make
predictions the demand for these roles
is kyotic but why because every industry
is becoming more data driven and
businesses need experts to make sense of
all that information so grab your
notebooks and let's Jump Right In so
first let's understand what data science
is data science is all about extracting
insights from data using Tech techniques
from statistics programming and machine
learning let's understand this with a
simple case study so imagine an
e-commerce company wants to improve its
sales by analyzing customer data a data
scientist can identify purchasing
patterns such as which products are
often brought together this Insight can
lead to strategic decisions like
bundling products or personalizing
marketing campaigns ultimately boosting
sales so to start with let's quickly see
what a data scientist life cycle looks
like the data life cycle is a structured
process that involves several key phases
it starts with problem identification
where the data scientist defines the
problem or question to be addressed next
is data collection where relevant data
is gathered from various sources then
comes data cleaning data cleaning
follows where raw data is processed to
remove inaccuracies and inconsistencies
in the exploratory data analysis phase
initial patterns and insights are
discovered this leads to model building
where algorithms are applied to predict
outcomes or classify data so the cycle
continues with model evaluation where
the performance of the model is tested
and refined and finally deployment and
monitoring are carried out where the
model is put into production and its
performance is continuously monitored to
ensure it remains accurate and effective
so now let's talk about the essential
skills you'll need to get started in
data science number one programming
languages you'll need to learn python or
R python is widely used because it's
easy to learn and has many useful
libraries for data science art is also
great especially for statistical
analysis then comes tools and
Technologies tools like Jupiter
notebooks make coding easier and more
interactive SQL is important for working
with databases allowing you to retrieve
and manage data for creating visuals
libraries like matplot lib and caborn
help you turn data into graphs and
charts then comes mathematics and
statistics you don't need to be a math
expert but it's helpful to understand
the basics of Statistics probably and
some linear algebra data manipulation
and cleaning this is a big part of data
scientist job you'll often spend time
cleaning up data to make sure it's
accurate and ready for analysis python
Panda's library is perfect for sorting
filtering and transforming data and
finally learning about machine learning
algorithms is crucial this is where you
use data to make predictions and
decisions which is a key part of data
science so how do you learn all this so
that you can quickly reach your goal
next up we are diving into an exciting
two-month plan to Kickstart your journey
to become a data scientist so without
any further Ado let's get started so in
week one start with python a mustn
programming language for any aspiring
data scientist you'll begin by
familiarizing yourself with python
syntax essentially how to write code and
understanding different data types like
numbers and strings you should also
learn about variables which act as
containers for storing data next you can
explore how to use operators and control
flow tools the these include Loops which
are useful for repeating tasks and
conditionals like IFL statements that
make your code more Dynamic and
responsive as you become comfortable you
should move on to intermediate topics
such as lists for storing multiple items
dictionaries for storing data in key
value Pairs and file handling for
reading and writing files these skills
are essential for efficiently managing
and manipulating data now week two
consists of mathematics and statistics
in week two focus on math side of data
science start with basic algebra
Concepts and then move on to linear
algebra which is crucial for
understanding data relationships and
Transformations you should also learn
about statistics a vital tool in data
science this includes understanding
descriptive statistics which help
summarize data and probability
distributions which provide insights
into Data patterns and likelihoods now
in week three you can go with data
manipulation and cleaning focus on data
manipulation and cleaning you can use
pandas a powerful python library to
handle data frames merge data sets and
perform basic data Transformations
pandas is your go-to tool for making
data analysis straightforward and
efficient you should also learn data
cleaning techniques such as handling
missing values like deciding how to deal
with blanks in your data and detecting
outliers clean data is essential for
accurate analysis and reliable results
now it's time to talk about databases
you can learn SQL the language used for
managing and quitting databases start
with basic commands like select for
choosing data we for filtering data and
order by for sorting data so simply
learns tutorials can be a great resource
to guide you through these foundational
steps and as you progress you should
delve into more advanced topics like
joints for combining data from different
tables and aggregations for summarizing
data so these skills are crucial for
handling complex queries and gaining
deeper insights from your data now
moving on to month two begin the second
month by mastering data visualization
you can learn to create simple charts
like bar graphs line charts and and
Scatter Plots using ma plot lip a widely
used visualization Library these visuals
are essential for communicating your
data findings clearly and effectively so
once you're comfortable with the basics
you should explore plotly for creating
interactive and more complex
visualizations this skill will enable
you to present data in an engaging and
dynamic way making your analysis more
impactful so you can start by
understanding the basics of machine
learning including supervised learning
where the computer learns from labeled
data and unsupervised learning where the
computer finds patterns in data on its
own these are foundational Concepts that
will open up many possibilities in data
science you should get familiar with key
algorithms like linear regression used
for predicting values and key means
clustering used for grouping similar
data points these algorithms are the
building blocks of machine learning and
are widely used in various applications
now in this week take your machine
learning skills to the next level you
can learn about more advanced algorithms
like decision trees great for both class
classification and regression tasks then
comes random fors which improve accuracy
through assemble learning and support
Vector machines used for classification
tasks these tools will help you build
more sophisticated and accurate models
and as you up toward the end of your
Learning Journey it's time to put your
skills to the test you should work on
real world data sets practice building
and deploying machine learning models
and create data visualization dashboards
these projects are crucial for
reinforcing what you have learned and
gaining practical experience finally
complete a Capstone project to Showcase
your ability to handle N2 and data
science tasks this comprehensive project
is a great way to demonstrate your
skills and knowledge in a real world
context simply learn offers Project
based resources to support you in this
final step now to boost your data
science learning try these additional
tools and resources number one kaggle
this site lets you work with real world
data sets and join competitions it's a
great way to practice your skills and
see different approaches to solving
problems then comes GitHub Hub explore
open-source projects to see what others
are working on you can also share your
own projects and get feedback from the
community and then Google collab this
cloud-based tool allows you to write and
run python code directly into your
browser it's ideal for trying out code
without having to set up anything on
your computer deep learning deep
learning was first introduced in the
1940s deep learning did not develop
suddenly it developed slowly and
steadily over seven decades many thesis
and discoveries were made on deep
learning from the 1940s to 2000 thanks
to companies like Facebook and Google
the term deep learning has gained
popularity and may give the perception
that it is a relatively New Concept deep
learning can be considered as a type of
machine learning and artificial
intelligence or AI that imitates how
humans gain certain types of knowledge
deep learning include statistics and
predictive modeling deep learning makes
processes quicker and simpler which is
advantageous to data scientists to
gather analyze and interpret massive
amounts of data having the fundamentals
discussed let's move into the different
types of deep learning neural networks
are the main component of deep learning
but neural networks comprise three main
types which contain artificial neural
networks orn convolution neural networks
or CNN and recurrent neural networks or
RNN artificial neural networks are
inspired biologically by the animal
brain convolutional neural networks
surpass other neural networks when given
inputs such as images Voice or audio it
analyzes images by processing data
recurrent neural networks uses
sequential data or series of data
convolutional neural networks and
recurrent neural networks are used in
natural language processes speech
recognition image recognition and many
more machine learning the evolution of
ml started with the mathematical
modeling of neural networks that served
as the basis for the invention of
machine learning in 1943 neuroscientist
Warren mccullock and logician Walter
pittz attempted to quantitatively map
out how humans make decisions and carry
out thinking processes therefore the
term machine learning is not new machine
learning is a branch of artificial
intelligence and computer science that
uses data and algorithms to imitate how
humans learn gradually increasing the
system's accuracy there are three types
of machine learning which include
supervised learning what is supervised
learning well here machines are trained
using labeled data machines predict
output based on this data now coming to
unsupervised learning models are not
supervised using a training data set it
is comparable to the learning process
that occurs in the human brain while
learning something new and the third
type of machine learning is
reinforcement learning here the agent
learns from feedback it learns to behave
in a given environment based on actions
and the result of the action this
feature can be observed in
robotics now coming to the evolution of
AI the potential of artificial
intelligence wasn't explored until the
1950s although the idea has been known
for centuries the term artificial
intelligence has been around for a
decade still it wasn't until British
polymath Alan Turing posed the question
of why machines could use knowledge like
humans do to solve problems and make
decisions we can Define artificial
intelligence as a technique of turning a
computer-based robot to work and act
like humans now let's have a glance at
the types of artificial intelligence
weak AI performs only specific tasks
like Apple Siri Google assistant and
Amazon's Alexa you might have used all
of these Technologies but the types I am
mentioning after this are under
experiment
General AI can also be addressed as
artificial general intelligence it is
equivalent to human intelligence hence
an AGI system is capable of carrying out
any task that a human can strong AI
aspires to build machines that are
indistinguishable from the human mind
both General and strong AI are
hypothetical right now rigorous research
is going on on this matter there are
many branches of artificial intelligence
which include machine learning deep
learning natural language processing
robotics expert systems fuzzy logic
therefore the correct answer for which
is not a branch of artificial
intelligence is option a data
analysis now that we have covered deep
learning machine learning and artificial
intelligence the final topic is data
science Concepts like deep learning
machine learning and artificial
intelligence can be considered a subset
of data science let us cover the
evolution of data science the phrase
data science was coined in the early
1960s to characterize a new profession
that would enable the comprehension and
Analysis of the massive volumes of data
being gathered at the time since its
Beginnings data science has expanded to
incorporate ideas and methods from other
fields including artificial intelligence
machine learning deep learning and so
forth data science can be defined as the
domain of study that handles vast
volumes of data using modern tools and
techniques to find unseen patterns
derive meaningful information and make
business decisions therefore data
science comprises machine learning
artificial intelligence and deep
learning there are a lot of areas where
data science can be used one of the very
common one is fraud detection or fraud
prevention there are a lot of fraudulent
activities or transactions primarily on
the Internet it's very easy to commit
fraud and therefore we can use data
science to a prevent or detect fraud
there are certain algorithms machine
learning algorithms that can be used
like for example some outlier techniques
clustering techniques that can be used
to detect fraud and prevent fraud as
well so who is the data scientist ra it
is actually a very generic role that
defines somebody who is working with
data is known as a data scientist but
there can be very specific activities
and the roles can be actually much more
specific what exact ly a person does
within the area of data science can be
much more specific but broadly anybody
working in the area of data science is
known as a data scientist so what does a
data scientist do these are some of the
activities data acquisition data
preparation data mining data modeling
and then model maintenance we will talk
about each of these in a great detail
but at a very high level the first step
obviously is to get the raw data which
is known as data acquisition it can be
all kinds of format and it could be
multiple sources but obviously that raw
data cannot be used as it is for
performing data mining activities or
data modeling activities so the data has
to be cleaned and prepared for using in
the data models or in the data mining
activity so that is the data preparation
then we actually do the data mining
which can also include some exploratory
activities and then if we have to do
stuff like machine learning then you
need to build a machine learning model
and test the model get insights out of
it and then if um the model is fine you
deploy it and then you need to maintain
the model because over a period of time
it is possible that you need to tweak
the model because of change in the
process or change in the data and so on
so that all comes under the model
maintenance so let's take deeper look at
each of these activities let's start
with data acquisition so in the stage of
data acquisition basically the data
scientist will collect raw data from all
possible sources so this could be
typically an rdbms which is a relational
database or it can also be a non rdbms
or could be flat files or unstructured
data and so on so we need to bring all
that data from different sources if
required we need to do some kind of
homogeneous formatting so that it all
fits into in in a looks at least format
from a format perspective it looks
homogeneous so that may be requiring
some kind of transformation very often
this is loaded into what is known as
data warehouse so this can also be
sometimes referred to as ETL or extract
transform and load so a data warehouse
is like a common place where data from
different sources is brought together so
that people can perform data science
activities like reporting or data mining
or the statistical analysis and so on so
data from various sources is put in a
centralized place which is known as a
data warehouse so that is also known as
ETL and and in order to do this there
can be the data scientists can take help
of some ETL tools there are some
existing tools that a data scientists
can take help of like for example data
stage or Talent OR Informatica these are
pretty good tools for performing these
ETL activities and getting the data the
next stage now that you have the raw
data into a data warehouse you still
probably are not in a position to
straight away use this data for
performing the data mining activity so
that is where data preparation comes
into play and there are multiple reasons
for that one of them could be the data
is dirty there are some missing values
and so on and so forth so a lot of time
is actually spent in this particular
state so data scientist spends a lot of
time almost 60 to 70% of the time in
this part of the project or the process
which is data preparation so there are
again within this there can be multiple
sub activities starting from let's say
data cleaning you will probably have
missing values the data there is some
columns the values are missing or the
values are incorrect there are null
values and so on and so forth so that is
basically the data cleaning part of it
then you need to perform certain
Transformations like for example
normalizing the data and so on right so
you could probably have to modify
categorical values into numerical values
and so on and so forth so these are
transformation
activities then we may have to handle
outliers so the data could be such that
there are a few values which are Way
Beyond the normal behavior of the data
for whatever reason either people have
keyed in wrong values or for some reason
some of the values are completely out of
range so those are known as outliers so
there are certain ways of handling these
outliers and detecting and handling
these outliers so this is a part of what
is known as exploratory analysis so you
quickly explore the data to find out are
there so and you can use visual tools
like plots and identify what are the
outliers and see how we can get rid of
the outliers and so on then the next
part could be data Integrity data
Integrity is to validate for example if
there are some primary keys that all the
primary keys are populated there are
some foreign Keys then at least most of
the foreign Keys should be populated and
otherwise when we are trying to query
the data you may get wrong values and so
on so that is the data Integrity part of
it and then we have what is known as
data reduction sometimes we may have
duplicate values we may have columns
that may be duplicated because they're
coming from different sources the same
values are there and so on so a lot of
this can be done using what is known as
data reduction and thereby you can
reduce the size of the data drastically
because very often this could be written
in data which can be removed and so on
so let's take a look at what what are
the various techniques that are used for
data cleaning so we need to ensure that
the data is valid and it is consistent
and uniform and accurate so these are
the various parameters that we need to
ensure as a part of the data cleaning
process now what are the techniques that
that are used for data cleaning or so we
will see what each of these are in this
particular case and uh so what is the
data set that we have we have uh data
about a bank and it's customer details
so let's take an example and see how we
go about cleaning the data and in this
particular example we're assuming we are
using python so let's assume we loaded
this data which is the raw file. CSV
this is how the customer data looks like
and um we will see for example if we
take a closer look at the geography
column we will see that there are quite
a few blank spaces so how do we go about
when we have some blank spaces or if it
is a string value then we put a empty
string here or we just use a space or
empty string if they are numerical
values then we need to come up with a
strategy uh for example we put the mean
value so wherever it is missing we find
the mean for that particular column so
in this case let's assume we have credit
score and we see that quite a few of of
these values are missing so what do we
do here we find the mean for this column
for all the existing values and we found
that the mean is equal to
6386 so we kind of write a piece of code
to replace wherever there are blank
values n an is basically like null and
uh we just go ahead and say fill it with
the mean value so this is the piece of
code we are writing to fill it so all
the blanks or all the null values get
replaced with the mean value now one of
the reasons for doing this is that very
often if you have some such situation
many of your statistical functions may
not even work so that's the reason you
need to fill up these values or either
get rid of these records or fill up
these values with something meaningful
so this is one mechanism which is
basically using a mean there are a few
others as we move forward we can see
what are the other ways for example we
can also say that any missing value in a
particular row if even one column the
value is missing you just drop that
particular row or delete all rows where
even a single column has missing values
so that is one way of dealing now the
problem here can be that if a lot of
data has let's say one or two columns
missing and uh we drop many such rows
then overall you may lose out on let's
say 60% of the data as some value or the
other missing 60% of the rows then it
may not be a good idea to delete all the
rows like in that manner because then
you're losing pretty much 60% of your
data Therefore your analysis won't be
accurate but if it is only 5 or 10% then
this will work another way is only to
drop values where or rather drop rows
where all the columns are empty which
makes sense because that means that
record is of really no use because it
has no information in it so there can be
some situations like that so we can
provide a condition saying that drop the
records where all the columns are blank
or not applicable we can also specify
some kind of a threshold let's say you
have 10 or 20 columns in a row you can
specify that maybe five columns are
blank or null then you drop that record
so again we need to take care that such
a condition such a situation the amount
of data that has been removed or
excluded is not large if it is like
maybe 5% maximum 10% then it's okay but
by doing this if you're losing out on a
large chunk of data then it may not be a
good idea you need to come up with
something big better what else we need
to do next is so the data preparation
part is done so now we get into the data
mining part so what exactly we do in
data mining primarily we come up with
ways to take meaningful decisions so
data mining will give us insights into
the data what is existing there and then
we can do additional stuff like maybe
machine learning and so on to get
perform Advanced analytics and so on so
the one of the first steps we do is what
is known as as data Discovery and uh
which is basically like exploratory
analysis so we can use tools like tblo
for doing some of this so let's just
take a quick look at how we go about
that so tblo is excellent data mining or
actually more of a reporting or a bi
tool and you can download a trial
version of tblo at table.com or there is
also Tableau public which is free and
you can actually use and play around
however if if you want to use it for
Enterprise purpose then it is a
commercial software so you need to
purchase license and you can then run
some of the data mining activities say
your data source your data is in some
Excel sheet so you can select the source
as Microsoft Excel or any other format
and the data will be brought into the
tblo environment and then it will show
you what is known as dimensions and uh
measures so dimensions are all the
descriptive columns so and Tableau is
intelligent enough to actually identify
these dimensions and measures so
measures are the numerical value so as
you can see here uh customer ID gender
geography these are all Dimensions non-
numeral values whereas age balance
credit score and so on are numeric
values so they come under measures so
you've got your data into Tablo and then
you want to let's say build a small
model and you want to let's say solve a
particular problem so what is the
problem statement all right let's say we
want to analyze why customers are
leaving the bank which is known as exit
and we want to analyze and see what are
some of the factors for exiting the bank
and we want to Let's assume consider
these uh three of them like let's say
gender credit card and geography these
as a criteria and analyze if these are
in any way impacting or have some
bearing on the customer exiting or the
customer exit Behavior okay so let's um
use TBL loo and very quickly we will be
able to find out how these parameters
are affecting all right so let's see so
this is our customer data so from our
Excel sheet we have data set about let's
say 10,000 rows and we want to find out
what is the criteria let's start with
gender let's say we want to first use
gender as a criteria so Tablo really
offers an easy drag and drop kind of a
mechanism so that makes it really really
easy to perform this kind of analysis so
what we need to do is exited says
whether the customer has exited or not
so it has a value of zero and one and
then of course you have gender and so on
so we will take these two and simply
drag and drop okay so exit it and then
we will put gender and if we drag and
drop into the analysis side of of TBL
Loop all right so here what we are doing
is we are showing male female as two
different columns here and and zero for
people who did not exist and one for
people who exited and that is colorcoded
so the blue color means people who did
not exit and uh this yellow color means
people who did exit all right so now if
we pull the data here create like bar
graphs this is how it would look uh so
what is yellow let's go back so yellow
is uh who exited and uh for the male
only
16.45% have exited and we can also draw
a ref line that will help us or even
provide aliases so these are a lot of
fancy stuff that is um provided by tblo
you can create aliases and um so that it
looks good rather than basic labels and
you can also add a reference line so you
add a reference line something like this
from here we can make out that on an
average female customers exit more than
the male customers right so that is what
we are seeing here on an average so we
have uh analyzed based on gender we do
see that there is some difference in the
male and female Behavior now let's take
the next criteria which is the credit
card so let's see if having a credit
card has any impact on the customer exit
Behavior so just like before we drag and
drop the credit card has credit card
column if we drag and drop here and then
we will see that there is pretty much no
difference between people having credit
card and not having credit card 20.8 1%
of people who have no credit card have a
exited and similarly 20.18% of people
who have credit card have also exited so
the credit card is not having much of an
impact that's what this piece of
analysis shows last we will basically go
and check how the geography is impacting
so once again we can drag and drop
geography column onto this side and uh
if we see here there are geographies
like I think there are about three
geographies like France Germany and uh
Spain and um we see that there is some
kind of a impact with the geography as
well okay so what we derive from this is
that the credit card is really we can
ignore the credit card variable or
feature from our analysis because that
doesn't have any impact but gender and
geography we can keep and do further
analysis okay all right so what are some
of the advantages of data mining bit
more detailed analysis can can help us
in predicting the future Trends and it
also helps in identifying customer
Behavior patterns okay so you can take
informed decisions because the data is
telling you or providing you with some
insights and then you take a decision
based on that if there is any fraudulent
activity data mining will help in
quickly identifying such a fraud as well
and of course it will also help us in
identifying the right algorithm for
performing more Advanced Data Mining
activities activities like machine
learning and so on all right so the next
activity now that we have the data we
have prepared the data and performed
some data mining activity the next step
is model building let's take a look at
model building so what is model building
if we want to perform a more detailed
data mining activity like maybe perform
some machine learning then you need to
build a model and how do you build a
model first thing is you need to select
which algorithm you want to use to solve
the problem on hand and also what kind
of data that is available and so on and
so forth so you need to make a a choice
of the algorithm and based on that you
go ahead and create a model train the
model and so on now machine learning is
kind of at a very high level classified
into supervised and unsupervised so if
we want to predict a continuous value
could be a price or a temperature or or
a height or a length things like that so
those are continuous values and if you
want to find some of those then you use
techniques like regression linear
regression simple lar regression
multiple linear regression and so on so
these are the algorithms on the other
hand there will be situations or there
may be situations where you need to
perform unsupervised learning case of
unsupervised learning you don't have any
historical labeled data so to learn from
so that is when you use unsupervised
learning and uh some of the algorithms
in unsupervised learning learning are
clustering K means clustering is the
most common algorithm used in
unsupervised learning and similarly in
supervised learning if you want to
perform some activity on categorical
values like for example it is not
measured but it is counted like you want
to classify whether this image is a cat
or a dog whether you want to classify
whether this customer will buy the
product or not or you want to classify
whether this email is a spam or not spam
so these are examples of categorical
value vales and uh these are examples of
classification then you have algorithms
like logistic regression K nearest
neighbor or KNN and support Vector
machine so these are some of the
algorithms that are used in this case
and similarly in case of unsupervised
learning if you need to perform on
categorical values you have some
algorithms like Association analysis and
hidden Marco model okay so in order to
understand this better let's take uh an
example and uh take you through the
whole process and then we will also see
how the code can be written to perform
this now let's take our example here
where we want to perform supervised
learning which is basically we want to
do a multi-linear regression which means
there are multiple independent variables
and then you want to perform a linear
regression to predict certain value so
in this particular example we have World
happiness data so this is a data about
the happiness quotient of people from
various countries and and we are trying
to predict and see whether our how our
model will perform so what is the
question that we need to ask first of
all how to describe the data and then
can we make a predictive model to
calculate the happiness score right so
based on this we can then decide on what
algorithm to use and what model to use
and so on so variables that are
available or used in this model this is
a list of variables that are available
there is a happiness rank I'll load the
data and or I'll show you the data in a
little bit so becomes clear what are
these so there is what is known as a
happiness rank happiness score which is
happiness score is more like absolute
value where as rank is what is the
ranking and then which country we are
talking about and within that country
which region and what kind of economy
and whether the family which family and
health details and freedom trust
generosity and so on and so forth so
there are multiple variables that are
available to us and uh the specific
details probably are not required and
there can be um in another example the
variables can be completely different so
we don't have to go into the details of
what exactly these variables are but
just enough to understand that we have a
bunch of these variables and now we need
to use either all or some of these
variables and then which we also
sometimes refer to as features and then
we need to build our model and train our
model all right so let's assume we will
use python in order to perform this
analysis or perform this machine
learning activity
and I will actually show you in our lab
in in a little bit this whole thing we
will run the Live code but quickly I
will run you through the slides and then
we will go into the lab so what are we
doing here first thing we need to do is
import a bunch of libraries in Python
which are required to perform our
analysis most of these are for
manipulating the data preparing the data
and then psychic learn or SK learn is
the library which we will use actually
for this particular machine learning
activity which is line regression so we
have numpy we have pandas and so on and
so forth so all these libraries are
imported and then we load our data and
the data is in the form of a CSV file
and there are different files for each
year so we have data for 2015 16 and 17
and uh so we will load this data and
then combine them concatenate them to
prepare a single data frame and uh here
we are making an assumption that you are
familiar with python so it becomes easy
easier if you are familiar with Python
programming language or at least some
programming language so that you can at
least understand by looking at the code
so we are reading the file each of these
files for each year and this is
basically we are creating a list of all
the names of the columns we will be
using later on you will see in the code
so we have loaded 2015 then
2016 and then also 2017 so we have
created um three data frames and then we
concatenate all these three data frames
this is what we doing here then we
identify which of these columns are
required which for example some of the
categorical values do we really need we
probably don't then we drop those
columns so that we don't unnecessarily
use all the columns and make the
computation complicated we can then
create some plots using plotly library
and it has some powerful features
including creation or creation of maps
and so on just to understand the pattern
the happiness Co or how the happiness is
across all the countries so it's a nice
visualization we can see each of these
countries how they are in terms of their
happiness score this is the legend here
so the lighter colored countries have
lower ranking and so these are the lower
ranking ones and these are higher
ranking which means that the ones with
this dark colors are the happiest ones
so as you can see here Australia and
maybe this side uh us and so on are the
happiest ones okay the other thing that
we need to do is the correlation between
the happiness score and happiness rank
we can find a correlation using a
scatter plot and we find that yes they
are kind of inversely proportion which
is obvious so if the score is high
happiness score is high then they are
ranked number one for example highest is
scored as number one so that's the idea
behind this so the happiness score given
here and the happiness rank is actually
given here so they are inversely
proportional because the higher the
score then the absolute value of the
rank will be lower right number one has
the highest value of the score and so on
so they are inversely correlated but
there is a strong what this graph shows
is that there is a strong correlation
between happiness Rank and happiness
score and then we do some more plots to
visualize this we determine that
probably Rank and score are pretty much
conveying the same message so we don't
need both of them so we will kind of
drop one of them and uh that is what we
are doing here so we dro the happiness
Rank and similarly so this is one
example of how we can remove some
columns which are not adding value so we
will see in the code as well how that
works moving on this is a correlation
between pretty much each of the columns
with the other columns so this is a
correlation you can plot using plot
function and uh we will see here that
for example happiness score and
happiness score are correlated strongest
correlation right because every variable
will be highly correlated to itself so
that's the reason so the darker the
color is the higher the correlation and
as so the and correlation in numerical
terms goes from 0 to one so one is the
highest value and it can only be between
0o and one correlation between two
variables can be only have a value
between 0 and one so the numerical value
can go from 0 to one and one here is
dark color and um zero is kind of dark
but it is blue color from Red it goes
down the dark blue color color indicates
pretty much no correlation so the from
this heat map we see that happiness and
economy and family are probably also
help probably are the most correlated
and then it keeps decreasing after
Freedom kind of keeps decreasing and
coming to pretty much uh zero all right
so that is a correlation graph and then
we can probably use this to find out
which are the columns that need to be
dropped which do not have very high
correlation and uh we take only those
columns that we will need so this is a
code for dropping some of the columns
once we have prepared the data when we
have the required columns then we use
psyit learn to actually split the data
first of all this is a normal machine
learning process you need to split the
data into training and test data set in
this case we are splitting into 80/20 so
80 is the training data set and 20 is
the test data set so that's what we are
doing here so we use uh train test split
method or function so you have all your
training data uh in xcore train the
labels in Yore train similarly xor test
has the test data the inputs whereas the
labels are in Yore test so that's how
and this value whether it is 8020 or
50/50 that is all individual preference
so in our case we are using 8020 all
right and uh then the next is to create
a linear regression instance so this is
what we're doing we're creating an
instance of linear regression and then
we train the model using the fit
function and uh we are passing X and Y
which is the x value and the label data
regular input and the label data label
information then we do the test We Run
The or we perform the evaluation on the
test data set so this is what we are
doing with the test data set and then we
will evaluate how accurate the model is
and using the psyit learn function ality
itself we can also see what are the
various parameters and what are the
various coefficients because in linear
regression you will get like a equation
of like a straight line Y is equal to
Beta 0 plus beta 1 X1 plus beta 2 X2
those beta 1 beta 2 Beta 3 are known as
the coefficients and beta 0 is The
Intercept after the training you can
actually get these information of the
model what is The Intercept value what
are the coefficients and so on by using
these functions so let's take quickly go
into the lab and take a look at our code
okay so this is my lab this is my
Jupiter notebook where the code I have
the actual code and I will take you
through this code to run this linear
regression on the world happiness data
so we will import a bunch of libraries
numpy pandas plot plotly and so on also
yeah psychic learn that's also very
important so that's the first step then
I will import my data and uh the data is
in three parts there are three files one
for each year 2015 2016 and 2017 and it
is a CSV file so I've imported my data
let's take a look at the data quickly
glance at data so this is how it looks
we have the country region happiness
Rank and then happiness score there are
some standard errors and then what is
the per capita family and so on so and
then we will keep going we will create a
list of all these column names we will
be using later so for now just I will
run this code and no need of major
explanation at this point we know that
some of these columns probably are not
required so you can use this drop
functionality to remove some of the
columns which we don't need like for
example region and standard error will
not be contributing to our model so we
will basically drop those values out
here so we use the drop and then we
created a vector of with these names
column names that's what we are passing
here instead of giving the names of the
columns here we can pass a vector so
that's what we are doing so this will
drop from our data frame it will remove
region and standard error these two
columns then the next step we will read
the data for 2016 and also
2017 and then we will concatenate this
data so let's do that so we have now
data frame called Happiness which is a
concatenation of both all the three
files let's take a quick look at the
data now so most of the unwanted columns
have been removed and you have all the
data in one place for all the three
years and this is how the data looks and
if you want to take a look at the
summary of The Columns you can say
describe and uh you will get this
information for example for each of the
columns what is the count what what is
the mean value standard deviation
especially the numeric values okay not
the categorical values so this is a
quick way to see how the data is and uh
initial little bit of exploratory
analysis can be done here so what is the
maximum value what's the minimum value
and so on for each of the columns all
right so then we go ahead and create
some visualizations using plotly so let
us go and build a plot so if we see here
now this is the relation correlation
between happiness Rank and happiness
score this is what what we have seen in
the slides as well we can see that there
is a tight correlation between them only
thing is it is inverse correlation but
otherwise they are very tightly
correlated which also says that they
both probably provide the same
information so there is not not much of
value add so we'll go ahead and drop the
happiness rank as well from our columns
so that's what we're doing here and now
we can do the creation of the
correlation heat map let us plot the
correl ation heat map to see how each of
these columns is correlated to the
others and we as we have seen in the
slides this is how it looks so happiness
score is very highly correlated so this
is the legend we have seen in the slide
as well so blue color indicates pretty
much zero or very low correlation deep
red color indicates very high
correlation and the value correlation is
a numeric value and the value goes from
0 to one if the two items or two
features or columns are highly
correlated then they will be as close to
one as possible and two columns that are
not at all correlated will be as close
to zero as possible so that's how it is
for example here happiness score and
happiness score every column or every
feature will be highly correlated to
itself so it is like between them there
will be correlation value will be one so
that's why we see deep red color but
then others are for example with higher
values are economy and then health and
then maybe family and freedom so these
are osity and Trust are not very highly
correlated to happiness score so that is
uh one quick exploratory analysis we can
do and uh therefore we can drop the
country and the happiness rank because
they also again don't have any major
impact on the analysis on our analysis
so now we have prepared our data there
was no need to clean the data because
the data was clean but if there were
some missing values and so on as we have
discussed in the slides we would have
had to perform some of the data cleaning
activities as well but in this case the
data was clean all we needed to do was
just the preparation part so we removed
some unwanted columns and we did some
exploratory data analysis now we are
ready to perform the machine learning
activity so we use psychic learn for
doing the machine learning psychic learn
is python library that is available for
performing our uh machine learning once
again we will import some of these
libraries like pandas and numpy and U
also psychic learn first step we will do
is split the data in 2080 format so you
have all the test data which is 20% of
the data is test data and 80% is your
training data so this test size
indicates how much of it is in the what
is the size of the test data so
remaining which is point here we are
saying 0 2 therefore that means training
is 08 so training data is 80% all right
so we have executed that split the data
and now we create an instance of the
linear regression model so LM is our
linear regression model and we pass X
and Y the training data set and call the
function fit so that the model gets
trained so now once that is done
training is done training is completed
and now what we have to do is we need to
predict the values for the test data so
the next step is using so you see here
fit will basically run the Training
Method predict will actually predict the
values so we are passing the input
values which is the independent
variables and we are asking for the
values of the dependent variable which
is which we are capturing in Yore PR and
we use the predict method here lm.
predict so this will give us all the
predicted y values and remember we
already have Yore test has the actual
values which are the labels so that we
can use these two to compare and find
out how much of it is errored so that's
what we are doing here we are trying to
find the difference between the
predicted value and the actual value
Yore test is the actual value for the
test data and Yore predict is the
predicted value we just found out the
predicted value so we will run that and
we can do a quick check as to how the
data looks how is the difference so in
some cases it is positive some cases it
is negative but in most of the cases I
think the difference is very small this
is exponential to the power of 0us 04
and so on so looks like our model has
performed reasonably well we can now
check some of the parameter of our model
like the intercept and the coefficients
so that's what we are doing here so
these are the coefficients of the
various parameters that we are the
coefficients of the various independent
variables okay so these are the values
then we can quickly go ahead and list
them down as well against the
corresponding independent variable so
the coefficients against the
corresponding independent variable so
1.51 is the coefficient for economy
99983 is for family coefficient for
family and health and so on and so forth
right so that's what this is showing now
we can use the functionality readily
available functionality of psychic learn
and then plot that to find some of the
parameters which determine the accuracy
of this model like for example what is
the mean square error and so on so
that's what we are doing here so let's
just go ahead and run this so you can
see here that the root mean square error
is pretty low which is a good sign and
uh which is one of the measures of um
how well our model is performing we can
do one more quick plot to just see how
the actual values and the predicted
values are looking and once again you
can see that as we have seen from the
root mean square error the root mean
square error is very very low that means
that the actual values and the predicted
values are pretty much matching up
almost matching up and this plot also
shows the same so this line is going
through the predicted values and the
actual values and the difference is very
very low so again this is actual data
this is one example where the the
accuracy is high and the predicted
values are pretty much matching with the
actual values but in real life you may
find that these values are slightly more
scattered and you may get the error
value can be relatively On The Higher
Side the root me squ okay so this was a
good and quick example of uh the code to
perform data science activity or a
machine learning or data mining activity
in this case we did what is known as
linear regression so let let's go back
to our slides and see what else is there
so we saw this these are the
coefficients of each of the features in
our code and uh we have seen the root
mean square error as well and uh with we
can take just few hundred countries
certain values and actually predict to
see if how the model is performing and I
think we have done this as well and in
this case as we have seen pretty much
the predicted values and the actual
values are pretty much matching which
means our model model is almost 100%
accurate as I mentioned real life it may
not be the case but in this particular
case we have got a pretty good model
which is very good also subsequently we
can assume that this is how the equation
in linear regression the model is
nothing but an equation like Y is equal
to Beta 0 plus beta 1 X1 plus beta 2 X2
plus beta 3 X3 and so on so this is what
we are showing here so this is our
intercept which is beta 0 and then we
have beta one into economy value beta 2
into the family value beta 3 into health
value and so on so that is what is shown
here okay so I think the next step once
we have the results from the data mining
or machine learning activity the next
step is to communicate these results to
the appropriate stakeholders so that is
what we will see here now so how do we
communicate usually you take these
results and then either prepare a
presentation or put it in a document and
then show show them these actionable
results or actionable insights and uh
you need to find out who are your target
audience and uh put all the results in
context and uh maybe if there was a
problem statement you need to put this
results in the context of the problem
statement what was our initial goal that
we wanted to achieve so that we need to
communicate here based on you remember
we started off with what is the question
and what is the data and so on and and
then what is the answer so we we need to
put the results and then what is the
methodology that we have used all that
has to be put and clearly communicated
in business terms so that the people
understand very well from a business
perspective so once the model building
is done once the results are published
and communicated the last part is
maintenance of this model now very often
what can happen is the model may have to
be subsequently updated Orel modified
because of multiple reasons either the
the data has changed the way the data
comes has changed or the process has
changed or for whatever reason the
accuracy may keep changing once you have
trained the model the for example we got
a very high accuracy but then over a
period of time there can be various
factors which can cause that so from
time to time we need to check whether
the model is performing well or not the
accuracy needs to be tested once in a
while and if required you may have have
to rebuild or retrain the model so you
do the assessment you you see if it
needs any tweaks or changes and then if
it is required you need to probably
retrain the model with the latest data
that you have and then you deploy it you
build the model train it and then you
deploy it so that is like the
maintenance cycle that you may have to
take the model data analist versus data
engineer versus data scientist which one
to choose this is one of the most
popular questions asked by Learners
looking for a career in data and
analytics I'm sure you two would have
come across these job roles in the ever
growing data science landscape though
they all deal with data these jobs are
not the same there are significant
differences between what a data analyst
data engineer and a data scientist does
we will look at these job roles and the
differences in
detail
first let's look at some data analytics
and data science trends
the analytics and data science Market is
thriving data analytics data engineering
and data science are the key trends in
today's accelerating Market as per
status.com the global big data analytics
Market Revenue will grow at a cagr of
30% with Revenue reaching over 68
billion US by
2025 according to technavio the
Enterprise data management Market Market
is expected to increase by
64.8 billion US by
2025 as per markets and markets.com the
big data market size is projected to
grow from1 62.6 billion US in
2021 to
273.5 billion US in
2026 now another report from Research
Drive says that the data science
platform Market is estimated to reach
$24.3 billion US by
2026 so with so much data available and
companies making huge Investments to
drive business insights the job
opportunities for data analysts data
engineers and data scientists are going
to increase in 2022 and over the coming
years now let's learn the major
differences between data analyst versus
data engineer versus data
scientist so who are they
a data analyst analyzes and interprets V
volumes of data in order to extract
meaningful information out of it they
find solutions to a business problem and
make critical business decisions the
insights provided by data analysts are
important to companies that want to
understand the needs of their end
customers we talking about who are data
engineer is a data engineer on the other
hand builds infrastructure and scalable
Pipelines to manage the flow of data and
prepare it for
analysis so basically they optimize the
systems that enable data analyst and
data scientists to perform their job
efficiently data scientists are
professionals who analyze and visualize
existing data and use algorithms to
build predictive models for making
future decisions they also engage with
Business Leaders to understand their
needs and present complex
findings with that let's look at the
primary roles and responsibilities of
these three job
roles data analysts are responsible to
collect clean store and process data
they discover hidden patterns from data
by performing explorat data analysis and
visualize data by creating charts and
graphs acquiring data from primary and
secondary sources is one of their key
tasks the build reports and dashboards
and also maintain databases
now talking about the roles and
responsibilities of a data engineer a
data engineer performs data acquisition
the design build and test data as well
to develop and maintain data
architecture data Engineers are tasks
with testing integrating managing and
optimizing data from a variety of
sources so they integrate data into
existing data pipelines prepare data for
modeling and perform various ETL
operations
now talking about the roles and
responsibilities of a data scientist so
data scientists develop machine learning
models to identify Trends in data for
making decisions they develop hypothesis
and use the knowledge of Statistics data
visualization and machine learning to
forecast the future for the business
data scientists visualize data and use
storytelling techniques and also write
programs to automate data collection and
processing now move on to the the skills
possessed by data analysts data
engineers and data
scientists to become a data analyst you
need to have good hands-on experience
with writing SQL queries you should have
excellent Microsoft Excel skills for
analyzing data data analysts are also
good at programming and they need to
know how to visualize data solve
business problems and possess domain
knowledge data Engineers should have a
solid understanding of SQL mongod DB and
programming they need to have a good
command of data architecture scripting
data warehousing and ETL data Engineers
are also good at Hadoop based
analytics now talking about the skills
for a data scientist so a data scientist
should have experience with programming
in Python and R they should have a very
good understanding of mathematics and
statistics as well data scientists need
to possess analytical thinking and data
visualization skills as as
well Machine learning deep learning and
decision making are other critical
skills every data scientist should
have now we look at the salaries of a
data scientist a data analyst as well as
a data
engineer so a data analyst in the United
States earns over $70,000 perom while in
India a data analyst can earn nearly 7
lakh 25,000 rupees per anom
a data engineer in the United States can
earn over $112,500 per year and in India
you can earn over 9 lakh rupees
perom talking about the salary of a data
scientist a data scientist in the United
States earns over
$117,000 per anom and in India a data
scientist can earn over 11 lakh rupees
perom coming to the final section of
this video we'll look at the top
companies hiring for data analysts data
engineers and data
scientists so we have the first company
is Google then we have Tesla next we
have the e-commerce giant Amazon the
internet giant Facebook or the social
media giant Facebook we have the TP
giant Oracle we also have Verizon and
arbnb so these are some of the top
companies that hire for the three roles
if you are an aspiring data scientist
who's looking out for online training
and certification in data science from
the best universities and Industry
experts then search no more simply
learns postgraduate program in data
science from Caltech University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below now let's talk
about the life cycle of a data science
Pro project okay the first step is the
concept study in this step it involves
understanding the business problem
asking questions get a good
understanding of the business model meet
up with all the stakeholders understand
what kind of data is available and all
that is a part of the first step so here
are a few examples we want to see what
are the various specifications and then
what is the end goal what is the budget
is there an example of of this kind of a
problem that has been maybe solved
earlier so all this is a part of the
concept study and another example could
be a very specific one to predict the
price of a 1.35 karat diamond and there
may be relevant information inputs that
are available and we want to predict the
price the next step in this process is
data preparation data Gathering and data
preparation also known as data monging
or sometimes it is is also known as data
manipulation so what happens here is the
raw data that is available may not be
usable in its current format for various
reasons so that is why in this step a
data scientist would explore the data he
will take a look at some sample data
maybe pick there are millions of Records
pick a few thousand records and see how
the data is looking are there any gaps
is the structure appropriate to be fed
into the system are there some columns
which are probably not adding value may
not be required for the analysis very
often these are like names of the
customers they will probably not add any
value or much value from an analysis
perspective the structure of the data
Maybe the data is coming from multiple
data sources and the structures may not
be matching what are the other problems
there may be gaps in the data so the
data all the columns all the cells are
not filled if you're talking about
structured data there are several blank
records or blank columns so if you use
that data directly you'll get errors or
you'll get inaccurate results so how do
you either get rid of that data or how
do you fill this gaps with something
meaningful so all that is a part of data
monging or data manipulation so these
are some additional sub topics within
that so data integration is one of them
if there are any conflicts in the data
there may be data may be redundant yeah
data redundancy is another issue there
may be you have let's say data coming
from two different systems and both of
them have customer table for example
customer information so when you merge
them there is a duplication issue so how
do we resolve that so that is one data
transformation as I said there will be
situations where data is coming from
multiple sources and then when we merge
them together they may not be matching
so we need to do some transformations to
make sure or everything is similar we
may have to do some data reduction if
the data size is too big you may have to
come up with ways to reduce it
meaningfully without losing information
then data cleaning so there will be
either wrong values or you null values
or there are missing values so how do
you handle all of that a few examples of
very specific stuff so if there are
missing values how do you handle missing
values or null values here in this
particular particular slide we are
seeing three types of issues one is
missing value then you have null value
you see the difference between the two
right so in the missing value there is
nothing blank null value it says null
now the system cannot handle if there
are null values similarly there is
improper data so it's supposed to be
numeric value but there is a string or a
non-numeric value so how do we clean and
prepare the data so that our system can
work flawlessly so there are multiple
ways and and there is no one common way
way of doing this it can vary from
Project to project it can vary from what
exactly is the problem we trying to
solve it can vary from data scientist to
data scientist organization to
organization so these are like some
standard practices people come up with
and and of course there will be a lot of
trial and error somebody would have
tried out something and it worked and
it'll continue to use that mechanism so
that's how we need to take care of data
cleaning now what are the various ways
of doing you know if if values are
missing how do you take care of that now
if the data is too large and um only a
few records have some missing values
then it is okay to just get rid of those
entire rows for example so if you have a
million records and out of which 100
records don't have full data so there
are some missing values in about 100
records so it's absolutely fine because
it's a small percentage of the data so
you can get rid of the entire records
which have missing values but that's not
a very common situation very often you
will have multiple or at least you know
large number of data set for example out
of million records you may have 50,000
records which are like having missing
values now that's a significant amount
you cannot get rid of all those records
your analysis will be inaccurate so how
do you handle such situation so there
are again multiple ways of doing it one
is you can probably if a particular
values are missing in a particular
column you can probably take the mean
value for that particular column and
fill all the missing values with the
mean value so that first of all you
don't get errors because of missing
values and second you don't get results
that are way off because these values
are completely different from what is
there so that is one way then a few
other could be either taking the median
value or depending on what kind of data
we are talking about so something
meaningful we will have put in there if
we are doing some machine learning
activity then obviously as a part of
data preparation you need to split the
data into training and test data set the
reason being if you try to test with a
data set which the system has already
seen as a part of training then it will
tend to give reasonably accurate results
because it has already seen that data
and that is not a good measure of the
accuracy of the system so typically you
take the entire data set the input data
set and split it into two parts and
again the ratio can vary from person to
person individual preferences some
people like to split it into 50/50 some
people like it as
6333 and 33.3 is is basically 2/3 and
1/3 and some people do it as 80/20 804
training and 20 for testing so you split
the data perform the training with the
80% and then use the remaining 20% for
testing all right so that is one more
data preparation activity that needs to
be done before you to start analyzing or
applying the data or putting the data
through the model then the next step is
model planning now this models can be
statistical models this could be machine
learning models so you need to decide
what kind of models you're going to use
again it depends on what is a problem
you're trying to solve if it is a
regression problem you need to think of
a regression algorithm and come up with
a regression model so it could be linear
regression or if you're talking about
classification then you need to pick up
an appropriate classification algorithm
like logistic regression or decision
tree or svm and then you need to train
that particular model so that is the
model building or model planning process
and the cleaned up data has to be fed
into the model and apart from cleaning
you may also have to in order to
determine what kind of model you will
use you have to perform some exploratory
data analysis to understand the
relationship between the various
variables and U see if the data is
appropriate and so on right so that is
the additional preparatory step that
needs to be done so little bit of
details about exploratory data analysis
so what exactly is exploratory data
analysis is basically to as the name
suggest you're just exploring you just
received the data and you're trying to
explore and uh find out what are the
data types and what is the is the data
clean in in each of the columns what is
the maximum minimum value so for example
there are out of the box functionality
available in tools like R so if you just
ask for a summary of the table it will
tell you for each column it will give
some details as to what is the mean
value what is the maximum value and so
on and so forth so this exercise or this
exploratory analysis is to get an
understanding of your data and then you
can take steps to during this process
you find there are a lot of missing
values you need to take steps to fix
those you will also get an idea about
what kind of model to be used and so on
and so forth what are the various
techniques used for exploratory data
analysis typically these would be
visualization techniques like you use
histograms uh you can use box plots you
can use cater plots so these are very
quick ways of identifying the patterns
or a few of the trends of the data and
so on and then once your data is ready
you you've decided on the model what
kind of model what kind of algorithm
your going to use if you're trying to do
machine learning you need to pass your
80% the training data or rather you use
that training data to train your model
and the training process itself is
iterative so the training process you
may have to perform multiple times and
once the training is done and you feel
it is giving good accuracy then you move
on to test so you take the remaining 20%
of the data remember we split the data
into training and test so the test data
is now used to check the accuracy or how
well our model is performing and if if
there are further issues let's say and
model is still during testing if the
accuracy is not good then you may want
to retrain your model or use a different
model so this whole thing again can be
iterative but if the test process is
passed or if the model passes the test
then it can go into production and it
will be deployed all right so what the
various tools that we use for model
planning R is an excellent tool in a lot
of ways whether you're doing regular
statistical analysis or machine learning
or any of these activities are in along
with our studio provides a very powerful
environment to do data analysis
including visualization it has a very
good integrated visualization or plot
mechanism which can be used for doing
exploratory data analysis and then later
on to do analysis detail analysis and
machine learning and so on and so forth
then of course you can write python
programs python offers a rich library
for performing data analysis and machine
learning and so on mat lab is a very
popular tool as well especially during
education so this is a very easy to
learn tool so matlb is another uh tool
that can be used and then last but not
least SAS SAS is again very powerful it
is uh reparatory to tool and it has all
the components that are required to
perform very good statistical analysis
or perform data science so those are the
various tools that would be required for
or that that can be used for model
building and uh so the next step is
model building so we have done the
planning part we said okay what is the
algorithm we are going to use what kind
of model we going to use now we need to
actually train this model or build the
model rather so that it can then be
deployed so what are the various uh ways
or what are the various types of model
building activities so it could be let's
say in this particular example that we
have taken you want to find out the
price of 1.35 karat diamond so this is
let's say a linear regression problem
you have data for various carrots of
diamond and you use that information you
pass it through a linear regression
model or you create a linear regression
model which can then predict your price
for 1.35 karat so this is one example of
model building and then little bit
details of how linear regression works
so linear regression is basically coming
up with a relation between an
independent variable and a dependent
variable so it is pretty much like
coming up with equation of a a straight
line which is the best fit for the given
data so like for example example here Y
is equal mx + C so Y is the dependent
variable and X is the independent
variable we need to determine the values
of M and C for our given data so that is
what the training process of uh this
model does at the end of the training
process you have a certain value of M
and c and um that is used for predicting
the values of any new data that comes
all right so the way it works is we use
the training and the test data set to
train the model and then validate
whether the model is working fine or not
using test data and uh if it is working
fine then it is taken to the next level
which is put in production if not the
model has to be retrained if the
accuracy is not good enough then the
model is retrained maybe with more data
or you come up with a newer model or
algorithm and then repat process so it
is an iterative process once the
training is completed training and test
then this model is deployed and we can
use this particular model to determine
what is the price of 1.35 karat diamond
remember that was our problem statement
so now that we have the best fit for
this given data we have the price of
1.35 karat diamond which is 10,000 so
this is one example of how this whole
process works now how do we build the
model there are multiple ways you can
use Python for example and use libraries
like pandas or numpy to build a model
and implement it this will be available
as a separate tutorial a separate video
in this playlist so stay tuned for that
moving on once we have the results the
next step is to communicate this results
to the appropriate stakeholders so which
is basically taking this result results
and preparing like a presentation or a
dashboard and communicating these
results to the concerned people so
finishing or getting the results of the
analysis is not the last step but you
need to as a data scientist take this
results and present it to the team that
has given you this problem in the first
place and explain your findings explain
the findings of this exercise and
recommend maybe what steps they need to
take in order to to overcome this
problem or solve this problem so that is
the pretty much once that is accepted
and the last step is to operationalize
so if everything is fine your data
scientists presentations are accepted
then they put it into practice and
thereby they will be able to improve or
solve the problem that they stated in
step one okay so quick summary of the
life cycle you have a concept study
which is basically understanding the
problem asking the right write questions
and trying to see if there is uh enough
data to solve this problem and then even
maybe gather the data then data
preparation the raw data needs to be
manipulated you need to do data monging
so that you have the data in a certain
proper format to be used by the model or
our analytics system and then you need
to do the model planning what kind of a
model what algorithm you will use for a
given problem and then the model
building so the exact execution of that
model
it happens in step four and you
implement and execute that model and uh
put the data through the analysis in
this step and then you get the results
this results are then communicated
packaged and presented and communicated
to the stakeholders and once that is
accepted that is operationalized so that
is the final let's begin this lesson by
defining the term statistics statistics
is a mathematical science pertaining to
The Collection presentation analysis and
interpretation of data it's widely used
to understand the complex problems of
the real world and simplify them to make
well-informed decisions several
statistical principles functions and
algorithms can be used to analyze
primary data build a statistical model
and predict the
outcomes an analysis of any situation
can be done in two ways statistical
analysis or a non-statistical analysis
statistical analysis is the science of
collecting exploring and presenting
large amounts of data to identify the
patterns and Trends statistical analysis
is also called quantitative analysis
non-statistical analysis provides
generic information and includes text
sound still images and moving images
non-statistical analysis is also called
qualitative analysis although both forms
of analysis provide results statistical
analysis gives more insight and a
clearer picture a feature that makes it
vital for
businesses there are two major
categories of Statistics descriptive
statistics and inferential statistics
descriptive statistics helps organize
data and focuses on the main
characteristics of the data it provides
a summary of the data numerically or
graphically numerical measures such as
average mode standard deviation or SD
and correlation are used to describe the
features of a data set suppose you want
to study the height of students in a
classroom in the descriptive statistics
you would record the height of every
person in the classroom and then find
out the maximum height minimum height
and average height of the
population inferential statistics
generalizes the larger data set and
applies probability Theory to draw a
conclusion it allows you to infer
population parameters based on the
sample statistics and to model
relationships within the data modeling
allows you to develop mathematical
equations which describe the interner
relationships between two or more
variables consider the same example of
calculating the height of students in
the classroom in inferential statistics
you would categorize height as tall
medium and small and then take only a
small sample from the population to
study the height of students in the
classroom the field of Statistics
touches our lives in many ways from the
daily routines in our homes to the
business of making the greatest cities
run the effective statistics are
everywhere there are various statistical
terms that one should be aware of while
dealing with Statistics population
sample variable quantitative variable
qualitative variable discret variable
continuous
variable a population is the group from
which data is to be
collected a sample is a subset of a
population
a variable is a feature that is
characteristic of any member of the
population differing in quality or
quantity from another member a variable
differing in quantity is called a
quantitative variable for example the
weight of a person number of people in a
car a variable differing in quality is
called a qualitative variable or
attribute for example color the degree
of damage of a car in an
accident a discrete variable able is one
which no value can be assumed between
the two given values for example the
number of children in a
family a continuous variable is one in
which any value can be assumed between
the two given values for example the
time taken for a 100 meter run typically
there are four types of statistical
measures used to describe the data they
are measures of frequency measures of
central tendency measures of spread
measures of position let's learn each in
detail frequency of the data indicates
the number of times a particular data
value occurs in the given data set the
measures of frequency are number and
percentage central tendency indicates
whether the data values tend to
accumulate in the middle of the
distribution or toward the end the
measures of central tendency are mean
median and
mode spread describes how similar or VAR
the set of observed values are for a
particular variable the measures of
spread are standard deviation variance
and quartiles the measure of spread are
also called measures of
dispersion position identifies the exact
location of a particular data value in
the given data set the measures of
position are percentiles quartiles and
standard scores statistical analysis
system or SAS provides a list of
procedures to perform descriptive
statistics
they are as follows proc print proc
contents proc means proc frequency proc
univariant proc
gchart proc boxplot proc
gplot proc print it prints all the
variables in a SAS data set proc
contents it describes the structure of a
data
set proc means it provides data
summarization tools to compute
descriptive statistics for variables
across all observations and within the
groups of
observations proc frequency it produces
oneway to inway frequency and cross
tabulation tables frequencies can also
be an output of a SAS data
set proc univariate it goes beyond what
proc means does and is useful in
conducting some basic statistical
analyses and includes high resolution
graphical features
proc gchart the g- chart procedure
produces six types of charts block
charts horizontal vertical bar charts Pi
donut charts and Star Charts these
charts graphically represent the value
of a statistic calculated for one or
more variables in an input SAS data set
the Tred variables can be either numeric
or
character proc box plot the box plot
procedure creates side by-side box and
whisker plots of measurements organized
in groups a box and whisker plot
displays the mean quartiles and minimum
and maximum observations for a
group proc gplot gplot procedure creates
two-dimensional graphs including simple
Scatter Plots overlay plots in which
multiple sets of data points are
displayed on one set of axis plots
against the second vertical axis bubble
plots and logarithmic plots in this demo
you'll learn how to use use descriptive
statistics to analyze the mean from the
electronic data set let's import the
electronic data set into the SAS console
in the left plane rightclick the
electronic. xlsx data set and click
import
data the code to import the data
generates automatically copy the code
and paste it in the new window
the proc means procedure is used to
analyze the mean of the imported data
set the keyword data identifies the
input data set in this demo the input
data set is
electronic the output obtained is shown
on the
screen note that the number of
observations mean standard deviation and
maximum and minimum values of the
electronic data set are obtained
this concludes the demo on how to use
descriptive statistics to analyze the
mean from the electronic data set so far
you have learned about descriptive
statistics let's now learn about
inferential
statistics hypothesis testing is an
inferential statistical technique to
determine whether there is enough
evidence in a data sample to infer that
a certain condition holds true for the
entire population to understand the
characteristics of the general
population we take take a random sample
and analyze the properties of the sample
we then test whether or not the
identified conclusions correctly
represent the population as a whole the
population of hypothesis testing is to
choose between two competing hypotheses
about the value of a population
parameter for example one hypothesis
might claim that the wages of men and
women are equal while the other might
claim that women make more than
men hypothesis testing is formulated in
terms of two hypotheses null hypothesis
which is referred to as H null
alternative hypothesis which is referred
to as
H1 the null hypothesis is assumed to be
true unless there is strong evidence to
the contrary the alternative hypothesis
is assumed to be true when the null
hypothesis is proven false let's
understand the null hypothesis and
alternative hypothesis using a general
example null hypothesis attempts to show
that no variation exist between
variables and alternative hypothesis is
any hypothesis other than the null for
example say a pharmaceutical company has
introduced a medicine in the market for
a particular disease and people have
been using it for a considerable period
of time and it's generally considered
safe if the medicine is proved to be
safe then it is referred to as null
hypothesis to reject null hypothesis we
should prove that the medicine is unsafe
if the null hyp hypothesis is rejected
then the alternative hypothesis is
used before you perform any statistical
tests with variables it's significant to
recognize the nature of the variables
involved based on the nature of the
variables it's classified into four
types they are categorical or nominal
variables ordinal variables interval
variables and ratio
variables nominal variables are ones
which have two or more categories and
it's impossible to order the values
examples of nominal variables include
gender and blood group ordinal variables
have values ordered logically however
the relative distance between two data
values is not clear examples of ordinal
variables include considering the size
of a coffee cup large medium and small
and considering the ratings of a product
bad good and best interval variables are
similar to ordinal variables except that
the values are measured in a way where
their differences are meaningful with an
interval scale equal differences between
scale values do have equal quantitative
meaning for this reason an interval
scale provides more quantitative
information than the ordinal scale the
interval scale does not have a true zero
point a true zero point means that a
value of zero on the scale represents
zero quantity of the construct being
assessed examples of interval variables
include the Fahrenheit scale used to
measure temperature and distance between
two compartments in a
train ratio scales are similar to
interval scales and that equal
differences between scale values have
equal quantitative meaning however ratio
scales also have a true zero point which
give them an additional property for
example the system of inches used with a
common ruler is an example of a ratio
scale there is a true zero point because
0 in does in fact indicate a complete
absence of length
in this demo you'll learn how to perform
the hypothesis testing using
SAS in this example let's check against
the length of certain observations from
a random
sample the keyword data identifies the
input data
set the input statement is used to
declare the Aging variable and cards to
read data into SAS
let's perform a t test to check the null
hypothesis let's assume that the null
hypothesis to be that the mean days to
deliver a product is 6
days so null hypothesis equals 6
Alpha value is the probability of making
an error which is 5% standard and hence
Alpha equals
0.05 the variable statement names the
variable to be used in the
analysis the output is shown on the
screen
note that the P value is greater than
the alpha value which is
0.05 therefore we fail to reject the
null
hypothesis this concludes the demo on
how to perform the hypothesis testing
using
SAS let's now learn about hypothesis
testing procedures there are two types
of hypothesis testing procedures they
are parametric tests and non-parametric
tests in statistical inference or hypoth
ois testing the traditional tests such
as test and Anova are called parametric
tests they depend on the specification
of a probability distribution except for
a set of free parameters in simple words
you can say that if the population
information is known completely by its
parameter then it is called a parametric
test if the population or parameter
information is not known and you are
still required to test the hypothesis of
the population then it's called a
non-parametric test non-parametric tests
do not require any strict distributional
assumptions there are various parametric
tests they are as follows test Anova
chai squared linear regression let's
understand them in detail T Test a t
test determines if two sets of data are
significantly different from each other
the T test is used in the following
situations
to test if the mean is significantly
different than a hypothesized value to
test if the mean for two independent
groups is significantly different to
test if the mean for two dependent or
paired groups is significantly
different for example let's say you have
to find out which region spends the
highest amount of money on shopping it's
impractical to ask everyone in the
different regions about their shopping
expenditure in in this case you can
calculate the highest shopping
expenditure by collecting sample
observations from each region with the
help of the T Test you can check if the
difference between the regions are
significant or a statistical
fluke
Anova an NOA is a generalized version of
the T Test and used when the mean of the
interval dependent variable is different
to the categorical independent variable
when we want to check variance between
two or more groups we apply the Anova
test for example let's look at the same
example of the T Test example now you
want to check how much people in various
regions spend every month on shopping in
this case there are four groups namely
East West North and South with the help
of the Anova test you can check if the
difference between the regions is
significant or a statistical
fluke chai
square chai square is a statistical test
used to compare observed data with data
you would expect to obtain according to
a specific
hypothesis let's understand the high
Square test through an example you have
a data set of male Shoppers and female
Shoppers let's say you need to assess
whether the probability of females
purchasing items of $500 or more is
significantly different from the
probability of males purchasing items of
$500 or more linear
regression there are two types of linear
regression simple linear regression and
multiple linear regression simple linear
regression is used when one wants to
test how well a variable predicts
another variable multiple linear
regression allows one to test how well
multiple variables or independent
variables predict a variable of interest
when using multiple linear regression We
additionally assume the predictor
variables are
independent for example finding
relationship between any two variables
say sales and profit is called Simple
linear regression finding relationship
between any three variables say sales
cost telemarketing is called multiple
linear regression some of the
non-parametric tests are wi coxen rank
sum test and crcll Wallace h test wi
coxen rank sum test the wi cooin signed
rank test is a non-parametric
statistical hypothesis test used to
compare related samples or matched
samples to assess whether or not their
population mean ranks differ in W coxen
rank sum test you can test the null
hypothesis on the basis of the ranks of
the observations crusco Wallace h test
crusco Wallace h test is a rank-based
non-parametric test used to compare
independent samples of equal or
different sample sizes in this test you
can test the null hypothesis on the
basis of the ranks of the independent
samples the advantages of parametric
tests are as follows provide information
about the population in terms of
parameters and confidence
intervals easier to use in modeling
analyzing and for describing data with
Central Tendencies and data
Transformations Express the relationship
between two or more
variables don't need to convert data
into rank order to
test the disadvantages of parametric
tests are as follows follow s only
support normally distributed data only
applicable on variables not
attributes let's Now list the advantages
and disadvantages of non-parametric
tests the advantages of non-parametric
tests are as follows simple and easy to
understand do not involve population
parameters and sampling Theory make
fewer
assumptions provide results similar to
parametric procedures
the disadvantages of non-parametric
tests are as follows not as efficient as
parametric tests difficult to perform
operations on large samples manually
we'll discuss the types of distribution
in
statistics but before we move ahead
let's have a brief introduction on what
is probability distribution a
probability distribution is a list of
all of the possible outcomes of a random
variable along with the corresponding
probability values
and it is used in many fields but we
rarely do explain what they are so in
this video we'll discuss the three main
types of probability distribution that
is normal binomial and poison
distribution so let's move
ahead so what is normal
distribution normal distribution is a
continuous probability density that has
a probability density function which
gives us a symmetrical bell curve now
data can be distributed or spread out in
different ways but there are many cases
where the data tends to be around a
central value with no bias to the left
or right which means that it doesn't
show any particular spikes towards the
left or the right and it gets close to a
normal distribution half of the data
will fall on the left of the mean and
the other half will fall on the right
now let's take a look at a graph which
shows the height distribution in a
class as you can see the average height
is in the middle and the data to the
left of the average height represents
the short people and the data to the
right of it represents the taller
people the Y AIS shows us the likelihood
of any of these Heights occurring the
average height has the most distribution
or it has the most number of cases in
the class and as the height decreases or
increases the number of people who have
that height also decreases
this kind of a distribution is called a
normal distribution where the average or
the mean is always the highest point and
any other point after that or before
that is significantly
lower the resulting data gives us a bell
curve and as you can see there is no
abrupt bias or spike in the data
anywhere except for the average height
so this kind of a curve is called a bell
curve and it's usually seen in a normal
distribution
the reason we call this a normal
distribution is because the data is
normally distributed with the average
being the highest and all the other data
points having a lower
likelihood now we came across two terms
which are associated with normal
distribution continuous probability
density and probability density function
what is continuous probability density
continuous probability density is a
probability distribution where the
random variable X can take any given
value because there are infinite values
that X could assume the probability of X
taking on any specific value zero for
example let's say you have a continuous
probability density for a men's height
what is the probability that a man will
have the exact height of 70
in it is impossible to find this out
because the probability of one man
measuring exactly 70 in is very low it
is more probable that he will measure
around 70.1 in or maybe 69 97 in and it
doesn't stop
there the fact is that it's impossible
to exactly measure any variable that's
on a continuous scale and because of
this it's impossible to figure out the
probability of one exact measurement
which is occurring in a continuous
probability
density next we have the probability
density function it's nothing but a
function or an expression which is used
to define the range of values that a
continuous random variable can take an
example of this would be to Gorge the
risk and reward of a
stock a probability density function is
a statistical measure which is used to
gge the likelihood of a discrete value a
discrete variable can be measured
exactly while a continuous variable can
have infinite
values however for both continuous as
well as discrete variables we can define
a
function which gives us the range of
values within
which these variables will fall and that
function is known as the probability
density
function now let's take a look at
standard
deviation what is standard deviation
standard deviation is used to measure
how the values in your data differ from
one another or how spread out your data
is a standard deviation ation is a
statistic that measures the dispersion
of a data set relative to its mean the
standard deviation is calculated as the
square root of variance by determining
each data Point's deviation relative to
the mean if the data points are further
from the mean that means that there's a
higher deviation within the data set and
then the data is set to be more spread
out this leads to a higher standard
deviation to Let's take an example of
income in rural and urban areas in rural
areas let's say such as farming areas
the income doesn't differ that much more
or less everyone earns the same because
of this our bell curve has a very low
standard deviation and it has a very
narrow
Peak however in urban areas the well
distribution is very uneven some people
can have very high incomes and can be
earning a lot while other people can
have very low incomes the furthermore
the data distribution between these two
income points is going to be more spread
out because there are lot more people
living there who work in various fields
and who have various incomes because of
this our standard deviation is more
spread out and a bell curve will also
have a wider
Peak now how can we find the standard
deviation standard deviation is obtained
by subtracting each data value from the
mean and finding the squared average of
these values let's look at how we can do
this with the help of an example these
values correspond to the height of
various
dogs we can find the mean by finding the
average of all these values which is
nothing but adding all the values and
dividing it by the total number of
values the mean that we get is
394 this means that the average height
of a dog is 394
mm to find the standard deviation first
we need to subtract the height from the
mean this will tell us how far from the
mean our data points actually
are next we will square up all of these
differences and add them up and again
divide it by the total number of values
that we have this is called the
variance the variance that we get in
this case is
21704 finally when we find the square
root of this value we will get the
standard deviation the standard
deviation here is 147 the standard
deviation will tell us how our data
points differ from the average and it
gives us a basic value suggesting how
spread out our data is from the very
middle or from the mean so when we plot
these values this value 147 will mean
that a curve will have a width of 147
points around the
mean now what is the standard normal
distribution the standard normal
distribution is a type of normal
distribution that has a mean of zero and
a standard deviation of one this means
that the normal distribution has its
Center at zero and it has intervals
which increase by one all normal
distributions like the standard normal
distribution are unimodel and
symmetrically distributed with a
bell-shaped curve however a normal
distribution can take on any value as
its mean and standard deviation in the
standard normal distribution however the
mean and standard deviation are always
fixed when you standardize a normal
distribution the mean becomes zero and
the standard deviation becomes one this
allows you to easily calculate the
probability of certain values occurring
in your distribution or to compare data
sets with different mean and standard
deviations the curve shows a standard
normal distribution as you can see again
the data is centered at
zero this does not mean that the data
necessarily starts at zero this means
that after standardizing this point is
where our mean will
in a standard normal distribution the
standard deviation is one so all the
data points will increase or decrease in
steps of one let's better understand a
standard normal distribution with the
help of an example again as you can see
the data is centered around zero which
is nothing but the
mean let's again consider the weights of
students in class 8th the average weight
here is around 50 kgs and the data
increases and decreases in steps of five
the data over here in this curve is
evenly distributed along these steps
this is what a standard normal
distribution will look like we already
know that the mean of our data is 50 and
because the data is increasing and
decreasing in equal steps we can just
standardize it and take it to mean that
the data is increasing and decreasing in
steps of one this is what a standard
normal distribution look looks like and
when you have a data which looks like
this you can always standardize it and
convert it into a standard normal
distribution now standard normal
distribution has a couple of properties
which makes calculation comparatively
easy the first one is that 68% of the
values fall within the first standard
deviation which means that 68% of all
data values on this Curve will fall
between the range of minus1 to 1 or the
first interval ranging from minus1 to 1
the second property is that 95% of the
rest of the values are within the second
standard deviation or from the second
negative point to the second positive
point and
finally
99.7% of the values fall within the
third standard deviation or from the
third negative point to the third
positive point this makes calculations
on standard normal distribution fairly
easy you can compare scores on different
distri distributions with different
means and standard deviations you can
normalize scores for statistical
decision making using standard normal
distribution you can find the
probability of observations in a
distribution which fall above or below a
given value and finally you can find the
probability that a mean significantly
differs from a population
mean now let's take a look at
zcore so what is a zcore a zcore is used
to tell us how far from the mean our
data point actually
is it is calculated using the mean and
standard deviation so it can be said
that the Z score is how many standard
deviations below the mean our data is
basically by using the Z score we can
get an approximate location of where our
data point lies on the graph with
regards to the mean now the Z score is
given by subtracting the data point from
the mean and dividing it by standard
deviation this can also be written as x
- mu / Sigma now any normal distribution
can be standardized by converting its
values into Z scores the Z score will
tell you how many standard deviation
from the mean each values lie while data
points are referred to as X in a normal
distribution they are called Zed or Z
scores in the Zed distribution a z score
is a standard score that will tell you
how many standard deviations away from
the mean and individual point will lie a
positive Z score will mean that your x
value is greater than the mean and a
negative Z score will mean that your x
value is less than the mean a z square
of 0 will mean that your x value is
equal to the mean and again to
standardize a value from a normal
distribution all we have to do is
convert it to a z score by subtracting
the mean from our individual value and
dividing it by the standard deviation
now let's see how we can find the Z
score from data points with the help of
a solved example
let's do a case
study in this case study we'll be taking
the summary of daily travel time of a
person who's commuting to and from work
all these values are in minutes and
using these values we have to calculate
the mean the standard deviation and the
Z score these values are as shown as we
can see there are 13 values in total
let's start by finding the mean the mean
is the average and it can be gotten by
adding all of these values Val and
dividing it by the total number of
values this gives us a value of
38.6 the mean tells us the average of
all our data points which means on an
average he travels for 38.6 minutes to
reach work next let's subtract the
individual values from our
mean and calculate the variance and
standard
deviation the values on the left give us
the values that we get after subtracting
it from the mean and the variance can be
calculated by squaring all of these
values adding up all of the squared
values and dividing it by the total
number of values at the end of the day
we get a variance of
140 to calculate this standard deviation
all we have to do is take a square root
of the variance which gives us a value
of
11.8 now the mean signifies the average
of our values and we already know this
it gives us the average time which is
taken to travel but the standard
deviation will tell us the average value
of how much our data points differ from
the mean it tells us the deviation
within our own data and it tells
us how far away on an average a point is
from the mean now the value that we get
is 11.8 which means that on an average a
single data point is around 11.8 data
points away from the
mean now let's calculate the Z score
the Z score is given by subtracting
individual data points from the mean and
dividing it by the standard deviation we
know that we have a standard deviation
of 11.8 and a mean of
38.6 using these values we can calculate
the Z scores for individual X values now
we know that a negative Z score means
that our x value is lower than our mean
but what does the number 1.06 mean this
means that the Z score for 26 is
1.06 standard
deviations away from the mean the
negative symbol here means that our x
value is less than the mean and by how
less 1.06 * the standard deviation now
we know that the negative value of a z
score means that our x value is less
than our mean but what does the number
1.06 mean this means that the Z score is
1.06 times the standard deviation less
than the
mean the same thing can be said for the
Z score of 33 it is 0.47 * the standard
deviation
less than the
mean the z s of 65 is 2.23 * the
standard deviation more than the mean
that means it has to be added to the
mean the reason that we know it's more
than the mean is because this has a
positive value so this this means that
using Z scores we can know where our
data points fall relative to other
points on the graph the Z score will
tell us how far away from the mean a
point is in steps of our standard
deviation Basics and
terminology the first one is
outcome whenever we do an experiment
like flipping a coin or rolling a dice
we get an outcome for example if we flip
a coin we get an outcome of heads or
tals and if we roll a die we get an
outcome of 1 2 3 4 5 or
six random experiment a random
experiment is any well- defined
procedure that produces an observable
outcome that could not be perfectly
predicted in advance a random experiment
must be well defined to eliminate any
vagueness or
surprise it must produce a definite
observable outcome so that you know what
happened after the random experiment is
run random
events consider a simple
example let us say that we toss a coin
up in the air what can happen when it
gets back it either gives a head or a
tail these two are known as outcome and
the occurrence of an outcome is an event
does the event is the outcome of some
phenomenon the last one is sample
space a sample space is a collection or
a set of possible outcomes of a random
experiment the sample space is
represented using the symbol
S the subset of all possible outcomes of
an experiment is called events and a
sample space may contain a number of
outcomes that depends on the experiment
if it contains a finite number of
outcomes then it is known as a discrete
or finite sample
spaces now let's discuss what is random
variable a random variable is a
numerical description of the outcome of
a statistical experiment a random VAR
variable that may assume only a finite
number of values is set to be discrete
one that may assume any value in some
interval on the real number line is set
to be
continuous let's see an example let X be
a random variable defined as a sum of
numbers when two dices are
rolled X can assume the values 2 3 4 5 6
7 8 9 10 11 and 12 notice there's no one
here because the sum of the two dice can
never be one now that we know the basics
let's move on to binomial
distribution the binomial distribution
is used when there are exactly two
mutually exclusive outcomes of a trial
these outcomes are appropriately labeled
success and failure the B distribution
is used to obtain the probability of
observing X successes in N number of
trials with the probability of success
on a single trial denoted by P the banal
distribution assumes that P is fixed for
all the trials
here's a real life example of a bomal
distribution suppose you purchase a
lottery ticket then either you are going
to win the lottery or not in other words
the outcome will be either success or
failure that can be proved through bomal
distribution there are four important
conditions that needs to be fulfilled
for an experiment to be a binomial
experiment the first one is there should
be a fixed number of end trials carried
out the outcome of a given tral is only
two that is either a success or a
failure the probability of success
remains constant from trial to trial it
does not changes from one trial to
another and the trials are independent
the outcome of a trial is not affected
by the outcome of any other
trial to calculate the binomial
coefficient we use the formula which is
NC into p ^ R into 1 - P to the^ n minus
r where R is the number of success in N
number of Trials and P is the
probability of success
1 - P denotes the probability of a
failure now let's use this formula to
solve an
example suppose a d is toss three times
what is the probability of No 5 turning
up 15 and 35s turning up to calculate
the No 5 turning up here R is equal to 0
and N is equal to 3 substituting the
value in the formula we have 33 0 into
1X 6 ^ 0 into 5x 6^ 3 where 1X 6 is the
probability of success and 5x 6 is the
probability of failure calculating this
equation we'll get the value to be 0.578
7 in a similar manner to calculate the
probability of 15 turning up we'll
replace r with 1 and N will be 3 so P X1
will be equal to 3 C1 into 1X 6 ^ 1 into
5x 6^ 2 which will come out to be
0.347 and for 35 turning up we
substitute Ral to 3 and the formula will
remain the same and we will get the
value to be
0.46 now that we are done with the
concepts of bomal probability
distribution here's a problem for you to
solve post your answers in the comment
section and let us
know a PO distribution is a probability
distribution used in statistics to show
how many times an event is likely to
happen over a given period of time to
put it another way it's account
distribution poison distribution are
frequently used to comprehend
independent event at a constant rate
over a given interval of
time the poison distribution was
developed by French mathematician Simon
Dennis poison in
1837 a poison distribution is used in
cases where the chances of any
individual event being a success is very
small the number of defective pencils
per box of a 6,000 pencil the number of
plane crashs in India in one year or the
number of printing mistakes in each page
of a
book all of these example can have use
of poison
distribution the poison distribution can
be used to calculate How likely it is
that something will happen X number of
times a random variable X has a poison
distribution with parameter Lambda and
the formula for that is e ^ minus Lambda
into Lambda ^ x / X factorial where X
can be the number of times the event is
happening the value of e is taken as
27182 let's discuss some application of
poison
distribution if you want to calculate
the number of deaths per day or week due
to rare disease in the hospital you can
use the poison distribution in a similar
manner the count of bacteria per CC in
blood or the number of computers
infected with virus per week the number
of mishandled baggage per thousand
passengers can also have an application
for poon
distribution let's discuss one example
to see how we can calculate the poon
distribution suppose on an average a
cancer kills five people each year in
India what is the probability that one
person is killed this year we'll assume
all these events are independent random
events so by the
formula we have x equal to 1 because we
have to calculate the probability of 1
person that is killed this year so p x =
1 will be equal to e ^ - 5 into 5 ^ 1 /
1 factorial which will come out to be
0.33 which will be near to
3.3% so the probability that only one
person is killed this year due to cancer
is 3.3% if you are an aspiring data
scientist who's looking out for online
training and certification in data
science from the best universities and
Industry experts then search no more
simply learn postgraduate program in
data science from Caltech University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box
below hello everyone welcome to another
session by simply learn today we are
going to discuss the base theorem an
important subtopic that comes under
probability Theory we'll start this
video by talking about probability and
conditional probability after that we'll
move on to the base theorem and
understand its formula and a real life
example where the base teror can be used
so let's get started what is
probability probability is the branch of
mathematics concerning numerical
descriptions of How likely an event is
to occur or How likely it is that a
proposition is true the probability of
an event is a number between 0 and one
but roughly speaking zero indicates the
impossibility of the event and one
indicates
certainty the higher the probability of
an event the more likely is that the
event will occur let's look at an
example a simple example is the tossing
of a fair unbiased coin since the coin
is fair the outcome that is heads and
the tails are both equally probab the
probability of heads equals the
probability of the Tails and since no
other outcomes are possible the
probability of either heads or tails can
be said to be 1 by two which is also 50%
the probability of an event can be
calculated by number of ways it can
happen divided by the total number of
outcomes now that we know about the
probability let's see if you can answer
this question what is the probability of
drawing a Jack and a queen consecutively
from a deck of 52 cards without
replacement here are your
options post your answers in the comment
section and let us know now let's move
on to conditional probability
let A and B be the two events associated
with a random experiment then the
probability of A's occurrence under the
condition that B has already occurred
and probability of B is not equal to
zero is called the condition probability
it is denoted by P
A/B thus we can say that p a/ b is equal
to p a intersection B / P of B where P
A/B is the probability of occurrence of
a given that B has already occurred and
P B is the probability of occurrence of
B to know more about conditional
probability you can check our previous
video which is specifically on
conditional probability now let's move
on to base
theorem the base theorem is a
mathematical formula for calculating
condition probability in probability and
statistics in other words it is used to
figure out how likely an event is
associated on its proximity to
another base law or Bas rule are the
other name names of this theorem the
formula for the base theorem can be
written in a variety of ways the most
common version is p a/ b is equal to P
of b/ a into P of a / P of B where P A/B
is the conditional probability of event
a occurring given that b is true and P A
and B of B are the probabilities of A
and B occurring independently of one
another let's solve a problem using the
base theorem to understand it better
there is a cricket match tomorrow and in
recent years it has rained only 5 days
each year unfortunately the
meteorologist has predicted the rain for
tomorrow now when it rains the
meteorologist correctly forecast rain
90% of the time and when it doesn't rain
he incorrectly forecast rain 10% of the
time let's calculate what is the
probability that it will rain on the
match day so the two sample spaces here
are the events that it rains and it does
not rain additionally a third event is
also there that meterologist predicts
the rain so the notation for these
events appear below event A1 is equal to
it rains on the match Day event A2 that
it does not rain on the match day and
event B is the meterologist predicting
the rain now in terms of probability we
know the following probability of A1 is
5X 365 that it rains 5 days in a year
which will come out to be
0.0136 P A2 is 360 by 365 that is no
days for 360 days in an year which will
come out to be
986 B b/ A1 is
.9 this signifies when it rains the
meteorologist predicts the r 90% of the
time in a similar manner B by A2 is 0.1
that it does not rain the meteorologist
predict the rain 10% of the
time combining all this we can calculate
p a 1/b that is the probability It Will
Rain on the given match day given a
forecast of Rain by meterologist the
answer can be determined using the base
theorem as shown below so here's the
formula of the base theorem and putting
all the values that we have calculated
in the previous Slide the probability
that it will rain on the match day given
a forecast of the rain by meterologist
will come out to be
0.111 which will be equal to
11.11% so there's an 11% chance that it
will rain on the match day given that
the meteorologist has predicted the rain
I hope this example is clear to you it's
a weekend and John decided to watch the
latest movie recommended by Netflix at
his friend's place before heading out he
asked Siri about the weather and
realized it would rain so he decided to
take his Tesla for the long journey and
switch to autopilot on the highway after
coming home from the eventful day he
started wondering how technology has
made his life easy he did some research
on the internet and found out that
Netflix Siri and Tesla are all using AI
so what is ai ai or artificial
intelligence is nothing but making
computers based machines think and act
like humans artificial intelligence is
not a new term John McCarthy a computer
scientist coined the term artificial
intelligence back in
1956 but it took time to evolve as it
demanded heavy computing power
artificial intelligence is not confined
to just movie recommendations and
virtual assistance broadly classifying
there are three types of AI artificial
narrow intelligence also called weak AI
is the stage where machines can perform
a specific task Netflix Siri chatbots
spatial recommendation systems are all
examples of artificial narrow
intelligence next up we have artificial
general intelligence referred to as an
intelligent agent's capacity to
comprehend or pick up any intellectual
skill that a human camp we are halfway
into successfully implementing this
space IBM's Watson supercomputer and
gpt3 fall under this category and lastly
artificial
superintelligence it is the stage where
machines surpass human intelligence you
might have seen this in movies and
imagined how the world would be if
machines occupy it fascinated by this
John did more research and found out
that machine learning deep learning and
natural language processing are all
connected with artificial intelligence
machine learning a subset of AI is the
process of automating and enhancing how
computers learn from their experiences
without human health machine learning
can be used in email spam detection
medical diagnosis Etc deep learning can
be considered a subset of machine
learning it is a field that is based on
learning and improving on its own by
examining computer algorithms while
machine learning uses simpler Concepts
deep learning works with artificial
neural networks which are designed to
imitate the human brain this technology
can be applied in face recognition
speech recognition and many more
applications natural language processing
popularly known as NLP can be defined as
the ability of machines to learn human
language and translate it chatbots fall
under this category artificial
intelligence is advancing in every
crucial field like healthcare education
robotics banking e-commerce and the list
goes on like in healthcare AI is used to
identify diseases helping healthcare
service providers in their patients make
better treatment in lifestyle decisions
coming to the education sector AI is
helping teachers automate grading
organizing and facilitating parent
Guardian
conversations in robotics AI powered
robots employ real-time updates to
detect obstructions in their path and
instantaneously design their routes
artificial intelligence provides
Advanced data analytics that is
transforming banking by reducing fraud
and enhancing compliance with this
growing demand for AI more and more
Industries are looking for AI Engineers
who can help them develop intelligent
systems and offer them lucrative
salaries going north of
$120,000 the future of AI looks
promising with the AI Market expected to
reach $190 billion by
2025 we know humans learn from their
past experiences and machines follow
instructions given by
humans but what if humans can train the
machines to learn from their past data
and do what humans can do and much
faster well that's called machine
learning but it's a lot more than just
learning it's also about understanding
and reasoning so today we will learn
about the basics of machine learning so
that's Paul he loves listening to new
songs he either likes them or dislikes
them Paul decides this on the basis of
the song's Tempo Jor intensity and the
gender of voice for Simplicity let's
just use Temple and intensity for now so
here Tempo is on the x-axis ranging from
relaxed to fast whereas intensity is on
the y axis ranging from light to Soaring
we see that Paul likes the song with
fast tempo and soaring intensity while
he dislikes the song with relaxed Tempo
and light intensity so now we know
Paul's choices let's say Paul listens to
a new song Let's name it as song a song
a has fast temp Tempo and a soaring
intensity so it lies somewhere here
looking at the data can you guess
whether Paul will like the song or not
correct so Paul likes this song by
looking at Paul's past choices we were
able to classify the unknown song very
easily right let's say now Paul listens
to a new song Let's label it as song b
so song b lies somewhere here with
medium Tempo and medium intensity
neither relaxed nor fast neither light
nor soaring now can you guess whether
Paul likes it or not not able to guess
whether Paul will like it or dislike it
are the choices unclear correct we could
easily classify song A but when the
choice became complicated as in the case
of song b yes and that's where machine
learning comes in let's see how in the
same example for song b if you draw a
circle around the song b we see that
there are four Wes for like whereas one
vot for dislike if we go for the
majority Wes we can say that Paul will
definitely like the song that's all this
was a basic machine learning algorithm
also it's called K nearest neighbors so
this is just a small example in one of
the many machine learning algorithms
quite easy right believe me it is but
what happens when the choices become
complicated as in the case of song b
that's when machine learning comes in it
learns the data builds the prediction
model and when the new data point comes
in it can easily protect for it more the
data better the model model higher will
be the accuracy there are many ways in
which the machine learns it could be
either supervised learning unsupervised
learning or reinforcement learning let's
first quickly understand supervised
learning suppose your friend gives you 1
million coins of three different
currencies say one rupee 1 euro and 1
dirham each coin has different weights
for example a coin of 1 rupee weighs 3 g
1 euro weighs 7 G and 1 Duram weighs 4 G
your model will predict the currency of
the coin here your weight becomes the
feature of coins while currency becomes
the label when you feed this data to the
machine learning model it learns which
feature is associated with which label
for example it will learn that if a coin
is of Three G it will be a 1 rupe coin
let's give a new coin to the machine on
the basis of the weight of the new coin
your model will predict the currency
hence supervised learning uses labeled
data to train the model here the machine
knew the feature of the object and also
the labels associated with those
features on this note let's move to
unsupervised learning and see the
difference suppose you have Cricket data
set of various players with their
respective scores and the wickets taken
when you feed this data set to the
machine the machine identifies the
pattern of player performance so it
plots this data with the respective
wickets on the x-axis while runs on the
Y AIS while looking at the data you'll
clearly see that there are two clusters
the one cluster are the players who
score scored High runs and took less
wickets while the other cluster is of
the players who scored less runs but
took many wickets so here we interpret
these two clusters as batsmen and
Bowlers the important point to note here
is that there were no labels of batsmen
and Bowlers hence the learning with
unlabeled data is unsupervised learning
so we saw supervised learning where the
data was labeled and the unsupervised
learning where the data was unlabeled
and then there is reinforcement learning
which is reward-based learning or we can
say that it works on the principle of
feedback here let's say you provide the
system with an image of a dog and ask it
to identify it the system identifies it
as a cat so you give a negative feedback
to the machine saying that it's a dog's
image the machine will learn from the
feedback and finally if it comes across
any other image of a dog it'll be able
to classify it correctly that is
reinforcement learning to generalize
machine learning model let's see a
flowchart input is given to a machine
learning model which then gives the
output according to the Alor gthm
applied if it's right we take the output
as a final result else we provide
feedback to the training model and ask
it to predict until it learns I hope
you've understood supervised and
unsupervised learning so let's have a
quick quiz you have to determine whether
the given scenarios uses supervised or
unsupervised learning simple right
scenario one Facebook recognizes your
friend in a picture from an album of
tagged
photographs scenario 2 Netflix
recommends new movies based on someone's
Past movie
choices scenario three analyzing Bank
data for suspicious transactions and
flagging the fraud transactions think
wisely and comment below your answers
moving on don't you sometimes wonder how
is machine learning possible in today's
era well that's because today we have
humongous data available everybody's
online either making a transaction or
just surfing the internet and that's
generating a huge amount of data every
minute and that data my friend is the
key to analysis also the memory handling
capabilities of computers have largely
increased which helps them to process
such huge amount of data at hand without
any delay and yes computers now have
great computational Powers so there are
a lot of applications of machine
learning out there to name a few machine
learning is used in healthcare where
Diagnostics are predicted for doctor's
review the sentiment analysis that the
tech Giants are doing on social media is
another interesting application of
machine learning from detection in the
finance sector and also to predict
customer turn in the e-commerce sector
while booking a gap you must have
encountered searge pricing often where
it says the fair of your trip has been
updated continue booking yes please I'm
getting late for office well that's an
interesting machine learning model which
is used by Global Taxi giant Uber and
others where they have differential
pricing in real time based on demand the
number of cars available bad weather
Rush R Etc so they use the search
pricing model to ensure that those who
need a cab can get one also it uses
predictive modeling to predict where the
demand will be high with a goal that
drivers can take care of the demand and
search pricing can be minimized great
hey Siri can you remind me to book a cab
at 6:00 p.m. today okay I'll remind you
thanks no problem if you are an aspiring
data scientist who's looking out for
online training and certification in
data science from the best universities
and industri experts then search no more
simply learns postgraduate program in
data science from calch University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below let's dive in a
little deeper and see how machine
Learning Works let's say you provide a
system with the input data that carries
the photos of various kinds of fruits
now you want the system to figure out
what are the different fruits and group
them accordingly so what the system does
it analyzes the input data then it tries
to find patterns patterns like shapes
size and
color based on these patterns the system
will try to predict the different types
of fruit and segregate them finally it
keeps track of all such decisions it
took in the process to make sure it's
learning the next time you ask the same
system to predict and segregate the
different types of fruits it won't have
to go through the entire process again
that's how machine learning
works now let's look into the types of
machine learning machine learning is
primarily of three types first one is
supervised machine learning as the name
suggest you have to supervise your
machine learning while you train it to
work on its own it requires labeled
training data next up is unsupervised
learning wherein there will be training
data but it won't be labeled finally
there's reinforcement learning wherein
the system learns on its own let's talk
about all these types in detail let's
try to understand how supervised
Learning Works look at the pictures very
very carefully the monitor depicts the
model or the system that we are going to
train this is how the training is done
we provide a data set that contains
pictures of a kind of a fruit say an
apple then we provide another data set
which lets the model know that these
pictures were that of a fruit called
Apple this ends the training phase now
what we we will do is we provide a new
set of data which only contains pictures
of apple now here comes the fun part the
system can actually tell you what fruit
it is and it will remember this and
apply this knowledge in future as well
that's how supervised Learning Works you
are training the model to do a certain
kind of an operation on its own this
kind of a model is generally used into
filtering spam mails from your email
account as well yes surprise aren't you
so let's move on to un super provid
learning now let's say we have a data
set which is cluttered in this case we
have a collection of pictures of
different fruits we feed these data to
the model and the model analyzes the
data to figure out patterns in it in the
end it categorizes the photos into three
types as you can see in the image based
on their
similarities so you provide the data to
the system and let the system do the
rest of the work simple isn't it this
kind of a model is used by flip cart to
figure out the products that are well
suited for you honestly speaking this is
my favorite type of machine learning out
of all the three and this type has been
widely shown in most of the scii movies
lately let's find out how it works
imagine a newborn baby you put a burning
candle in front of the baby the baby
does not know that if it touches the
flame its fingers might get burned so it
does that anyway and gets hurt the next
time you put that candle in front of the
baby it will remember what happened the
last time and would not repeat what it
did
that's exactly how reinforcement
learning works we provide the machine
with a data set wherein we ask it to
identify a particular kind of a fruit in
this case an Apple so what it does as a
response it tells us that it's a mango
but as we all know it's a completely
wrong answer so as a feedback we tell
the system that it's wrong it's not a
mango it's an apple what it does it
learns from the feedback and keeps that
in mind when the next time when we ask a
same question it gives us the right
answer it is able to tell us that it's
actually an apple that is a reinforced
response so that's how reinforcement
learning works it learns from his
mistakes and experiences this model is
used in games like Prince of Persia or
Assassin's Creed or FIFA where in the
level of difficulty increases as you get
better with the games just to make it
more clear for you let's look at a
comparison between supervised and
unsupervised learning firstly the data
involved in case of supervised learning
is l
as we mentioned in the examples
previously we provide the system with a
photo of an apple and let the system
know that this is actually an apple that
is called label data so the system
learns from the label data and makes
future
predictions now unsupervised learning
does not require any kind of label data
because its work is to look for patterns
in the input data and organize it the
next point is that you get a feedback in
case of super VIIs learning that is once
you get the output the system tends to
remember that and uses it for the next
operation that does not happen for
unsupervised learning and the last point
is that supervised learning is mostly
used to predict data whereas
unsupervised learning is used to find
out hidden patterns or structures in
data I think this would have made a lot
of things clear for you regarding
supervised and unsupervised
learning now let's talk about a question
that everyone needs to answer before
building a machine learning model what
kind of a machine learning solution
should we
use yes you should be very careful with
selecting the right kind of solution for
your model because if you don't you
might end up losing a lot of time energy
and processing cost I won't be naming
the actual Solutions because you guys
aren't familiar with them yet so we will
be looking at it based on supervised
unsupervised and reinforcement learning
so let's look into the factors that
might help us select the right kind of
machine learning solution first first
factor is the problem statement
describes the kind of model you will be
building or as the name suggests it
tells you what the problem is for
example let's say the problem is to
predict the future stock market prices
so for anyone who is new to machine
learning would have trouble figuring out
the right solution but with time and
practice you will understand that for a
problem statement like this solution
based on supervised learning would work
the best for obvious reasons then comes
the size quality and nature of the data
if the data is cluttered you go for
unsupervised if the data is very large
and categorical we normally go for
supervised learning Solutions finally we
choose the solution based on their
complexity as for the problem statement
wherein we predict the stock market
prices it can also be solved by using
reinforcement learning but that would be
very very difficult and time consuming
unlike supervised
learning algorithms are not types of
machine learning in the most simplest
language they are methods of solving a
particular problem so the first kind of
method is classification which falls
under supervised learning classification
is used when the output you are looking
for is a yes or a no or in the form a or
b or true or false like if a shopkeeper
wants to predict if a particular
customer will come back to his shop or
not he will use a classification
algorithm the algorithms that fall under
classification are decision tree knife
base random Forest logistic regression
and
KNN the next kind is regression this
kind of a method is used when the
predicted data is numerical in nature
like if the shopkeeper wants to predict
the price of a product based on its
demand it would go for
regression the last method is
clustering clustering is a kind of
unsupervised learning again it is used
when the data needs to be organized
most of the recommendation system used
by flip cart Amazon Etc make use of
clustering another major application of
it is in search engines the search
engines study your old search history to
figure out your preferences and provide
you the best search
results one of the algorithms that fall
under clustering is K
means now that we know the various
algorithms let's look into four key
algorithms that are used widely we will
understand them with very simple
examples the four algorithms that we
will try to understand are K nearest
neighbor linear regression decision tree
and knife Pace let's start with our
first machine learning solution K
nearest neighbor K nearest neighbor is
again a kind of a classification
algorithm as you can see on the screen
the similar data points form
clusters the blue
one the red one
and the green one there are three
different clusters now if we get a new
and unknown data point it is classified
based on the cluster closest to it or
the most similar to it k in KN andn is
the number of nearest neighboring data
points we wish to compare the unknown
data with let's make it clear with an
example let's say we have three clusters
in a cost to durability graph first
cluster is of
footballs the second one is of tennis
balls and the third one is of
basketballs from the graph we can say
that the cost of footballs is high and
the durability is less the cost of
tennis balls is very less but the
durability is high and the cost of
basketballs is as high as the durability
now let's say we have an unknown data
point we have a black spot which can be
one kind of the balls but we don't know
what kind it is
so what we'll do we'll try to classify
this using KN andn so if we take K is
equal to 5 we draw a circle keeping the
unknown data point is the center and we
make sure that we have five balls inside
that Circle in this case we have a
football a basketball and three tennis
balls now since we have the highest
number of tennis balls inside the circle
the classified ball would be a tennis
ball so that's how k larest neighbor
classification is done linear regression
is again a type of supervised learning
algorithm this algorithm is used to
establish linear relationship between
variables one of which would be
dependent and the other one would be
independent like if we want to predict
the weight of a person based on his
height weight would be the dependent
variable and height would be
independent let's have a look at it
through an
example let's say we have a graph here
showing a relationship between between
height and weight of a person let's put
the y- axis as
H and the x-axis as
weight so the green dots are the various
data points these green dots are the
data points and D is the mean squared
error that is the perpendicular
distances from the line to the data
points are the error
values this error tells us how much the
predicted values vary from the original
value Let's ignore this blue line for a
while so let's say if this is our
regression line you can see the distance
from all the data points from this line
is very
high so if we take this line as a
regression line the error in the
prediction will be too
high so in this case the model will not
be able able to give us a good
prediction let's say we draw another
regression line here like this even in
this case you can see that the
perpendicular distance of the data
points from the line is very high so the
error value will still come as high as
the last one so this model will also not
be able to give us a good
prediction so what to
do so finally we draw a line which is
this blue line so here we can see that
the distance of the data points from the
line is very less relative to the other
two lines we
drew so the value of D for this line
will be very less so in this case if we
take any value on the x- axis the
corresponding value on the y- axis will
be our
prediction and given the fact that the D
is very low our prediction should be
good
also this is how regression works we
draw a line a regression line that is in
in such a way that the value of D is the
least eventually giving us good
predictions this algorithm that is
decision tree is a kind of an algorithm
you can very strongly relate to it uses
a kind of a branching method to realize
the problem and make decisions based on
the conditions let's take this graph as
an example imagine yourself sitting at
home getting bored you feel like going
for a swim what you do is you check if
it's sunny outside so that's your first
condition condition if the answer to
that condition is yes you go for a swim
if it's not sunny and the next question
you would ask yourself is if it's
raining outside so that's condition
number two if it's actually raining you
cancel the plan and stay indoors if it's
not raining then you would probably go
outside and have a walk so that's the
final node that's how decision tree
algorithm works you probably use this
every day it realizes a problem and then
takes the decisions based on the answers
to every
conditions nbis algorithm is mostly used
in cases where a prediction needs to be
done on a very large data set it makes
use of conditional
probability conditional probability is
the probability of an event say a
happening given that another event B has
already happened this algorithm is most
commonly used in filtering spam mails in
your email account let's say you receive
a mail the model goes through your old
spam mail
records then it uses space theorem to
predict if the present male is a spam
male or not so p c of a is the
probability of event C occurring when a
has already occurred B A of C is the
probability of event a occurring when C
has already occurred and P C is the
probability of event C occurring and Pa
a is the probability of event a occuring
let's try to understand KN base with a
better example night base can be used to
determine on which days to play cricket
based on the probabilities of a day
being rainy windy or sunny the model
tells us if a match is possible if we
consider all the weather conditions to
be event a for
us and the probability of a match being
possible event
C so the model applies the probabilities
of event A and C into the base theorem
and predicts if a game of cricket is
possible on a particular day or not in
this case if the probability of C of a
is more than 0.5 we can be able to play
a game of cricket if it's less than 0.5
we won't be able to do that that's how
Nas algorithm Works we're going to cover
reinforcement learning today and what's
in it for you we'll start with why
reinforcement learning we'll look at
what is reinforcement learning we'll see
what the different kinds of learning
strategies are that are being used today
in computer models under supervised
versus unsupervised versus
reinforcement we'll cover important
terms specific to reinforcement learning
we'll talk about markov's decision
process and we'll take a look at a
reinforcement learning example well
we'll teach a tic-tac-toe how to play
why reinforcement learning training a
machine learning model requires a lot of
data which might not always be available
to us further the data provided might
not be reliable learning from a small
subset of actions will not help expand
the vast of solutions that may work for
a particular problem and you can see
here we have the robot learning to walk
um very complicated setup when you're
learning how to walk and you'll start
asking questions like if I'm taking one
step forward and left what happens if I
pick up a 50 pound object how does that
change how a robot would walk these
things are very difficult to program
because there's no actual information on
it until it's actually tried out
learning from a small subset of actions
will not help expand the vast realm of
solutions solons that may work for a
particular
problem and we'll see here it learned
how to walk this is going to slow the
growth that technology is capable of
machines need to learn to perform
actions by themselves and not just learn
off
humans and you see the objective climb a
mountain real interesting point here is
that as human beings we can go into a
very unknown environment and we can
adjust for it and kind of explore and
play with it most of the models the
non-reinforcement models in computer um
machine learning aren't able to do that
very well uh there's a couple of them
that can be used or integrated to see
how it goes is what we're talking about
with reinforcement learning so what is
reinforcement learning reinforcement
learning is a subbranch of machine
learning that trains the model to return
an Optimum solution for a problem by
taking a sequence of decisions by itself
consider a robot learning to go from one
place to another the robot is given a
scenario and must arrive at a solution
by itself the robot can take different
paths to reach the
destination it will know the best path
by the time taken on each path it might
even come up with a unique solution all
by itself and that's really important is
we're looking for Unique Solutions uh we
want the best solution but you can't
find it unless you try it so we're
looking at uh our different systems or
different model we have supervised
versus unsupervised versus reinforcement
learning and with the supervised
learning that is probably the most
controlled environment uh we have a lot
of different supervised learning models
whether it's linear regression neural
networks um there's all kinds of things
in between decision trees the data
provided is labeled data with output
values specified and this is important
because when we talk about supervised
learning you already know the answer for
all this information you already know
the picture has a motorc cycle in it so
you're supervised learning you already
know that um the outcome for tomorrow
for you know going back a week and
looking at stock you can already have
like the graph of what the next day
looks like so you have an answer for
it and you have label data which is used
you have an external supervision and
solves Problems by mapping labeled input
to No One output so very
controlled unsupervised learning and
unsupervised learning is really
interesting because this now taking part
in many other models they start with an
you can actually insert an unsupervised
learning model um in almost either
supervised or reinforcement learning as
part of the system which is really cool
uh data provided is unlabeled data the
outputs are not specified machine makes
its own predictions used to solve
association with clustering problems
unlabeled data is used no supervision
solves Problems by understanding
patterns and discovering output
uh so you can look at this and you can
think um some of these things go with
each other they belong together so it's
looking for what connects in different
ways and there's a lot of different
algorithms that look at this um when you
start getting into those are some really
cool images that come up of what
unsupervised learning is how it can pick
out say uh the area of a donut one model
will see the area of the donut and the
other one will divide it into three
sections based on this location versus
what's next to it
so there's a lot of stuff that goes in
with unsupervised learning and then
we're looking at reinforcement learning
probably the biggest industry in today's
market uh in machine learning or growing
Market it's very it's very infant stage
uh as far as how it works and what's
going to be capable of the machine
learns from its environment using
rewards and errors used to solve reward
based problems no predefined data is
used no supervision follows Trail and
error problem solving a approach uh so
again we have a random at first you
start with a random I try this it works
and this is my reward doesn't work very
well maybe or maybe doesn't even get you
where you're trying to get it to do and
you get your reward back and then it
looks at that and says well let's try
something else and it starts to play
with these different things finding the
best route so let's take a look at
important terms in today's reinforcement
model and this has become pretty
standardized over the last uh few years
so these are really good to know
we have the agent uh agent is the model
that is being trained via reinforcement
learning so this is your actual uh
entity that has however you're doing it
whether you're using a neural network or
a q table or whatever combination
thereof this is the actual agent that
you're using this is the
model and you have your environment uh
the training situation that the model
must optimize to is called its
environment uh and you can see here I
guess we have a robot who's trying to
get uh chest full of gyms or whatever
and that's the output and then you have
your action this is all possible steps
that can be taken by the model and it
picks one action and you can see here
it's picked three different uh routes to
get to the chest of diamonds and
gems we have a state the current
position condition returned by the
model and you could look at this uh if
you're playing like a video game this is
the screen you're looking at uh so when
you go back here uh the environment is a
whole whole game board so if you're
playing one of those mobus games you
might have the whole game board going on
uh but then you have your current
position where are you on that game
board what's around that what's around
you um if you were talking about a robot
the environment might be moving around
the yard where it is in the yard and
what it can see what input it has in
that location that would be the current
position condition returned by the model
and then the reward uh to help the model
move in the right direction Direction it
is rewarded points are given to it to
appraise some kind of action so yeah you
did good or if uh didn't do as good
trying to maximize the reward and have
the best reward
possible and then policy policy
determines how an agent will behave at
any time it acts as a mapping between
action and present State this is part of
the model what what is your action that
you're you're going to take what's the
policy you're using to have an output
from your agent one of the reasons they
separate uh policy as its own entity is
that you usually have a prediction um of
a different options and then the policy
well how am I going to pick the best
based on those predictions I'm going to
guess at different options and we'll
actually weigh those options in and find
the best option we think will work uh so
it's a little tricky but the policy
thing is actually pretty cool how it
works let's go Ahad and take a look at a
reinforcement learning examle
and just in looking at this we're going
to take a look uh consider what a dog um
that we want to train uh so the dog
would be like the agent so you have your
your puppy or whatever uh and then your
environment is going to be the whole
house or whatever it is where you're
training them and then you have an
action we want to teach the dog to
fetch so action equals
fetching uh and then we have a little
biscuit so we can get the dog to perform
various actions by offering incentives
such as a dog biscuit as a
reward the dog will follow a policy to
maximize this reward and hence will
follow every command and might even
learn new actions like begging by itself
uh so you have B you know so we start
off with fetching it goes oh I get a
biscuit for that it tries something else
and you get a handshake or begging or
something like that and then goes oh
this is also reward-based and so it kind
of explores things to find out what will
bring it as biscuit and that's very much
like how a reinforced model goes is it
uh looks for different rewards how do I
find can I try different things and find
a reward that
works the dog also will want to run
around and play and explorers
environment uh this quality of model is
called exploration so there's a little
Randomness going on in
Exploration and explores new parts of
the house climbing on the sofa doesn't
get a reward in fact it usually gets
kicked off the
sofa so let's talk a little bit about
markov's decision process uh markov's
decision process is a reinforcement
learning policy used to map a current
state to an action where the agent
continuously interacts with the
environment to produce new Solutions and
receive rewards and you'll see here's
all of our different uh uh vocabulary we
just went over we have a reward our
state or agent our environment inter our
action and so even though the
environment kind of contains everything
um that you you really when you're
actually writing the program your
environment's going to put out a reward
in state that goes into the agent uh the
agent then looks at this state or it
looks at the reward usually um first and
it says okay I got rewarded for whatever
I just did or I didn't get rewarded and
then looks at the state then it comes
back and if you remember from policy the
policy comes in um and then we have a
reward the policy is that part that's
connected at the bottom and so it looks
at thatc policy and it says hey what's a
good action that will probably be
similar to what I did or um sometimes
they're completely random but what's a
good action that's going to bring me a
different
reward so taking the time to just
understand these different pieces as
they go is pretty important in most of
the models today um and so a lot of them
actually have templates based on this
you can pull in and start using um
pretty straightforward is as far as once
you start seeing how it works uh you can
see your environment send it says hey
this is the agent did this if you're a
character in a game this happened and it
shoots out a reward in a state the agent
looks at the reward looks at the new
state and then takes a little guess and
says I'm going to try this action and
then that action goes back into the
environment it affects the environment
the environment then changes depending
on what the action was and then it has a
new state and a new reward that goes
back to the agent
so in the diagram shown we need to find
the shortest path between Noe A and D
each path has a reward associated with
it and the path with the maximum reward
is what we want to choose the nodes a b
c d denote the nodes to travel from node
uh A to B is an action reward is the
cost of each path and policy is each
path
taken and you can see here a can go uh
to b or a can go to C right off the bat
or it can go right to D and if you
explored all three of these uh you would
find that a going to D was a zero reward
um a going to C and D would generate a
different reward or you could go AC b d
there's a lot of options here um and so
when we start looking at this diagram
you start to
realize that even though uh today's
reinforced learning models do really
good uh um finding an answer they end up
trying almost all the different
directions you see and so take up a lot
of work uh or a lot of processing time
for reinforcement learning they're right
now in their infant stage and they're
really good at solving simple problems
and we'll take a look at one of those in
just a minute in a tic tac toe game uh
but you can see here uh once it's gone
through these and it's explored it's
going to find the
ACD is the best reward it gets a full 30
points for it so let's go ahead and take
a look at a reinforcement learning
demo uh in this demo we're going to use
reinforcement learning to make a tic tac
toe game you'll be playing this game
Against the Machine learning
model and we'll go ahead we're doing it
in Python so let's go ahead and go
through I always uh not always actually
have a lot of python tools let's go
through um Anaconda which will open up a
Jupiter notebook seems like a lot of
steps but it's worth it to keep all my
stuff separate and it's also has a nice
display when you're in the Jupiter
notebook for doing
python so here's our Anaconda Navigator
I open up the notebook which is going to
take me to a web page and I've gone in
here and created a new uh python folder
in this case I've already done it and
enabled it to change the name to
tic-tac-toe uh and then for this example
uh we're going to go ahead
and import a couple things we're going
to um import numpy as NP we'll go ahead
and import pickle numpy of course is our
number array and pickle is just a nice
way sometimes for storing uh different
information uh different stat that we're
going to go through on
here uh and so we're going to create a
class called State we're going to start
with
that and there's a lot of lines of code
to this uh class that we're going to put
in here don't let that scare you too
much there's not as much here um it
looks like there's going to be a lot
here but there really is just a lot of
setup going on in the in our class date
and so we have up here we're going to
initialize it um we we have our board um
it's a tic teac toe board so we're only
dealing with nine spots on the board uh
we have player one player
two uh is in we're going to create a
board hash uh we'll look at that in just
a minute we're just going to stir some
information in there symbol of player
equals one um so there's a few things
going on as far as the
initialization uh then something simple
we're just going to get the hash um of
the board we're g get the information
from the board on there which is uh
columns and rows we want to know when a
winner occurs uh so if you get three in
a row that's what this whole section
here is for um let me go ahead and
scroll up a little bit and you can get a
copy of this code if you send a note
over to Simply learn we'll send you over
um this particular file and you can play
with it yourself and see how it's put
together I don't want to spend a huge
amount of time on this uh because this
is just some real General python coding
uh but you can see here we're just going
through um all the rows and you add them
together and if it equals three three in
a row same thing with
columns um diagonal so you got to check
the diagonal that's what all this stuff
does here is it just goes through the
different areas actually let me go ahead
and
put there we
go um and then it comes down here and we
do our sum and it says true uh minus
three just says did somebody win or is
it a tie so you got to add up all the
numbers on there anyway just in case
they're all filled up and next we also
need to know available positions um
these are ones that don't no one's ever
used before this way when you try
something or the computer tries
something uh it's not going to give it
an illegal move that's what the
available positions is doing uh then we
want to update our state and so you have
your position going in we're just
sending in the position that you just
chose and you'll see there's a little
user interface we put in there you P
pick the row and column in
there and again I mean this is a lot of
code uh so really it's kind of a thing
you'd want to go through and play with a
little bit and just read through it get
a copy of it uh great way to understand
how this works and here is a given
reward um so we're going to give a
reward result equals self winner this is
one of the hearts of what's going on
here uh is we have a result self. winner
so if there's a winner then we have a
result if the result equals one here's
our
feedback uh if it doesn't equal one then
it gets a zero so it only gets a reward
in this particular case if it
wins and that's important to know
because different uh systems of
reinforced learning do rewarding a lot
differently depending on what you're
trying to do this is a very simple
example with a a 3X3 board imagine if
you're playing a video game uh certainly
you only have so many actions but your
environment is huge you have a lot going
on in the environment and suddenly a
reward system like this is going to be
just um it's going to have to change a
little bit it's going to have to have
different rewards and different setup
and there's all kinds of advanced ways
to do that as far as weighing you add
weights to it and so they can add the
weights up depending on where the reward
comes in so it might be that you
actually get a reward in this case case
you get the reward at the end of the
game and I'm spending just a little bit
of time on this because this is an
important thing to note but there's
different ways to add up those rewards
it might have like if you take a certain
path um the first reward is going to be
weighed a little bit less than the last
reward because the last reward is
actually winning the game or scoring or
whatever it is so this reward system
gets really complicated on some of the
more advanced uh
setups um in this case though you can
see right here that they give a um a 0.1
and a 0. five
reward um just for getting a picking the
right value and something that's
actually valid instead of picking an
invalid value so rewards again that's
like key it's huge how do you feed the
rewards back in uh then we have a board
reset that's pretty straightforward it
just goes back and resets the board to
the beginning cuz it's going to try out
all these different things while it's
learning it's going to do it by trial
and error so you have to keep resetting
it
and then of course there's the play we
want to go ahead and play uh rounds
equals 100 depends on what you want to
do on here um you can set this different
you obviously set that to higher level
but this is just going to go through and
you'll see in here uh that we have
player one and player two this is this
is the computer playing itself uh one of
the more powerful ways to learn to play
a game or even learn something that
isn't a game is to have two of the these
models that are basically trying to beat
each other and so they they keep finding
explore new things this one works for
this one so this one tries new things it
beats this we've seen this in um chess I
think was a big one where they had the
two players in chess with reinforcement
learning uh was one of the ways they
train one of the top um computer chess
playing
algorithms uh so this is just what this
is it's going to choose an action it's
going to try something and the more it
tries stuff
um the more we're going to record the
hash we actually have a board hash where
they self get the hash setup on here
where it stores all the
information and then once you get to a
win one of them wins it gets the reward
uh then we go back and reset and try
again and then kind of the fun part we
actually get down here is uh we're going
to play with a human so we'll get a
chance to come in here and see what that
looks like when you put your own
information in and then it just comes in
here does the same thing it did above
it gives it a reward for its things um
or sees if it wins or ties um looks at
available positions all that kind of fun
stuff and then finally we want to show
the board uh so it's going to print the
board out each
time really um as an integration is not
that exciting what's exciting uh in here
is one looking at this reward system
whoops Play One More up the reward
system is really the heart of this how
do you reward the different uh setup and
the other one is when it's playing it's
got to take an action and so what it
chooses for an action is also the heart
of reinforcement learning how do we
choose that action and those are really
key to right now where reinforcement
learning is uh in today's uh technology
is uh figuring this out how do we reward
it and how do we guess the next best
action so we have our uh environment and
you can see the environment is we're
going to be or the state uh which is
kind of like what's going on we're going
to return the state depending on what
happens and we want to go ahead and
create our agent uh in this place our
player so each one is me go and grab
that and so we look at a class player um
this is where a lot of the magic is
really going on is what how is this
player figuring out how to maneuver
around the board and then the board of
course returns a St
uh that it can look at and a reward uh
so we want to take a look at this we
have a name uh self State this is class
player and when you say class player
we're not talking about a human player
we're talking about um just a uh the
computer players and this is kind of
interesting so remember I told you
depending on what you're doing there's
going to be a Decay gamma um explor rate
uh these are what I'm talking about is
how do we train it um
as you try different moves it gets to
the end the first move is important but
it's not as important as the last one
and so you could say that the last one
has the heaviest weight and then as you
as you get there the first one let's see
the first move gives you a five reward
the second gives you a two reward and
the third one gives you a 10 reward
because that's the final ending you got
it the 10's going to count more in the
first step uh and here's our uh we're
going to you know get the board
information coming in
and then choose an action this was the
second part that I was talking about
that was so important uh so once you
have your training going on we have to
do a little Randomness and you can see
right here is our NP random uh uniform
so it's picking out a random number take
a random action this is going to just
pick which row and which column it is um
and so choosing the action this one you
can see we're just doing random States
uh Choice length of positions action
position and then it skips in there and
takes a look at the board uh for p and
positions you it's actually storing the
different boards each time you go
through so it it has a record of what it
did so it can properly weigh the values
and this simply just ains a hash State
what's the last date pin it to the uh um
to our states on here here's our
feedback reward so the reward comes in
and it's going to take a look at this
and say is it none uh what is the reward
and here is that formula remember I was
telling you about up here um that was
important because it has Decay gamma
times the reward this is where as it
goes through each step and this is
really important this is this is kind of
the heart of this of what I was talking
about earlier uh you have step
one and this might have a a reward of
two you have step two I should probably
should have done ABC this has a step
three uh step
four so on till you get to step in and
this might have a reward of
10 uh so reward a
10 we're going to add that but we're not
adding uh let's say this one right here
uh let's say this reward here right
before 10 was um let's say it's also 10
it just makes a the math easy so we had
10 and 10 we had 10 this is 10 and 10 in
whatever it is but it's time it's
0.9 uh so instead of putting a full 10
here we only do 9 that's a 0.9
time
10 and so this
formula um as far as the Decay times the
reward minus the cell State value uh it
basically adds in it says here's one or
here's two I'm sorry I should have done
this AB BC would have been easier uh so
the first move goes in here and it puts
two in
here uh then we have our s uh setup on
here you can see how this gets pretty
complicated in the math but this is
really the key is how do we train our
states and we want the the final State
the win to get the most points if you
win you get most points um and the first
step gets the least amount of points so
you're really training this almost in
Reverse you're training you're training
it from the last place where you have
like it says okay this is now where I
need to sum up my rewards and I want to
sum them up going in reverse and I want
to find the answer in Reverse kind of an
interesting uh uh play on the mind when
you're trying to figure this stuff
out and of course we want to go ahead
and reset the board down here uh and
save the policy load
policy these are the different things
that are going in between the agent and
the state to figure out what's going on
let's go ahead and load that up and then
finally we want to go ahead and create a
human
player and the human players going to be
a little different uh in that uh you
choose an action row and column here's
your action uh if action is if action in
positions meaning positions that are
available uh you return the action if
not it just keeps asking you until you
get an action that actually works and
then we're going to go ahead and a PIN
to the hash state which uh we don't need
to worry about because it Returns the
action up
here and feed forward uh again this is
because it's a
human um at the end of the game bat
propagate and update State values this
part isn't being done because it's not
programming uh the model uh the model is
getting its own rewards so we've gone
ahead and loaded this in here uh so
here's all our pieces and the first
thing we want to do
is set up uh P1 player one uh P2 player
two and then we're going to send our
players to our state so now it has P1 P2
and it's going to play and it's going to
play 50,000 rounds now we can probably
do a lot less than this and it's not
going to get the full results in fact
you know what uh let's go ahead and just
do five um just to play with it because
I want to show you something
here oops somewhere in there I forgot to
load something
there we go I must have forgot to run
this
run oops forgot a reference there for
the board rows and columns
3x3 um there is actually in the state it
references that we just tack it on on
the end it was supposed to be at the
beginning uh so now I've only set this
up with um see where are we going here
I've only set this up to train
five times and the reason I did that is
we're going to uh come in and actually
play it and then I'm going to change
that and we can see how it differs on
there there we go and I didn't even make
it through a run and we're going to go
ahead and save the
policy um so now we have our player one
and our player two policy uh the way we
set it up it has two separate policies
loaded up in
there and then we're going to come in
here and and we're going to do uh player
one is going to be the computer
experience rate zero load policy one
human player human and we're going to go
ahead and play this I remember I only
went through it um uh just one round of
training in fact minimal training and so
it puts an X there and I'm going to go
ahead and do row zero column one you can
see this is very uh basic on here and so
I put in my zero and then I'm going to
go zero block it zero
zero and you can see right here it let
me win uh just like that I was able to
win
zero two and woo human winds so I only
trained it five times we're going to run
this again and this time uh instead of
five let's do 5,000 or 50,000 I think
that's what the guys in the back had and
this takes a while to train it this is
where reinforcement learning really
falls apart look how simple this game is
we're talking about uh 3x3 set of
columns and so for me to train it on
this um I could do a q table which would
take which would go much quicker um you
could build a quick Q table with almost
all the different options on there and
uh you would probably get a the same
result much quicker we're just using
this as an example so when we look at
reinforcement learning you need to be
very careful what you apply it to it
sounds like a good deal until you do
like a large neural network where you're
doing um you set the neural network to a
learning increment of one so every time
it goes through it
learns and then you do your action so
you pick from the learning uh setup and
you actually try actions on the learning
setup until you get the what you think
is going to be the best action so you
actually feed what you think is right
back through the neural network there a
whole layer there which is really fun to
play
with and then it has an output well
think of all those processes I mean that
is just a huge amount of work it's going
to do uh let's go ahead and Skip ahead
here give it a moment it's going to take
a a minute or two to go ahead and
run now to train it uh we went ahead and
let it run and it took a while this this
took um I got a pretty powerful
processor and it took about 5 minutes
plus to run it and we'll go ahead and
uh run our player setup on here oops I
brought in the last whoops I brought in
the last round so give me just a moment
to redo the policy save there we go I
forgot to save the policy back in
there and then go ahead and run our
player again so we we've saved the
policy and then we want to go ahead and
load the policy for P1 as a computer and
we can see the computer's gone in the
bottom right corner I'm going to go
ahead and go uh one one which is the
center and it's gone right up the top
and if you have ever played Tic Tac Toe
you know the computer has me uh but
we'll go ahead and play it out row zero
column
two there it is and then it's gone here
and so I'm going to go ahead and go row
01 two no 01 there we go and column zero
that's I wanted oh and it says I okay
you your action there we go boom uh so
you can see here we've got a didn't
catch the win on this it said Tai um
kind of funny they didn't catch the win
on
there but if we play this a bunch of
times you'll find it's going to win more
and more the more we train it the more
the reinforcement
happens this lengthy training process uh
is really the stopper on reinforcement
learning as this changes reinforcement
learning will be one of the more
powerful uh packages evolving over the
next decade or two in fact I would even
go as far as to say it is the most
important uh machine learning tool and
artificial intelligence tool out there
as it learns not only a simple Tic Tac
Toe board but we start learning
environments and the environment would
be like in language if you're
translating a language or something from
one language to the other so much of it
is lost if you don't know the context
it's in what's the environments it's in
and so being able to attach environment
and context and all those things
together is going to require
reinforcement learning to
do so again if you want to get a copy of
the Tic Tac Toe board it's kind of fun
to play with uh run it you can test it
out you can do um you know test it for
different uh uh values you can switch
from P1
computer um where we loaded the policy
one to load the policy 2 and just see
how it varies there's all kinds of
things you can do on there supervised
learning uses labeled data to train
machine learning models
label data means that the output is
already known to you the model just
needs to map the inputs to the outputs
an example of supervised learning can be
to train a machine that identifies the
image of an animal below you can see we
have a trained model that identifies the
picture of a cat unsupervised learning
uses unlabeled data to train machines
unlabelled data means there is no fixed
output variable the model learns from
the data discovers patterns and features
in the data and Returns the output here
is an example of an unsupervised
learning technique that uses the images
of vehicles to classify if it's a bus or
a truck so the model learns by
identifying the paths of a vehicle such
as the length and width of the vehicle
the front and rear end covers roof hoods
the types of Wheels used Etc based on
these features the model classifies if
the vehicle is a bus or a
truck reinforcement learning trains a
machine to take suitable accs and
maximize reward in a particular
situation it uses an agent and an
environment to produce actions and
rewards the agent has a start and an end
state but there might be different parts
for reaching the end State like a maze
in this learning technique there is no
predefined target variable an example of
reinforcement learning is to train a
machine that can identify the shape of
an object given a list of different
objects such as square triangle
rectangle or a circle in the example
shown the model tries to predict the
shape of the object which is a square
here now let's look at the different
machine learning algorithms that come
under these learning techniques some of
the commonly used supervised learning
algorithms are linear regression
logistic regression support Vector
machines K nearest neighbors decision
tree random forest and knife
base examples of unsupervised learning
algorithms are K means clustering
hierarchical clustering DB scan
principal compon component analysis and
others choosing the right algorithm
depends on the type of problem you're
trying to
solve some of the important
reinforcement learning algorithms are Q
learning Monte Carlo sarsa and deep Q
Network now let's look at the approach
in which these machine learning
techniques
work so supervised learning takes
labeled inputs and Maps it to known
outputs which means you already know the
target
variable unsupervised learning finds
patterns and understand the trends in
the data to discover the output so the
model tries to label the data based on
the features of the input
data while reinforcement learning
follows trial and error method to get
the desired solution after accomplishing
a task the agent receives an award an
example could be to train a dog to catch
the ball if the dog learns to catch a
ball you give it a reward such as a
biscuit now let's discuss the training
process for each of these learning
methods
so supervised learning methods need
external supervision to train machine
learning models and hence the name
supervised they need guidance and
additional information to return the
result unsupervised learning techniques
do not need any supervision to train
models they learn on their own and
predict the output similarly
reinforcement learning methods do not
need any supervision to train machine
learning models and with that let's
focus on the types of problems that can
be solved solved using these three types
of machine learning techniques so
supervised learning is generally used
for classification and regression
problems we'll see the examples in the
next
slide and unsupervised learning is used
for clustering and Association problems
while reinforcement learning is
reward-based so for every task or for
every step completed there will be a
reward received by the agent and if the
task is not achieved correctly there
will be some penalty used
now let's look at a few applications of
supervised unsupervised and
reinforcement
learning as we saw earlier supervised
learning are used to solve
classification and regression problems
for example You can predict the weather
for a particular day based on humidity
precipitation wind speed and pressure
values you can use supervised learning
algorithms to forecast sales for the
next month or the next quarter for
different products similarly you can use
it for stock price analysis or
identifying if a cancer cell is
malignant or
benign now talking about the
applications of unsupervised learning we
have customer segmentation So based on
customer Behavior likes dislikes and
interests you can segment and cluster
similar customers into a group another
example where unsupervised learning
algorithms are used is customer churn
analysis now let's see what applications
we have in reinforcement learning so
reinforcement learning algorithms are
widely used in the gaming Industries to
to build games it is also used to train
robots to perform human tasks if you are
an aspiring data scientist who looking
out for online training and
certification in data science from the
best universities and Industry experts
then search no more simply learns
postgraduate program in data science
from Caltech University in collaboration
with IBM should be the right choice for
more details on this program please use
the link in the description box below
often professionals want to know if
there is a relationship between two or
more variables for instance is there a
relationship between the grade on the
third French exam a student takes and
the grade on the final exam if yes then
how is it related and how strongly
regression can be used here to arrive at
a conclusion this is an example of by
variant data that is two variables
however statisticians are mostly
interested in multivariate data
regression analysis is used to to
predict the value of one variable the
dependent variable on the basis of other
variables the independent variables in
the simplest form of regression linear
regression you work with one independent
variable the formula for simple linear
regression is shown on the screen in the
next screen we'll look at a few examples
of regression
analysis regression analysis is used in
several situations such as those
described on the screen in example one
using the data given on the screen you
have to analyze the relation between the
size of a house and its selling price
for a realer in example two you need to
predict the exam scores of students who
study for 7.2 hours with the help of the
data shown on the slide a couple more
examples are given on the screen in
example three based on the expected
number of customers and the previous day
data given you need to predict the
number of burgers that will be sold by a
KFC Outlet in example four you have to
calculate the life expect for a group of
people with the average length of
schooling based on the data
given let's look at the two main types
of regression analysis simple linear
regression and multiple linear
regression both of these statistical
methods use a linear equation to model
the relationship between two or more
variables simple linear regression
considers one quantitative and
independent variable X to predict the
other quantitative but dependent
variable y multiple linear regression
considers more than one quantitative and
qualitative variable to predict a
quantitative and dependent variable y
we'll look at the two types of analyses
in more detail in the slides that follow
in simple linear regression the
predictions of the explained variable y
when plotted as a function of the
explanatory variable X from a straight
line the best fitting line is called the
regression line the output of this model
is a function to predict the dependent
variable on the basis of the values of
the independent variable the dependent
variable is continuous and the
independent variable can be continuous
or discreete let's look at the different
kinds of linear and nonlinear analyses
list of linear techniques are simple
method of least squares coefficient of
multiple determination standard error of
the estimate dummy variable and
interaction similarly there are many
nonlinear techniques available such as
polom logarithmic square root reciprocal
and exponential to understand this model
we'll first look at a few assumptions
the simple linear regression model
depicts the relationship between one
dependent and two or more independent
variables the assumptions which justify
the use of this model are as follows
linear and additive relationship between
the dependent and independent variables
multivariate normality little or no
collinearity in the data little or no
autocorrelation in the data homos
acticity that is variance of Errors same
across all values of X the equation for
this model is shown on the screen a more
descriptive graphical representation of
simple linear regression is given on the
screen beta KN represents the slope a
slope with two variables implies that
one unit changes in X result in a two
unit change in y beta 1 represents the
estimated change in the average value of
y as a result of one unit change in x
Epsilon represents the estimated average
value of y when the value of x is
zero this demo will show the steps to do
simple linear regression in
r in this demo you'll learn how to do
simple linear
regression let's use X and Y vectors
that we have created in the previous
demo
we also ensured there exists a
relationship between X and Y visually by
plotting a
graph to build a simple linear
regression model let's use the LM
function
to see how the linear model fits into X
and Y let's plot the linear line by the
ab line
function let's use the predict function
to test or predict the linear model we
can pass a known variable to predict
prict the unknown variables
let's look at an example of a common use
for linear regression profit estimation
of a company if I was going to invest in
a company I would like to know how much
money I could expect to make so we'll
take a look at a venture capitalist firm
and try to understand which companies
they should invest in so we'll take the
idea that we need to decide the
companies to invest in we need to
predict the profit the company makes and
we're going to do it based on the
company's expenses and even just a
specific expense in this case we have
our company we have the different
expenses so we have our R&D which is
your research and development we have
our marketing uh we might have the
location
we might have what kind of
administration it's going through based
on all this different information we
would like to calculate the profit now
in actuality there's usually about 23 to
27 different markers that they look at
if they're a heavy duty investor we're
only going to take a look at one basic
one we're going to come in and for
Simplicity let's consider a single
variable R&D and find out which
companies to invest in based on that so
when we take a R&D and we're plotting
The Profit based on the R&D expenditure
how much money they put into the
research and development and then we
look at the profit that goes with that
we can predict a line to estimate the
profit so we draw a line right through
the data when you look at that you can
see how much they invest in the R&D is a
good marker as to how much profit
they're going to have we can also note
that companies spending more on R&D make
good profit so let's invest in the ones
that spend a higher rate in their R&D
what's in it for you first we'll have an
introduction to machine learning
followed by Machine learning algorithms
these will be specific to linear
regression and where it fits into the
larger model then we'll take a look at
applications of linear regression
understanding linear regression and
multiple linear regression finally we'll
roll up our sleeves and do a little
programming in use case profit
estimation of companies let's go ahead
and jump in let's start with our
introduction to machine learning along
with some machine learning algorithms
and where that fits in with learning
your regression let's look at another
example of machine learning based on the
amount of rainfall how much would be the
crop yield so we here we have our crops
we have our rainfall and we want to know
how much we're going to get from our
crops this year so we're going to
introduce two variables independent and
dependent the independent variable is a
variable whose value does not change by
the effect of other variables and is
used to manipulate the dependent
variable it is often denoted as X in our
example rainfall is the independent
variable this is a wonderful example
because you can easily see that we can't
control the rain but the rain does
control the crop so we talk about the
independent variable controlling the
dependent variable let's Define
dependent variable as a variable whose
value change when there is any
manipulation the values of the
independent variables it is often
denoted as why and you can see here our
crop yield is dependent variable and it
is dependent on the amount of rainfall
received now that we've taken a look at
a real life example let's go a little
bit into the theory and some definitions
on machine learning and see how that
fits together with linear regression
numerical and categorical values
let's take our data coming in and this
is kind of random data from any kind of
project we want to divide it up into
numerical and categorical so numerical
is numbers age salary height where
categorical would be a description the
color a dog's breed gender categorical
is limited to very specific items where
numerical is a range of information now
that you've seen the difference between
numerical and categorical data data
let's take a look at some different
machine learning definitions when we
look at our different machine learning
algorithms we can divide them into three
areas supervised
unsupervised reinforcement we're only
going to look at supervised today
unsupervised means we don't have the
answers and we're just grouping things
reinforcement is where we give positive
and negative feedback to our algorithm
to program it and it doesn't have the
information till after the fact but
today we're just looking at supervised
cuz that's where linear regression fits
in in supervised data we have our data
already there and our answers for a
group and then we use that to program
our model and come up with an answer the
two most common uses for that is through
the regression and classification now
we're doing linear regression so we're
just going to focus on the regression
side and in the regression we have
SIMPLE linear regression we have
multiple linear regression and we have
polom linear regression now on these
three simple linear regression is the
examples we've looked at so far where we
have a lot of data and we draw a
straight line through it multiple linear
regression means we have multiple
variables remember where we had the
rainfall and the crops we might add
additional variables in there like how
much food do we give our crops when do
we Harvest them those would be
additional information add into our
model and that's why it' be multiple
linear regression and finally we have
polinomial linear regression that is
instead of drawing a line we can draw a
curved line through it now that you see
where regression model fits into the
machine learning algorithms and we're
specifically looking at linear
regression let's go ahead and take a
look at applications for linear
regression let's look at a few
applications of linear regression
economic growth used to determine the
economic growth of a country or a state
in the coming quarter can also be used
to predict the GDP of a country product
price can be used to predict what would
be the price of a product in the future
we can guess whether it's going to go up
or down or should I buy today housing
sales to estimate the number of houses a
builder would sell and what price in the
coming months score predictions Cricut
fever to predict the number of runs a
player would score in the coming matches
based on the previous performance I'm
sure you can figure out other
applications you could use linear
regression for so let's jump in and
let's understand linear regression and
dig into the theory understanding linear
regression linear regression is the
statistical model used to predict the
relationship ship between independent
and dependent variables by examining two
factors the first important one is which
variables imp particular are significant
predictors of the outcome variable and
the second one that we need to look at
closely is how significant is the
regression line to make predictions with
the highest possible accuracy if it's
inaccurate we can't use it so it's very
important we find out the most accurate
line we can get since linear regression
is based on drawing a line through data
we're going to jump back and take a look
at some ukian geometry the simplest form
of a simple linear regression equation
with one dependent and one independent
variable is represented by y = m * x + C
and if you look at our model here we
plotted two points on here uh X1 and y1
X2 and Y2 y being the dependent variable
remember that from before and X being
the independent variable so y depends on
whatever X
m in this case is the slope of the line
where m equals the difference in the Y
2us y1 and X2 - X1 and finally we have C
which is the coefficient of the line or
where happens to cross the zero axis
let's go back and look at an example we
used earlier of linear regression we're
going to go back to plotting the amount
of crop yield based on the amount of
rainfall and here we have our rainfall
remember we Cann change rainfall and we
have our crop yield which is dependent
on the rainfall so we have our
independent and our dependent variables
we're going to take this and draw a line
through it as best we can through the
middle of the data and then we look at
that we put the red point on the Y AIS
is the amount of crop yield you can
expect for the amount of rainfall
represented by the Green Dot so if we
have an idea what the rainfall is for
this year and what's going on then we
can guess how good our crops are going
to be and we've created a nice line
right through the middle to give us a
nice mathematical formula let's take a
look and see what the math looks like
behind this let's look at the intuition
behind the regression line now before we
dive into the math and the formulas that
go behind this and what's going on
behind the
scenes I want you to note that when we
get into the case study and we actually
apply some python script that this math
that you're going to see here is already
done automatically for you you don't
have to have it memorized it is however
good to have have an idea what's going
on so if people reference the different
terms you'll know what they're talking
about let's consider a sample data set
with five rows and find out how to draw
the regression line we're only going to
do five rows because if we did like the
rainfall with hundreds of points of data
that would be very hard to see what's
going on with the mathematics so we'll
go ahead and create our own two sets of
data and we have our independent
variable X and our dependent variable Y
and when X was 1 we got y = two when X
was uh 2 y was 4 and so on and so on if
we go ahead and plot this data on a
graph we can see how it forms a nice
line through the middle you can see
where it's kind of grouped going upwards
to the right the next thing we want to
know is what the means is of each of the
data coming in the X and the Y the means
doesn't mean anything other than the
average so we add up all the numbers and
divide by the total so 1 + 2 + 3 + 4 + 5
over 5 = 3 and the same for y we get
four if we go ahead and plot the means
on the graph we'll see we get 3 comma 4
which draws a nice line down the middle
a good estimate here we're going to dig
deeper into the math behind the
regression line now remember before I
said you don't have to have all these
formulas memorized or fully understand
them even though we're going to go into
a little more detail of how it works and
if you're not a math whz and you don't
know if you've never seen the sigma
character before which looks a little
bit like an e that's opened up that just
means summation that's all that is so
when you see the sigma character it just
means we're adding everything in that
row and for computers this is great
because as a programmer you can easily
iterate through each of the XY points
and create all the information you need
so in the top half you can see where
we've broken that down into pieces and
as it goes through the first two points
it computes the squared value of x the
squared value of y and x * Y and then it
takes all of X and adds them up all of Y
adds them up all of X2 adds them up and
so on and so on and you can see we have
the sum of equal to 15 the sum is equal
to 20 all the way up to x * Y where the
sum equals 66 this all comes from our
formula for calculating a straight line
where y equals the slope * X plus the
coefficient C so we go down below and
we're going to compute more like the
averages of these and we're going to
explain exactly what that is in just a
minute and where that information comes
from it's called called the square means
error but we'll go into that in detail
in a few minutes all you need to do is
look at the formula and see how we've
gone about Computing it line by line
instead of trying to have a huge set of
numbers pushed into it and down here
you'll see where the slope m equals and
in the top part if you read through the
brackets you have the number of data
points times the sum of x * Y which we
computed one line at a time there and
that's just the 66 and take all that and
you subtract it from the sum of x times
the sum of Y and those have both been
computed so you have 15 * 20 and on the
bottom we have the number of lines times
the sum of X squ easily computed as 86
for the sum minus I'll take all that and
subtract the sum of X2 and we end up as
we come across with our formula you can
plug in all those numbers which is very
easy to do on the computer you don't
have to do the math on a piece of paper
or calcul C and you'll get a slope of 6
and you'll get your C coefficient if you
continue to follow through that formula
you'll see it comes out as equal to 2.2
continuing deeper into what's going
behind the scenes let's find out the
predicted values of Y for corresponding
values of X using the linear equation
where M = 6 and C = 2.2 we're going to
take these values and we're going to go
ahead and plot them we're going to
predict them so y = 6 * or x = 1 + 2.2 =
2 .8 so on and so on and here the Blue
Points represent the actual y values and
the brown points represent the predicted
y values based on the model we created
the distance between the actual and
predicted values is known as residuals
or errors the best fit line should have
the least sum of squares of these errors
also known as e square if we put these
into a nice chart where you can see X
and you can see Y what we actual values
were and you can see why predicted you
can easily see where we take Yus y
predicted and we get an answer what is
the difference between those two and if
we square that y- y prediction squared
we can then sum those squared values
that's where we get the 64 plus the 36 +
1 all the way down until we have a
summation equals 2.4 so the sum of
squared errors for this regression line
is 2.4 we check this error for each line
and conclude the best fit line having
the least e Square value in a nice
graphical representation we can see here
where we keep moving this line through
the data points to make sure the best
fit line has the least Square distance
between the data points and the
regression line now we only looked at
the most commonly used formula for
minimizing the distance there are lots
of ways to minimize the distance between
the line and the data points like sum of
squared errors sum of absolute errors
root mean square error Etc what you want
to take away from this is whatever
formula is being used you can easily
using a computer programming and
iterating through the data calculate the
different parts of it that way these
complicated formulas you see with the
different summations and absolute values
are easily computed one piece at a time
up until this point we've only been
looking at two values X and Y well in
the real world it's very rare that you
only have two values when you're
figuring out a solution so let's move on
to the next topic multiple linear
regression let's take a brief look at
what happens when you have multiple
inputs so in multiple linear regression
we have uh well we'll start with the
simple linear regression where we had y
= m + x + C and we're trying to find the
value of y now with multiple linear
regression we have multiple variables
coming in so instead of having just X we
have X1 X2 X3 and instead of having just
one slope each variable has its own
slope attached to it as you can see here
we have M1 M2 M3 and we still just have
the single coefficient so when you're
dealing with multiple linear regression
you basically take your single linear
regression and you spread it out so you
have y = M1 * X1 + M2 * X2 so on all the
way to m to the nth x to the nth and
then you add your coefficient on there
implementation of linear regression now
we get into my favorite part let's
understand how multiple linear
regression works by implementing it in
Python if you remember before we were
looking at a company and just based on
its R&D trying to figure out its profit
we're going to start looking at the
expenditure of the company we're going
to go back to that we're going to
predict this profit but instead of
predicting it just on the R&D we're
going to look at other factors like
Administration cost marketing costs and
so on and from there we're going to see
if we can figure out what the profit of
that company's going to be to start our
coding we're going to begin by importing
some basic libraries and we're going to
be looking through the data before we do
any kind of linear regression we're
going to take a look at the data see
what we're playing with then we'll go
ahead and format the data to the format
we need to be able to run it in the
linear regression model and then from
there we'll go ahead and solve it and
just see how valid our solution is so
let's start with importing the basic
libraries now I'm going to be doing this
in Anaconda Jupiter notebook a very
popular IDE I enjoy it CU it's such a
visual to look at it's so easy to use um
just any ID for python will work just
fine for this so break out your favorite
python IDE so here we are in our Jupiter
notebook let me go ahead and paste our
first piece of code in there and let's
walk through what libraries we're
importing first we're going to import
numpy as NP and then I want you to skip
one line and look at import pandas as PD
these are very common tools that you
need with most of your linear regression
the numpy which stands for number python
is usually denoted as NP and you have to
almost have that for your SK learn
toolbox you always import that right off
the beginning panda does although you
don't have to have it for your sklearn
libraries it does such a wonderful job
of importing data setting it up into a
data frame so we can manipulate it
rather easily and it has a lot of tools
also in addition to that so we usually
like to use the pandas when we can and
I'll show you what that looks like the
other three lines are for us to get a
visual of this data and take a look at
it so we're going to import matplot
library. pyplot as PLT and then caborn
as SNS caborn works with the map plot
libr Library so you have to always
import matplot library and then Seaborn
sits on top of it and we'll take a look
at what that looks like you could use
any of your own plotting libraries you
want there's all kinds of ways to look
at the data these are just very common
ones and the caborn is so easy to use it
just looks beautiful it's a nice
representation that you can actually
take and show somebody and the final
line is the Amber sign matap plot
library in line that is only because I'm
doing an inline IDE my interface in the
Anaconda Jupiter notebook require as I
put that in there or you're not going to
see the graph when it comes up let's go
ahead and run this it's not going to be
that interesting because we're just
setting up variables in fact it's not
going to do anything that we can see but
it is importing these different
libraries and setup the next step is
load the data set and extract
independent and dependent variables now
here in the slide you'll see companies
equals pd. read CSV and it has a long
line there with the file at the end
1,000 companies. CSV you're going to
have to change this to fit whatever
setup you have and the file itself you
can request just go down to the
commentary below this video and put a
note in there and simply learn we'll try
to get in contact with you and Supply
you with that file so you can try this
coding yourself so we're going to add
this code in here and we're going to see
that I have companies equals pd. reader
CSV and I've changed this path to match
my computer c/s simplylearn
1000 companies. CSV and then below there
we're going to set the x equals to
companies under the I location and
because this is companies is a PD data
set I can use this nice notation that
says take every row that's what the
colon the first colon is comma except
for the last column that's what the
second part is where we have a colon
minus one and we want the values set
into there so X is no longer a data set
a pandis data set but we can easily
extract the data from our pandis data
set with this notation and then y we're
going to set equal to the last row well
the question is going to be what are we
actually looking at so let's go ahead
and take a look at that and we're going
to look at the companies. head which
lists the first five rows of data and
I'll open up the file in just a second
so you can see where that's coming from
but let's look at the data in here as
far as the way the panda sees it when I
hit run you'll see it breaks it out into
a nice setup this is what pandas one of
the things pandas is really good about
is it looks just like an Excel
spreadsheet you have your rows and
remember when we're programming we
always start with zero we don't start
with one so it shows the first five rows
0 1 2 3 4 and then it shows your
different columns R&D spend
Administration marketing spend State
profit it even notes that the top are
column names it was never told that but
pandas is able to recognize a lot of
things that they're not the same as the
data rows why don't we go ahead and open
this file up in a CSV so you can
actually see the raw data so here I've
opened it up as a text editor and you
can see at the top we have R&D spin
comma Administration comma marketing
spin comma State comma profit carries
return I don't know about you but i' go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an Excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is where I can see
the data to me it doesn't mean a whole
lot maybe you're an expert in business
and Investments and you stand what
$165,300
compared to the administration cost of
$1
13689 780 so on so on helps to create
the profit of
$2,261 83 cents that makes no sense to
me whatsoever no pun intended so let's
flip back here and take a look at our
next set of code where we're going to
graph it so we can get a better
understanding of our data and what it
means so at this point we're going to
use a single line of code code to get a
lot of information so we can see where
we're going with this let's go ahead and
paste that into our uh notebook and see
what we got going and so we have the
visualization and again we're using SNS
which is pandas as you can see we
imported the map plot library. pylot as
PLT which then the caborn uses and we
imported the caborn as SNS and then that
final line of code helps us show this in
our um inline coding without this it
wouldn't display and you could display
it to a file and other means and that's
the matap plot library in line with the
Amber sign at the beginning so here we
come down to the single line of code
caborn is great cuz it actually
recognizes the panda data frame so I can
just take the companies. core for
coordinates and I can put that right
into the Seaborn and when we run this we
get this beautiful plot and let's just
take a look at what this plot means if
you look at this plot on mine the colors
are probably a little bit more purplish
and blue than the original one uh we
have the columns and the rows we have R
and D spending we have Administration we
have marketing spending and profit and
if you cross index any two of these
since we're interested in profit if you
cross index profit with profit it's
going to show up if you look at the
scale on the right way up in the dark
why because those are the same data they
have an exact correspondence so R&D
spending is going to be the same as R&D
spending and the same thing with
Administration cost so right down the
middle you get this dark row or dark um
diagonal row that shows that this is the
highest corresponding data that's
exactly the same and as it becomes
lighter there's less connections between
the data so we can see with profit
obviously profit is the same as profit
and next it has a very high correlation
with R&D spending which we looked at
earlier and it has a slightly less
connection to marketing spending and
even less to how much money we put into
the administration so now that we have a
nice look at the data let's go ahead and
dig in and create some actual useful
linear regression model so that we can
predict values and have a better profit
now that we've taken a look at the
visualization of this data we're going
to move on to the next step instead of
just having a pretty picture we need to
generate some hard data some hard values
so let's see what that looks like we're
going to set up our linear regression
model in two steps the first one is we
need to prepare some of our data so it
fits correctly and let's go ahead and
paste this code into our Jupiter
notebook and what we're bringing in is
we're going to bring in the SK K learn
pre-processing where we're going to
import the label encoder and the one hot
encoder to use the label encoder we're
going to create a variable called label
encoder and set it equal to capital L
label capital E encoder this creates a
class that we can reuse for transferring
the labels back and forth now about now
you should ask what labels are we
talking about let's go take a look at
the data we processed before and see
what I'm talking about here if you
remember when we did the companies. head
and we printed the top five rows of data
we have our columns going across we have
column zero which is R&D spending column
one which is Administration column two
which is marketing spending and column
three is State and you'll see under
State we have New York California
Florida now to do a linear regression
model it doesn't know how to process New
York it knows how to process a number so
the first thing we're going to do is
we're going to change that New York
California and Florida and we're going
to change those to numbers that's what
this line of code does here x equals and
then it has the colon comma 3 in
Brackets the first part the colon comma
means that we're going to look at all
the different rows so we're going to
keep them all together but the only row
we're going to edit is the third row and
in there we're going to take the label
coder and we're going to fit and
transform the X also the third row so
we're going to take that third row we're
going to set it equal to a
transformation and that transformation
basically tells it that instead of
having a uh New York it has a zero or
one or a two and then find we need to do
a one hot encoder which equals one hot
encoder categorical features equals
three and then we take the X and we go
ahead and do that equal to one hot
encoder fit transform X to array this
final transformation preps our data
Force so it's completely set the way we
need it as just a row of numbers even
though it's not in here let's go ahead
and print X and just take a look at what
this data is doing you'll see you have
an array of arrays and then each array
is a row of numbers and if I go ahead
and just just do row zero you'll see I
have a nice organized row of numbers
that the computer now understands we'll
go ahead and take this out there because
it doesn't mean a whole lot to us it's
just a row of numbers next on setting up
our data we have avoiding dummy variable
trap this is very important why because
the computer has automatically
transformed our header into the setup
and it's automatically transformed all
these different variables so when we did
the encoder the encoder created two two
columns and what we need to do is just
have the one because it has both the
variable and the name that's what this
piece of code does here let's go ahead
and paste this in here and we have xal X
colon comma 1 colon all this is doing is
removing that one extra column we put in
there when we did our one hot encoder
and our label encoding let's go ahead
and run that and now we get to create
our linear regression model and let's
see what that looks like here and we're
going to do that in two steps the first
step is going to be in splitting the
data now whenever we create a uh
predictive model of data we always want
to split it up so we have a training set
and we have a testing set that's very
important otherwise we'd be very
unethical without testing it to see how
good our fit is and then we'll go ahead
and create our multiple linear
regression model and train it and set it
up let's go ahead and paste this next
piece of code in here and I'll go ahead
and shrink it down a size or two so it
all fits on one line so from the sklearn
module selection
we're going to import train test split
and you'll see that we've created four
completely different variables we have
capital x train capital X test smaller
case y train smaller case y test that is
the standard way that they usually
referen these when we're doing different
uh models you usually see that a capital
x and you see the train and the test and
the lowercase Y what this is is X is our
data going in that's our R&D spin our
Administration our marketing and and
then Y which we're training is the
answer that's the profit because we want
to know the profit of an unknown entity
that's what we're going to shoot for in
this tutorial the next part train test
split we take X and we take y we've
already created those X has the columns
with the data in it and Y has a column
with profit in it and then we're going
to set the test size equals 0.2 that
basically means 20% So 20% of the rows
are going to be tested we're going to
put them off to the side so since we're
using a th000 lines of data that means
that 200 of those lines we're going to
hold off to the side to test for later
and then the random State equals zero
we're going to randomize which ones it
picks to hold off to the side we'll go
ahead and run this it's not overly
exciting it's setting up our variables
but the next step is the next step we
actually create our linear regression
model now that we got to the linear
regression model we get that next piece
of the puzzle let's go ahead and put
that code in there and walk through it
so here we go we're going to paste it in
there and let's go ahead and and since
this is a shorter line of code let's
zoom up there so we can get a good luck
and we have from the SK learn. linear
model we're going to import linear
regression now I don't know if you
recall from earlier when we were doing
all the math let's go ahead and flip
back there and take a look at that do
you remember this where we had this long
formula on the bottom and we were doing
all this suiz and then we also looked at
setting it up with the different lines
and then we also looked all the way down
to multiple linear regression where
we're adding all those formulas together
all of that is wrapped up in this one
section so what's going on here is I'm
going to create a variable called
regressor and the regressor equals the
linear regression that's a linear
regression model that has all that math
built in so we don't have to have it all
memorized or have to compute it
individually and then we do the
regressor do fet in this case we do X
train and Y train because we're using
the training data X being the data in
and Y being profit what we're looking at
and this does all that math for us so
within one click and one line we've
created the whole linear regression
model and we fit the data to the linear
regression model and you can see that
when I run the regressor it gives an
output linear regression it says copy x
equals True Fit intercept equals true in
jobs equal one normalize equals false
it's just giving you some general
information on what's going on with that
regressor model now that we've created
our linear regression model let's go
ahead and use it and if you remember we
kept a bunch of data aside so we're
going to do a y predict variable and
we're going to put in the X test and
let's see what that looks like scroll up
a little bit paste that in here
predicting the test set results so here
we have y predict equals regressor do
predict X test going in and this gives
us y predict now because I'm in Jupiter
in line I can just put the variable up
there and when I hit the Run button
it'll print that array out I could have
just as easily done print y predict so
if you're in a different IDE that's not
an inline setup like the Jupiter
notebook you can do it this way print y
predict and you'll see that for the 200
different test variables we kept off to
the side it's going to produce 200
answers this is what it says the profit
are for those 200 predictions but let's
don't stop there let's keep going and
take a couple look we're going to take
just a short detail here and calculating
the coefficients and the intercepts this
gives us a quick flash at what's going
on behind the line we're going to take a
short detour here and we're going to be
calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has a
coefficients for us and we can simply
just print regressor do coefficient
uncore when I run this you'll see our
coefficients here and if we can do the
regressor coefficient we can also do the
regressor intercept and let's run that
and take a look at that this all came
from the multiple regression model and
we'll flip over so you can remember
where this is going into and where it's
coming from you can see the formula down
here where y = M1 * X1 + M2 * X2 and so
on and so on plus C the coefficient so
these variables fit right into this
formula y = slope 1 * column 1 variable
plus slope 2 * column 2 variable all the
way to the m to the n and x to the N
plus C the coefficient or in this case
you have-
8.89 to the power of two etc etc times
the First Column and the second column
and the third column and then intercept
is the minus
10309 Point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure it's a valid model that this
model works and understand how good it
works so calculating the r s value
that's what we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in code and so we're going to use
this from sklearn docs we're going to
import R2 score that's the R squar value
we're looking at the error so in the R2
score we take our y test versus our y
predict y test is the actual values
we're testing that was the one that was
given to us so we know our true the Y
predict of those 200 values is what we
think it was true and when we go ahead
and run this we see we get a
9352 that's the R2 score now it's not
exactly a straight percentage so it's
not saying it's 93% correct but you do
want that in the upper 90s oh and higher
shows that this is a very valid
prediction based on the R2 score and if
r squ value of. 91 or 92 as we got on
our model remember it does have a random
generation involved this proves the
model is a good model which means
success yay we success trained our model
with certain predictors and estimated
the profit of the companies using linear
regression so now that we have a
successful linear regression model let's
take a look at what we went over today
and take a look at our key takeaways
first if you are an aspiring data
scientist who's looking out for online
training and certification in data
science from the best universities and
Industry experts then search no more
simply learns postgraduate program in
data science from calic University in
collaboration with IBM should be the
right choice for more details on this
program please use the link in the
description box below what is logistic
regression let's say we have to build a
predictive model or a machine learning
model to predict whether the passengers
of the Titanic ship have survived or not
the Shipwreck so how do we do that so we
use logistic regression to build a model
for this how do we use logistic
regression so we have have the
information about the passengers their
ID whether they have survived or not
their class and name and so on and so
forth and we use this information where
we already know whether the person has
survived or not that is the labeled
information and we help the system to
train based on this information based on
this labeled data this is known as
labeled data and during the process of
building the model we probably will
remove some of the non-essential
parameters or attributes here we only
take those attributes which are really
required to make these predictions and
once we train the model we run new data
through it whereby the model will
predict whether the passenger has
survived or not so let's see what we
will learn in this video we will talk
about what is supervised learning and we
will go into details about
classification which is one of the
techniques for supervised learning and
then we will further focus on logistic
regression is which is one of the
algorithms for performing classification
especially binary classification then we
will compare linear and logistic
regression and what are some of the
logistic regression applications and
finally we will end with a use case or a
demo of actual python code for doing
logistic regression in Jupiter non all
right so let's start with what is
supervised learning supervised learning
is one of the two main types of machine
learning methods here we use what is
known as labeled data to help the system
learn this is very similar to how we
human beings learn so let's say you want
to teach a child to recognize an apple
how do we do that we never tell the
child okay this is an apple has a
certain diameter on the top certain
diameter at the bottom and this has a
certain RGB color no we just show an
apple to the child and tell the child
this is Apple and then next time when we
show an Apple child immediately
recognizes yes this is an apple
supervised learning works very similar
on the similar lines so where does
logistic regression fit into the overall
machine learning process machine
learning is divided into two types
mainly two types there is a third one
called reinforcement learning but we
will not talk about that right now so
one is supervised learning and the other
is unsupervised learning unsupervised
learning uses techniques like clustering
and Association and supervised learning
uses techniques like classification and
regression now supervised learning is
used when you have labeled data you have
historical data then you use supervised
learning when you don't have labeled
data then you used unsupervised learning
it's in supervised learning there are
two types of techniques that are used
classification and regression based on
what is the kind of problem we are
solved let's say we want to take the
data and classify it it could be binary
classification like a zero or a one an
example of classification we have just
seen whether the passenger has survived
or not survived like a zero or one that
is known as binary classification
regression on the other hand is you need
to predict a value what is known as a
continuous value classification is for
discrete values regression is for
continuous values let's say you want to
predict the share price or you want to
predict the temperature that will be
there what will be the temperature
tomorrow that is where you use
regression whereas classification are
discrete values is will the customer buy
the product or will not buy the product
will you get a promotion or you will not
get a promotion I hope you're getting
the idea or it could be multiclass
classification as well let's say you
want to build an image classification
model so the image classification model
would take an image as an input and
classify into multiple classes whether
this image is of a cat or a dog or an
elephant or a
so there are multiple classes so not
necessarily binary classification so
that is known as multiclass
classification so we are going to focus
on classification because logistic
regression is one of the algorithms used
for classification now the name may be a
little confusing in fact whenever people
come across logistic regression it
always causes confusion because the name
has regression in it but we are actually
using this for performing classification
okay so yes it is logistic regression
but it is used for classification and in
case you're wondering is there something
similar for regression yes for
regression we have linear regression
keep that in mind so linear regression
is used for regression logistic
regression is used for classification so
in this video we are going to focus on
supervised learning and within
supervised learning we're going to focus
on classification and then within
classification we are going to focus on
logistic regression algorithm so first
of all classification so what are the
various algorithms available for
performing classification the first one
is decision tree there are of course
multiple algorithms but here we will
talk about a few dision trees are quite
popular and very easy to understand and
therefore they used for classification
then we have K nearest neighbors this is
another algorithm for performing
classification and and then there is
logistic regression and this is what we
are going to focus on in this video and
we are going to go into a little bit of
details about logistic regression all
right what is logistic regression as I
mentioned earlier logistic regression is
an algorithm for performing binary
classification so let's take an example
and see how this works let's say your
car has not been serviced for quite a
few years and now you want to find out
if the is going to break down in the
near future so this is like a
classification problem find out whether
your car will break down or not so how
are we going to perform this
classification so here's how it looks if
we plot the information along the X and
Y AIS X is the number of years since the
last service was performed and why is
the probability of your car breaking
down and let's say this information was
this data rather was collected from
several car users it's not just your car
but several car users so that is our
labeled data so the data has been
collected and um for for the number of
years and when the car broke down and
what was the probability and that has
been plotted along X and Y AIS so this
provides an idea or from this graph we
can find out whether your car will break
down or not we'll see how so first of
all the probability can go from 0 to one
as you all aware probability can be
between 0er and one and as we can
imagine it is intuitive as well as the
number of years are on the Lower Side
maybe one year 2 years or 3 years till
after the service the chances of your
car breaking down are very limited right
so for example chances of your car
breaking down or the probability of your
car breaking down within 2 years of your
last service are 0.1 probability
similarly 3 years is Maybe three and so
on but as the number of years increases
let's say if it was six or seven years
there is almost a certainty that your
car is going to break down that is what
this graph shows so this is an example
of a application of the classification
algorithm and we will see in little
details how exactly logistic regression
is applied here one more thing needs to
be added here is that the dependent
variables outcome is discrete so if we
are talking about whether the C is going
to break down or not so that is a
discrete value the Y that we are talking
about the dependent variable that we are
talking about what we are looking at is
whether the car is going to break down
or not yes or no that is what we are
talking about so here the outcome is
discrete and not a continuous value so
this is how the logistic regression
curve looks let me explain a little bit
what exactly and how exactly we are
going to uh determine the class outcome
rather so for a logistic regression
curve a threshold has to be set saying
that because this is a probability
calculation remember this is a
probability calculation and the
probability itself will not be zero or
one but based on the probability we need
to decide what the outcome should be so
there has to be a threshold like for
example 0.5 can be the threshold let's
say in this case so any value of the
probability below 05 is considered to be
zero and any value above 0.5 is
considered to be one so an output of
let's
say8 will mean that the car will break
down so that is considered as an output
of one and let's say an output of. 29 is
considered as zero which means that the
car will not break down so that's the
way logistic regression works now let's
do a quick comparison between logistic
regression and linear regression because
they both both have the term regression
in them so it can cause confusion so
let's try to remove that confusion so
what is linear regression linear
regression is a process is once again an
algorithm for supervised learning
however here you're going to find a
continuous value you're going to
determine a continuous value it could be
the price of a real estate property it
could be your hike how much hike you're
going to get or it could be a stock
price these are all continuous values
these are not screen compared to a yes
or a no kind of a response that we are
looking for in logistic regression so
this is one example of a linear
regression let's say at the HR team of a
company tries to find out what should be
the salary hike of an employee so they
collect all the details of their
existing employees their ratings and
their salary hikes what has been given
and that is the labeled information that
is available and the system learns from
this it is trained and it learns from
this labeled information so that when a
new employees information is fed based
on the rating it will determine what
should be the height so this is a linear
regression problem and a linear
regression example now salary is a
continuous value you can get 5,000
5,500 5,600 it is not discrete like a
cat or a dog or an apple or a banana
these are discrete or a yes or a no
these are discrete values right so this
where you are trying to find continuous
values is where we use linear regression
so let's say just to extend on this
scenario we now want to find out whether
this employee is going to get a
promotion or not so we want to find out
that is a discrete problem right a yes
or no kind of a problem in this case we
actually cannot use linear regression
even though we may have labeled data so
this is the labeled data So based on the
employee rating these are the ratings
and then some people got the promotion
and this is the ratings for which people
did not get promotion that is a no and
this is the rating for which people got
promotion we just plotted the data about
whether a person has got an employee has
got promotion or not yes no right so
there is nothing in between and what is
the employees rating okay and ratings
can be continuous that is not an issue
but the output is discrete in this case
whether employee got promotion yes yes
no okay so if we try to plot that and we
try to find a straight line this is how
it would look and as you can see doesn't
look very right because looks like there
will be lot of Errors th root mean
square error if you remember for linear
regression would be very very high and
also the the values cannot go beyond
zero or Beyond one so the graph should
probably look somewhat like this clipped
at zero and one but still the straight
line doesn't look right therefore
instead of using a linear equation we
need to come up with something different
and therefore the logistic regression
model looks somewhat like this so we
calculate the probability and if we plot
that probability not in the form of a
straight line but we need to use some
other equation we will see very soon
what that equation is then it is a
gradual process right so you see here
people with some of these ratings are
not getting any promotions and then
slowly uh at certain rating they get
promotion so that is a gradual process
and U this is how the math behind
logistic regression looks so we are
trying to find the odds for a particular
event happening and this is the formula
for finding the odd so the probability
of an event happening divided by the
probability of the event not happening
so P if it is the probability of the
event happening probability of the
person getting a promotion and divided
by the probability of the person not
getting a promotion that is 1 minus
P so this is how you measure the odds
now the values of the odds range from 0o
to Infinity so when this probability is
zero then the odds will the value of the
odds is equal to zero and when the
probability becomes one then the value
of the odds is 1 by 0 that will be
Infinity but the probability itself
remains between 0 and 1 now this is how
an equation of a straight line Looks So
Y is equal to Beta 0 + beta 1 x where
beta 0 is the Y intercept and beta 1 is
the slope of the line if we take the
odds equation and take a log of both
sides then this would look somewhat like
this and the term logistic is actually
derived from the fact that we are doing
this we take a log of PX by 1 - PX this
is an extension of the calculation of
odds that we have seen right and that is
equal to Beta 0 + beta 1 x which is the
equation of the straight line and now
from here if you want to find out the
value of PX you will see we can take the
exponential on both sides and then if we
solve that equation we will get the
equation of PX like this PX is equal to
1 by 1 + e^ of- beta 0 + beta 1 x and
recall this is nothing but the equation
of the line which is equal to y y is
equal to Beta 0 + beta 1 x so that this
is the equation also known as the
sigmoid function and this is the
equation of the logistic regression Al
all right and if this is plotted this is
how the sigmoid curve is obtained so
let's compare linear and logistic
regression how they are different from
each other let's go back so linear
regression is solved or used to solve
regression problems and and logistic
regression is used to solve
classification problems so both are
called regression but linear regression
is used for solving regression problems
where we predict continuous values
whereas logistic regression is used for
solving classification problems where we
have had to predict discrete values the
response variables in case of linear
regression are continuous in nature
whereas here they are categorical or
discrete in nature and U the linear
regression
helps to estimate the dependent variable
when there is a change in the
independent variable whereas here in
case of logistic regression it helps to
calculate the probability or the
possibility of a particular event
happening and linear regression as the
name suggests is a straight line that's
why it's called linear regression
whereas logistic regression is a sigmoid
function and the curve is the shape of
the curve is s it's an s-shaped curve
this is another example of application
of logistic regression in weather
prediction whether it's going to rain or
not rain now keep in mind both are used
in weather prediction if we want to find
the discrete values like whether it's
going to rain or not rain that is a
classification problem we use logistic
regression but if we want to determine
what is going to be the temperature
tomorrow then we use linear regression
so just keep in mind that in weather
prediction we actually use both but
these are some examples of logistic
regression so we want to find out
whether it's going to be rain or not
it's going to be sunny or not whe it's
going to snow or not these are all
logistic regression examples a few more
examples classification of objects this
is a again another example of logistic
regression now here of course one
distinction is that these are multiclass
classification so logistic regression is
not used in its original form but it is
used in a slightly different form so we
say whether it is a dog or not a dog I
hope you understand so instead of saying
is it a dog or a cat or a elephant we
convert this into saying so because we
need to keep it to Binary classification
so we say is it a dog or not a dog is it
a cat or not a cat so that's the way
logistic regression can be used for
classifying objects otherwise there are
other techniques which can be used for
performing multiclass classification in
healthcare logistic regression is used
to find the survival rate of of a
patient so they take multiple parameters
like trauma score and age and so on and
so forth and they try to predict the
rate of survival all right now finally
let's take an example and see how we can
apply logistic regression to predict the
number that is shown in the image so
this is actually a live demo I will take
you into Jupiter notebook and uh show
the code but before that let me take you
through a couple of slides to explain
what we're trying to do so let's say you
have an 8 by8 image and the the image
has a number 1 2 3 4 and you need to
train your model to predict what this
number is so how do we do this so the
first thing is obviously in any machine
learning process you train your model so
in this case we are using logistic
regression so and then we provide a
training set to train the model and then
we test how accurate our model is with
the test data which means that like any
machine learning process we split our
initial data into two parts training set
and test set with the training set we
train our model and then with the test
set we we test the model till we get
good accuracy and then we use it for for
inference right so that is typical
methodology of uh uh training testing
and then deploying of machine learning
models so let's uh take a look at the
code and uh see what we are doing so
I'll not go line by line but just take
you through some of the block blocks so
first thing we do is import all the
libraries and then we basically take a
look at the images and see what is the
total number of images we can display
using matplot lip some of the images or
a sample of these images and um then we
split the data into training and test as
I mentioned earlier and we can do some
exploratory analysis and uh then we
build our model we train our model with
the training set and then we test it
with our test set and find out how
accurate our model is using the
confusion Matrix the heat map and use
heat map for visualizing this and I will
show you in the code what exactly is the
confusion Matrix and how it can be used
for finding the accuracy in our example
we got we get an accuracy of about 0.94
which is pretty good or 94% which is
pretty good all right so what is the
confusion Matrix this is an example of a
confusion Matrix and this is used for
identifying the accuracy of a
classification model or like a logistic
regression model so the most important
part in a confusion Matrix is that first
of all this as you can see this is a
matrix and the size of the Matrix
depends on how many outputs uh we are
expecting right so the the most
important part here is that the model
will be most accurate when we have the
maximum numbers in its diagonal like
like in this case that's why it has
almost 93 94% because the diagonals
should have the maximum numbers and the
others other than diagonals the cells
other than the diagonals should have
very few numbers so here that's what is
happening so there is a two here there
are there's a one here but most of them
are along the diagonal this what does
this mean this means that the number
that has been fed is zero and the number
that has been Det detected is also zero
so the predicted value and the actual
value are the same so along the
diagonals that is true which means that
let's let's take this diagonal right if
if the maximum number is here that means
that like here in this case it is 34
which means that 34 of the images that
have been fed or rather actually there
are two misclassifications in there so
36 images have been fed which have
number four and out of which 34 have
been predicted correctly as number four
and one has been predicted as number
eight and another one has been predicted
as number nine so these are two
misclassifications okay so that is the
meaning of saying that the maximum
number should be in the diagonal so if
you have all of them so for an ideal
model which has let's say 100% accuracy
everything will be only in the diagonal
there will be no numbers other than zero
in all other cells so that is like a
100% accurate model Okay so that's uh
gist of how to use this Matrix uh how to
use this uh confusion Matrix I know the
name uh is a little funny sounding
confusion Matrix but actually it is not
very confusing it's very straightforward
so you are just plotting what has been
predicted and what is the labeled
information or what is the actual data
that's also known as the ground truth
sometimes okay these are some fancy
terms that are use so predicted label
and actual label that's all it is okay
yeah so we are showing a little bit more
information here so 38 have been
predicted and here you will see that all
of them have been predicted correctly
there have been 38 zeros and the the
predicted value and the actual value is
is exactly the same whereas in this case
right it has there are I think 37 + 5
yeah 42 have been fed the images 42
images are of Digit three and uh the
accuracy is only 37 of them have been
accurately predicted three of them have
been predicted as number seven and two
of them have been predicted as number
eight and so on and so forth okay all
right so with that let's go into Jupiter
notebook and see how the code looks so
this is the code in in Jupiter notebook
for logistic regression in this
particular demo what we are going to do
is train our model to recognize digits
which are the images which have digits
from let's say 0 to 5 or 0 to 9 and um
and then we will see how well it is
trained and whether it is able to
predict these numbers correctly or not
so let's get started so the first part
is as usual we are importing some
libraries that are required and uh then
the last line in this block is to load
the digits so let's go ahead and run
this code then here we will visualize
the shape of these uh digits so we can
see here if we take a look this is how
the shape is
1797 by 64 these are like 8 by8 images
so that's that's what is reflected in
this shape now from here onwards we are
basically once again importing some of
the libraries that are required like
numpy and map plot and we will take a
look at uh some of the sample images
that we have loaded so this one for
example creates a figure uh and and then
we go ahead and take a few sample images
to see how they look so let me run this
code and so that it becomes easy to
understand so these are about five
images sample images that we are looking
at 0 1 2 3 4 so this is how the images
this is how the data is okay and uh
based on this we will actually train our
logistic regression model and then we
will test it and see how well it is able
to recognize so the the way it works is
the pixel information so as you can see
here this is an 8 by 8 pixel kind of a
image and uh the each pixel whether it
is activated or not activated that is
the information available for each pixel
now based on the pattern of this
activation and non-activation of the
various pixels this will be identified
as a zero for example right similarly as
you can see so overall each of these
numbers actually has a different pattern
of the pixel activation and that's
pretty much that our model needs to
learn uh for which number what is the
pattern of the activation of the pixels
right so that is what we are going to
train our model okay so the first thing
we need to do is to split our data into
training and test data set right so
whenever we perform any training we
split the data into training and test so
that that the training data set is used
to train the system so we pass this
probably multiple times uh and then we
test it with the test data set and the
split is usually in the form of there
and there are various ways in which you
can split this data it is up to the
individual preferences in our case here
we are splitting in the form of 23 and
77 so when we say test size as
2023 that means 23% of the entire data
is used for testing and the remaining
77% is used for training so there is a
readily available function which is uh
called train test split so we don't have
to write any special code for the
splitting it will automatically split
the data based on the proportion that we
give here which is test size so we just
give the test size automatically
training size will be determined and uh
we pass the data that we want to split
and the the results will be stored in
xcore train and Yore train for the
training data set and what is xcore
train this are these are the features
right which is like the independent
variable and Yore train is the label
right so in this case what happens is we
have the input value which is or the
features value which is in xor train and
since this is labeled data for each of
them each of the observations we already
have the label information saying
whether this digit is a zero or a one or
a two so that this this is what will be
Ed for comparison to find out whether
the the system is able to recognize it
correctly or there is an error for each
observation it will compare with this
right so this is the label so the same
way xcore train Yore train is for the
training data set xcore test Yore test
is for the test data set okay so let me
go ahead and execute the disc code as
well and then we can go and check
quickly what is the how many entries are
there and in each of this so xor train
the shape is
1383 by 64 and ycore train has 1383
because there is uh nothing like the
second part is not required here and
then xcore test shape we see is 414 so
actually there are 414 observations in
test and 1383 observations in train so
that's basically what these four lines
of code are are saying okay then we
import the logistic regression library
and uh which is a part of psychic learn
so we we don't have to implement the
logistic regression process itself we
just call this the function and uh let
me go ahead and execute that so that uh
we have the logistic regression Library
imported now we create an instance of
logistic regression right so logistic RR
is a is an instance of logistic
regression and then we use that for
training our model so let me first
execute this code so these two lines so
the first line basically creates an
instance of logistic regression model
and then the second line where is where
we are passing our data the training
data set right this is our the the
predictors and uh this is our Target we
are passing this data set to train our
model all right so so once we do this in
this case the data is not large but by
and large uh the training is what takes
usually a lot of time so we spend in
machine learning activities in machine
learning projects we spend a lot of time
for the training part of it okay so here
the data set is relatively small so it
was pretty quick so all right so now our
model has been trained using the
training data set and uh we want to see
how accurate this is so what we'll do is
we will test it out in probably faces so
let me first try out how well this is
working for uh one image okay I will
just try it out with one image my the
first entry in my test data set and see
whether it is uh correctly predicting or
not so and in order to test it so for
training purpose we use the fit method
there is a method called fit which is
for training the model and once the
training is done if you want to test for
a particular value new input you use the
predict method okay so let's run the
predict method and we pass this
particular image and uh we see that the
shape is or the prediction is four so
let's try a few more let me see for the
next 10 uh seems to be fine so let me
just go ahead and test the entire data
set okay that's basically what we will
do so now we want to find out how
accurately this has U performed so we
use the score method to find what is the
percentage of accuracy and we see here
that it has performed up to 94% Accurate
okay so that's uh on this part now what
we can also do is we can um also see
this accuracy using what is known as
confusion Matrix so let us go ahead and
try that as well uh so that we can also
visualize how well uh this model has uh
done so let me execute ured this piece
of code which will basically import some
of the libraries that are required and
um we we basically create confusion
Matrix an instance of confusion matrix
by running confusion Matrix and passing
these uh values so we have so this
confusion underscore Matrix method takes
two parameters one is the Yore test and
the other is the prediction so what is
the Yore test these are the labeled
values which we already know for the
test data set and predictions are what
the system has predicted for the test
data set okay so this is known to us and
this is what the system has uh the model
has generated so we kind of create the
confusion Matrix and we will print it
and this is how the confusion Matrix
looks as the name suggests it is a
matrix and um the key point out here is
that the accuracy of the model is
determined by how many numbers are there
in the diagonal the more the numbers in
the diagonal the better the accuracy is
okay and first of all the total sum of
all the numbers in this whole Matrix is
equal to the number of observations in
the test data set that is the first
thing right so if you add up all these
numbers that will be equal to the number
of observations in the test data set and
then out of that the maximum number of
them should be in the D that means the
accuracy is predic good if the the
numbers in the diagonal are less and in
all other places there are a lot of
numbers which means the accuracy is very
low the diagonal indicates a correct
prediction that this means that the
actual value is same as the predicted
value here again actual value is same as
the predicted value and so on right so
the moment you see a number here that
means the actual value is something and
the predicted value is something else
right similarly here the actual value is
something and the predicted value is
something else so that is basically how
we read the confusion Matrix now how do
we find the accuracy you can actually
add up the total values in the diagonal
so it's like 38 + 44 Plus 43 and so on
and divide that by the total number of
test observations that will give you the
percentage accuracy using a confusion
Matrix now let us visualize this
confusion Matrix in a slightly more
sophisticated way uh using a heat map so
we will create a heat map with some
we'll add some colors as well it's uh
it's like a more visually visually more
appealing so that's the whole idea so if
we let me run this piece of code and
this is how the heat map looks uh and as
you can see here the diagonals again are
are all the values are here most of the
values so which means reasonably this
seems to be reasonably accurate and yeah
basically the accuracy score is 94% this
is calculated as I mentioned by adding
all these numbers divided by the total
test values so the total number of
observations in test data set okay so
this is the confusion Matrix for
logistic
regression all right so now that we have
seen the confusion Matrix let's take a
quick sample and see how well uh the
system has classified and we will take a
a few examples of the data so if we see
here we we picked up randomly a few of
them so this is uh number four which is
the actual value and also the predicted
value both are four this is an image of
zero so the predicted value is also zero
actual Valu is of course zero then this
is the image of nine so this is also
been predicted correctly 9 and actual
value is nine and this is a image of one
and again this has been predicted
correctly as like the actual value okay
so this was a quick demo of logistic
regression how to use logistic
regression to identify
images so we put them side to about side
we have our linear regression which is a
predictive number used to predict a
dependent output variable based on
Independent input variable accuracy is a
measured uh using least squares
estimated
so that's where you take uh you could
also use absolute value uh the least
squares is more popular there's reasons
for that mathematically and also for
computer
runtime uh but it does give you an an
accuracy based on the the least Square
estimation the best fit line is a
straight line and clearly that's not
always used in all the regression models
there's a lot of variations on that the
output is a predicted integer value
again this is what we're talking about
we're talking about linear regression
and we're talking about regression it
means the numers coming out linear
usually means we're looking for that
line versus a different model and it's
used in business domain forecasting
stocks uh it's used as a basis of almost
of most um uh predictions with numbers
so if you're looking at a lot of numbers
you're probably looking at uh a linear
regression
model uh for instance if you do just the
high lows of the stock exchange and
you're you're going to take a lot more
of that if you want to make money off
the stock you'll find that the linear
regression model fits uh probably better
than almost any of the other models even
you know highend neural networks and all
these other different machine learning
and AI models because they're numbers
they're just a straight set of numbers
you have a high value a low value volume
uh that kind of thing so when you're
looking at something that straight
numbers um and are connected in that way
usually you're talking about a linear
regression model and that's where you
want to start a logistic regression
model used to classify dependent output
variable based on Independent input
variable so just like the linear
regression model and like all of our
machine learning tools you have your
features coming in uh and so in this
case you might have uh label you know an
image or something like that is is
probably the very popular thing right
now labeling broccoli and vegetables or
whatever accuracy is measured using
maximum likelihood estimation the best
fit is given by a curve curve and we saw
that um we're talking about linear
regression you definitely are talking
about straight line although there is
other regression models that don't use
straight lines and usually when you're
looking at a logistic regression the
math as you saw was still kind of a
ukian line but it's now got that sigmoid
activation which turns it into um a
heavily weighted curve and the output is
a binary value between 0er and one and
it's used for classification image
processing as I mentioned is is what
people usually think of um although they
use it for classification of uh like a
window of things so you could take a
window of stock history and you could CL
generate classifications based on that
and separate the data that way if it's
going to be that this particular pattern
occurs it's going to be upward trending
or downward
trending in fact a number of stock uh uh
Traders use that not to tell them how
much to bid or what to bid uh but they
use it as to whether it's worth looking
at the stock or not whether the Stock's
going to go down or go up and it's just
a 01 do I care or do I even want to look
at it so let's do a demo so you can get
a picture of what this looks like in
Python code let's predict the price at
which insurance should be sold to a
particular customer based on their
medical history we will also classify on
a mushroom data set to find the
poisonous and nonpoisonous mushrooms
and when you look at these two datas the
first one uh we're looking at the price
so the price is a number um so let's
predict the price which the insurance
should be sold to and the second one is
we're looking at either it's poisonous
or it's not poisonous so first off
before we begin the demo I'm in the
Anaconda Navigator in this one I've
loaded the python
3.6 and using the Jupiter notebook and
you can use jupyter notebook by itself
um you can use the Jupiter lab which
allows multiple tabs it's basically the
notebook with tabs on it uh but the
jupyter notebook is just fine and it'll
go into uh Google Chrome which is what
I'm using for my Internet Explorer and
from here we open up new and you'll see
Python 3 and again this is loaded with
python
3.6 and we're doing the linear versus
logic uh regression or logit you'll see
L git T um is one of the one of the
names that kind of pops up when you do a
search on here uh but it is a logic
we're looking at the logistic regression
models and we'll start with the linear
regression uh because it's easy to
understand you draw a line through stuff
um and so in programming uh we got a lot
of stuff to unfold here in our in our uh
startup as we preload all of our
different
parts and let's go ahead and break this
up we have at the beginning import uh
pandas so this is our data frame uh it's
just a way of storing the data think of
a uh when you talk about a data frame
think of a spreadsheet you have rows and
columns it's a nice way of viewing the
data and then we have uh we're going to
be bringing in our pre-processing label
en Co coder I'll show you what that is
um when we get down to it it's easier to
see in the data uh but there's some data
in here like um sex it's male or female
so it's not like an actual number it's
either your one or the other that kind
of stuff ends up being encoded that's
what this label encoder is right here we
have our test split
model if you're going to build a model
uh you do not want to use all the data
you want to use some of the data and
then test it to see how good it is and
if it can't have seen the data you're
testing on until you're ready to test it
on there and see how good it is and then
we have our uh logistic regression model
our categorical one and then we have our
linear regression model these are the
two these right here let me just
um clear all that there we go uh these
two right here are what this is all
about logistic versus uh linear is it
categorical are we looking for a true
false or are we looking for um a
specific
number and then finally um usually at
the very end we have to take and just
ask how accurate is model did it work um
if you're trying to predict something in
this case we're going to be doing um uh
Insurance costs uh how close to the
insurance cost does it measure that we
expect it to be you know if you're an
insurance company you don't want to
promise to pay everybody's medical bill
and not be able
to and in the case of the mushrooms you
probably want to know just how much at
risk you are for following this model uh
as to far as whether you're going to get
a eat a poisonous mushroom and die or
not um so we'll look at both of those
and we'll get talk a little bit more
about the shortcomings and the um uh
value of these different processes so
let's go ahead and run this this is
loaded the data set on here and then
because we're in Jupiter notebook I
don't have to put the print on there we
just do data set and by and it prints
out all the different data on here and
you can see here for our insurance CU
that's what we're starting with uh we're
loading that with our panda
and it prints it in a nice format where
you can see the age sex uh body mass
index number of children smoker so this
might be something that the insurance
company gets from the doctor it says hey
we're going to this is what we need to
know to give you a quote for what we're
going to charge you for your
insurance and you can see that it has uh
1,338 rows and seven columns you can
count the columns 1 two 3 four five six
seven so there's seven columns on here
and the column we're really interested
in is charges um I want to know what the
charges are going to be what can I
expect not a very good Arrow drawn um
what to expect them to charge on there
uh so is this going to be you know is
this person going to cost me uh
$16,814.23
so that we can guess what the minimal
charge is for their
insurance and then there's one other
thing you really need to notice on this
data um and I mentioned it before but
I'm going to mention it again because
pre-processing data is so much of the
work in data science um sex well how do
you how do you deal with female versus
male um are you a smoker yes or no what
does that mean region how do you look at
Region it's not a number how do you draw
a line between Southwest and
Northwest um you know they they're
objects it's either your Southwest or
you're Northwest it's not like I'm
southwest I guess you could do longitude
and latitude but the data doesn't come
in like that it comes in as true false
or whatever you know it's either your
Southwest or your
Northwest so we need to do a little bit
of pre-processing of the data on here to
make this
work oops there we go Okay so let's take
a look and see what we're doing with
pre-processing and again this is really
where you spend a lot of time with data
Sciences trying to understand how and
why you need to do that and so we're
going to do uh you'll see right up
here label uh and then we're going to do
the do a label encoder one of the
modules we brought in so this is SK
learns uh label
encoder I like the fact that it's all
pretty much automated uh but if you're
doing a lot of work with the label
encoder you you should start to
understand how that
fits um and then we have uh label. fit
right here where we're going to go ahead
and do the data set uh. sex. drop
duplicates and then for data set sex
we're going to do the label transform
the data s sex and so we're looking
right here at um male or female and so
it usually just converts it to a zero1
because there's only two choices on here
same thing with the smoker it's 0 one so
we're going to transfer the trans change
the smoker uh 01 on this and then
finally we did region down here region
does it a little bit different we'll
take a look at that and um it's I think
in this case it's probably going to do
it because we did it on this label
transform um with this particular setup
it gives each region a number like 0 1
two three so let's go AE and take a look
and see what that looks like go and run
this and you can see that our new data
set um has age that's still a number uh
Sex Is Zero or one uh so zero is female
one is male number of children we left
that alone uh smoker one or zero it says
no or yes on there we actually just do
one for no zero or no yeah one for no
I'm not sure how it organized them but
it turns the smoker into zero or one yes
or no uh and then region it did this at
as uh 0 1 2 three so there three
regions now a lot of times in in when
you're working with data science and
you're dealing with uh regions or even
word
analysis um instead of doing one column
and labeling it 0 one two three a lot of
times you increase your features and so
you would have region Northwest would be
one column yes or no region Southwest
would be one column yes or no true 01 uh
but for this this this particular setup
this will work just fine on here now
that we spend all that time getting it
set up uh here's the fun part uh here's
the part where we're actually using our
setup on this and you'll see right here
we have our um y linear regression uh
data set drop the charges because that's
what we want to
predict and so our X I'm sorry our X
linear data set dro the charges because
that's where we're going to predict
we're predicting charges right here so
we don't want that as our input for our
features and our y output is charges
that's what we want to guess we want to
guess what the charges
are and then what we talked about
earlier is we don't want to do all the
data at once so we're going to take
um3 means 30% we're going to take 30% of
our data and it's going to be as the
train as the testing site so here's our
y test and our X test down there um and
so that part our model will never see it
until we're ready to test to see how
good it is and then of course right here
you'll see our training set and this is
what we're going to train it we're going
to trade it on 70% of the data and then
finally the big ones uh this is where
all the magic happens this is where
we're going to create our magic setup
and that is right here our linear model
we're going to set it equal to the
linear regression model and then we're
going to fit the data on here
and then at this point I always like to
pull up um if you if you if you're
working with a new model it's good to
see where it comes from and this comes
from the S kit uh learn and this is the
sklearn linear model linear regression
that we imported earlier and you can see
they have different parameters the basic
parameter works great if you're dealing
with just numbers uh mentioned that
earlier with stock high lows this model
will do as good as any other model out
there for doing if you're doing just the
very basic high lows and looking for a
linear fit a regression model fit um and
what you one of the things when I am
looking at this is I look for
methods and you'll see here's our fit
that we're using right now and here's
our
predict and we'll actually do a little
bit in the middle here as far as looking
at some of the parameters hidden behind
it the math that we talked about
earlier and so we go in this and we go
ahead and run this
you'll see it loads the linear
regression model and just has a nice
output that says hey I loaded the linear
regression model and then the second
part is we did the fit and so this model
is now trained our linear model is now
trained on the training
data and so one of the things we can
look at is the um um for idx and colon
name and enumerate X linear train
columns come of an interesting thing
this prints out the coefficients uh so
when you're looking at the back end of
the data you remember we had that
formula BX X1 plus bxx2 plus the plus
the uh intercept uh and so forth these
are the actual coefficients that are in
here this is what it's actually
multiplying these numbers
by and you can see like region gets a
minus value so when it adds it up I
guess a region you can read a lot into
these numbers uh it gets very
complicated there's ways to mess with
them if you're doing a basic linear
regression model you usually don't look
at them too closely uh but you might
start looking in these and saying hey
you know what uh smoker look how smoker
impacts the cost um it's just massive uh
so this is a flag that hey the value of
the smoker really affects this model and
then you can see here where the body
mass index uh so somebody who is
overweight is probably less healthy and
more likely to have cost money and then
of course age is a factor um and then
you can see down here we have uh sex is
than a factor also and it just it
changes as you go in there negative
number it probably has its own meaning
on there again it gets really
complicated when you dig into the um
workings in how the linear model works
on that and so um we can also look at
the intercept this is just kind of fun
um so it starts at this negative number
and then adds all these numbers to it
that's all that means that's our
intercept on there and that fits the
data we have on that and so you can see
right here we can go back and oops give
me just a second there we go we can go
ahead and predict the unknown data and
we can print that out and if you're
going to create a model to predict
something uh we'll go ahead and predict
it here's our y prediction value linear
model
predict and then we'll go ahead and
create a new data frame in this case
from our X linear test group
we'll go ahead and put the cost back
into this data frame and then the
predicted cost we're going to make that
equal to our y
prediction and so when we pull this up
uh you can see here that we have uh the
actual cost and what we predicted the
cost is going to
be there's a lot of ways to measure the
accuracy on there uh but we're going to
go ahead and jump into our mushroom
data and so in this you can see here we
we've run our basic model we built our
coefficients you can see the intercept
the back end you can see how we're
generating a number here uh now with
mushrooms we want to yes or no we want
to know whether we can eat them or not
and so here's our mushroom file we're
going to go and run this take a look at
the data and again you can ask for a
copy of this file uh send a note over to
Simply
learn.com and you can see here that we
have a class um the cap shape cap
surface and so forth so there's a lot of
feature in fact there's 23 different
columns in here going
across and when you look at this um I'm
not even sure what these particular like
PE PE I don't even know what the class
is on
this I'm going to guess by the notes
that the class is uh poisonous or
edible so if you remember before we had
to do a little precoding on our data uh
same thing with here uh we have our cap
shape which is b or X or k um we have
cap color uh these really aren't numbers
so it's really hard to do anything with
just a a single number so we need to go
ahead and turn those into a label
encoder which again there's a lot of
different encoders uh with this
particular label encoder it's just
switching it to 0 1 2 3 and giving it an
integer
value in fact if you look at all the
columns all of our column are labels and
so we're just going to go ahead and loop
through all the columns and the data and
we're going to transform it into a um
label encoder and so when we run this
you can see how this gets shifted from
uh xbxx K to 01 2 3 4 5 or whatever it
is class is 01 one being poisonous zero
looks like it's
editable and so forth on here so we're
just encoding it if you were doing this
project depending on the results you
might encode it differently like I
mentioned earlier you might actually
increase the number of features as
opposed to laboring 0 1 2 3 4 five um in
this particular example it's not going
to make that big of a difference how we
encode
it and then of course we're looking for
uh the class whether it's poisonous or
edible so we're going to drop the class
in our x uh Logistics model and we're
going to create our y Logistics model is
based on that class so here's our
XY and just like we did before we're
going to go ahead and split it uh using
30% for
test 70% to program the model on
here and that's right here whoops there
we go there's our uh uh train and
test and then you'll see here on this
next setup
um this is where we create our model all
the magic happens right here uh we go
ahead and create a logistics model I've
up the max
iterations if you don't change this for
this particular problem you'll get a
warning that says this has not
converged um because that that's what it
does is it goes through the math and it
goes hey can we minimize the error and
it keeps finding a lower and lower error
and it still is changing that number so
that means it hasn't conversed yet it
hasn't find the lowest amount of error
it can and the default is 100 uh there's
a lot of settings in here so when we go
in here to let me pull that up from the
sklearn uh so we pull that up from the
sklearn
model you can see here we have our
logistic it has our different settings
on here that you can mess with most of
these work pretty solid on this
particular setup so you don't usually
mess a lot usually I find myself
adjusting the um iteration and it'll get
that warning and then increase the
iteration on there and just like the
other model you can go just like you did
with the other model we can scroll down
here and look for our
methods and you can see there's a lot of
methods uh available on here and
certainly there's a lot of different
things you can do with it uh but the
most basic thing we do is we fit our
model make sure it's set right uh and
then we actually predict something with
it so those are the two main things
we're going to be looking at on this
model is fitting and predicting there's
a lot of cool things you can do that are
more advanced uh but for the most part
these are the two which um I use when
I'm going into one of these models and
setting them
up so let's go ahead and close out of
our sklearn setup on there and we'll go
ahead and run this and you can see here
it's now loaded this up there we now
have a uh uh logistic model and we've
gone ahead and done a predict here also
just like I was showing you earlier uh
so here is where we're actually
predicting the data so we we've done our
first two lines of codee as we create
the model we fit the model to our
training data and then we go ahead and
predict for our test data now in the
previous model we didn't dive into the
test score um I think I just showed you
a graph and we can go in there and
there's a lot of tools to do this we're
going to look at the uh model score on
this one and let me just go ahead and
run the model
score and it says that it's pretty
accurate we're getting a roughly 95%
accuracy well that's good one 95%
accuracy 95% accuracy might be good for
a lot of
things but when you look at something as
far as whether you're going to pick a
mushroom on the side of the trail and
eat it we might want to look at the
confusion Matrix and for that we're
going to put in our y listic test the
actual values of edible and unedible and
we're going to put in our prediction
value and if you remember on here uh
let's see I believe it's poisonous was
one uh zero is edible so let's go ahead
and run that 01 zero is good so here is
um a confusion Matrix and this is if
you're not familiar with these we have
true true true
false true false false false so it says
out of the edible mushrooms we correctly
labeled 1 121 mushrooms edible that were
edible and we correctly measured
1,113 poisonous mushrooms as
poisonous but here's the
kicker I labeled uh 56 edible mushrooms
as being um poisonous well that's not
too big of a deal we just don't eat them
but I measured 68 mushrooms as being
edible that were poisonous so probably
not the best choice to use this model to
predict whether you're going to eat a
mushroom or not and you'd want to dig a
Little Deeper before you U start e
picking mushrooms off the side of the
trail so a little warning there when
you're looking at any of these data
models looking at the error and how that
error fits in with what domain you're in
domain in this case being edible
mushrooms uh be a little careful make
sure that you're looking at them
correctly so we've looked at uh edible
or not edible we've looked at uh
regression model as far as uh the end
values what's going to be the cost and
what our predicted cost is so we can
start figuring out how much to charge
these people for their
insurance and so these really are the
fundamentals of data science when you
pull them together uh when I say data
science I'm talking about your machine
learning
code yeah step by step we will go
through all of this and uh we'll make
sure that we learn everything and we
bring everything together towards the
end right without further Ado let me
just straight up way deep dive to
business right to learn data
science
right and with this data science there
is also something which is prefixed
which is applied data
science and suffix for this is with
python right apply data science with
python right right so there are there
are two key Concepts which are going to
be a part of this course the first one
is the knowledge about data science that
what data science is and then because we
are doing an applied course right we are
doing an applied course I will try to
tie up these Concepts which we will
understand in data science with a tool
right which is python for you right we
already know about uh 60 65% of python
right which is the fundamental Python
and now we will be mov moving to the
next step to Advanced
Pyon right and using python right
leveraging python we will be solving a
lot of problems of data science right
using
this okay so the first few sessions
right the first few sessions will be
about making you a breast with python
what python is right what and what
packages we have how do they work in
reality right and all those things and
then we will be coupling it up with data
science Concepts and then finally
towards the end of the session in the
last few classes we will be doing uh we
will be taking a real data set and on
that data set we will be applying all
these Concepts right to understand the
data better and we will be drawing
inferences from that to convert that
into information to take actionable
insights or using that actionable
insights taking a better decision right
we we do all of that in in the actual
way okay so now guys if you understand
this then the next point of contention
is data
sense right one of the most famous
keywords on the planet right now right
one of the most famous keywords on the
planet right now do you think that these
two things okay let me put it
differently what do you think that can
be the possible
explanation about this term data science
you know data you know science what do
you think is going to follow in these
sessions what is data science to you as
per these two words okay so this is
people made up of two words right data
and science right so what we are trying
to do is we
are trying to understand data
right we are trying to understand data
right and then do something to it right
understanding its science understanding
the uh nature the behavior of this data
and converting it in something called as
information right do do we know
difference between data and information
data is something which is completely
raw okay it is completely raw it it has
no
meaning right it has no meaning isn't it
for
example I give you the stats of some
player like suppose say donon I give you
stats of Mahendra Singh dhoni right that
what what what were his scores uh what
is his name what is his age and you know
all those things now everything is there
right but we don't know what to do about
it right do you think the score of dhony
has has any context people it has any
context no right but when I deep down
but I when I go and deep dive about it
right what is the first thing you find
out of
scores what is the first thing you find
out of SC scores you try to find the
average of score isn't it that in last
10
Innings right in last 10 Innings before
this also you need something which is
called as a problem statement isn't it
now for example the problem statement is
selectors want to understand selectors
wants to understand that whether
Mahendra Singh dhoni should be picked up
so there's a problem now right selectors
want to see that whether dhon is fit for
the next tournament or not so what we
will do we will now try to take the mean
of the scores right for last 10 Innings
and if the score is suppose X we will
try to compare this with Y what is y y
is a
reference right Y is a reference that we
want to compare it against now when you
are doing this comparisons when you are
applying these techniques to this score
this is now slowly becoming information
right and at the end of the day once you
have the strike rate once you have the
mean score of duni once you have his age
once you have his Fitness score four all
those things will now help you to take
this particular decision because now
what you have is called as information
right because this has
context right this has
meaning and this is usually
processed right this is usually
processed right this is usually
processed now what did we do now what
did we do here if you will go and read
about data science data science says
data science
says it is the art of
collecting Right
cleaning
analyzing
modeling
improving right and visualizing
right
visualizing the data right if a person
is adept in doing all these things this
person people is cumulatively called a
data scientist
right that person is called a data
scientist right so before going to the
definition of data scientist now I will
give you some more examples right I'll
give you some more examples data science
people as I said is a combination of
these things right you have to collect
the
data right you have to collect the
data right and this has a lot of things
data can be connected from two types in
two types one is
primary and the second one is
secondary right what is the primary way
of collecting data from your iot devices
right from
sensors from your inbuilt machine
me right then from your
surveys which you float right
questionnaires right all these things
are primary ways what is the secondary
way of collecting
data purchasing
data right using internet
data because you have not generated it
you are just using someone El's data
right something like uh trans transer
learning what is transfer
learning transfer learning is a
technique where suppose I am Bank a and
you are Bank B right so Bank a has
created some model right trained on
their data now you are going to use the
exact same model right you're going to
use the exact same model right maybe
you're not seeing the data but you are
just using the property of data like
mean median mode and a lot of modeling
things which will come we will learn
about them and you use this model on
your particular data right so in a way
you did not have enough data to create
the model yourself but you are now using
someone else's model to run your data on
it right so this is called as transfer
learning so this kind of collection is
basically secondary data collection so
you can collect the data right then you
can perform data analysis
right you can perform data analysis
right how will you perform this data
analysis using
complex
algorithms right using complex
algorithms right some
statistics right you can use artificial
intelligence right artificial
intelligence you you can use machine
learning
right right you can use all these things
for data analysis then you can
transform
transform the
patterns into
predictions right you can transfer the
these patterns into predictions right
which can be used for
business decision
making right for business decision
making then you can validate the
results right and present the
results right so this is like a complete
life cycle of a data scientist right so
before going further let me give you
what combinations do you need to have to
become a data scientist the first one
is domain
knowledge right so what is domain
knowledge first of all I told you right
there will be a problem right you'll be
solving a problem in any project of data
science you'll be trying to solve a
problem right and the problem will be
belonging to a particular domain even if
you working for yourself right even if
you're an entrepreneur then also you'll
be solving a problem so this domain
knowledge part includes things like
understanding it's a very important
diagram understanding client
requirement right understanding the
client requirement
right important
Criterion right
important criteria
knowledge right for example to give an
example suppose we have created a
machine learning model OKAY
understanding the data we have created a
machine learning model whose accuracy is
90% right is 90% a good
accuracy yeah fairly decent accuracy yes
suppose you have to predict sales right
you're selling something suppose you are
selling clothes and you want to predict
what will be the sales for the next week
when you use this model whatever the
output model gives you what is going to
be the accuracy of your output using
this
model how much
accuracy
90% but my question is model is 90% but
my question to you is that if 90%
accuracy is on sales data a person like
me will be very very happy okay very
very happy I'll be probably dancing
right but if you try to apply the same
model right for a medical diagnosis case
will you be interested in getting
operated in such a hospital or an
institution where the accuracy is coming
as 90% domain knowledge right domain
knowledge we need to understand what are
the exact requirements we need to
understand what are the exact
expectations right and we need to know
how much do we need to Pivot right so
the first thing in data science is is
these accuracies and everything are
subjective right they are subjective so
for that you need domain knowledge
domain knowledge part very important
guys very important these three things
which I'm going to tell second part is
people the game changer right the second
part
is computer
science now if I take you back in
history okay if I take you back in
history in 1980s or somewhere then do
you okay how many of you think that data
science is a New
Concept how many of you think that data
science is a New
Concept I hope you all know it's not a
New Concept everyone knows that yep it
has been happening for ages just like
you guys will be shocked if you already
don't know AI was coined in the year
1956 1956 at the University of dmouth AI
was coined by Paul McCarthy right and we
saw the boom of AI in the year
2010 right such a long journey same case
with data science because people back in
the day data science was called as data
mining everyone heard about it data
mining knowledge databases yeah we need
we used to mine the data now what were
the problems what were the hiccups of
data mining the hiccups for data mining
was that we were doing everything
everything
manually right now if I give you 100
points can you calculate the
mean or let's say if I give you two
points to multiply 2 multiplied by three
how much time will you
take 2
seconds yep how much time a computer
will
take 2 seconds if I give you to multiply
2 2 48 8 9 multiplied by 200 how much
time will you take to calculate this say
5 Seconds how much time computer will
take 2 seconds now if I give you to
multiply 2 489 into 5 1 95 4 3 8 6 how
much time will you take to calculate
this manually maybe say 1
minute 80 seconds 1 minute how much time
a computer will take still 2 seconds
right still 2 seconds right and if I
give you to calculate this over 200
times you will take 200 minutes right
using parallel Computing computer will
still take about 3 to 5 Seconds right so
are you understanding the power of
computer do you understand this concept
in this relationship what was happening
back then what computer science did
people was it revolutionized the way
data mining was happening and that thing
now is called as that again that thing
now is called as data science in which
computer science is one of the most
important contributors so this is just
one reason now manually right manually
if I give you say 1 million rows of data
right 1 million rows of data right so
how many pages will you pages will you
need to store this data suppose your
notebook is like
this right these boxes and here you are
storing the data 1 million so maybe you
can buy n number of notebooks but now do
you think it's as easy in storing
something in computer because back in
the day people we had memory issues
isn't it memory
constraints so there is something called
as M's law right which says as
the advancement in microprocessors will
increase the price of microprocessor
will decrease right so this is what is
happening right now back in 1980s if I
show you right guys right yeah 2.5 kg
exactly right it was size of a fridge
hard disk but now it fits in your palm
right so this was enabled the storage
techniques right the processing
techniques the infrastructure right
things like big data what kind of data
do you think we will be dealing with
people in data science you all know the
term very famous term the kind of data
big data right everyone knows about Big
Data what is Big Data yes a data which
is fast right it has velocity
veracity variety right so this kind of
data needs to be stored this kind of
data needs to be processed so which
thing brought all these things into data
science it was given to us by computer
science right so computer science people
included things like database management
right data validation right data
infrastructure right data
infrastructure right then we had
languages computer
languages which is python right now for
us right again do you think when you do
this thing manually right suppose you do
this thing manually how easy do you
think it will become using something
like python or any other computer
language to create complex models how
easy it will be to do that to create the
complexity in models right where you can
capture the nonlinear nature isn't it
people isn't
it for example for example let me tell
you this 2 4 6 8 10 Dash what do you
think is the next number guys 12 if I
tell you to Define this to
me in okay let leave that what do you
think is going to be the next number
here what is the next number
11 next
number
25 perfect now guys if I ask you to
write these numbers right the way you
predicted them can you give me a
function f ofx is equal to what is the F
ofx
here it's 2x right it's 2x F ofx equal
to 2x if I tell you to create a function
it will be F of xal 2x what will be the
function here
guys F
ofx will be equal
to x + 1 yeah x + one y and here F ofx
will be equal
to x yeah now the last example right
last
example what is the next number here I
don't want the number I want the
function I want this so that I can
generalize isn't it how did you reach
this
figure how many of you think it's not
possible to determine this how
many of
you
think it is
not
possible to determine
this yeah how many of you think what if
I just change this question and ask you
how many of you think it is not possible
to determine this
manually same response but now I say how
many of you think it is not possible to
determine
this with
computers will your answer still remain
no do you think I cannot approximate
this function using
computers we have something called as
deep neural
networks and they are called as
universal function approximation
right so this is the problem people this
is the problem right I will show this to
you when the time comes right I will
remember this example and I will show
this to you but now what I'm trying to
tell you is the things which seemed
impossible manually was solved by what
it was solved by computers the
distribution of this is like this can
you figure it out yourself no
right we cannot isn't it we cannot do
that so this kind kind of approximation
will be given by what it will be only
given by machines right and this is
people what data science is all about
right it is what computer science did
inside data science right I hope this is
clear I'm assuming a lot of you will be
going for interviews and everything
after these course right so this will be
a very very important thing for you to
know right often it is asked why data
science is having computer science in it
right the reason is this okay so this is
the role of computer science inside data
science now people the third thing the
third circle which is one of the
most Parts is
maths and staffs right mathematics
statistics which was
optimization right optimization of your
models
right design
of
model right now guys if you look
carefully if you look carefully in order
to approximate this what you what will
you be playing with you will be playing
with a lot of data you will be playing
with a lot of mathematical tool
mathematical Concepts and statistical
Concepts isn't it how did you what do
you call this this is maths right this
is statistics and Mathematics finding
mean median mode standard deviations
probability statistics all these will
lead to this kind of result isn't it so
this becomes the Third Wheel of this
particular uh of of this particular
diagram and this point of
intersection right the site point of
intersection is basically data
science Y is particularly data science
right so now people this point okay this
point is basically representing data
engineering right data engineering right
data engineering is people the part of
data science which enables us to capture
the correct data right how the data will
flow how the data will be stored how the
data will be clean right all this is
done by whom it is done by data
engineering
just because so am I right guys you will
go for interviews right after this and
try to fetch yourself jobs in this
domain data science AI ml if my
understanding is correct is that the a
yes so now guys there will be three
types of
companies or let's say to simplify let's
say two types one is
small and the other one is Big right so
in a small organization if you become a
part of small organization and you are
the data scentist there you you can be
involved in all of these things right
all of these things possible Right your
bosses and your management will expect
you to construct all these flows right
no computer science you should know
maths and stats and you should have the
domain knowledge and you will be asked
to do all of this but if you are going
to become a part of a big organization
usually all these roles are fragmented
all of these roles are fragmented right
there's a separate data infrastructure
team there's a separate data governance
team now guys when you go on to collect
the data can you collect any sensitive
data about is it possible ethically it's
not right and legally also it's not
right my question to you is who will
look after this compliance whose
responsibility indeed it is to look
after this compliance data scientist so
this is about fragmentation if you are
part of a big organization this thing
will be taken by someone else right but
if you are a part of a small
organization you will be know you will
be expected to do this all by yourself
in if if you are part of a big
organization then do you think you need
to have this domain
knowledge the answer is no why because
there will be separate set of people who
are called as what who are called as
business analyst have you heard about
this position people business
analyst what is a business analyst role
it is basically a technical translator
right who knows technical who knows
domain and that person goes and talks to
the client talks to the client in a
Layman language convert it into
technical requirement coupled with the
domain knowledge and give you the
document this is basically a medical
engineering problem so we need this this
this this this and you need to fulfill
this this this this criteria but again
if you're part of a small organization
who needs to take care of that who needs
to make sure that you know everything
about a domain you yourself right you
yourself right then people this area
usually represents whom this area
represents
research and
Analysis
why because they we have people who have
domain knowledge and we have people who
are knowledge of math stats have you
heard about a position called as atery
in the world aeral science ateres are
people who are basically dealing with uh
domains which are very very heavily data
intrinsic right for example Finance
domain right Finance is all about
numbers so there we go atal science and
we have math stats optimization model
development all of those things
happening there we are also at at this
point of time people belonging to with
Section research and data analysis right
and then people there's the third
intersection right there's a third the
third intersection which is this part
and this people is called as machine
learning right machine learning why
machine learning if you can combine the
power of maths stats right and you can
combine the power of computer science
you will find yourself to be in a
position where you can call yourself a
machine learning engineer right how does
a machine learning engineer becomes a
data scientist when they couple it up
with the domain expertise right so in
this course people in this course we
will teach you computer science we will
teach you a little bit about math stats
but what we cannot teach you is domain
knowledge yeah and making the base of
what we are about to do very very
important to understand right if you
understand this then half the battle is
won right so now to answer the question
which was posted earlier uh answer to
the question which was posted earlier
there are different things in the world
of data science right so you can pick
and choose anything or you can do
everything by yourself if you guys are
engineers then I think you can be at
this sweet spot going forward in life if
you choose a domain for yourself for
example you choose to be a part of
automobile industry you choose to be a
part of medical industry you choose to
be a part of say retail industry you
choose to be a part of Aeronautics
industry right you choose to be a part
of Finance industry right so whatever
you will choose this thing will get
developed over time right this is the
most difficult out of these three try to
give you the example right my domain was
agricultural industry right the agre
products I have worked extensively in
agricultural industry right so again for
now in my current role this is something
which I don't have I have this expertise
I have this expertise so same will be
with you and you guys will develop this
knowledge over the time now I will try
to give you an example right elections
to make you understand how data science
can be used in one particular use case
okay maybe we can extend that to a lot
of other use cases and examples right
talking about election season right
talking about the election season right
we will we will try to understand how do
we use data science because it is very
extensively used in this data science
right now let me talk about the first
phase let's say this is pre election
phase
right right this is pre-election phase
in this pre-election phase what do you
think will be the tasks with which an
agency like Election Commission of India
will be doing the first task can be that
they will
be doing the
voter
registration right voter registration
and data management isn't it
yeah it will start with
that and what will be the things inside
this the first thing will be data
collection right first thing will be
data collection so you will collect the
data from all the registered voters
right maybe it can be their demographic
information where they live what is
their age what is their gender right
what is their past polling Behavior have
they turned out previously or not right
all these details we can collect then
can we also do data
cleaning because I've have told you
right that there can be a lot of
redundancies some person's name can
appear twice right some people can be
mismatched suppose for example we have
learned this in Python
ragav
ragav
ragav ragav
right right all these are what
people this is belonging to the same
name right this is me but for a computer
for a computer how many rages are there
different ones all are different right
so example like these right some people
might have died they might not be
existing anymore right so all this part
will be taken care where people in the
data cleaning process right removing the
duplicates updating the new addresses
right correcting the information about
every voter all those things right and
now lastly we can also include a flavor
of data
analytics right what will data analytics
include in this we can analyze the
demographic data to identify eligible
voters isn't it yeah people till now we
haven't understood this why it is not
automated how will you pick up all the
how how will you pick up all the details
and nuances suppose you're filling a
form right by mistake you have suppose
you are 25 years of age suppose you have
written 250 so does that mean I remove
this I remove this entry of yours
because by mistake you have written your
age as
250 is age 250 possible in our current
world never right so I will have to tell
the machine that because this is a
mistake please convert it to 25 isn't it
this is called as imputation so this is
mostly a manual task not manual but you
have to understand the problem manually
and then code it on computer suppose
someone has written their
state
as e d LH I right and
Country as i d i n right so what is this
state refering what is this country
refering to we humans are very
smart India right we can say this is
India and if this is India then what is
this pointing out
to Delhi just because you said and it's
a it's a leading question I wanted to
explain this tell me is it possible to
do this automatically no right we have
to employ manual rules right we have to
tell because we who's more intelligent
humans or machines humans right machines
are just more optimized right so we know
through human intelligence that this is
pointing to Delhi and this is not edhi
so this is data cleaning right lastly we
have data analytics so do you think
people based on these data points we can
understand that who are the eligible
voters and maybe who are not registered
yet maybe who have not voted in the past
can we do all those
analytics and reach out to those peoples
and persons that's the first part this
just the first part pre-election phase
now moving on to the second part right
moving on to the second part let's say
uh we say
public
opinion regarding the polling right the
polling which is about to happen the
first thing will be you want to collect
information about people right so can
can you go out and and reach all 1.8
billion people in this country that what
is their likable vote for which party is
it
possible 1.8 billion do you think it's
possible for 1
billion do you think it's possible for
500 million do you think it's possible
for 100 million no right so basically
I'm talking about what I have something
which is called as a
population right and if I have to study
about this population which is 1.8
billion people which is impossible which
you just said what do I need to do
should I stop my process no right I will
go and collect something which is called
a
sample right we always work in samples
right suppose someone says that a
Coca-Cola bottle does not contain 500 mL
of liquid which it claims right suppose
someone has put this allegation possible
that Coca-Cola bottles do not have 500
ml liquid which they promise now there
are two ways to deal with this right
there are two ways to deal with this
either I go and collect all the bottles
of Coca-Cola in the world
possible never right so what will I do I
will go and pick up handful of bottles
right handful of bottles so what is that
handful of bottles those are called as
samples one last thing suppose someone
says that because of an
industry all the fishes of the lake are
dying or they are infected is it
possible to go and collect and check all
the fishes in the pond or lake no right
what will we do we will collect again
handful of fishes and we will test them
right again samples now how does the raw
data
collected raw data has in
I hope you understand this this is no
more about
population with this step being told now
you're dealing with samples so now do
you want to ask me how is sample created
yeah now I'm coming to that now guys
there are a lot of second point is how
to sample right how to
sample right so we have sampling
techniques people one is called as
probabilistic and one is called as
nonprobabilistic right I will not go in
detail right now I just want to tell you
an overview probabilistic is suppose uh
you are manufacturing t-shirts right
you're manufacturing t-shirts right and
suppose you
created 100
lots
of thousand t-shirts right so this is
box one box two box three box four up to
up to 100 right 100 boxes and in each
box how many t-shirts are there th000
right now suppose you are a Quality
Inspector you're a Quality Inspector is
it possible you for you to go over all
the 100 Lots with all checking all the
th t-shirts one by one no right what
will you do you will sample again you
will sample now the most common way of
sampling these kind of problems is
probabilistic sampling what is
probability what is the probability of
getting heads or tails when you spin the
when you flip the coin equally like
1X two and 1X two what is the
probability of getting 1 2 3 4 5 6 on a
roll of a dice 1X six now what is the
probability of picking any t-shirt from
this first slot out of thousand
t-shirts 1 by
th000 yes so do you think all the
t-shirts have equally probab equal
probability of being picked up without
any bias if you decide to draw five
t-shirts right from each of this box
right and suppose say two are defective
and three are not defective what will
you do will you accept the lot or reject
the lot we have majority of t-shirts of
non- defective or let's say we will
reject the lot we will reject the lot we
will reject the lot let's say we will
reject the lot okay though this is
basically subjective as per the company
policies but let's say we rejected now
people my question to you is what if
this entire batch had only two defected
t-shirts but now what will happen the
entire batch will be
rejected yes let's say you sampled one
t-shirt let's let's change the use case
I say you sampled only one t-shirt and
that t-shirt was defected now you will
reject the batch and that is equally
likely case so this is called as
probabilistic sampling people and there
is no way you can go back there is no
way you cannot say that Hey sir please
uh allow this batch to pass because
there is a chance that rest of the
t-shirts are not effective no it is not
the way that happens it happens randomly
so right this is called as random
sampling right random sampling now
suppose you are doing a cancer research
right you're doing a cancer research
right so for your cancer research people
what kind of people will you need people
who had had cancer in the past isn't it
to know more about their problem to know
more about their medical condition so is
it possible people that in this use case
you can go and pick up any person from
the population and ask them questions no
right that is not possible so now is the
probability equally likely or it has
changed when you pick the sample it has
changed now there is a bias which is
introduced
that you only want people who had cancer
right so that kind of sampling people is
called as nonprobabilistic
sampl right nonprobabilistic sampl clear
now sampling technique right now third
thing in this same scheme can be people
what it can be the data collection right
data collection mode right that how do
you collect the data you can float a
survey on say
internet you can go and stand outside a
mall or
office isn't it how will you how will
you you can probably interview
someone right interview someone right
you can have a group
discussion yeah all these
techniques yes no maybe right in the
same part public opinion polling right
now guys uh so this was a brief
introduction right and this can be
extended to any industry right as of now
you can have example in the Medical
Science right you can have an example in
automobile right you can have an example
in
retail right right then you you can have
example in
manufacturing right we can have example
in
education right you can have example in
sports right IPL analysis Cricket
analysis all these Sports analysis right
these are the most famous domains right
they are the most famous domains they
are not topics they are domains right in
which data science is used extensively
so I'm just going to check guys in
automobiles there's a biggest example
self-driving
cars yeah autonomous driving how do you
think that's possible data science
again like Tesla absolutely Tesla is
level three we have five
levels level three is narrow AI level
four is Agi and level five is super AI
we are going to move first right with
the technical aspect right with the
technical aspect in our data science
course
right right which is based on
python right because that is our base
language which we have learned so far
right so in Python people we will start
and cover four of the packages
right now and as we move on to machine
learning and other uh deep learning and
everything you will explore more and
more packages the first package we have
to cover will be
numpy right I'll explain you in detail
what numpy is then we will cover
pandas then we will cover matte plot
lip and then finally we will cover
something called as C
Bor right we will cover something called
as C so these four packages inherently
we have to cover in Python to make sure
we are able to
reduce the
time in coding right we are able to
reduce the timing coding and using these
packages immediately help us in getting
the desired results right I hope you all
remember the concept of
modules we have covered in Python you
all remember functions and modules you
can use jupyter notebook if your jupyter
notebook is not installed you can use
something which is called as Google
colab right go to Google type
collab right let's say you write
collab right and then you will see this
option click on Google collab and it
will allow you to code in Python right
so we are now going to discuss about
the numpy package in Python okay numpy
package in
Python so numpy is
a
fundamental
package for data science in py3 right it
is one of the most fundamental packages
for practicing data science in Python
right why is that why is so why is LP
why so fundamental what is so special
numpy
package gives us a new data
type for
handling data in
Python called
as n d array right ND arrays which
stands
for this stands for n
dimensional arrays right nend
dimensional arrays right this stands for
n dimensional
arrays so till
now till now we have
studied
about
list integer
tles
strings right out of
which out of
which the data
types such as list
pupil have been used to store data right
store data right
and
range used for
generating new data which is primarily
sequential
right now there is one now there is one
problem right there is one problem and
there should be a question that there
should be a
question why do we need a new
data
type to work with data
science right why do we need this y so
the answer to this question people the
answer to this question uh lies in a
small explanation right yeah lies in a
small explanation which is
that
python is a
high
level language right python is a
highlevel language right
and a high level
language is
usually
very
distant
from Hardware
a high level language is close to
Hardware or distant from the hardware
did you not attend the Python
Programming
Essentials what is the type of
programming language which is closest to
Hardware a low-level
language if this is my
Hardware yeah this is my
OS this is my application layer right so
hard level language high level languages
here and low level languages here right
Which is closest so binary
languages assembly
languages right all these are closest to
the hardware because where is the
processing happening where is the
processing happening of the data at the
hardware isn't it processing of
data is
happening at hard
Hardware no hry actually it is happening
at Hardware right what is processing
processing is signals of zeros and ones
right what are zeros and ones these are
electric
signals these are the electric signals
right which is basically on and off
right and it is communicated to the
hardware through the help of
resistors and
microprocessors right isn't it right why
a computer Only Knows zeros and ones
because zero is off and one is on which
is the electric
current right electric current to
activate or deactivate certain thing
right true and false Gates right so it
is happening at Hardware so now people
if you understand this part then try to
logically connect it to what I'm going
to say when you are studying data
science
what kind of data you will be dealing
with big
data right and as the name suggests it
will have a lot of
volume right it will have a lot of
volume other than a lot of other things
right it will be very very big and on
this large volume of data you'll be
doing
processing you'll be doing processing
what does processing
means what does processing means
operations
so where is this operation happening
this is happening in
hardware and for Hardware which is the
closest language to Hardware a lowlevel
language but now people but now we have
a situation in front of us what is the
situation that what are we trying to do
data science with what are we trying to
do data science
with python
right and python is what a high level
language isn't it yeah so there's a
discrepancy yeah a big
one because hard level high level
language
these are
slow in
processing right these are very slow in
processing right so for these kind of
languages to handle this kind of data
and these kind of operations yeah is
very difficult right so let's let's keep
let's keep this part aside if you
understand this now let's go to the
second point
right when you learned
python on a scale of 1 to 10 how easy
was
it the ease of use of python it's
relatively a higher number right
relatively a higher number
so now guys when I talk about data
science right when I talk
about data science
okay right when I talk about data
science my thing is that this will be
used by
masses right will be used by masses a
lot of people managers programmers
business analysts data analysts possible
Right who are from non-technical
background who don't know coding they
also can do data science because data
science is a general thing isn't it
understanding the data it should not be
limited by your capability to understand
the uh technicalities of a very complex
language so for these people which
language is suitable which is python
right it is easiest to understand it's a
high level language almost like English
yeah so python is a simple language so
in this part people python fits the bill
right which is a bigger thing in the
second part when we talk about the
operations we talk about the operations
in this part there's a problem right
python
fails right python fails right because
it's a high level language we said that
okay there is a language called as C
right which is a middle level language
right and C language people is used to
create OS operating systems it is used
to create
networks networking
applications it is used to create
games right all the things which are
close to
Hardware sees used right so C fits this
bill right C fits this
Bill Python said that okay if C fits the
bill and python is
written in C written on C right it's
written on top of C language now what
happened was python said okay if my
intrinsic data types my intrinsic
processing is not suitable for data
science but my high level nature is
let's do one thing let's take C language
and use its power right use its power
that it is very close to hardware and
let's create a new data type right let's
create a new data
type which is written on top of c and
which can integrate with python
seamlessly and people this new data type
was called as
array and this was given to you by
something called as numpy package right
numpy package it defined a new data type
which was array and along with defining
the array it gave various
operations right it gave various
operations one
could perform
on
arrays right one could perform on
arrays it's simple right program the the
power of processing lied with C right it
was lying with C so we developed a new
data type using C on top of python and
that new data type was called as array
and this array was defined in a new
module which was called as numpy module
which told you how to create the arrays
and then how to manipulate those arrays
for doing data science we had options
like Java we had options like C right we
had options like Photon to be used for
data science but we chose python because
of its Simplicity and the simple
syntaxes that people from
non-programming background could also
use Python to do data science
right now the limitation was that
because it is slow because of being high
level we needed something which could
make it fast and that was using an
external data type which is not internal
to Python and that was array and this
array is defined inside a new uh module
or a library called as numpy which is
numerical python right numerical python
back to the programming right where we
have understood right about this
question
right
so
numpy essentially is built on top
of
C language which
is
compatible with
python it leverages the power of
closeness of C with
Hardware right C with
Hardware which
eventually makes the
processing faster
in P
right so this is what the first part is
right this is what a first part is right
now the second thing is people so this
is about the performance bit right this
are the performance bit the
above is about the
performance of
5 right now coming to the next part
which is the memory efficiency right
memory efficiency right
so arrays
created by
numpy
in Python are less
memory
exhaustive than lists in Python right
and I will prove these points later on
to you through code right in list right
in list
each item is an object right each item
is an object right I hope you remember
this guys each item is an object
right and it holds right it
holds meta
information
like
references and
types right Etc right a lot of
information it holds
right this
makes list
consume more memory right but but arrays
in
numpy are
contiguous which
means that they do not create object
but
rather directly
store the
data
in
continuous
memory
blocks one after another right also the
arrays are homogeneous in nature right
you can only store the omous data in
Array unlike lists in list you could
show different different data types
right but in arrays you cannot right you
have to store the same kind of data in
the array homogeneous so it's a
contiguous memory block which is meaning
that you can store data in continuity
right one after another in the memory
block so the access is faster the memory
location and the memory efficiency is
very higher right as compared to the
native data type like lists or tles in
Python arrays are
basically vectorized
operations right I'll talk to about talk
about to you with vectors what are
vectors right they the vectorized
operations and they are way more
convenient to deal with as compared to
list
in Python right in list you have to go
through a lot of Loops right we saw that
you have to go through the list
comprehension but you will see in Python
pandas sorry in Python numpy the
vectorized operations are very very
simple right they are very very simple
right again for all this I will give you
examples but uh it will take some time
because you have to understand what
arrays are first right
okay
then the most
important other
packages which are
pandas matte plot
Li c bond SK learn
cpy all are written on
top of number tyy
package that's the reason for this
reason it's called
as
fundamental
package all the other packages which
make your life easier as a data
scientist where you don't have to worry
about
code all these
packages make our data
science Journey smooth
because we have to
worry less about code and more
about
logic yes so for these for the
understanding of these packages it's
very important that we understand numpy
first and then we move forward
right
now then let's get started so the first
step right the first step which you have
to uh see right the first step which you
have to see while using numpy package is
basically from where will you import the
package right from where will you import
the package so to import the package
numpy we write import numpy as NP right
where NP
is an alias right it's an
alias right so please do this import
numpy as
NP right if you don't get any result for
this then you can write pip install
numpy right pip install numpy and just
execute this right when you will execute
this it will give you this message or it
will give you it will download this
package for you right pip stands
for
python index
package
right it is basically like Play
store app
store right or Windows
store for
python right
you're just going to these Play
Store Windows store app store of your
Python and asking them to download this
for you right it is also called as the
package
manager right
pip if it is done you can also check the
version you can say NP
doore version uncore uncore and it will
give you the version of nump
right numpy is an
open-source
package right yeah that's about it yeah
it's an open source package
people right open source package and if
you want to see the code you can go to
GitHub
go to Google write the uh code for numpy
it will show you it on GitHub right done
so let's say I okay so I
say we have when we so okay before this
let me come to a little bit of theory
before I do this with you so now guys I
said that
numpy has
arrays as
data
type right and this is written on C
which runs
directly on
Hardware also this numpy
array is basically a
vector right it's basically a vector now
what is a vector people what is a vector
a v Vector is a
quantity which has
Sign Plus
magnitude right it has a sign and
magnitude if I say this is a cartisian
space this is I this is J and I say this
this is 3 I and 4 J right 3 i 4 J cap so
this is a vector right and this is the
direction
guys if you have studied Elementary
maths you would know
this yes people this is a vector if I
draw another like
this then this is another Vector so I
will call this
say uh 2 I and
5j Yep this is another vector and this
is the Theta right this is the
direction this is the direction and what
is the magnitude
3 I 3 2 + 4 S which is 9 + 16 which is
25 under root which is five so the
magnitude of vector is five and
direction is equal to Theta this is a
vector
quantity now what is this what is this
this is a
scalar this a scalar yeah only magnitude
isn't it this is scalar only magnitude
and then I have 2 comma 3 right this is
what this is a
vector it has two
Dimensions or one dimension only one
dimension
right this is one dimension
Vector y one dimension vector now if I
say this 2 3 4 5 what is this called
This is called a
matrix right this is called a matrix
which is what collection of
vectors and this collection of M vectors
Matrix is called as two dimensional
right it is called as two
dimensional now if you have this
so these are stacked behind each
other this is one this is two this is
three right so we have three
layers in
Matrix so how many dimensions will be
this people one dimension two Dimension
and three dimension this is a three
dimension
Matrix right or three dimension
vector or three dimension
array I'll repeat once again what is a
single value a single value is called as
a scalar right it only has magnitude now
when you have multiple values right this
is called as a vector it has a direction
it has a magnitude and this is single
Dimension now
multiple vectors right like this or
maybe you can say like this right are
you understanding why I'm calling it one
dimension it can be either this
Dimension or it can be this dimension in
any Dimension you stack two vectors you
will get yourself a matrix right you
will get yourself a matrix which is now
two Dimensions it has rows and it has
columns
right now if you stack multiple such
Matrix one after another
right this becomes a threedimensional
matrix and this can go up to how many
dimensions people how many dimensions
this can go up to it can go up to
this
can go up to n
Dimensions right n
Dimensions right so now do we get it why
do we call it n d
arrays yeah n Dimension
arrays right that why are we calling
something
what right nend Dimension arrays we are
only capable of viewing three dimensions
people it can go up to 100 Dimensions
500 Dimensions thousand Dimensions any
dimensions scalers are least important
vectors are more important so now we
have
multiple dimensions
in
arrays namely z
d
1D
2D 3D and so on till the n d right ND
arays right now let's create our first
array right let's create our first array
and this will be a
Zero
Dimensional array right zero Dimension
array how will you create this you will
say a
rr0 is equal to NP do array and you will
mention what a scalar value is what is a
scalar value it is a simple value value
I say two right np. array equal to two
and when you will now check or print the
type of ARR Z it will tell you class
numpy ND array right it is a zero
Dimension array and if you want to
check the
dimension you just have to write a rr0
do NM
right and it shows you that there is
zero Dimensions present so what is this
in short this is a scaler I have entered
a single value people 2 200 500 whatever
you want to enter and the syntax is np.
array np. array your instructing numpy
package to fetch the function array on
method array and convert this into that
particular data type right array
zero and when you check the type type is
ND array but what is the dimension of
this ND array this is zero which is
nothing but a scalar right we have
created
this this is what we have
created
right now
guys if I created this list
right say l is equal
to yeah so this was this was a list
right this was a list and this was this
is what people what is this that we have
just studied according to that what
dimension is this
list so what I'm trying to do is I will
create a one
the array with a or let's say from a
list right let's let me show you how do
we do that
okay so I
say har read the error name AR not
defined why because you have created
array from a a r z come on hurry
right I will create a one dimension ARR
how will I do that people I will say a
rr1 is equal to NP do array and can I
pass 1D list inside this or can I say I
can pass L inside this when I do this
people now see what will happen a
rr1 will be equal to this right and if
you say let me say print
arr1 it will be like this okay this is
your arr1 do
NM you will see that it gives you one
right this is a one dimension array
right one dimension array
yes so for
example when I say scalar right when I
say scalar I
say 35 right when I say
Vector 1D I
say uh so these are suppose my
marks right now I say
35 40
50 right so now what are these my marks
in three
subjects yeah marks in three subjects
this is my say Hindi this is English and
this is maths right or let's say science
because not everyone has Hindi science
English and maths right guys now if I
have to create a
matrix of two Dimension what will I
write so that means this is one student
this is one student people isn't it guys
Yes No Maybe So now in Matrix we will
have what we will have multiple
students yes no maybe in a matrix people
we will have multiple students suppose
this was S1 now you will have S1 S2 S3
S4 like
this and each student will have their
own individual list of marks so can I
say that I'm making a nested
list can I say that people I'm making a
nested list so now let's make it
okay from
a nested list okay a nested
list so I'll say arr2 is equal to np.
ARR list I will have to create a list
first lis2 is equal to so this is my
first bracket what is this bracket
representing this bigger bracket now I
will put another bracket inside this and
I will write 1 1 2 2 3 3 I'll put a
comma again right a comma then I will
say 4 4 55 66 comma 77 88 99 right I'll
do this right now I will say
2
2 when you will do this you will see
that an array like this has been created
right like this
arr2 right this has been created right
when we check the dimension it is two
Dimension right
y marks of three different students in
three different
subjects and this is the same technique
you can create a three dimension array
how will you create a three dimension
array
people if I go here how will you create
a three dimension array
now now suppose I have data in this and
this is my master list okay my master
list in this I have
data and this can be represent like
this this is
my first Matrix isn't it and this is the
vector inside
this V1 vs2
V3 then this can be called as M1 and now
to create a three dimension setup how
many m1s do you need you need m multiple
m1s isn't it you need M1 M2 M3 multiple
Matrix like this
people like this Matrix one Matrix 2
Matrix 3 so what will you do you will
have you will have what
people you will have another yellow
right and you will have inside this
yellow multiple purples
isn't
it right again like
this yeah like this you will have it
people so can I say people can I say
that as I am increasing the dimensions
as I am
increasing the
dimensions I am putting 1 D sorry 0 d
right and okay again in this V1 in this
V1 do you think you will have multiple
scalers people can I say
that can I say multiple scalers create a
vector multiple vectors create a matrix
and multiple Matrix create one
threedimensional
Matrix can I say that let me talk to you
about an image
Right image right what is an image made
up of
people what is an image made up
of H
pixels yes or no no not frames frames is
basically
videos pixels are creating an image
right so we have how many pixels here 1
2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
right
suppose this is one vector right this is
one vector and this is one
scalar right scaler one scalar 2 scaler
three scaler 4 and this will make Vector
V1 this is vs2 V3 V4 and together
together can I call this M1 and call
this
blue so for a colored image how many
channels are there people how many
channels are there what do we call it we
call it the image as
RGB RGB
image that means
red
green
blue so this is blue part so similarly
you will have a red part in front of it
then you will have a green part and then
finally you will have a blue part yeah
are you understanding guys why do we
require three dimensional arrays
yes suppose now you want to make a
change at this this pixel so you will go
to the third layer which is the blue
layer then you will go to the third
column you go to the third column and
third row and this is how you will reach
this pixel everyone yes no
maybe yes so this is the reason why we
need to create a 3D
array right to ingest information like
this right to just information like this
so we can create a 3D array
also
right right and how did I tell you how
many brackets will I have squared
brackets I will have three squared
bracket so now this is just one
student right now I'll put a comma
here right I'll put a comma here and I
will start
right I'll do this I'll say this list
three
a3+ 3
A3
A3 here
three right and now you will see that
there is a threedimensional
array right guys
right this is a three dimensional
are y moving on there are multiple
ways create arrays right the first one
we have
done so we have
done from
lists from list we have done
then second will
be from uh we can create a zero
array right we can
create ones
array right then we can create custom
array right I will show this all to
you right I'll show this all to you so
let's start with the one's array sorry
Zero's
array right what do you have to do you
have to write so let's say 0 underscore
ARR Z okay a zero Dimension zero array
so you say NP do
Z right and you
create a
two
right a two right and if I say
uh
0o do
endm right you will see it is a one
dimension
array right it is a one dimension
array right so two is by default taken
as so let me just show this to
you it will look like this right it is
taken as horizontal what is the
dimension of this guys a vector what is
the dimension of this vector
no no no it's one it is basically 2
cross one right 2 comma
1 sorry 1 comma 2 my
bad 1 comma 2 isn't it now what if you
had to create a 2 cross one what what if
you had to create a 2 cross one right so
let me just show that to
you so I say
wait like this
okay now when I do this I say
arr1 and I say
here 2 comma 1 right I say 2 comma 1 now
you will see people what will happen to
this
now because you have created right
specified two Dimensions right what will
this be converted to
now H what will this be converted to
this will be converted
to
a twood dimension vector by default it
was this right which was this
Vector right by default it was this
Vector right what is the dimension of
this Vector 1 cross how many values you
put here right 1 cross two like this so
we call this only one diens Dion we call
this only one dimension but now when I
will run this one you will see yes 2
cross 1 and now you will see this
is two Dimension right you see this is
two Dimension now clear people just the
orientation has changed but now you see
the brackets there are two brackets now
because what have you now instructed you
have now instructed Python and rather uh
numpy to create the vector as 2 cross
one so you have said give me this zero
and give me this zero here so the moment
you do this you are now specifying the
rows and
columns now this 21 right let's say this
is
21 now let me create a another two cross
two Dimension Matrix for you let me call
this
10 comma 10 right so how many rows and
columns will it have people
how many rows and columns will it have I
will say 2 1
Z it will have 10 rows and 10 columns
like this you saw this yeah 10 rows and
10
columns
right by default it is 1 cross two like
this it is a on dimensional vector and
this is a two dimensional Vector what
about a three dimensional vector 0 dot 0
uncore uh
arr3 is equal to NP do zeros right NP do
zeros
right H should I just write 3 comma 3
comma 3 and I should check for
this yep you will have a 3 cross three
Vector 3 cross three Matrix with three
Matrix stacked behind each other right
if I say four this will be four so how
do we read
this number of
layers number of rows and number of
columns so can you help me with a syntax
can you help me with a syntax which can
create me a 3D Matrix of five layers
three rows and three columns what will I
write five layers three rows and three
columns what will I
write 5 33 yeah you'll get five layers 1
2 3 4 five right like this suppose
suppose you have to
represent an
image with
with
say RGB
Channel
and uh say 256 and 256 pixels how will
you create this so I'll say IMG Right
image is equal to NP do Z and I will say
inside this three Channel 250 26 cross
256 right and when you will just run
this image it will be like this right it
will be like this this is one channel
this is two channel and this is three
Channel and uh we understood that numpy
is a fundamental package for data
science in Python right this package
gives us a new data type to work with
which is called as n dimensional arrays
right now what are arrays what are
arrays arrays are nothing but vectors
right which are stored in a contigous
block of memory which means they are
stored continuously one after another
and they do not get converted into the
object unlike the list and they are way
faster they are more memory efficient
than list I will prove this that to you
today with the help of example through
the help of code right so why numpy is
because numpy is a package which is
built on top of C language which is
compatible with python and C being a
middle level language interacts directly
with the hardware so whatever operation
you run in a fact that is getting
directly executed on the hardware itself
right that is the reason why the uh
performance is way better when we try to
use the end dimensional arrays along
with this the syntaxes the type of
syntaxes we used to type uh in list I
will show that to you also today with
the help of example are way simpler when
you try to do them with ND arrays right
so arrays are basically vectorized
operations right and they're also very
convenient to deal with as compared to
lists and other native data types in
Python right and the most important part
is that in our data science Journey
whatever other packages we will use
right again what are packages they have
predefined things stored for you so that
you can leverage them and focus Less on
code and more on LW LC right you need to
be aware about the logic more than the
knowledge of the code right so we have
packages for that which contains methods
inside them which you can use and uh
without any further calculations you can
work with them directly
right so that is how we started with
numpy right and the syntax to import
numpy was import numpy as NP where NP
was an alias right uh you could do pip
install numpy if someone did not have
access to numpy if numpy was not coming
by default you can use pip install numpy
which is python index package right and
uh this is like Play Store App Store
Windows store for python all the
packages are stored in PIP and you can
call PIP you can ask pip to download
that package for you so that you can use
that right it's basically the package
manager so and numpy is an open source
and if you want to see the code you can
go to GitHub and check the code out for
yourself right in numpy we have n
dimensional arrays now the question
arises people that why do we need numpy
right so my answer to this particular
question is that when you will deal in
data science right when you will become
a data scientist you will be dealing
with data right now my question is how
will you ingest how will you make the
machine ingest the data right there has
to be a way right for you to input the
data to the machine right to make
manipulations to the data to read the
data so all this is started as the base
package of numpy arrays right other than
that it becomes very difficult and
cumbersome for us to deal with that and
this provides us a lot of ease and
flexibility to deal with such massive
amounts of data which you will see along
with me as we will move forward in this
particular course yeah perfect now
people let me just uh pull up the
pbts this is what we discussing people
we start with something which is called
as scaler right we start with something
which is called as Scala which is a
quantity which only has magnitude right
in numpy language this is also called as
z d right it is called as zero Dimension
then people we have Vector right and
Vector has a constant dimens it has only
one dimension right you can interpret it
as a row or you can interpret it as a
column it doesn't really matter because
this is only one single Dimension right
so usually it will be written as five
comma blank right there will be nothing
written in front of it so this in numpy
terminology and nomenclature is called
as one dimension right when you move on
then you get combined couple of vectors
you get a shape and now that is called
as a matrix and in numpy terminology it
is called as two Dimension right and
when you try to stack multiple two
Dimension matrices one before one after
each other or one before each other then
they become something called as three
dimension and now in my capability I
don't know what four dimensional looks
like but there is a high possibility
that you have nend dimensional data
right it has it has end we are dealing
with n Dimension data right suppose with
this I also add time right at t equal to
1 at t equal to 2 that will serve as the
fourth dimension for this data but how
do how does it look like I don't really
know that right because humans are only
capable of visualizing 3D three
dimensions at Max right so you can go to
end dimensions and hence the name n
dimensional arrays right ND arrays post
this right post this we moved on to
create certain things and I tried to
explain you the data right so the data
will look to you like this right you
might have a scalar quantity which is
marks right one marks right now if I go
on to vectors in one day it can be marks
of one
student right marks of one student in
science English
English and maths right
35 40 and 50 out of say 50 right three
subjects so this will be characterized
as a vector right what will be the
dimension written for this it will be
three comma nothing this will be zero
right shape will be zero for this Matrix
suppose we have 1 2 3 4 students and
each student will have three marks
right each student will have three marks
so what will be the shape of this we
have four rows and three columns right
so this will be the shape right of this
2D Matrix right and now if you stack
images right one behind each other then
it will be like this right RGB image I
give you an example so this has 4 cross
4 pixels so the shape will be 3 cross 4
cross 4
right this will be the shape for this
particular 3D Matrix right guys so this
is how you input the data just to tell
you a little bit more since generative
AI is very popular these days right so
what if I tell you the fact that the
chat
GPT which you use or you might have used
right
has never
seen a single
single
word in its
lifetime right all it
sees is numbers right only numbers how
do we see numbers suppose I say
my
name
is
ragab
ragab is
uh
nice name right so these are two data
right these are two data points now we
all know that computers do not
understand these right computers do not
understand these right there is nothing
no understanding for computers to know
what text is right it only knows zero
and one yes dhika right it only knows
zeros and ones so see how we will
convert this so there is something
called as
vocabulary right so vocabulary are
nothing but the unique
words right how many unique words do I
have in this my name is ragav four this
is not unique this is repeating this is
repeating five six and this is repeating
so I have six words so now guys I will
do something called as word to
W right where I will convert these words
into vectors how will I convert them
look at this so I will have suppose this
is
S1 this is S1 and this is is S2 right so
I will
represent S1
as right I will have a
vector of size six how I will say 1 0 0
0 0 0 right 1 0 0 how many elements does
it have six elements right name will be
0 1 0 0 0 0 is will be 0 0 1 0 0 0 and
ragab will be 00 0 0 1 0 0 right this is
S1 my name is ragab now when it comes to
S2 right when it comes to S2 how will I
enter this
S2 0 0 0 1 0 0 ragav what is is here 0 0
1 0 0 0 what is a 00 00 0 1 0 or nice is
00 00 0 1 and name will be 0 1 0 0 0 0
right now this will be the vector
representation of these two
sentences just to tell you a fact gbd3
right gbd3 model right gbd3 or gbd
3.5 they have vocabulary
of 30,000 words right 30,000 words and
each word right each
word has
a
dimension of
12,500 numbers right what do I mean
suppose I say
ragav so it will be one word and it will
be represented by
5
11,500 different
numbers right and like ragav there will
be 30,000 words in this GPD
model right 30,000 words in this GPD
model right and this total number of
parameters which get trains in the
neural network which we will learn later
on neural networks they are almost close
to
1 7
5
billion parameters right 1.75 billion
parameters so why I'm telling you all
this because to Showcase to you that
what is the importance of vectors right
in the entire machine learning and data
science yeah people so this is the
reason people now my question is how
will you create these vectors how will
you read these vectors right the answer
is through numpy package because it is
the base package
clear people yeah I hope today's class
will be uh interesting for you because
you will know the context why are we
doing it yeah so I'll try to show that
to you how we convert things to
vectors right okay let me go here now
right let me go
here so people uh we started using numpy
so I started with the zero Dimension
arrays right zero Dimension arrays so
zero Dimension is nothing but a scalar
so I created ar0 which was NP array and
I entered a single word single element
inside this which is nothing but a
scalar and then I checked the type of a
r Zer also right and then I checked the
dimension also so the answer was two
class was numy ND aray and the dimension
was zero right exactly what I had
mentioned in my
PB right it will be having a zero
Dimension like this right same thing has
been Pro
right because it's a scalar now coming
on to one dimension right coming on to
one dimension I create a list which is
nothing but a one dimension data type
right now I create an array which is
arr1 from array from this particular
list Lis and then I check the type of
print arr1 check the type of arr1 and
the dimensions right so when I
execute right then you will see that it
was this numpy array and the dimension
was one right exactly like this so if I
show you something else say print arr1
dot
shape you will see it's 4 comma empty
right and I show it to you
here it will be empty right mty and this
will be comma one right so this means
that it has only four elements right if
I increase these elements to say 55 66
77 then it will become 7 comma blank
right which means it has seven elements
as a vector now we create something with
a nested list right which is like this
so with the nest I want one bracket
which is running outside right then
inside this I I have one two and three
lists inside one list right so this is a
nested list this is marks of First
Student second student and the third
student right so I do this and you see
it is this right and I will show you the
shape
also right it will be 3 cross 3 three
rows and three columns three rows and
three columns right now similarly we we
can also create
uh the 3D
Matrix right
with two levels right level one and
level two and three cross three so the
shape will be what people can someone
guess the
shape what will be the shape of this
I've shown you
here right if this is 344 then what will
be
this three rows and three columns right
so when you will execute you will get 2
3 3 right 2 3
3
right right now people there are
multiple ways right there are multiple
ways to create arrays and we should know
them because all of these comes very
very handy not right now I don't have
enough context to give you right right
now but later on you will see with me or
with some other trainer that how these
will be used in deep learning
specifically right they are the key of
deep learning algorithms right where we
initialize some weights we initialize
some biases and those initializations
are nothing but multi-dimensional numpy
arrays right numpy
arrays
okay right for example suppose I have
this I have to multiply this with some
random numbers right so how will you
generate these random numbers you will
generate them through numpy and you can
generate them in a specific kind of uh
shape right which is 2 cross 3 and then
you can multiply them you can multiply
the matrices and you can get your output
for yourself right so this is the way
they are used
so we saw the first thing from list we
have already covered then now we are
moving on to creating zero arrays right
so I create a zero array of one
dimension right of one dimension which
is 0 0 they are represented in floats
right they are represented in floats
0.0 right now you can create a two-
dimensional zero array right you can
create a two dimensional zero array
which is you have to mention just the
shape inside 2 cross one so it will have
two rows and one columns right two rows
and one columns the difference here is
the difference here is that these are
one dimension and these are two
Dimensions right you have explicitly
mentioned the rows and columns so you
can expand this to 10 cross 10
also right you can expand in 10 cross 10
or 10 cross 6 whatever you feel like
yourself right it will have 10 rows and
six columns right
now you can also
create the 3D arrays 3D zero
arrays
right which is 5 comma 3 comma 3 what
does five means what does five means
first element represents the number of
layers so you have five layers right
it's five layer deep then you have three
rows and three columns right so it will
look something like this
one 1 2 3 4 5 right like this something
like this right it will look like this
St 1 2 3 1 2 3 1 2 3 right 533 okay yes
the number of matrices Hur what I
presented right
layers
rows columns right layer rows and
columns
clear so now when you execute this you
will get an arrangement like this okay I
tried to show you this thing with
another example right which is I created
an image of an RGB image of 256 cross
256 pixels right which have all zeros
inside them and this is how it was
created right three layers RGB 256 256
so this is how the image will look
like
right this is how it will look
like now you can also
create arrays with ones right exactly
the same way you created it with zeros
I'll give it to you I'll give you 5
minutes time to create them I'll show
you one so I say
A1 is equal to NP
dot
on and inside I pass
two I check A1 so this is a array like
this right it is an array like this now
you
create create two
Dimension and three
dimension arrays of one it is
basically as a float har it's
represented as a float right it's
represented as a float
okay
right perfect right also guys uh with
this right also with this you can create
the custom arrays right you can create
the custom arrays right how do we create
custom aray people how do we create the
custom
aray you you have created now zeros you
have created now ones now what is left
that you create the custom arrays uh
forget about this I will come to this
later
on right let's
create custom arrays right so the syntax
Remains the Same right I say ccore ARR
is equal to NP do full right this is the
syntax np. full right and you will say 6
cross 6 and suppose you want an array of
all fours right this is the dimension 6
cross 6 and this value after comma is
basically the value which you want you
execute this and you copy this paste
this and you will get the arrays of
fours for yourself right if you want of
10 you will get of 10 if you want
10.3 you will get 10.3 right anything
which you want
if you want K's you will get K's right
all the examples so let me just show
that to
you y like this now guys how did we
create or how did we
use range in
Python can you
use range to
generate numbers between 20 to
50 right 20 to 50 can you give me the
syntax quickly how did you do that in
range how do we do that we said R is
equal to
range 20 to 51 right and then I said
I in
R print
I right and this is how I got the
numbers right so similar to range in
Python we
have a range in numpy right how do we
use a range I
say uh a
range underscore ARR is equal to n B do
a range right and then same syntax I
will say 20 to 51 right 20 to 51 and
that's it and when I will check my ARR
range AR you will see I have generated
myself numbers between 20 to 50 and a
range in
numpy right a range in numpy yes if you
want a interval so you can use this say
a range one and after comma you pass the
third argument suppose it's three so now
it will jump three times right 20 23 26
29 32 35 like this up till
50 now guys there is something which is
called as lens
space right what is LIN
space it stands
for linear space
which
means between two given
numbers this function will fit the
required number
of numbers right for example suppose for
example we need to create an
interval from 0 to 1 people there are
infinite numbers I can have between 0 to
1 isn't
it infinite numbers I can have between 0
to one
0.00000000 1 0.00 0 1 0 1 01 right I can
go in the infinite manner right now for
example you need to create numbers
between 0 to 10 right and you want to
create and want to have 10 numbers in it
right so how will you do this it's not
float it's about the number Theory right
between zero and one you have infinite
numbers right so you say length space is
equal to NP do lens space right NP do
Lin space you mention from 0 to 10 you
want to have 10 numbers right and when
you will create lens space you will see
that these are the numbers are there
which have been created right these are
the numbers which have been
created right nine numbers now I say 100
numbers I want evenly spaced 100 numbers
right evenly spaced 100 numbers how are
they even you can simply subtract one
number from another and the difference
for all the numbers will be exactly the
same 0.01 01 01 subtract any two numbers
it will be 0.01 01 01 right where do we
we need this we need this to plot the
axises right when you will plot graphs
you will need axis between this interval
you need five values that works like
this okay suppose you want from 0 to 10
five different values right you will
have five different values like this
right between the gap of
0.5 right if I say 1 to 10 you will have
values like
this right like this clear 100 value
like this yeah yeah clear guys how do we
use LM space suppose you want from 0 to
10 interval from 0 to 10 and 10 will be
included 0 will not be included right
will start from one so I go to one it
will be
from one why is zero not included then
yeah Z is included right 0 is also
included and 10 is also included 100
numbers between them
right clear this is what Lin
spaces now guys
now suppose
you laran Lin space is basically
used if you want to create n numbers
between the range of numbers right
between 0 to one right suppose between 0
to 1 you are trying to plot a graph okay
and your values are 0.2 0.3 0.6 right
and you want to draw a graph so you will
have to mark the axis right the xaxis
and the y axis so you can use lens space
there and what will it do it will take
the range it will take the interval in
between you want to add the equal space
numbers and then the third argument here
will be that how many numbers do you
want between them so this syntax tells
you
that
from 0 to 1 give give me 100 numbers how
are these numbers
equally
spaced numbers equally space numbers
right so when you will execute this you
will see that all there are 100 numbers
which have been generated right 100
numbers and all the numbers are
equidistant from each other because
difference of every single number from
the next number is 0.01 01 01
yep that's what it does right now guys
now suppose we want to
Generate random numbers right we want to
Generate random numbers right now we
interested in generating random numbers
so we have something called as
random do random right what will it do
random. random will
Generate
random float
numbers between 0 to 1 random float
numbers between 0 to 1 how will this
happen you will
say randor Rand is equal to np. random.
random and inside you will mention what
is the dimension that you seek suppose I
want 6 cross 6 so when you will check
this you will have all numbers for 6
cross 6 Dimension right 6 cross 6 Matrix
right now every time you rerun this the
numbers will change because all of these
are random
numbers right all of these are random
numbers now I say 100 multiplied by Rand
underscore
Rand you will see all of them all these
numbers will be multiplied by 100 right
all of
them in one
shop now just like this we can also
create random integers right how will we
create random integers
guys I say Rand
incore randint is equal to np. random do
Rand in right and here you will specify
that what is the range of numbers you
want from so I say between 20 to
25 I need random numbers and then I want
it from in a 3 cross three format right
and now when you will check your random
you will get random numbers generated
like this okay random numbers generated
like
this
right if you say 3 cross 3 cross 3 you
will get a 3 cross 3 cross 3 Matrix even
if you will only say three you will get
a 1
d right guys you can change the
dimension so this is the range from
which you want to choose the random
numbers and this is the dimension you
want this Matrix or vector to be
in now guys we'll move on to the next
part which is basically properties and
again there are a lot of operations
you'll have to see it yourself
right properties
and
attributes of
numpy arrays right property and
attributes of numpy
arrays okay now guys the first one in
this scheme of things is shape of array
right shape of array what is shape of
array it tells
you
the
dimensions of the
array
stored in
a couple right for example I say ARR
Z right AR r r 1 AR r r 2 AR rr3 right
and then I
say print
this do
shape right like this and you will see
that it will give you the shape of each
array right 0 D 1D 2D and 3D right
people shape of the
array please try it out we have used it
one or two times but this is what shape
of array actually
means second is
people
endm right is
endm right endm of array right it tells
you the rank of the array whether it's
one dimensional two dimensional Zero
Dimensional three dimensional four
dimensional so again I will do the same
thing
right I'll say endm and you will see it
will give you 0 1 2 3 zero Dimension
zero rank one rank two Rank and three
Rank and it can go all the way up to end
rank right before this I should have
also printed these arrays
right these are the
arrays
yep these are the arrays which we have
and these are the subsequent things
right Ram copy then guys the third thing
is the size of
array right it tells
you the number of elements inside how
many elements do we have inside this
array in arr1 how many elements do we
have 1 2 3 4 5 6 7 how many elements do
we have in this 2 cross2 a R2 1 2 3 4 5
6 7 8 9 which is rows multiplied by
columns 3 multiplied by 3 right and how
many elements do we have in this 3D
which is 2 * by 3 * by 3 which is 18
right so now when you copy this right
you can use
this and
say sides right it will say 17 98
yep fourth is
people the D
type of aray tells you the data type of
the array right and I've told you we
place
only
homogeneous data in arring right what
will happen if we don't do this I will
show that to you also right so we do
this right and we
say d type and you will see in 64 all of
of them are integers right all of them
are integers that's the reason we are
getting
m64 right suppose I create a new array
arore new right let me say
head right
hetro
heterogenous and I say it is like NP do
array let's say like this
right like this now people when you will
check
ARR do D type you will see it will give
you float just because of one floating
Point number inside this entire array it
gets converted to float right between
all the integers if you put one float
then it will be taking float directly
right
now let me just copy this and let's say
one and let me add another value which
is string and say
R right and when you will execute this
it will give you u32 u32 here is
representing strings right it is all
called as objects right these are all
string values right so preceden is
strings greatest then float and then
your uh integers right if you place the
heterogenous data inside the numpy array
right you only need to put homogenous
data in the
array now fifth is the item
size of array right what is item
size it gives
you the
bite
occupied by each element of an array
right because we assume that elements
will be homogeneous it will give you the
bite occupied by each element of the the
array only one element okay so how will
it happen so let's say ARR Z or let's
say arr1 dot item
size right and you will get eight right
so why 8
because because each data point right
each data point is
occupying the result is 8 by let me put
this
here right 8
byes because each element in arr1 is
occupying 64 bits which
are equivalent to which are equivalent
to 8 byes right one byte is equal to 8
Bits so 64 bits will be equal to 8 bytes
right that is how it is giving you the
result now if you are interested in
knowing the entire bytes right entire
bytes then you say n
bites will give you
the total
bytes
occupied by the elements of three AR
right you say
print arr1 Dot N bytes right and
write bytes it will be 56
bytes right by because how many elements
do we have in our ARR 1 1 2 3 4 5 6 7
right 7 8 are 56 right 56 total bytes
are being occupied with by
arr1 now guys the seventh
one
is as type
right in
Array this will help you change the data
type of the array right change the data
type of the array suppose I have arr1
DOD type which is uh so I say
print D type right this is N64 right and
now what I do is I say
print uh wait let me give you a
structured way print arr1 right let me
say there
is here one
right d type of
array
now arr2 sorry AR rr1 is equal to AR
rr1 dot as type right do as type and
let's say I want to convert this in NP
dot right NP
dot
uh int 32 right I want to create in
convert this in uh ARR int 32 right when
I do this and now when I will copy these
same things you will see for yourself
right now the D type was in 64 and now
the D type is int 32
right if I want I can do this conversion
in float
also float 64 right and then I will just
copy
this and I will paste it
here right and now you will see now the
floating Point has been
activated right it has been now
activated we can go till int we can go
till int
8 right I can go to
16 right and I can do
this I can go to int
eight
also yeah like this int also 64
32 so first one has to
be 32 low
whatever right like this okay you can
convert
this also people this is later on
conversion you can Define the data type
of the array while creation time also
how would you do that suppose you are
creating ar11 and you say np. array
right and suppose you take it from a
list and then you just put a comma and
say d type so what will be the default
data type here people if I just do this
if I just execute this what will be the
default data type in 64 is the
default isn't it but now suppose I want
to change it right here I say data type
is equal to float 32 right sorry float
64
NP
dot sorry my
bad float 64 right and I say AR rr11 and
this will be float 64 if you want float
32 it will also become float 32 right
right here while you
define instead of using as type you can
do it right here right these things will
come in very handy people because you
will have to save memory because when
your data becomes very very big you will
be always in a crunch for memory
like this you want integers then
integers will be like this random.
randant like this suppose you want to
generate 0 to six right and suppose you
want to
generate say 100 numbers like this 0 to
6 the
scores 1 to six like this randomly
right suppose you want to generate 100
scores for five different batsmen
randomly it will be like this batsman
number one batsman number two batsman
number three four and fifth
right
yep understand the data guys now it's
the time to understand the
data yep now guys we have methods in
numpy arrays right methods in numpy
arrays so what are these methods now the
first method we have to learn is called
as reshape right
reshape yeah so rehap is you can use
this to
create you can use this to create
a new shape of the array very very
powerful guys very powerful one of the
most powerful methods in numpy is uh the
reshape right and how do we use reshape
suppose we have a 1D array of 20
elements right now to reshape
this reshape it we need to find the
factors right factors of 20 they are
what 1 20 4
5 2
10 right the other
factors the other factors now see what
will I do right now see what will I do
let me create a say random array right
random array I I say
random uncore ARR is equal to entry.
random. Rand in right and let me say I
want to create it from 1 to 50 right and
I want it to be having 20 elements right
so I say random. ARR so you will have
random values inside this right random
20 values now see now re
shape
first I will reshape in 1 cross 20 right
1 comma 20 how will I do that you just
have to
write
uh
print a sorry random. AR random uncore
ARR dot reshape do re shape and you just
pass in the dimension I say 1 comma
20 right and when you will do this you
will see it is coming now in 1 comma 20
format right so let me just also write
print right
2D 1 comma 20
right and now what I'll do is I'll copy
this and I will paste this and say 20
comma
1 right you will see it will be like
this 20 comma 1 immediately with reshape
right let me copy this let me say 2D I'm
still at 2D let me say 2 comma 10 right
and you will see this is 2 comma 10 now
I can reshape it
in 10 comma
2 right 10 comma 2 right then I can
reshape the same
thing
in 4 comma 5 and I can reshape this in
5A 4 right like this guys are you able
to see the power one dimension I'm able
to create two dimensions and now I will
take it a step further and I will write
it in three dimension right how will I
write it in three dimension let me say
this 1
comma 2 comma 10 right this will also be
three dimensions let's sorry 2 comma 2
comma
5 let me say this and now you will see I
can have this in three dimensions
right yep I can also say in three
dimension
like this I want to have five
layers with two rows and two columns
right so you will have it like this also
right so this is how people we can
reshape the array very powerful very
very
powerful right very very
powerful right and you can take
this and save print
random do this and you can say
print do
shap right so this was the first
one y like
this also people if you want to
visualize you can also go this
route we can have 10 comma 2 comma 1
right it will look like this right 10
layers 10 layers you can
have right 10 layers you can
have great now second method which we
have to learn is called
as
transpose right transpose method right
what does that do it inter changes the
dimensions like rows and columns right
yes absolutely right suppose you have a
matrix right which
is 22 33 44 55 66 77 right this is a so
now when you will a transpose it right
the dimension right now the shape right
now now is 3 comma 2 now this will
become 2A 3 and how this will happen
rows will now become columns and columns
will now become rows right so let's make
column the rows right sorry columns the
rows it will be 22 44
66 33 55 77 right so people in transpose
no information in lost it is just a
change in the view right which is
happening
right 22 44
66 33 55
77 why do we need transposition suppose
we have two
Matrix this is 11th class mathematics
right one
has a dimension of n cross M and the
second has dimension of a cross P if
you want to
multiply these two Matrix
say M1 and
M2 right there needs to be a
satisfaction of condition M should be
equal to a right M should be equal to a
right M should be equal to a this should
be equal to this and the resultant
Vector the resultant Matrix which you
will get will be of n cross B Dimension
right so often times suppose this is n
cross M this is n cross M right M cross
n and you know that m is equal to a so
what will you do you will transpose this
Matrix right you will transpose this
Matrix then it will become n cross M and
then M can be equivalent to a right for
example what I'm saying we have one
Matrix which is 2 cross 3 and this
Matrix is 2 cross 5 right so can you
multiply these Matrix people is 3 equal
to 2 the answer is no right the answer
is no so what will you do you will just
transpose this and this will become 3
cross 2 and this is 2 cross 5 and now
you can multiply this and the resultant
will become 3 cross 5 Matrix right so
for operations like these we need
transposition right so how do we
transpose
it how do we transpose it so let's say
again uh arr2 right this is arr2 and I
want to transpose it so I say
a2core t is equal to uh NP do
transpose sorry sorry arr2 dot transpose
right and now when you will see a2core T
you will see rows and columns have
interchanged right rows and columns have
interchanged with each
other or let me give you one more
example uh if this is not clear let me
pick
up this again
right now I
say dot d shape into
say 2 cross 10 right so this is 2 cross
10 and now when you want to transpose
this so I'll say this underscore T is
equal to this dot transpose
R A and this and you can check this now
and it will be this sorry guys so this
is going to be it will be like this
right
transposed and now if you want to
see
this this was the original shape right
rows and columns have now been
interchanged transposed with each
other now guys the third
method the third method which is there
with us is called as plattin right it is
called as
flatten right what does flatten do it
reduces the
dimension to one dimension right any
Dimension you have it reduces it to one
dimension for example I have
this right and now when I
say
this do underscore F this will become
this Dot
platin and when you will check this up
you will see that it has now become one
dimension no matter how many dimensions
you have it will become one
dimension right let me take this again
to show you one more
example
right and here I say dot reshape
into
uh uh 5 cross 2 cross 2 right I do this
my random ARR is this right it has five
layers two rows and two columns right so
now I say this
underscore plattin is equal to this Dot
plattin and if you will check it now
again you will see it has now flattened
it up
y now why do we need this we need this
for a lot of statistical operations we
need this to feed the data into the
algorithms right as we will move forward
you will understand the use of
flattening right now guys moving on and
uh as discussed let me now show
you the power
of
numpy right
numpy
over lists and other data types right I
will not take a lot of examples just a
second
guys yeah
okay now guys I told you
that
lists take
up much
more
memory as compared to numpy array right
and I'm going to prove this to you right
now let me use let me create a random
sequence of random numbers using range
in Python right right and say I create
range of 10,000 numbers right range of
10,000 numbers so what will this give me
this will give me numbers from 0 to 9999
right continuous numbers right so this
is
range I will
use arrange
in numpy to
create a
similar series of numbers
right so let's say array is equal to np.
a
range right same thing same done by both
right I've shown you above also now let
me import
thisis library right says package and I
will use something called as get size of
right get size of what does this do get
size of it's a method
which
calculates the
bytes
occupied by a
single element
in vanilla python what is vanilla python
it is the traditional
python right vanilla
python so let me just show that to you
I'll use this and I will say
print now guys if I
get size of any r random number from
this range right any random number say I
get size of five right and I then
multiply that bytes with the length of
random right with the length of random
this ran
right right with the length of random do
you think I will get the bytes for the
entire data structure what am I saying
is
suppose uh I used
Range Five so what will this give me 0 1
2 3 4 right this will be the output so
now I say
get size of say I say two right so
suppose 2 is X and then I multiply this
with the length of this series which is
five so do you think I will get 5x which
will represent the number of bytes
occupied by the entire data type
anything randomly any random number this
can be three right why not
hurry why
not all of these are integers so
integers all of 64 bits assuming so if
you calculate the side of size of this
and if you multiply with the total
number of numbers you will get the total
size isn't it
huh index is in
t no no no it's not about it's about the
element right homogenous elements inside
this I'm saying when you use Range Five
what is going to be the output 0 1 2 3 4
right now all these are
elements elements of
range right all of them are elements of
range right now
I'm saying if I fetch the size of one
element and multiply it with the length
of the entire range will I get the bites
occupied by the entire range for example
if I do this right if I do
this this is 28 right 28 bytes people 28
bytes bytes are occup
IED by one element of range
right one element of range right now if
I just I'm saying I'm just saying if I
multiply to find how many numbers range
has how many
numbers range
has equal to 10,000
right so
total memory
occupied will be will be how much it
will be
28 multiplied by 10,000 which will be
equal to
28,000 yeah and how will you find this
you will say
print this multiplied by length of RAM
right 28,000 bytes clear now yes now
this is for the range now let me use
another thing so how will you calculate
the length of this array what what uh
property and attribute will you use
people n bytes right n bytes will give
you total bytes occupied by the elements
of the array right we will use n bytes
here so I come back down and I
say using n bytes for arrays right and
you will see what the result come
print uh array Dot N
bytes right and I say bytes are you
ready to see the result do you see what
has
happened how many bytes this was taking
it was taking 288,000 bytes how many
bytes this is taking this is taking
80,000 bytes
this is taking 2 lakh 80,000 this is
taking 80,000 2 lakh extra bytes of
memory is taken by
[Music]
range ran is
range yep and if I just go to Million
numbers right Million numbers in both
see the difference it
becomes right this is now 3 3
28 million bytes it is taking and it is
taking 8 million bytes 20 million extra
bytes are occupied right now people do
you believe me y that numpy are way more
efficient in memory management as
compared to the traditional data types
of python yes okay that's the first part
now second is people performance right
performance so what I'm going to do is
what I'm going to do is I am going
to
import time right it's a it's a module
in Python right suppose I say x is equal
to
range this much right okay and then I
have y is equal to range
say
this
to this right both of them will have
equal amount of numbers same numbers
both of them will have right this will
have say uh 1 2 3 1 2 3 10 million
values 10 million values this will also
have 10 million
values right both of them will have 10
million values now what I'm trying to do
is I want to add them up right bite bit
by bit I want to add them up right I
want to add first element of this to
first element of this second of this to
Second of this third of this to third of
this like this okay I want to do this
now what I'll do is I will run a counter
right I will run a counter which is the
start
time right and this is given by time dot
time right which will give you the
this will give you the current time
right after this I will run the
operation I will say C is equal to x +
y for X comma Y in
zip X comma y right in zip X comma y
Right add x + y bit by bit element by
element for X and Y in zip zip is a
function right which allows you to do
this operation sequentially right
sequentially Right add the
elements of X and
Y element by element right element by
element right element by element right
and then I'm going to print right so
start time will start and now I will say
time do time which is now the end time
minus start time so this will give me
the time taken for execution isn't it
guys will give
me Delta of time which is equal to time
taken for
operation
seconds right these many seconds will be
taken right so let me run this and it
takes around
say 4.3 seconds right to do this right
4.3 seconds now guys see what happens
you had to write this complex syntax in
the traditional python now let me show
this on arrays right what will happen on
arrays I will say a is equal to NP do
arrange right and inside a range I will
pass the same values what I have taken
above right and I will say B is equal to
NP
dot a range and I will pass the same
values
inside right exactly the same now what
I'll do is I will say same
thing right just I will change the
execution of C will now simply become
A + B what is simple this or
this this or
this two right do you see the power if
not I will show this to you again right
later on and let me run this and you see
the difference now let me just increase
a couple of zeros right a couple of
zeros two zeros I'm increasing in both
the use cases
see it is going on and on right let's
see how much time it will
take to add say 2 million 1 billion
numbers 1 billion numbers I have asked
my system to add and I want to see how
much time it
takes running running
running yeah Colonel has died Colonel
has died people
I'll have to
restart right I will have to
import
numpy as NP so let me just remove one
zero from
both it is taking 4 seconds removing one
zero from here also right and when I do
this it takes 1 second do you see guys
what is the difference in performance
also right for both of these
Y and if you didn't understand this let
me give you an
example Range Five this is 5 to 10
right and this is basically adding
elements
right 5 7 9 11 13 right so this will be
what will the output of this this will
be 0 1 2 3 4 and this will be output
what
5 6 7 8 9 right so 0 + 5 5 6 + 1 7 7 + 2
9 8 + 3 11 9 + 4 13 and the same thing
if I do
here then what will happen I say five I
say five and 10 right and I say C is
equal to a + b and I say C same thing
you get here right we can move on right
the next bit guys which we have to
understand the next bit which we have to
understand is called as the indexing in
numpy arrays right indexing in num py
arrays right how do we index the
elements right how do we index the
elements indexing in
numpy arrays right indexing in numpy
arrays so again you know indexing from
basic python so let's start with 1D for
1 D arrays right I will use arr1
yeah this is arr1 now right this is
arr1
okay now people what I want to do is
what I want to do is I want to fetch
right I want to petch right you can
slice and dice let's say
dice 33 right so again as for a normal
indexing of list what is
33 what is the IND so 33
people two so you will say the same
thing print right a r R1 squared bracket
two and you will get 33 for yourself
right if you wish to slice same thing
right 33 to say 66 what is the
index 33 is 2 two which one 3 four five
and six right you will write three to
six not five hurry five is not included
remember say
print ARR 1 2 is to 6 and you will get
33 44 55
66 right simple indexing please try it
out please try it out and if you want
you can have this code also you can
write this code you will always have
Clarity that why do we use
it moving on people moving on let's see
indexing in a 2d array right and before
I explain this to you let me take you
here right so a 2d array will be
what right this is a 2d array it has
three
rows and three columns right rows
columns so now for rows indexing will
start from zero so if you have to pitch
this particular row right so what will
be the index it will be row Zer if you
have to fetch this particular Row the
index will be one and if you have to
fetch this particular row this the index
will be two similarly for column if you
have to fetch this particular column
right you will have column is equal to Z
this particular column column equal to 1
this particular column column equal to 2
right indexing will start from minus one
again right
012 so let me just show that to you
right let's say I call arr2 this is my
ar2
now
indexing first row right how will I do
that I will say print
arr2 and I will write how how will I
write write
this H I will write row zero right row
zero so what will this give me this will
give me this right the syntax
is row space column right so now if you
just pass one it will give you only rows
right
r one row two Row three right
similarly if you want to create it for
columns right what will you
say
sorry uh or columns
[Music]
it
was
Zero column 1 column
2 column three
right
right so what is my column 1 76 89 98 76
89
98 right and if you want column
two uh
sorry if you want column two this is the
column two and this is column three
right guys 90 99
99 independently now if I ask you to
fetch me a particular
element element right say I want you to
fetch me 7 8 right how will you fit 78
you will say print arr2 what is the row
for 78 which row does it belong to 012
to which row 78 belongs one row which
column it belongs to
012 one hurry check again whether it
belongs to zero column First Column or
second
column right and you will get uh
sorry 0 1 so this is
2 right 78 right second row First Column
right second row First
Column how to define it as
in right this is your Matrix right now
what are the index positions for this
this row is zero Row first row second
row this is your zeroth column First
Column second column
yeah yes no maybe are we understanding
this this much is clear the indexing of
rows and columns now now if you have two
so the
syntaxes syntaxes say this is Matrix a
so you will say a in this a matrix you
will write row comma column right
suppose I want to access only zero row
so there will be no column so what will
you get row number
zero right row number zero and you will
just put it like this or you can put a
comma and put colon colon means what
take everything so I want all three
columns together zero row and all three
columns so this will be
your show let me show that to
you right see this zero row and all the
columns so what will be your answer 76
88 90 right then if you want to access
the second row 89 90 99 89 1990 99 one
and like this
clear now similarly for columns what
will happen you will take all the rows
comma which column do you want if you
say two what will be the result for this
what numbers will you get for this colon
comma
2 you will get 33 66
99 now coming to the element right
suppose now you want to fetch 55 right
so what will you write you will write
which row does it belong follow the
syntax it belongs to the first row which
column does it belong
to First Column so what will you get 55
come to come come to this example now
you want 78 in this particular array or
Matrix right where is 78 which row does
it belong
to is it first row check
carefully zero throw first row second
row yeah so I put two here now which
column does it belong to First Column
second column sorry zeroth column First
Column second column belongs to the
First Column so I put one here so when
you put this syntax you will get
78 clear now the third thing is
slicing through the array right now for
this I want I want 90 99 78 99 that
means what I want this this this and
this let me take you to the pp first now
what I'm asking you to fetch me I'm
asking you to fetch me these four
numbers right these four numbers so here
what will you write which rows are
included in this people which rows are
included in
this Row one to
all right one to
all yeah not two one to all
right and you leave everything like this
if you have the last row you leave it
empty after the colon comma which
columns do you want for this one and two
right and when these will intersect when
these will intersect you will get this
area you will get this shaded area green
shaded area so I will say I need from
column one to all the columns right so
what will this fetch you what will this
fetch you this will fetch you
44 55
66 and 77 88
99 right what will this fetch you this
will fetch you 22 55 88 33 66 99 what
are the commonalities between both of
these
access both of these slices what are the
commonalities it is only only this much
right 55 66 88 99 right so do you think
you will get your result yeah let's
check it here right I say print
arr2 right and this I
say I need row zero sorry Row one to
empty and then column 1 empty and you
will get 90 99 78
indexing
yeah now guys let's check it
for three dimensions right
3D I say arr3 right this is my arr3
right so let me just put an example here
suppose my arrays are 1 1 22 3 3 4 4 55
66 77 88 91 right this is my first then
behind this I have another Matrix which
is
uh 11 1 222
333 4 44 555 666 777 888
999 right and in
my third one I have 111 1 2 2 2 2 3 3 3
3 4 4 4 4 55 55 6 6
66 7 8 8
88 9 9 9 is that is it qualifying for a
3D array can you access anything on this
particular array if given a chance
separately like this one now people if
you want to move between layers right if
you want to move between layers do you
think I have told you one particular
axis which is what which is the
layers so what number will be given to
this layer layer number
zero layer number
one and layer number two so now if you
have to access this
555 which layer you will go to First you
will go to first layer then which row
will you go to you will go to first row
and First Column so if you pass this
syntax what will you get you will get
555 right problem solved the only bottom
neck was the layers part and you have
additional parameter or argument for
this
layer so if I come back to my example
and if you have to access this 555 right
how will you do that I say print
arr2 AR R3 right and this I say and in
this I say what I have to access this
555 which which layer number is this
this is layer number zero right this is
layer number zero and this is layer
number one so I say layer number one
which row is
this in this particular layer it is
layer number sorry it is row number one
and column number number one and when
you do this you will get 5x 5 right if
you want 999 what you will do you will
change this to 2 comma 2
right if you want 777 or let's say if
you want 98 what will you do for
98 layer number zero row number two
column number zero and you will get this
98 clear
guys y layer row column in the same way
you will slice it right you will slice
it
y right I'll write the
syntax so that you don't get confused it
is array squ bracket layer
row
column right
layer
row column
right this is the
synx
perfect right great we can also perform
some operations right plus minus
multiplication division between two
arrays very very easily right it should
not pose any problem to us right we can
do all the operations which we want to
right between two arrays right for
example right you had you
have right operations on
arrays so let's say
uh let's see as a list right list so we
have list is equal to 1 2 3 4 5 right
now suppose you want to
square each element of the list right
what will you do you will say s s qore
l is equal to X to the^ of 2 for X
in L right and then you will say print
sqor L and you will get the squared of
list
right original list Square L
now in Array what will happen suppose I
say A 1 is equal to NP do array and L I
create the same array out of this list I
say
print
original
array right I say A1 right this my
original array same as this now I want
to square it
Square the elements of array very very
simple nothing you require you just say
you just
say s qcore A1 is equal to S qcore A1 is
equal to A1 to the power of 2 right A1
to the power of 2 right and then you
print the
right you get the square R
suppose you want to find the mean
[Music]
of the mean of
numbers using list right so what will
you do you will
say mean is equal to sum of L right list
divided by length of list right and you
will get the
mean right which is three for this one
right this original list now let me show
you in
arrays right how will you do this you
will say mean is equal to NP do mean and
you will just pass
A1 right and when you will check mean
you will get 3.0 right nothing like this
direct right direct like this right you
can you have I've already showed you add
I've already showed you uh square and
then I believe you can understand that
what all operations are possible using
the arrays right leveraging the power of
arrays right also guys in the arrays
right in the arrays what you can do is
you can
perform you
can perform some string operations right
very powerful string operations right so
for example let's say I have uh I I have
a array right I have an array of names
of people right so I say names is equal
to np. array right and I
say
ragab uh then I say
d right and then I
say Madu right these three names I have
right so you can check the names they
will be like in the array right and the
data type will be U6 which is a
representation of strings right now guys
suppose you want to capitalize you wish
to capitalize the names right of people
what will you do you will say print
me NP docare right NP docare dot
capitalize right capitalize and inside
this you will pass
names and you will see all the names
have been
capitalized right you see this R has
been capitalized d has been capitalized
M has been
capitalized
right you can convert them into upper if
you want
print np. carare do
uper names and you will have all of them
in caps loog you can say print NP docare
do
lower you will have them in lower
right you can put the title right
suppose I say
uh t tle e title is equal to NP do array
and I
say
RAV
goyel right then I
say Den
right and I
say
Madu
Verma right I say these three things now
if I say
print np. car
do title right and I
say titles you will see that all the
words will be in capitalized mode ragav
goyel D and S of DH senapati M and V of
madua are now capitalized I can
also replace something if I wish to
suppose I want to replace uh Madu with
say Su right I will say
print right
np. dot
replace replace right and you will say
where you want to replace I say
title and in this I want to
replace
Madu
with
Su right and you will say it will be
suore right Madu has been replaced
with right
right if you want to
calculate the characters of
strings right you can do that print
np. Dost strore length
of titles and you will get 11 characters
are there in here 15 are there here and
11 are here in this
particular right you can do much more
powerful things also let me show you one
complex function right suppose I have F
name is equal to NP do
array right and we have
ragav
denesh
Madu right and we have lore name
AR ver right we have these two things
now I can create a new array full uncore
name by simply right by simply saying NP
doc dot add right and I say uh F name
right
comma
lore name
right
plus
yeah refh
full
name yeah R I just was trying to add a
space in
between anyway right we can do
that right you can also people search in
arrays right very powerful again search
in arrays using
where
right right you can say suppose a 2 is
equal
to np. array right and I say 1 1 2 2 3 3
4 4 5 5 6 6 right and now you have to
say a is equal to NP do where right and
in this you say A2
greater than
20 right and when you will check
a uh it's giving me the index why is it
giving me the
index is it giving me the
index does it always return
index one more thing is you can find uh
can find and
a
letter right through a letter you can
find an
element
through a letter Guys these are all some
tricks which you should know because you
will be dealing with data and you need
to pull data right you need to pull data
a lot right based on conditions and
based on things suppose you want to find
out the names which have g in them right
or r a in them so how will you do this I
I will say print right and I will say uh
np. do find right and I will say find
this inside full
underscore names right and find me RA a
right ra a
true false
false right it returns true because it
has found it here right so I don't want
to tell you indexing through this but
anyway you should know this just just
assume this that I'm telling you to
write this okay because this is much
easier when we will go to pandas
right just uh write it as a syntax okay
greater than equal to zero I I hope this
is clear so let's start with the data
science interv questions and answers and
the number one problem we will be facing
is real world problem solving and the
question one is handling missing data in
predictive modeling so imagine you have
given a data set where 30% of the data
for key predictive variable is missing
this variable is crucial for a
predictive model how would you handle
this situation to ensure the integrity
and performance of a model and please
describe your approach step by step so
starting with the answer you can start
with handling missing data set is a
common challenge in data science and
it's important to address it carefully
to maintain the accuracy of your model
and here's how you could approach this
situation the number one point would be
identify the missing data so first you
need to understand where the missing
values are in your data set you can do
this by using a simple code in Python
with libraries like mandas for example
you can use the data do isnull do sum
function that will show you the count of
missing values in each column then you
can analyze the pattern determine if
there's a pattern to the missing data is
it random or is it missing for a reason
this can affect your approach if the
data is missing at random the methods
you use might be different than if the
data is missing systematically so
choosing a method for imputation let's
see the next method that is choosing a
method for imputation so if the missing
data is numeric you might replace
missing values with the mean or median
of that column this is simple and
effective but can be used primarily when
the data is missing completely at random
then comes model based imputation
sometimes you can use other variables in
the data to predict missing values using
a regression model this can be more
accurate but is also more complex then
we'll use the K nearest neighbors can
algorithm but before that we have a Cod
snippet here that could be used for the
implementation of imputation you could
use python or R and now moving on we'll
see see the K nearest neighbors
algorithm so this method predicts the
missing values based on how closely
related the data points are to each
other so after imputation it's crucial
to check how your changes have affected
the overall data set and model
performance sometimes filling in too
many missing values can introduce bias
and then we have visualization to help
understand before and after the
imputation you could visualize the
distribution of the variable using
histograms or box plots this helps in
seeing how the imputation has Chang the
statistical properties of the data and
by following these steps you can handle
missing data thoughtfully and maintain
the Integrity of your predictive model
now moving to the question number two
that is based on evaluating model
overfitting so the question is you have
developed a predictive model but you
suspect it might be overfitting the
training data how would you test and
address the issue please explain your
steps and the techniques you would use
so you could start the answer by
explaining what is overfitting so
overfitting is a common problem where
model performs well on training data but
poorly on unseen data indicating it's
too closely fitted to the training data
specific details and noise so now we'll
see a step by-step guide on how to
address this the number one step is
gross validation so one effective way to
test for all fitting is by using cross
validation technique cross validation
involves splitting your training data
into multiple smaller sets that is false
and then training a model on some of
these set and validate
it on the others so this helps you
understand if the model's good
performance is consistent across
different subsets of data for example in
Python you can use the crosscore Valore
score function from SK learn. modore
selection so this is the code and this
is the code snippet of python that you
can use for the cross validation and
here we are importing from skarn that is
the module and we're importing crosscore
wellcore score and here we have used the
cross Val score function and then we
have printed the average cross
validation score and the next step we
will do is running cross validation
model so this is your predictive model
that you have already built using pyit
learn and here's the X train these are
the X input features of your training
data and Y train these are the output
labels of training data so we are
running gross validation model here this
is your predictive model that you have
already built using pyit Lear so X train
here that means these are the input
features of your training data and Y
train here means these are the output
labels of training data and CV equal to
5 this parameter test the function to
split the data into five parts that is
fals and the model is strained on four
of these parts and the remaining part is
used for testing so this process rotates
until each part has been used for
testing once and the printing results
that is score do mean so this calculates
the average of the scores obtained from
each cross validation for this average
score gives you an idea of how well your
model is likely to perform on unseen
data a consistent score across different
poll suggest your model is generalizing
well rather than or fitting to the
training data so now moving to the next
point that is training versus validation
error supp plot the training and
validation errors as a function of
training EPO or complexity of the model
a model that overfits will show a low
error on training data and a high error
on validation data as it trains further
then we have Runing the model if you
confirm that the model is overfitting
consider simplifying it this might mean
reducing the number of parameters by
selecting fewer features using
regularization techniques like lasso or
Ridge or choosing a less complex model
after this step we will move to
regularization technique step so these
techniques add a penalty to the loss
function used to train the model which
can discourage complex models that
overfit then we have common methods that
include L1 that is lasso and L2 rage
regularization and here's how you can
add L2 regularization in Python so this
is the C snippet here and what we have
done here is we are creating the ridge
model and we have applied Alpha equal to
1.0 so this parameter controls the
strength of the regularization a higher
Alpha value increases the regularization
effect which helps reduce model
complexity and combat overfitting the
alpha value can be tuned to find the
optimal balance between bias and
variance and now coming for the fitting
the model so model do fit and in that we
have X train and Y train that trains the
ridge model on the training data it
adjust the weight of the feature in X
train to predict the Y train while also
considering the regularization term this
helps prevent the model from fitting too
closely to the noisy aspects of the
training data and then we are
reevaluating the model after making
adjustments it's important to reevaluate
the model again using the same cross
validation technique to see if the issue
of or fitting has improved and then we
have visualization to help illustrate or
fitting you could create a plot showing
the training and validation errors or
the number of epo or model complexity so
by using these techniques you can
identify if a model is overfitting and
take steps to correct it ensuring it
performs well not only on the training
data but also on new unseen data so now
mve to the next question that is
question number three and it is based on
realtime data stream processing and the
question is you are tasked with building
a model to predict stock prices in real
time the data comes in every second and
you need to update your predictions
accordingly describe how you would set
up your system to handle this type of
data effectively and what tools and
techniques would you use and why so you
could start answering this question with
handling realtime data so handling
realtime data especially for something
as volatile and fastpaced as stock
prices requires a robust system that can
process and analyze data quickly and
accurately so here's how you could
approach this we will set up such a
system and we will have some steps so
starting with the steps so the first
step is choosing the right tools the
right tool would be Apache Kafka so this
is a popular tool for handling realtime
data that streams because it allows you
to publish And subscribe to streams of
records that is data and it can handle
high throughput with low latency Kafka
acts as a buffer and manages the flow of
data ensuring that your system doesn't
get overwhel and you can also use Apache
spark especially spark streaming is
excellent for processing the data it can
process data in real time and perform
complex operations like window grouping
data into chunks of a specified time
period and aggregating that is
summarizing data so you can modify it
and perform the predicting of stock
prices and then the step is data
processing Pipeline and the first step
comes here is injection data first
enters the system typically through
Kafka which collects data sent from the
stock market and then we do the
processing so spark streaming takes
overhead
here you can apply Transformations and
run your predictive models on the data
for example you might calculate moving
averages or other indicators that feed
into your stock price prediction model
and then comes the output finally the
predictions are outputed this could be
to a dashboard for Traders and automated
trading system or even stored for
further analysis and then we develop the
model now comes the model development
you would likely use a machine learning
model that can update quickly and
incorporate new data as it arrives model
such as arima for time series
forecasting or more complex machine
learning models like Rec neural networks
rnns can be suitable the model should be
retrained or fine-tuned periodically
with new data to ensure it stays
accurate now we'll come to scalability
and reliability so ensured your system
can scale as data volume increases this
might mean adding more servers or
optimizing your data processing code
Implement monitoring to catch any issues
early like delays in data processing or
model performance stopes and now we'll
see the step that is visualization and
monitoring consider setting up a
realtime dashboard that shows key
metrics like prediction accuracy and
processing time this helps in quickly
sporting when something goes wrong by
setting up your system with these tools
and strategies you can effectively
handle the challenge of predicting stock
prices in real time so now we'll move to
the next question that is question
number four and this will based on
scalable data analytics so we have
covered two questions that were a bit
code based questions and now we'll see
other questions that would be based on
scalable data analytics or they might be
on different areas and with the 13th
question we will start again with the
coding ones so moving with the question
four that is based on scale level data
analytics and the question is given a
scenario where your organization
suddenly needs to scale its data
analysis capabilities due to an influx
of data that would be 10 times the
normal volume how would you handle this
situation to ensure your data analytics
processes remain efficient and accurate
what technologies would you consider and
what steps would you take so you can
start answering this question with
handling a sudden increase in data
volume requires a strategic approach to
scaling your analytics infrastructure
without compromising on efficiency or
accuracy so we'll see some steps from
that you could effectively manage this
scenario that you would start answering
the interviewer that we can start by
evaluating the current infrastructures
ability to handle increased loads this
includes assessing your databases
servers and analytical tools to identify
potential bottlenecks or limitations
then you could move to Next Step that
would be choosing scalable Technologies
to manage the increased data volume
consider leveraging cloud-based
Solutions such as Amazon web services
Google Cloud platform or Microsoft Azure
these platforms offer scalable resources
which can be adjusted accordingly to the
data load uring you only pay for what
you use integrate Big Data Technologies
like Apache Hado for distributed storage
and P spark for fast data processing
these tools are designed to handle
massive volumes of data efficiently and
can scale up to meet standard increase
demands Now we move to the next step
that would be optimizing data processing
so Implement data partitioning and
indexing strategies to improve the
efficiency of data queries this will
help in managing large data sets by
breaking them into smaller manageable
chunks and speeding up search operations
and use real-time data processing
Frameworks like apach Kafka or apach
link which can handle high throughput
and provide timely insights from large
data streams and the next step would be
Automation and monitoring automate
routine data processing task to reduce
the manual effort and speed up the
analysis this can be done through
scripting or using workflow automation
tools set up comprehensive monitoring
systems to track the performance of your
data processes tools like promus for
system monitoring and graph for
analytics and monitoring dashboards are
useful here they help ensure that the
system is running smoothly and alert you
to potential issues before they become
critical and the next step will be
regular evaluation and scaling
continuously evaluate the performance of
analytics infrastructure as your data
grows keep adjusting and scaling your
resources to maintain Optimal
Performance plan for periodic reviews of
your technology stack and infrastructure
to ensure they remain aligned with your
data needs and organizational goals by
following these steps you can ensure
that your data analytics processes are
prepared to handle sudden surges in data
volume effectively maintaining the
integrity and speed of insights so this
was all for the question four now mov to
the question five and this is based on
integrating machine learning models into
production and the question is you have
developed a machine learning model that
performs F in testing environment now
you need to integrate it into your
production environment where it will be
used in real-time applications what
steps would you take to ensure the
successful deployment and operations of
the model in production so we'll start
answering this by successfully deploying
a machine learning model into production
involves several critical steps to
ensure it performs as well in realtime
operations as it does in testing so you
would have a clear pathway to make the
interv understand we will start with the
pathway with the first step that would
be model validation so before moving
anything into production revalidate your
model's performance using a separate
validation data set this helps confirm
that the model generalizes well to new
unseen data The Next Step will be
preparing the production environment
ensure that the production environment
is ready to handle the model this
includes setting up the necessary
hardware and software ensuring that it
can handle the expected load and that
all dependencies are correctly installed
and configured then the next step comes
that is model wrapping WRA your model in
an API that is application programming
interface making it accessible to other
parts of your software infrastructure
Frameworks like flask for python can be
used to create a simple web server that
listens for inputs and provides model
outputs then comes the next step that is
deployment strategies consider using
containerization tools like toker which
can help encapsulate your model and its
environment ensuring that it works
uniformly across different development
and production settings and then we'll
use deployment strategies like blue
green deployment or Canary releases to
minimize downtime and reduce the risk of
introducing a faulty model into
production and then comes the next step
that is monitoring and logging Implement
logging and monitoring to track the
model's performance and health in real
time tools like promas for monitoring
and Elk elastic search log stretch
kibana for logging help in quickly
identifying and diagnosing issues in
production and then comes the next step
that is Performance Tuning monitor the
model's performance over time if the
model's performance degrades or if new
data shows different patterns you may
need to retrain or fine tune the model
to maintain accuracy and and after this
step there's a step for feedback loop
set a feedback loop where predictions
and outcomes can be compared this
feedback is crucial for continuously
improving the model and catching any
drif in data or changes in external
conditions that affect the model and
after this comes a last step that is
legal and compliance checks ensure all
the data used by the model in production
complies with privacy laws and
regulations this is crucial for
maintaining trust and legality
especially when handling sensitive
information
so by carefully planning and executing
these steps you can smoothly transition
your machine learning model from a
testing environment to a fully
functional component of a production
system so this was all about the
question number five now move to the
question number six that would be based
on datadriven decision making and the
question is your company wants to shift
towards more data driven decision making
you have been tasked with developing a
strategy to implement this what steps
would you take to ensure that the data
at all levels of the organization is
utilized effectively to make informed
decisions and what challenges might you
face and how would you address them so
you can start answering this by
implementing a datadriven decision-
making strategy that will require a
comprehensive approach to ensure that
reliable data is accessible and
effectively used across all levels of
the organization and now we can develop
and deploy this strategy and similarly
you could tell this strategy to the
interviewer so the number one step will
be assessing current data infrastructure
start by valuating the existing data
infrastructure to understand what data
is available how it is stored and how it
is currently used this assessment will
help identify gaps in data collection
storage and access that need to be
addressed Now we move to the next step
that is developing a data governance
framework Implement a data governance
framework that defines who can access
data how it can be used and who is
responsible for maintaining its quality
this framework ensures data integrity
and security which are critical for
making reliable decisions now we'll move
to the next step that is training and
empowerment so train employees at all
levels on the importance of data driven
decision making and provide them with
the tools and knowledge necessary to
analyze and interpret data this might
include training sessions workshops and
ongoing support to ensure everyone can
use data effectively now mve to the next
step that is implementing analytical
tools so deploy userfriendly analytical
tools that can integrate seamlessly into
the daily workflows of employees tools
like w Microsoft powerbi or even
Advanced Excel techniques can provide
powerful data analysis capabilities
without requiring extensive technical
knowledge after this we'll move to the
step that would be creating a
centralized data platform developer
centralized data platform where all
organizational data can be accessed and
analyzed this platform should be
scalable and secure providing a single
source of Truth for the organization and
then we have the promoting a data driven
culture so foster culture that values
datadriven decision making encourage
experimentation and learning from data
driven initiatives celebrate successes
and learn from failures to continually
improve the use of data driven in
decision making and there would be some
challenges and solutions for that so one
major challenge we know here is
resistance to change as some employees
may prefer traditional decision-making
methods so address this by demonstrating
the tangible benefits of data D
decisions through pilot projects and
success stories so data seos can also
Hender effective data use promote cross
Department collaboration and integrate
disparate data sources to overcome this
challenge after that you can monitor and
do continuous Improvement so by
systematically implementing these steps
you can transform your organization into
one that leverages data at all levels to
make informed and effective decisions
and after answering in these steps you
could make the interor have a truth and
a faith in you that you could make these
models now mve to the next question that
is question number seven and that is
based on handling large data set and the
question is your project involves
analyzing extremely large data sets
potentially exceeding terabytes in size
what strategies would you use to manage
and analyze such large data sets
effectively describe the tools and
techniques you might employ and you
could start this with answering that
working with large data sets especially
those in terabyte range presents unique
challenges in terms of storage
processing and Analysis so we'll have
have a structured approach to handle
these challenges effectively we'll start
with the data storage that would be use
distributed file systems consider using
a distributed file systems like Hado
distributed file system hdfs or Amazon
S3 these systems are designed to store
vast amounts of data across many servers
offering High availability and full
tolerance and then comes the next step
that is data processing leverage big
data processing Frameworks tools like
apage spark are ideal for processing
large data sets they handle distributed
computing effectively spark can perform
data processing task much faster than
traditional disk based processing due to
its inmemory Computing capabilities and
next we could start with efficient data
sampling so there are many sampling
techniques that we can use so when the
data set is too large to handle even
with powerful tools consider using data
sampling techniques to reduce the size
to a manageable level without losing
significant insights ensure that the
sample represents the whole data set
accurately and then comes optimization
of data queries indexing and
partitioning optimize your data queries
by implementing indexing and
partitioning this can drastically reduce
the time it takes to perform queries by
limiting the amounts of data scan and
then we can do scalable analytics and
then we'll move to the next step that is
scalable analytics and in that we could
start with the parallel Computing use
parallel Computing capabilities of
Frameworks like spark or Dusk to analyze
data across multiple notes this helps in
SC in up your analytics operations to
handle large data sets effectively and
now we'll move to the cloud-based
analytical tools so consider using cloud
services like Google big query or AWS
red shift which are designed to handle
massive data sets and complex analytics
with these and after this step we'll
move to data cleaning and pre-processing
here we will automate pre-processing
task we'll use automated tools to clean
and pre-process data this includes
handling missing values normalizing data
and removing duplicates which can be
particularly challenging with large data
set and after this step we'll move to
the step that will visualize large data
set so we'll use specialized tools that
tools could be table or powerbi that can
handle large data set by aggregating
data and using efficient back end
Technologies for more detailed
exploration tools like plotly or bouet
can be used as they offer capabilities
to interactively visualize large volumes
of data and after that there would be
step for regular maintenance and updates
that could be continuously monitoring
the data quality as new data comes in
you can continuously monitor its quality
and after this step you could integrate
all these strategies and tools into your
workflow and you can effectively manage
and extract valuable insights from
extremely large data sets thereby
supporting robust datadriven decision
making and you could answer the whole
strategy to the interviewer now move to
the question number eight that is based
on optimizing machine learning models
and the question is during model
development you have noticed that your
machine learning model is
underperforming what steps would you
take to diagnose the problem and
optimize the model's performance what
techniques and tools would you use so
you can answer this by starting with the
optimizing and optimizing a machine
learning model that is underperforming
involves several steps to diagnose and
improve its accuracy and efficiency and
here we will have structured approach to
tackle this issue and you could start
this with the number one step that is
diagnosing the problem evaluate model
metrics start by thoroughly evaluating
the performance metrics of a model for
classification task for classification
task look at accuracy precision recall
and the F1 score for regression task
consider R squ mean squ error that is
MSC and mean absolute error that is ma
and then you can move to the next step
that is use plots like Roc curves for
classification models and residual plots
for regression to visually assess where
the model is going wrong after that
we'll move to the next step that is data
quality quality and quantity check
inspect the data that is sometimes the
quality and quantity of data can be the
root cause of poor model performance
ensure the data is clean well
pre-processed and sufficient look for
issues like missing values outliers or
imbalanced classes and after this we'll
move to the feature engineering step
that would be experiment with creating
new features or transforming existing
ones to provide better predictive power
and then we have the next step that is
model tuning and configuration after
feature engineering and move to the next
step that is model tuning and
configuration so hyper parameter tuning
use techniques like grid search or
random search to find the optimal
settings for your models parameters
tools like pyit learns grid search CV or
randomized search CV can automate this
process and there's a cross validation
that would Implement cross validation to
ensure that the model's performance is
consistent across different subsets of
the data set and then we have the next
step that is trying different models so
experiment with algorithms here if
initial models are underperforming try
different algorithms that might be
better suited for the problem for
instance if you started with linear
regression and it's not performing well
consider more complex models like random
forest or gradient boosting machines and
after this we have emble methods that we
can use techniques like bagging boosting
or stacking to combine the predictions
of multiple models to improve overall
performance after this step we have
feature selection that includes reduce
dimensionality use techniques like
principal component analysis that is PCA
to reduce the number of features which
might help in improving model
performance by removing noise and idency
and then we have select important
features so use model based technique to
identify and keep only the most
important features that impact the
outcome and then comes the last step
that is regular updates and retraining
so here you can monitor an update that
could be continuously monitoring the
model's performance over time as new
data becom available update and retrain
the model to adapt to any changes in
underlying patterns and after that you
could have a consultation and
collaboration work with the other teams
and by methodically addressing each of
these areas you can diagnose why your
machine learning model is
underperforming and can take steps to
optimize its accuracy and efficiency so
this was all about question number eight
so let's start with the question number
nine and this is based on handling
unstructured data so the question is you
are given a large amount of unstructured
data in including text images and videos
what strategies would you use to manage
and analyze this type of data
effectively describe the tools and
techniques you might employ so you can
start answering this question by
describing that dealing with
unstructured data can be challenging due
its lack of predefined format or
structure however with the right
strategies and tools you can effectively
manage and analyze it to extract
valuable insights and there will be a
approach how you can do that so we will
discuss the approach here and starting
with the steps so the number one step
will be data categorization and
organization so the number one step in
this step will be sorting and tagging we
will Begin by categorizing the data into
types that will be text images or videos
use tagging to add metadata which helps
in organizing the data and makes it
easier to access and analyze later then
and after that particularly for text
Data we'll use natural language
processing NLP we will employ n&p
techniques to extract useful information
from text tools like nltk Spacey or even
more advanced models like bird can help
you perform tasks such as sentiment
analysis entity recognition and topic
modeling after that we will do text
indexing we can use elastic search or
Apache so to index large volumes of text
these tools provide powerful search
capabilities and can handle complex
queries efficiently and after that we'll
move to image data and to structure
image data we'll use image processing
we'll use libraries like open CV for
basic image processing tasks such as
filtering and Transformations for more
advanced image analysis consider deep
learning models using Frameworks like
tensorflow or py to and then we'll
feature extraction apply techniques to
extract features from images such as
edges textures or key points which can
be used for further analysis or machine
learning and then we'll come to video
data and here we'll do video processing
and we'll use the tools like fmeg that
can be used for basic video processing
tasks such as format conversion or
extracting frames for analyzing video
content look at machine learning models
that can classify or recognize
activities in the video and after this
we'll move to temporal analysis for
videos temporal components are imported
techniques like sequence modeling or
recurrent Neal networks rnns can be
useful to analyze sequences or frames
for activities or EVs and then we'll
move to data storage and management here
will use the given volume and complexity
of unstructured data and use Big Data
platforms like Hadoop or cloud services
like AWS S3 for storage these platforms
can scale up to handle large data sizes
and provide the necessary infrastructure
to store and retrieve unstructured data
efficiently and then we have
visualization and Reporting custom
dashboards that we will create here we
will develop custom dashboards using
tools like table or powerbi which can
integrate different data types and
provide a unified view of the analyzed
data and after that we will do data
summarization tools that provide
summarization capabilities can help in
considering large volumes of
unstructured data into more manageable
and interpretable forms and after that
we leverage these strategies and tools
and can effectively manage analyze and
derive insights from unstructured data
which can be crucial for making informed
decisions in various applications and
this is the path that you can explore
and explain to the interviewer if this
question has been asked now moving to
the question number 10 and that will be
based on scaling AI solution in
Enterprise and the question is your
company wants to scale its AI operations
from a few initial pilot projects to
Enterprise wide implementation what are
the key considerations and steps you
would take to ensure the successful
scaling of AI Solutions across the
organization and what challenges might
you face and how would you address them
so you can start answering this question
with the scaling AI Solutions you could
answer him that scaling AI Solutions
across an Enterprise requires careful
planning and strategic implementation to
ensure success and alignment with
business objectives and there should be
a strategic approach to implement this
so starting with the approach and the
number one step will be that will be
strategic alignment so identify business
objectives start by identifying the
business objectives that the AI
Solutions are intended to support this
ensures that the AI initiatives are
aligned with the company's strategic
goals and can demonstrate clear business
value and then comes the stakeholder
engagement so engage stakeholders from
various departments early in the process
to gather input and build support this
helps in understanding diverse needs and
ensures broader acceptance of the AI
Solutions and after that comes the
infrastructure and technology so there's
an option that is assess and upgrade
infrastructure evaluate whether your
current it infrastructure can support
the expanded use of AI you might need to
upgrade Hardware invest in Cloud
Solutions or adopt technologies that
facilitate AI processing and data
handling and and after that we have
standardization of tools standardize the
tools and platforms used for air
development to ensure compatibility and
ease of maintainance across the
organizations and after that we'll move
to data management so robust data
governance that is to implement a strong
data governance framework to manage
Enterprise data effectively this
includes policies for data Quality
Security and compliance especially
important when scaling AI solutions that
rely on vast amounts of data and after
that we will come to data accessibility
so ensure that data is accessible across
the organization but also secure against
unauthorized access this involves
setting up secure data laks or
warehouses that centralized data while
allowing controlled access and then we
come to the next step that is talent and
training so build AI competency that is
develop in-house AI expertise through
training programs and hiring so this
build the necessary skills within the
organization to develop manage and scale
AI Solutions and after that you can also
perform cross functional AI teams that
could would be forming cross functional
teams that include data scientists it
professionals and domain experts so this
Fosters collaboration and ensure that a
Solutions are developed with a
comprehensive understanding and after
forming these collaborative teams we'll
move to scalable deployment models so
pilot test and phase roll out before a
full scale roll out conduct pilot test
to go the AI solution Effectiveness and
integration capabilities based on
feedback adjust and then gradually
deploy the solutions across the
organization and then we have modular
and flexible design so design AI systems
to be modular and scalable allowing for
adjustments and expansions as needs and
then we'll Monitor and do the continuous
Improvement so there will be performance
metrics that would establish metrics to
regularly assess the performance of AI
systems we will monitor these systems to
ensure they met expected outcomes and
adep as necessary and after that we have
next step that is addressing challenges
so there could be cultural resistance
that there could be employees that would
be resisting to the changes but we have
to address this through continuous
education and by showcasing successful
AI use cases within the organizations
and by carefully considering these
aspects and methodically implementing
steps you can successfully scale AI
Solutions across your Enterprise driving
significant business value and
Innovation and that's all for question
number 10 now we'll move to question
number 11 and that is based on ethical
considerations in data science so the
question is in your data science
projects how do you ensure that ethical
considerations are addressed describe
the steps you take to identify and
mitigate ethical risk in your projects
what Frameworks or guidelines do you
follow so you could start answering this
question with ethical considerations
that they're crucial in data science to
ensure that the solutions and analyzes
do not advertently cause harm or bias
here's how you can ensure that so there
are some steps and we will discuss those
steps starting with the number one that
is educate on ethical standards so stay
informed about the ethical standards in
data science such as fairness
accountability transparency and privacy
organizations like the data science
Association and the ACM have codes of
Ethics that we refer to as guidelines
and then we have ethical risk assessment
identify potential ethical issues that
would be at the beginning of each
project conduct a thorough assessment to
identify any potential ethical risk such
as biases and data or impact on vulnerab
groups this involve reviewing the source
of data the methodologies used for data
collection and the intended use of the
data analytics results and then we have
stakeholder analysis engage with
stakeholders to understand the diverse
perspectives and potential impact of the
project this helps in identifying
ethical issues that may not be apparent
from a purely technical standpoint and
then we will move to mitigation
strategies implementing bias mitigation
techniques we will use statistical and
machine learning techniques to detect
and mitigate biases in data this might
involve techniques like resampling ring
or using algorithms designed to be fair
and then we have privacy preserving
methods employe methods such as data
anonymization encryption or differential
pracy to protect individual privacy when
analyzing sensitive data then we have
other methods that is transparency and
explainability then we have model
explainability and after that coming to
documentation and Reporting so we have
to maintain thorough documentation of
data sources model decisions and
methodologies and then we have
continuous monitoring and feedback there
you have to monitor outcomes and the
feedback mechanisms should be applied
and then we have the panels that is
collaboration and advisory panels then
we have ethical review boards so for
complex projects setting up or
Consulting with an ethical review board
can provide oversight and diverse
perspectives on the ethical implications
of project methodologies so by
proactively addressing ethical
considerations through these steps you
can ensure that your data science
projects uphold High ethical standards
and positively contribute to society
while minimizing harm so this was all
about question 11 now moving to question
number 12 that is based on time series
forecasting for business decisions so
the question number 12 is you are tasked
with forecasting monthly sales for a
retail company using time series data
from the past 5 years what steps would
you take to prepare and analyze this
data to make accurate forecast what
specific tools or techniques would you
use and why so we can start answering
this by time series forecasting and we
could address them that it's a powerful
tool for predicting future events based
on past data especially in business
context like retail sales so we will
have a structured approach here and
we'll start with data collection and
cleaning first we'll gather data and
ensure that you have collected all
relevant data including monthly sale
figures from the past 5 years and also
considering including external factors
that might affect sales such as economic
indicators holidays and promotional
activities and then we'll proceed to
clean data we will check for and handle
any inconsistencies or missing values
and then we have data visualization we
will plot the data we'll use plotting
libraries like M prot or cbone in Python
to visualize the data this will help in
identifying patterns Trends and
seasonality and then we have
decomposition of data so there's a
seasonal decomposition and we'll use
statistical techniques to decompose the
data into and seasonally and residuals
so this can be comped with tools like
the seasonal decompose function from the
stats models library in Python and we'll
understand these components separately
and can improve the accuracy of our
forecast and then the next step is model
selection and forecasting so there are
two models that is Aima and Sima models
so we have to choose appropriate
forecasting models based on data's
characteristics for instance arima that
is auto regressive integrated moving
average it is effective for non season
data while Sima that is seasonal arima
that is suitable for data with seasonal
patterns and after choosing the model
we'll move to cross validation we will
Implement time series specific cross
validation techniques like time B
splitting to evaluate model performance
and this will ensure your model
generalizes well on unseen data and then
we have model fitting and Diagnostics we
will fit the model that is by using the
simx class from stat models that will
fit your model to the data and then we
will carefully select parameters based
on AIC that is aake information
Criterion that scores or thoro grid
research technique and then we can do
the Diagnostics and forecast and
validation and after forecast validation
will move to iterative Improvement so
there's a feedback loop that should be
mandatory and there should be a regular
update for the model with new sales data
and refining the model as needed so this
continuous Improvement cycle helps adapt
to changing patterns in sales data and
by following these steps and using these
tools you can create robust forecast
that help the retail company plan better
and make informed decisions so this was
all about question number 12 now moving
to question number 13 that is based on
customer segmentation using machine
learning so the question is you are
given a data set containing demographic
and purchasing Behavior data for a group
of customers your task is to segment
these customers into distinct groups
based on similari in the purchasing
behavior and demographics so what steps
would you take to perform this
segmentation and can you provide a
sample python code stet to illustrate
the initial stages of data handling and
model application so we can start this
by explaining customer segmentation that
it's a powerful approach to tailor
marketing strategies and improve
customer service by identifying distinct
groups based on their behavior and
characteristics and here also we have a
detailed approach for this task so we'll
start with number one step that would be
data exploration and pre-processing so
there will be initial exploration that
is beginning by examining the data set
to understand the features available
such as age income purchase frequency
Etc then we'll look for missing values
or anomalies and decide how to handle
them that could be using imputation and
then we'll move to feature engineering
we will create new features that might
be useful for segmentation such as
customer lifetime value or average
transaction amount you'll also use
normalization that is normalize the data
to ensure that one feature doesn't
disproportionately influence the model
due to its scale we'll use standard
scaling or minmax scaling as appropriate
so then we'll come to the next step that
is choosing the segmentation technique
and here we have K means clustering so
this is a popular method for customer
segmentation here we will decide on the
number of clusters by using techniques
like the elbow method orot analysis to
determine the optimal cluster count and
then we have model implementation and in
that that we will use data preparation
and we'll prepare the data by selecting
the relevant features and applying any
final Transformations and then we have
model fitting we'll fit the K's
clustering model to the data and
evaluate and interpret analyzing
clusters and after analyzing clusters
we'll move to the next step that is
strategic insights we'll provide
actionable insights based on cluster
characteristics such as targeted
marketing strategies for each segment
and then we have iterative refinement
that is feedback incorporation and we'll
use business feedback to refine the
segmentation if additional data becomes
available Incorporated to enhance the
model and now we'll see the sample
python code so for this first we'll
import the libraries and modules as you
can see on the screen we have imported
pandas random Forest classifier train
test split standard scaler
classification report and after that
we'll load the data and and for that we
have used the pandas to read the data
that is read SSV and after that we are
processing the data that is data
pre-processing we are handling missing
values and using the forward fill or fil
to fill missing values in the data set
and then we are featuring scaling that
is normalizing the selected features
that is feature one feature two and
feature three using standard scaler and
then we'll move to the next step that is
data splitting we'll split the data set
into training and testing sets so that
test size equal to Z 0.2 parameters
specifies that 20% of the data will be
used for testing and then we'll train
the model we'll initialize and train a
random forest classifier with 100 trees
and a random state for reproductibility
and then we'll evaluate the model we'll
make predictions on the test set that is
xcore test using the train model and
print a classification report showing
Precision recall F1 score and support
for each class so this code demonstrates
the process of loading pre-processing
training and evaluating a machine
learning model That Is Random forest
classifier for predicting equipment
failures in a manufacturing plant the
use of techniques such as data
pre-processing and splitting along with
the random Forex classifier highlights a
standard flow for building predictive
maintenance models so this was all about
the question number 13 so now moving to
the question number 14 that is based on
predictive customer churn and the
question is your tasked with developing
a model to predict which customers are
likely to churn from a subscription
service so what steps would you take to
build this model and can you provide a
sample python code to illustrate the
data preparation and model training
process so we'll start answering this
question about depicting what is
predicting customer CH so predicting
customer churn is crucial for businesses
to implement retention strategies
proactively and we'll have a detailed
approach for building a predictive model
for this purpose starting with data
collection and exploration and in this
we will collect data and after that
we'll perform the exploratory data
analysis that is Eda we'll perform an
initial analysis to understand patterns
and Trends and then we have feature
engineering we will create new features
and derive new feature that might
influence J such as change in usage
pattern or service upgrades and then
we'll handle the missing values if we
found any and then we'll encode
categorial variables we'll use
techniques like one hot encoding or
label encoding for categorial variables
and then we have scale features to
normalize or standardized numerical
features to ensure they contribute
equally to the model's performance and
then we'll select the model that is
we'll choose the appropriate model and
start with for the knowing handling
binary classification task that could be
with logistic regression random forest
or gradient boosting machines and after
selecting the model we will train the
model and evaluate it so fit your model
on the training data and after that
evaluate the model using appropriate
metrics like accuracy precision recall
F1 score and Roc to go its performance
and then we'll optimize the model using
hyper parameter tuning we'll optimize in
the model parameter using grid search or
random search to improve performance and
then we have feature importance that is
analyze and rank features by their
importance in predicting churn to refine
the model further and then and then last
step is deployment and monitoring we'll
deploy the model once validated deploy
the model into production environment
where it can predict real time CH so
after deploying the model regularly
monitor the model to ensure it remains
effective over time as new data comes in
so now we'll see the sample python code
for this example so starting with the
importing of libraries we will import
pandas numpy pyit learn SK learn
tensorflow and the tensorflow kasas and
Co bags and after importing the modules
we'll start with data loading we'll load
the data set from a a CSV file named
equipment dat. CSV and that to with the
pandas data frame and after that we'll
do the data pre-processing we'll handle
missing values and for that we'll use
forward fill to fill missing values in
the data set and then we have feature
scaling that will normalize the selected
features that is feature one feature two
feature three using standard scaler and
after that we'll use the data splitting
we'll split the data set into training
and testing sets and the test size will
be equal to 0.2 and this parameter
specifies that 20% of the data will be
used for testing and after that we'll
start with building the model first
we'll see sequential model that
initializes a sequential model technique
and then we have dense layers that adds
two dense layers with 64 units and R
activation function then we have Dropout
layers that adds two Dropout layers with
a dropout rate of 0.5 to reduce over
fitting after that we'll do the model
compilation we'll compile the model
using the Adam Optimizer and binary
crossentropy loss function for binary
classification and there will be an
early stopping that will define an early
stopping call back to stop training when
the validation loss metric has stopped
improving after three Bs and after
training the model we will evaluate the
model and evaluating the model on the
test data and print the laws and
accuracy metrics so this code
demonstrates the process of loading
preprocessing building compiling
training and evaluating a deep learning
model using tensor flow and care as for
predicting equipment failures in a
manufacturing plant so the use of
techniques such as data pre-processing
Dropout regularization and early
stopping helps in building a robust deep
learning model for predictive
maintenance so that's all with question
number 14 now we start with question
number 15 that is based on deep learning
in NLP and your question is you are
tasked with developing a sentiment
analysis model using deep learning to
understand customer opinions from
reviews so what steps would you take to
build this model and can you provide a
sample python Cod snippet to illustrate
how you would pre-process data and train
a simple deep learning model so you
start answering this with sentiment
analysis that sentiment analysis using
deep learning allows businesses to coach
customer sentiment from text Data like
reviews or comments effectively and
we'll have a detailed approach for
building a sentiment analysis model
we'll start with data collection and
cleaning we will collect the data gather
a substantial data set of text reviews
and their Associated sentiments
typically labeled as positive negative
or neural and then we'll clean the data
pre-process the data by removing noise
such as HTML tags special characters and
so words and we'll normalize the text by
converting into lowercase and then we
have text pre-processing we'll convert
text into tokens words or phrases and
then we have vectorization that
transforms tokens into numerical format
using techniques like word embeddings or
tfidf that is term frequency in worse
document frequency and then we'll use
the padding and then we have the option
of model selection we'll choose a model
architecture based on a basic approach
and use a RNN or more advanced
architecture like lstm that is long
short-term memory or Gru that is cated
recurrent units which are effective for
sequence data like text and then we have
model training we'll compile the model
Define the model architecture and
compile it with the loss function suited
for classification like categorial cross
and tropy and Optimizer like Adam and
then we'll train the model we'll fit the
model on our pre-process data we'll
evaluate and optimize it evaluating
model performance here use the metrics
such as accuracy precision recall and F1
score to assess the model and then we
have hyper parameter tuning we'll
optimize the model by adjusting
parameters like learning rate number of
layers and units per layer and then
coming to deployment we'll deploy the
model and integrate the model into the
existing review processing pipeline so
it can automatically classify new
reviews so let's see the sample python
code and we'll have a basic approach for
that here will import nump Tor flow
sequential embedding lstm d drop out so
embedding converts positive integers
that is indexes into dense vectors of
fixed size and lstm that is long
shortterm memory layer that is used for
learning dependences in sequence data
and then we have dense that is a
regularly densed connected NL
and then we will import padore sequences
and after that we have the data set and
the sample text Data representing
customer reviews that will store in
variable text and then we have labels
that has binary labels indicating
sentiment one for positive 0 for
negative and now we'll start with the
pre-processing of data here we have
declared that tokenizer we will
initialize a tokenizer that will help
only the top thousand most frequent
words and then we have fitcore own
underscore text that is update the
internal vocabulary based on the list of
text it essentially creates a dictionary
of word to index Pairs and then we have
text two sequences that will transform
each text in text to a sequence of
integers and then we have pad sequences
that will ensure all sequences have the
same length by padding shorter sequences
with zeros up to the maximum length and
then we'll start building the model here
we have sequential model that will set
up a linear stack of layers and then we
have embedding layer that will map each
word index to an embedding Vector of
size 64 so the input length is set to 10
that is the length of the input
sequences then we'll start with lstm
layers so two lstm layers are added the
first one returns sequences to allow the
next lstm layer to process these
sequences and after that we have the
Dropout layer that applies Dropout with
a rate of 0.5 with the first lstm layer
to reduce overfitting and after that
we'll come to dense layer
that has output of a single scaler that
represents the predicted senent and
using sigmoid activation to Output of
probability and now we'll start with
model compilation and training so we'll
configure the model for training and
we'll use binary crossentropy as the
loss function that is suitable for
binary classification and the adom
optimizer and tracks constantly accuracy
as a metric and then we have the fit
that trains the model for a specified
number of epo that is iterations or the
entire data set and then we'll predict
the model that is after training the
model can predict the sentiment of the
reviews in the data set this is useful
for checking how the model performs on
the training data itself so this
breakdown explains each step of the
coding process detailing how the data is
prepared and how the model is configured
and then we'll compile it and use for
training and prediction so this detailed
explanation should help in understanding
how to implement a simple lstm model for
sentiment analysis intensive flow now
moving to the question number 16 so
let's start with question number 16 that
is based on anomaly detection in
transaction data so the question is you
are tasked with identifying unusual
transactions in a company's financial
data that might suggest FR L activity so
what steps would you take to develop an
anomaly detection model and can you
provide a sample python code snippet to
illustrate how you would pre-process the
data and apply an anomally detection
technique so we'll start answering this
with anomal detection technique
that is anomaly detection is essential
for preventing fraud by identifying
transactions that deviate significantly
from typical patterns and now we'll see
the structured approach to building an
anomaly detection model for transaction
data we'll start with data collection
and cleaning and we'll collect all the
compiling transaction data which should
include details like transaction amount
time user ID and transaction type then
we'll move to feature engineering and
develop features that capture the
essence of transaction such as time of
day and and the day of the week and then
we have data normalization we'll use
scaling techniques such as main Max
scaling or standardization to ensure
that the model is perfectly normalized
and then we have choosing the anomaly
detection technique so here we have to
choose the technique which is effective
for high dimensional data sets and works
for isolating anomalies instead of
profiling normal data points after
choosing the anomal technique we'll
train anomal identification we'll fit
the choosen model to the data and the
anomalies that would have been chosen
will be those transactions that the
model identifies and after this we come
to the last step that is review an
action here we have manual review that
is transactions flagged as potential
anom should be reviewed manually to
confirm frent activity and then we have
continuous Improvement that is we can
regularly update the model with the new
data and feedback from the review
process to improve accuracy and now
moving to the prediction that is after
training the model we can predict the
sentiment of the reviews in the data set
and this is useful for checking how the
model performs on the training data
itself now we'll see the python code to
see how you can set up this model for
anomaly detection we'll start by eming
the libraries and module and after that
we load and prepare data that is we will
load transaction data from a CSA file
into the pandas data frame and after
that we'll convert that transaction time
column to dat time format which allows
the extraction of additional time based
features and after that we perform
feature engineering that will extract
the hour of the day from the transaction
time column this feature can be
important as transactions occurring at
unusual hours may be indicative of Fraud
and then we'll move to the normalization
of data this will apply standard scaling
to the amount and hour of the day
feature this normalization process
involves subtracting the mean and
dividing by the standard deviation for
each feature uring that the feature
contribute equally to the analysis and
improving the performance of many
machine learning algorithms
and after that we'll start with anomaly
detection with isolation Forest that's a
technique we'll initialize an isolation
forest model with 100 trees that is n
estimators equal to 100 setting the
proportions of outliers that is
contamination to 1% of the data so this
parameter is crucial as it influences
the threshold of marking and observation
as an anomaly then we fit the model to
the scaled amount and R of the DAT data
and predict the anomaly status for each
transaction and then we'll start with
filter and display anomalies we'll
filter our transactions identified as
anomalies that is anomaly equal equal to
minus one we display these transactions
which can be reviewed manually to
determine if they represent actual frent
activity so this code snippet provides a
systematic approach to detecting
anomalies in transaction data leveraging
the isolation Forest algorithm's ability
to handle complex and high dimensional
data set effectively so the
pre-processing steps ensured that the
data is approp rately formatted and
normalized for optional model
performance so this was all about
question number 16 now moving to
question number 17 and that is based on
integrating machine learning models into
web applications and your question is
you have developed a machine learning
model to predict real estate prices
based on various features like location
size and amenities how would you
integrate this model into a web
application to allow users to get
realtime price predictions can you
provide a sample python code snippet to
illustrate how you would prepare the
model for integration and handle user
request so starting with the approach
that is integrating a machine learning
model into a web application this will
invol several steps to ensure the model
is accessible and perform well in a live
environment so here's how you can
approach this task we could divide into
steps and we'll start with number one
step that is model preparation we'll
finalize and save the model so once your
model is stained and validated save it
using a format that can be easily loaded
into a web application so Python's
pickle module or tza CLA same model
format are commonly used for this
purpose then we can use web application
backend setup for this select a suitable
web framework so flask is popularly
known for its Simplicity and
Effectiveness in integrating python
based machine learning models and after
that we'll develop the API after
developing the API within your flas cap
that you can receive user inputs for
model features load the model make
prediction and return the result and
after this we'll develop the UI we'll
design a userfriendly interface we'll
create a simple and intuitive UI that
lets users input the feature like
location size and submit them for
prediction and after that we'll move to
the deployment phase we'll use a cloud
platform like Heroku AWS or Google Cloud
to deploy your flask application and
then we have the maintenance and updates
will Monitor and update regularly for
the performance and use the model as
needed based user feedback so now move
to the python code and see how this
model can be created so here we'll start
importing the libraries and module and
we are using flask pickle and gon5 and
we will start with app initialization
we'll initialize a new flask web
application that would be a special
variable which gives python files a
unique name to differentiate between
them when they are important into other
scripts and after that we'll load the
model so loading a pre machine learning
model from the file system so this model
is assumed to be saved in the same
directory as this script so the model is
loaded in RB mode which stands for read
binary and after that we'll move to API
route and prediction function so we will
define an API endpoint at predict that
listens for post request this is the URL
that the front end of the web
application will call to send data to
the back end and after that we'll start
with predicting the function and here we
have extract Fe fees that retries data
sent into the Json format from the post
request that is request getor Json and
the force we have set it as true here
and forcefully formats the request data
into Json ensuring compatibility and
then we'll extract the relevant features
that is location size and amenities from
the Json object and store them in a list
as expected by the model and after
preparing the features we make the
prediction we'll use the loaded model to
make a prediction based on the provided
feature and then we have the return
prediction method here we will convert
the prediction result into Json format
using jsonify and send it back to the
client and this will ensure that the
response can be easily handled by the
client application so this was all about
the question number 17 now moving to the
question number 18 that is based on
analyzing and now we'll move to the
question number 18 that is based on
analyzing geospatial data and your
question is you are tasked with
analyzing GEOS spal data to help a city
improve its public transportation system
the data includes GPS code coordinates
of bus stops ridership numbers and
traffic patterns what steps would you
take to analyze this data and can you
provide a sample python code snippet to
illustrate how you might visualize bus
stop location and ridership so you can
start answering this question that just
Patt data analysis can provide critical
insights into how effectively a public
transportation system serves its City
and guide improvements and this an
detailed approach for this and we can
start with data preparation and in this
we will do data collection and data
cleaning and after this step we'll move
to the next step that is explor data
analysis and in this we'll have
statistical summary we'll generate
descriptive statistics and then we have
correlation analysis and after moving
that we have ju patial visualization
that is mapping bus stop we'll plot the
locations of bus stop on a map to
visually assess their distribution
across the city and after that we have
heat maps that will create ridership
data to identify hot spots and areas
with potential service gaps and after
geospatial visualization we'll move with
spatial analysis we have proximity
analysis that will analyze the proximity
of bus stop to key areas like commercial
centers or residential areas and now
moving to the fifth step that is
optimization and recommendation so we'll
have a route optimization that will
suggest modifications to route based on
traffic patterns and ridership demand
and the policy recommendations that will
provide actionable recommendations for
improving bus frequency es now mov to
the sample python code where we can
Define this model and use it accordingly
and here we'll start importing the
libraries and modules and here we'll
start with importing geopandas and M Pro
lip. pipel and after importing we'll
start with data loading so we will
declare a variable bscore stopes and
load the bus stops data from a shape
file so shape files are popular GEOS
spal Vector data formats for geographic
information system software and then we
have the wrers ship that will load wrers
ship data from a CSV file which includes
columns for longitude latitude and
ridership levels and after that we'll
create Geo data frame that will convert
the ridership data frame into a Geo data
frame and this step involves creating a
geometry column from the longitude and
latitude columns and then we have the
plotting one here we will plot the
graphs that would be figures and x's and
create a figure for the same single
subplot with a specified size that is 10
cross 10 in and then we have cityor map.
plot it is assumed that there is a base
map of the city loaded as a Geo data
frame named city map this is ploted
first with a light gray color to serve
as a background for the other layers so
this was all about question number 18
now moving to question number 19 that is
based on predictive maintenance using
machine learning and the question is you
are tasked with developing a predictive
maintenance system system for a
manufacturing plant that relies heavily
on automated Machinery so the data
available includes machine operational
parameters maintainance history and
failure incidents what steps would you
take to develop a predictive model and
can you provide a sample python code so
you can start with predictive
maintenance that is essential in
manufacturing as it helps prevent
equipment failures reducing downtime and
maintenance cost and here you would have
a detailed approach or predictive model
for this starting with data collection
and integration then you can do Eda that
is exploratory data analysis and then we
can perform feature engineering and then
move to data pre-processing task and
then the selection model and training
and after that we have model evaluation
and deployment technique that we can do
for the model using appropriate metrics
such as Precision recall and F1 score so
this was all about question number 19 so
now move to question number 20 that is
based on personalization using machine
learning and your question is you are
tasked with developing a machine
learning model to personalize content
recommendations for users on a media
streaming platform the data available
includes user demographic details
viewing history and ratings so what
steps would you take to build a model
for personalized recommendations and can
you provide a sample python code for
that so you can start answering this
with creating of personalized
recommendation systems this would be
essential for engaging users by
providing content that is relevant to
their interest and there will be a
systematic approach or personal ized
content recommendation we'll start with
data collection and integration and
after that we'll perform Eda that is
explorate data analysis and then we have
feature Engineering in this will'll
interact features and the temporal
features we include time waste features
to capture Trends and seasonality in
viewing behavior and then we'll select
the model that is by collaborative
filtering and Hybrid models and then
we'll train the model and validation and
Implement and monitor them and after
that will deploy the model integrate the
recommendation system thanks for
completing the data science full course
by simply learn you have gained a solid
foundation in one of the most critical
Fields today with your new data science
skills you are ready to tackle real
world problem make data driven decisions
and explore career opportunities if you
like this video do hit the notification
Bell for more videos by simply
learn staying ahead in your career
requires continuous learning and
upscaling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in cuttingedge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click here